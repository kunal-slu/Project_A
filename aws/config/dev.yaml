env: dev
environment: aws  # Set to 'aws' for AWS/EMR execution

project_name: project-a

aws:
  region: us-east-1
  account_id: "424570854632"
  kms_key_arn: "arn:aws:kms:us-east-1:424570854632:key/66576e98-a4e4-4b87-8b61-4357c39d0886"

buckets:
  lake: "my-etl-lake-demo-424570854632"
  artifacts: "my-etl-artifacts-demo-424570854632"
  logs: "my-etl-logs-demo-424570854632"
  code: "my-etl-code-demo-424570854632"

glue:
  bronze_db: "project-a_bronze_dev"
  silver_db: "project-a_silver_dev"
  gold_db: "project-a_gold_dev"

emr:
  application_id: "00g0tm6kccmdcf09"
  execution_role_arn: "arn:aws:iam::424570854632:role/project-a-dev-emr-exec"
  driver:
    cores: 2
    memory: "4G"
  executor:
    cores: 2
    memory: "8G"

# AWS S3 paths - matching local structure
paths:
  bronze_root: "s3://my-etl-lake-demo-424570854632/bronze"
  silver_root: "s3://my-etl-lake-demo-424570854632/silver"
  gold_root: "s3://my-etl-lake-demo-424570854632/gold"
  checkpoints_root: "s3://my-etl-lake-demo-424570854632/_checkpoints"
  dq_results_root: "s3://my-etl-lake-demo-424570854632/_dq_results"

# Source file definitions - pointing to S3 bronze data
# NOTE: S3 uses "snowflakes" (plural) but we'll handle this in the loader
sources:
  crm:
    base_path: "s3://my-etl-lake-demo-424570854632/bronze/crm"
    files:
      accounts: "accounts.csv"
      contacts: "contacts.csv"
      opportunities: "opportunities.csv"
  
  redshift:
    base_path: "s3://my-etl-lake-demo-424570854632/bronze/redshift"
    files:
      behavior: "redshift_customer_behavior_50000.csv"
  
  snowflake:
    # Note: S3 path is "snowflakes" (plural) but we keep config as "snowflake" for consistency
    base_path: "s3://my-etl-lake-demo-424570854632/bronze/snowflakes"
    files:
      customers: "snowflake_customers_50000.csv"
      orders: "snowflake_orders_100000.csv"
      products: "snowflake_products_10000.csv"
  
  fx:
    type: json
    base_path: "s3://my-etl-lake-demo-424570854632/bronze/fx"
    raw_path: "s3://my-etl-lake-demo-424570854632/bronze/fx"
    bronze_path: "s3://my-etl-lake-demo-424570854632/bronze/fx/delta"
    silver_path: "s3://my-etl-lake-demo-424570854632/silver/fx"
    files:
      daily_rates: "fx_rates_historical_730_days.csv"
      daily_rates_json: "fx_rates_historical.json"
    base_currency: "USD"
    expected_currencies: ["USD", "EUR", "GBP", "JPY", "INR", "CHF", "CAD", "AUD", "CNY"]
  
  kafka:
    local_bootstrap_servers: "localhost:9092"
    msk_bootstrap_servers: "${ENV:MSK_BOOTSTRAP_SERVERS}"
    topic: "orders_events"
    consumer_group: "project-a-etl-consumer"
    seed_file: "s3://my-etl-lake-demo-424570854632/bronze/kafka/stream_kafka_events_100000.csv"
  
  kafka_sim:
    base_path: "s3://my-etl-lake-demo-424570854632/bronze/kafka"
    files:
      orders_seed: "stream_kafka_events_100000.csv"

# Table name definitions for Silver and Gold layers
# MUST match local config exactly
tables:
  silver:
    customers: "customers_silver"
    orders: "orders_silver"
    products: "products_silver"
    behavior: "customer_behavior_silver"
    fx_rates: "fx_rates_silver"
    order_events: "order_events_silver"
  
  gold:
    fact_orders: "fact_orders"
    dim_customer: "dim_customer"
    dim_product: "dim_product"
    dim_date: "dim_date"
    customer_360: "customer_360"
    product_performance: "product_performance"

dq:
  mode: "strict"
  fail_on_error: true
  alert_on_warning: true

sinks:
  redshift:
    jdbc_url: "jdbc:redshift://HOST:5439/DB"
    user: "redshift_user"
    password: "******"
  snowflake:
    url: "account.snowflakecomputing.com"
    user: "SF_USER"
    password: "******"
    database: "ANALYTICS"
    schema: "PUBLIC"
    warehouse: "COMPUTE_WH"
    role: "SYSADMIN"

lineage:
  enabled: true
  backend: "openlineage"
  endpoint: "http://marquez:5000"
  namespace: "project-a-dev"

sla:
  bronze_ready_by: "02:30"
  silver_ready_by: "03:15"
  gold_ready_by: "04:00"

source_connections:
  snowflake:
    secret_name: "project-a-dev/snowflake/conn"
    default_warehouse: "DEV_WH"
    default_database: "CUSTOMER_DB"
    default_schema: "PUBLIC"
  redshift:
    secret_name: "project-a-dev/redshift/conn"
  kafka:
    secret_name: "project-a-dev/kafka/conn"
  salesforce:
    secret_name: "project-a-dev/salesforce/conn"
  fx_api:
    secret_name: "project-a-dev/fx/conn"

# Spark configuration for AWS/EMR
spark:
  enable_delta: true  # Enable Delta Lake for AWS
  extra_conf:
    spark.sql.adaptive.enabled: "true"
    spark.sql.adaptive.coalescePartitions.enabled: "true"
    spark.eventLog.enabled: "true"
    spark.eventLog.dir: "s3://my-etl-lake-demo-424570854632/logs/spark-events/"

