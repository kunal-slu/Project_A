env: dev
environment: emr  # Set to 'emr' for EMR Serverless execution

project_name: project-a

aws:
  region: us-east-1
  account_id: "424570854632"
  kms_key_arn: "arn:aws:kms:us-east-1:424570854632:key/66576e98-a4e4-4b87-8b61-4357c39d0886"

buckets:
  lake: "my-etl-lake-demo-424570854632"
  artifacts: "my-etl-artifacts-demo-424570854632"
  logs: "my-etl-logs-demo-424570854632"
  code: "my-etl-code-demo-424570854632"

glue:
  bronze_db: "project-a_bronze_dev"   # actual TF name
  silver_db: "project-a_silver_dev"
  gold_db: "project-a_gold_dev"

emr:
  application_id: "00g0tm6kccmdcf09"
  execution_role_arn: "arn:aws:iam::424570854632:role/project-a-dev-emr-exec"
  driver:
    cores: 2
    memory: "4G"
  executor:
    cores: 2
    memory: "8G"

paths:
  bronze_root: "s3://my-etl-lake-demo-424570854632/bronze"
  silver_root: "s3://my-etl-lake-demo-424570854632/silver"
  gold_root:   "s3://my-etl-lake-demo-424570854632/gold"
  checkpoints_root: "s3://my-etl-lake-demo-424570854632/_checkpoints"
  dq_results_root:  "s3://my-etl-lake-demo-424570854632/_dq_results"

# Source file definitions - single source of truth
sources:
  crm:
    base_path: "s3://my-etl-lake-demo-424570854632/bronze/crm"
    files:
      accounts: "accounts.csv"
      contacts: "contacts.csv"
      opportunities: "opportunities.csv"
  
  redshift:
    base_path: "s3://my-etl-lake-demo-424570854632/bronze/redshift"
    files:
      behavior: "redshift_customer_behavior_50000.csv"
  
  snowflake:
    base_path: "s3://my-etl-lake-demo-424570854632/bronze/snowflake"
    files:
      customers: "snowflake_customers_50000.csv"
      orders: "snowflake_orders_100000.csv"
      products: "snowflake_products_10000.csv"
  
  fx:
    type: json
    base_path: "s3://my-etl-lake-demo-424570854632/bronze/fx"
    raw_path: "s3://my-etl-lake-demo-424570854632/bronze/fx/json/"
    bronze_path: "s3://my-etl-lake-demo-424570854632/bronze/fx/delta/"
    silver_path: "s3://my-etl-lake-demo-424570854632/silver/fx/"
    files:
      daily_rates: "fx_rates_historical_730_days.csv"
      daily_rates_json: "fx_rates_historical.json"  # JSON Lines format
    base_currency: "USD"
    expected_currencies: ["USD", "EUR", "GBP", "JPY", "INR", "CHF", "CAD", "AUD", "CNY"]
  
  kafka:
    local_bootstrap_servers: "localhost:9092"  # Not used on AWS, but kept for consistency
    msk_bootstrap_servers: "${ENV:MSK_BOOTSTRAP_SERVERS}"  # Set via environment variable
    topic: "orders_events"
    consumer_group: "project-a-etl-consumer"
    # Seed file location (for reference, not used in production)
    seed_file: "s3://my-etl-lake-demo-424570854632/bronze/kafka/stream_kafka_events_100000.csv"
  
  kafka_sim:
    base_path: "s3://my-etl-lake-demo-424570854632/bronze/kafka"
    files:
      orders_seed: "stream_kafka_events_100000.csv"

# Table name definitions for Silver and Gold layers
tables:
  silver:
    customers: "customers_silver"
    orders: "orders_silver"
    products: "products_silver"
    behavior: "customer_behavior_silver"
    fx_rates: "fx_rates_silver"
    order_events: "order_events_silver"
  
  gold:
    fact_orders: "fact_orders"
    dim_customer: "dim_customer"
    dim_product: "dim_product"
    fact_customer_24m: "fact_customer_24m"

dq:
  mode: "strict"           # stop pipeline on critical failures
  fail_on_error: true
  alert_on_warning: true

sinks:
  redshift:
    jdbc_url: "jdbc:redshift://HOST:5439/DB"
    user: "redshift_user"
    password: "******"
  snowflake:
    url: "account.snowflakecomputing.com"
    user: "SF_USER"
    password: "******"
    database: "ANALYTICS"
    schema: "PUBLIC"
    warehouse: "COMPUTE_WH"
    role: "SYSADMIN"

lineage:
  enabled: true
  backend: "openlineage"
  endpoint: "http://marquez:5000"   # placeholder
  namespace: "project-a-dev"

sla:
  bronze_ready_by: "02:30"
  silver_ready_by: "03:15"
  gold_ready_by: "04:00"

# Legacy source connection configs (for live connections)
source_connections:
  snowflake:
    secret_name: "project-a-dev/snowflake/conn"
    default_warehouse: "DEV_WH"
    default_database: "CUSTOMER_DB"
    default_schema: "PUBLIC"

  redshift:
    secret_name: "project-a-dev/redshift/conn"

  kafka:
    secret_name: "project-a-dev/kafka/conn"

  salesforce:
    secret_name: "project-a-dev/salesforce/conn"

  fx_api:
    secret_name: "project-a-dev/fx/conn"

# Sink configurations (dual sink: S3 + Snowflake)
sinks:
  snowflake:
    enabled: true
    account: "${SNOWFLAKE_ACCOUNT}"  # e.g., "xy12345.us-east-1"
    user: "PROJECT_A_INGEST"
    password_secret_name: "/project-a/dev/snowflake/password"  # AWS Secrets Manager
    role: "DATA_ENG_ROLE"
    warehouse: "WH_DATA_ENG"
    database: "PROJECT_A"
    schema: "ANALYTICS"
    tables:
      fact_orders: "FACT_ORDERS_DAILY"
      dim_customer: "DIM_CUSTOMER"
      dim_product: "DIM_PRODUCT"
      customer_360: "CUSTOMER_360"
    # Snowflake Spark connector options
    connector_jar: "net.snowflake:snowflake-jdbc:3.13.29,net.snowflake:spark-snowflake_2.12:2.12.0-spark_3.3"
