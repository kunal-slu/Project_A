name: CI/CD Pipeline

on:
  pull_request:
    branches: [ main, feature/* ]
  push:
    branches: [ main, feature/aws-production ]
  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM

env:
  PYTHON_VERSION: '3.11'
  AIRFLOW_VERSION: '2.9.3'

jobs:
  lint-and-format:
    name: Lint and Format
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt || pip install ruff mypy pytest yamllint sqlfluff detect-secrets
        
    - name: Run Ruff (linting)
      run: ruff check . --output-format=github
      
    - name: Run Ruff (formatting)
      run: ruff format . --check
      
    - name: Run MyPy (type checking)
      run: mypy src/ || true  # Allow failures for now
      
    - name: Run YAML linting
      run: yamllint . || true  # Allow failures for now
      
    - name: Run SQL linting
      run: sqlfluff lint . || true  # Allow failures for now
      
    - name: Run secret detection
      run: detect-secrets scan --update .secrets.baseline || true

  test:
    name: Test Suite
    runs-on: ubuntu-latest
    needs: lint-and-format
    strategy:
      matrix:
        python-version: ['3.10', '3.11']
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
        
    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('**/requirements*.txt') }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt || pip install pytest pytest-cov
        
    - name: Run unit tests
      run: |
        pytest tests/unit/ -v --cov=src --cov-report=xml --cov-report=html
        
    - name: Run integration tests
      run: |
        pytest tests/integration/ -v || echo "No integration tests found"
        
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v4
      if: matrix.python-version == '3.11'
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  airflow-dag-test:
    name: Airflow DAG Test
    runs-on: ubuntu-latest
    needs: lint-and-format
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install Airflow
      run: |
        pip install apache-airflow==${{ env.AIRFLOW_VERSION }}
        pip install -r requirements.txt
        
    - name: Initialize Airflow database
      run: |
        export AIRFLOW_HOME=$(pwd)/airflow
        airflow db init
        
    - name: Test DAG imports
      run: |
        export AIRFLOW_HOME=$(pwd)/airflow
        python -c "
        import sys
        import time
        import pathlib
        import pkgutil
        import importlib
        
        start_time = time.time()
        dags_dir = pathlib.Path('airflow/dags')
        
        for mod in pkgutil.iter_modules([str(dags_dir)]):
            try:
                importlib.import_module(f'airflow.dags.{mod.name}')
                print(f'✅ DAG imported successfully: {mod.name}')
            except Exception as e:
                print(f'❌ DAG import failed: {mod.name} - {e}')
                sys.exit(1)
        
        elapsed = time.time() - start_time
        print(f'✅ All DAGs imported in {elapsed:.2f} seconds')
        assert elapsed < 2.0, f'DAG import took too long: {elapsed:.2f}s'
        "
        
    - name: List DAGs
      run: |
        export AIRFLOW_HOME=$(pwd)/airflow
        airflow dags list

  delta-smoke-test:
    name: Delta Lake Smoke Test
    runs-on: ubuntu-latest
    needs: lint-and-format
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install Spark and Delta
      run: |
        pip install pyspark==3.5.1 delta-spark==3.2.0 pyarrow
        
    - name: Run Delta smoke test
      run: |
        python - <<'PY'
        from pyspark.sql import SparkSession
        import tempfile
        import os
        
        # Create Spark session with Delta
        spark = (SparkSession.builder
                .appName("delta-smoke-test")
                .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
                .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
                .config("spark.sql.adaptive.enabled", "false")  # Faster for tests
                .getOrCreate())
        
        # Create test data
        df = spark.range(10).withColumn("value", spark.range(10).id * 2)
        
        # Test Delta operations
        with tempfile.TemporaryDirectory() as tmp_dir:
            delta_path = os.path.join(tmp_dir, "test_delta")
            
            # Write to Delta
            df.write.format("delta").mode("overwrite").save(delta_path)
            print(f"✅ Delta write successful: {delta_path}")
            
            # Read from Delta
            read_df = spark.read.format("delta").load(delta_path)
            count = read_df.count()
            print(f"✅ Delta read successful: {count} records")
            
            # Test time travel
            history = spark.sql(f"DESCRIBE HISTORY delta.`{delta_path}`")
            history_count = history.count()
            print(f"✅ Delta time travel successful: {history_count} versions")
            
            assert count == 10, f"Expected 10 records, got {count}"
            assert history_count >= 1, f"Expected at least 1 version, got {history_count}"
        
        print("✅ All Delta Lake smoke tests passed!")
        spark.stop()
        PY

  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    needs: lint-and-format
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install security tools
      run: |
        pip install safety bandit semgrep
        
    - name: Run Safety (vulnerability scan)
      run: safety check --json || true
      
    - name: Run Bandit (security linting)
      run: bandit -r src/ -f json || true
      
    - name: Run Semgrep (static analysis)
      run: semgrep --config=auto src/ || true

  build-docker:
    name: Build Docker Image
    runs-on: ubuntu-latest
    needs: [test, airflow-dag-test, delta-smoke-test]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      
    - name: Login to Docker Hub
      uses: docker/login-action@v3
      with:
        username: ${{ secrets.DOCKER_USERNAME }}
        password: ${{ secrets.DOCKER_PASSWORD }}
        
    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        push: true
        tags: |
          ${{ secrets.DOCKER_USERNAME }}/project-a:latest
          ${{ secrets.DOCKER_USERNAME }}/project-a:${{ github.sha }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
