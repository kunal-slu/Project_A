name: CI/CD Pipeline - Complete

on:
  push:
    branches: [main, develop, feature/*]
  pull_request:
    branches: [main, develop]
  schedule:
    # Run nightly tests at 2 AM UTC
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.11'
  POETRY_VERSION: '1.7.0'

jobs:
  # ============================================
  # Job 1: Code Quality & Linting
  # ============================================
  lint:
    name: Code Quality & Linting
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install ruff black mypy isort pylint bandit safety

      - name: Run Ruff (Fast Python Linter)
        run: |
          ruff check src/ tests/ --output-format=github
        continue-on-error: true

      - name: Run Black (Code Formatter Check)
        run: |
          black --check src/ tests/
        continue-on-error: true

      - name: Run isort (Import Sorter)
        run: |
          isort --check-only src/ tests/
        continue-on-error: true

      - name: Run MyPy (Type Checker)
        run: |
          mypy src/ --ignore-missing-imports
        continue-on-error: true

      - name: Run Pylint
        run: |
          pylint src/pyspark_interview_project/ --disable=C0111,C0103,R0913,R0914
        continue-on-error: true

  # ============================================
  # Job 2: Security Scanning
  # ============================================
  security:
    name: Security Scanning
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install bandit safety detect-secrets

      - name: Run Bandit (Security Linter)
        run: |
          bandit -r src/ -f json -o bandit-report.json
        continue-on-error: true

      - name: Run Safety (Dependency Vulnerability Check)
        run: |
          pip install -r requirements.txt
          safety check --json --output safety-report.json
        continue-on-error: true

      - name: Run detect-secrets
        run: |
          detect-secrets scan --all-files --force-use-all-plugins
        continue-on-error: true

      - name: Upload Security Reports
        uses: actions/upload-artifact@v3
        with:
          name: security-reports
          path: |
            bandit-report.json
            safety-report.json

  # ============================================
  # Job 3: YAML & SQL Linting
  # ============================================
  lint-configs:
    name: Lint Configuration Files
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install yamllint
        run: pip install yamllint

      - name: Lint YAML files
        run: |
          yamllint config/*.yaml k8s/**/*.yaml
        continue-on-error: true

      - name: Install SQLFluff
        run: pip install sqlfluff

      - name: Lint SQL files
        run: |
          sqlfluff lint scripts/*.sql --dialect postgres
        continue-on-error: true

  # ============================================
  # Job 4: Unit Tests with Coverage
  # ============================================
  test:
    name: Unit Tests & Coverage
    runs-on: ubuntu-latest
    needs: [lint]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
          pip install pytest pytest-cov pytest-xdist pytest-timeout coverage[toml]

      - name: Run Unit Tests with Coverage
        run: |
          pytest tests/ \
            --cov=src/pyspark_interview_project \
            --cov-report=xml \
            --cov-report=html \
            --cov-report=term-missing \
            --cov-fail-under=60 \
            -v \
            -n auto \
            --timeout=300
        continue-on-error: false

      - name: Upload Coverage Reports
        uses: actions/upload-artifact@v3
        with:
          name: coverage-reports
          path: |
            htmlcov/
            coverage.xml

      - name: Upload Coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella
          fail_ci_if_error: false

  # ============================================
  # Job 5: Integration Tests
  # ============================================
  integration-test:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [test]
    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_USER: airflow
          POSTGRES_PASSWORD: airflow
          POSTGRES_DB: airflow
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-integration

      - name: Run Integration Tests
        env:
          DATABASE_URL: postgresql://airflow:airflow@localhost:5432/airflow
        run: |
          pytest tests/integration/ -v --timeout=600
        continue-on-error: true

  # ============================================
  # Job 6: Airflow DAG Validation
  # ============================================
  dag-validation:
    name: Airflow DAG Validation
    runs-on: ubuntu-latest
    needs: [lint]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Airflow
        run: |
          pip install apache-airflow==2.9.3 apache-airflow-providers-apache-spark

      - name: Validate DAG Imports
        run: |
          python -c "
          import sys
          sys.path.append('airflow/dags')
          
          errors = []
          dags = [
              'delta_lake_etl_pipeline_dag',
              'delta_maintenance_dag'
          ]
          
          for dag_name in dags:
              try:
                  module = __import__(dag_name)
                  print(f'✅ {dag_name} imported successfully')
                  
                  # Check for DAG object
                  if hasattr(module, 'dag'):
                      print(f'   Found DAG: {module.dag.dag_id}')
                  else:
                      print(f'   ⚠️  No dag object found')
              except Exception as e:
                  errors.append(f'❌ {dag_name}: {e}')
                  print(f'❌ {dag_name} failed: {e}')
          
          if errors:
              print(f'\n❌ {len(errors)} DAG(s) failed validation')
              sys.exit(1)
          else:
              print(f'\n✅ All DAGs validated successfully')
          "

      - name: Check DAG Structure
        run: |
          python tests/test_dag_imports.py
        continue-on-error: true

  # ============================================
  # Job 7: Delta Lake Smoke Tests
  # ============================================
  delta-smoke-test:
    name: Delta Lake Smoke Tests
    runs-on: ubuntu-latest
    needs: [test]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Run Delta Lake Smoke Test
        run: |
          python -c "
          import sys
          sys.path.append('src')
          
          from pyspark_interview_project.utils.safe_writer import SafeDeltaWriter
          from pyspark_interview_project.schemas.production_schemas import get_schema
          from pyspark_interview_project.dq.great_expectations_runner import GreatExpectationsRunner
          
          print('✅ SafeDeltaWriter import successful')
          print('✅ Schema registry import successful')
          print('✅ GE Runner import successful')
          print('\n✅ All Delta Lake components available')
          "

  # ============================================
  # Job 8: Docker Build & Push
  # ============================================
  docker-build:
    name: Build & Push Docker Image
    runs-on: ubuntu-latest
    needs: [test, security]
    if: github.ref == 'refs/heads/main'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}
        if: github.event_name != 'pull_request'

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ secrets.DOCKER_USERNAME }}/pyspark-etl
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=sha

      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: .
          platforms: linux/amd64,linux/arm64
          push: ${{ github.event_name != 'pull_request' }}
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ secrets.DOCKER_USERNAME }}/pyspark-etl:latest
          format: 'sarif'
          output: 'trivy-results.sarif'
        continue-on-error: true

      - name: Upload Trivy results
        uses: github/codeql-action/upload-sarif@v2
        with:
          sarif_file: 'trivy-results.sarif'
        continue-on-error: true

  # ============================================
  # Job 9: Performance Benchmarks
  # ============================================
  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    needs: [test]
    if: github.event_name == 'schedule' || github.ref == 'refs/heads/main'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest-benchmark memory_profiler

      - name: Run Benchmarks
        run: |
          pytest tests/benchmarks/ --benchmark-only --benchmark-json=benchmark-results.json
        continue-on-error: true

      - name: Upload Benchmark Results
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results
          path: benchmark-results.json

  # ============================================
  # Job 10: Deployment (Production)
  # ============================================
  deploy-prod:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [test, integration-test, dag-validation, docker-build]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    environment:
      name: production
      url: https://airflow.example.com
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'

      - name: Set Kubernetes Context
        run: |
          echo "${{ secrets.KUBE_CONFIG }}" > kubeconfig
          export KUBECONFIG=kubeconfig
          kubectl config use-context production

      - name: Deploy to Kubernetes
        run: |
          kubectl apply -f k8s/base/
          kubectl rollout status deployment/airflow-webserver -n pyspark-etl
          kubectl rollout status deployment/airflow-scheduler -n pyspark-etl

      - name: Run Smoke Tests
        run: |
          kubectl run smoke-test --image=${{ secrets.DOCKER_USERNAME }}/pyspark-etl:latest \
            --restart=Never \
            --command -- python -m pyspark_interview_project.cli --config config/prod.yaml --env prod --cmd validate

      - name: Notify Deployment
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          text: 'Production deployment completed'
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
        if: always()

  # ============================================
  # Job 11: Cleanup Old Artifacts
  # ============================================
  cleanup:
    name: Cleanup Artifacts
    runs-on: ubuntu-latest
    needs: [deploy-prod]
    if: always()
    steps:
      - name: Delete old artifacts
        uses: actions/github-script@v6
        with:
          script: |
            const days_to_keep = 30;
            const cutoff_date = new Date();
            cutoff_date.setDate(cutoff_date.getDate() - days_to_keep);
            
            const artifacts = await github.rest.actions.listArtifactsForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              per_page: 100
            });
            
            for (const artifact of artifacts.data.artifacts) {
              const artifact_date = new Date(artifact.created_at);
              if (artifact_date < cutoff_date) {
                await github.rest.actions.deleteArtifact({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  artifact_id: artifact.id
                });
                console.log(`Deleted artifact: ${artifact.name}`);
              }
            }

