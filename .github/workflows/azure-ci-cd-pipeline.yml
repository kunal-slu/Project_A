name: Azure CI/CD Pipeline - PySpark Data Engineering

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy to'
        required: true
        default: 'dev'
        type: choice
        options:
        - dev
        - staging
        - prod

env:
  AZURE_REGION: East US
  PYTHON_VERSION: '3.11'
  SPARK_VERSION: '3.5.0'
  DELTA_VERSION: '2.4.0'

jobs:
  # Code Quality & Testing
  code-quality:
    name: Code Quality & Testing
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        if [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi
        
    - name: Run linting
      run: |
        if command -v flake8 >/dev/null 2>&1; then flake8 src/ tests/ --max-line-length=120 --extend-ignore=E203,W503 || true; fi
        if command -v black >/dev/null 2>&1; then black --check --diff src/ tests/ || true; fi
        if command -v isort >/dev/null 2>&1; then isort --check-only --diff src/ tests/ || true; fi
        
    - name: Run type checking
      run: |
        if command -v mypy >/dev/null 2>&1; then mypy src/ --ignore-missing-imports || true; fi
        
    - name: Run security scan
      run: |
        bandit -r src/ -f json -o bandit-report.json || true
        
    - name: Run unit tests
      run: |
        if [ -d tests ]; then pytest tests/ --cov=src --cov-report=xml --cov-report=html --junitxml=test-results.xml; else echo "No tests directory"; fi
        
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: code-coverage
        
    - name: Upload test results
      uses: actions/upload-artifact@v3
      with:
        name: test-results
        path: |
          test-results.xml
          coverage.xml
          htmlcov/
          bandit-report.json

  # Integration Testing
  integration-test:
    name: Integration Testing
    runs-on: ubuntu-latest
    needs: code-quality
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
          
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        
    - name: Set up test environment
      run: |
        export POSTGRES_HOST=localhost
        export POSTGRES_PORT=5432
        export POSTGRES_USER=postgres
        export POSTGRES_PASSWORD=postgres
        export POSTGRES_DB=test_db
        
    - name: Run integration tests
      run: |
        if [ -d tests/integration ]; then pytest tests/integration/ --junitxml=integration-test-results.xml; else echo "No integration tests"; fi
        
    - name: Upload integration test results
      uses: actions/upload-artifact@v3
      with:
        name: integration-test-results
        path: integration-test-results.xml

  # Build & Package
  build:
    name: Build & Package
    runs-on: ubuntu-latest
    needs: [code-quality, integration-test]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install build dependencies
      run: |
        python -m pip install --upgrade pip
        pip install build wheel setuptools
        
    - name: Build package
      run: |
        python -m build
        
    - name: Create deployment package
      run: |
        mkdir -p deployment
        cp dist/*.whl deployment/
        cp -r config/ deployment/
        cp -r src/ deployment/
        cp requirements.txt deployment/
        mkdir -p deployment/scripts
        if [ -d scripts ]; then cp -r scripts/* deployment/scripts/; fi
        cp README.md deployment/
        tar -czf pyspark-pipeline-azure.tar.gz deployment/
        
    - name: Upload deployment package
      uses: actions/upload-artifact@v3
      with:
        name: deployment-package
        path: pyspark-pipeline-azure.tar.gz

  # Deploy to Azure Dev
  deploy-dev:
    name: Deploy to Azure Dev
    runs-on: ubuntu-latest
    needs: build
    if: github.ref == 'refs/heads/develop' || github.event_name == 'workflow_dispatch'
    environment: azure-dev
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Download deployment package
      uses: actions/download-artifact@v3
      with:
        name: deployment-package
        
    - name: Azure Login
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS_DEV }}
        
    - name: Set Azure CLI version
      run: az version
      
    - name: Set Azure subscription
      run: |
        az account set --subscription ${{ secrets.AZURE_SUBSCRIPTION_ID_DEV }}
        
    - name: Deploy to Azure Storage
      run: |
        # Extract deployment package
        tar -xzf pyspark-pipeline-azure.tar.gz
        
        # Upload to Azure Storage
        az storage blob upload-batch \
          --account-name ${{ secrets.AZURE_STORAGE_ACCOUNT_DEV }} \
          --auth-mode key \
          --account-key ${{ secrets.AZURE_STORAGE_KEY_DEV }} \
          --source deployment/ \
          --destination artifacts \
          --overwrite

    - name: Trigger Databricks Job (optional)
      run: |
        if [ "${{ secrets.AZURE_DATABRICKS_WORKSPACE_URL_DEV }}" != "" ] && [ "${{ secrets.AZURE_DATABRICKS_TOKEN_DEV }}" != "" ]; then \
          echo "Triggering Databricks job run" && \
          curl -sS -X POST "${{ secrets.AZURE_DATABRICKS_WORKSPACE_URL_DEV }}/api/2.1/jobs/run-now" \
            -H "Authorization: Bearer ${{ secrets.AZURE_DATABRICKS_TOKEN_DEV }}" \
            -H 'Content-Type: application/json' \
            -d '{"job_id": '${{ secrets.AZURE_DATABRICKS_JOB_ID_DEV }}', "notebook_params": {"CONFIG_PATH": "config-azure-dev.yaml"}}' | cat; \
        else \
          echo "Databricks trigger skipped - secrets not configured"; \
        fi
          
    - name: Notify deployment
      uses: 8398a7/action-slack@v3
      with:
        status: success
        text: 'PySpark pipeline deployed to Azure Dev environment'
        webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}
      if: always()

  # Deploy to Azure Staging
  deploy-staging:
    name: Deploy to Azure Staging
    runs-on: ubuntu-latest
    needs: build
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.environment == 'staging'
    environment: azure-staging
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Download deployment package
      uses: actions/download-artifact@v3
      with:
        name: deployment-package
        
    - name: Azure Login
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS_STAGING }}
        
    - name: Set Azure subscription
      run: |
        az account set --subscription ${{ secrets.AZURE_SUBSCRIPTION_ID_STAGING }}
        
    - name: Deploy to Azure Storage
      run: |
        tar -xzf pyspark-pipeline-azure.tar.gz
        
        az storage blob upload-batch \
          --account-name ${{ secrets.AZURE_STORAGE_ACCOUNT_STAGING }} \
          --auth-mode key \
          --account-key ${{ secrets.AZURE_STORAGE_KEY_STAGING }} \
          --source deployment/ \
          --destination artifacts \
          --overwrite
          
    - name: Trigger Databricks Job (optional)
      run: |
        if [ "${{ secrets.AZURE_DATABRICKS_WORKSPACE_URL_STAGING }}" != "" ] && [ "${{ secrets.AZURE_DATABRICKS_TOKEN_STAGING }}" != "" ]; then \
          echo "Triggering Databricks job run (staging)" && \
          curl -sS -X POST "${{ secrets.AZURE_DATABRICKS_WORKSPACE_URL_STAGING }}/api/2.1/jobs/run-now" \
            -H "Authorization: Bearer ${{ secrets.AZURE_DATABRICKS_TOKEN_STAGING }}" \
            -H 'Content-Type: application/json' \
            -d '{"job_id": '${{ secrets.AZURE_DATABRICKS_JOB_ID_STAGING }}', "notebook_params": {"CONFIG_PATH": "config-azure-staging.yaml"}}' | cat; \
        else \
          echo "Databricks trigger skipped - secrets not configured"; \
        fi
          
    - name: Wait for pipeline completion
      run: |
        echo "Waiting for pipeline to complete..."
        sleep 300  # Wait 5 minutes
        
    - name: Run validation script
      run: |
        if [ -f scripts/smoke_validate.py ]; then python scripts/smoke_validate.py; else echo "No smoke test"; fi
          
    - name: Notify deployment
      uses: 8398a7/action-slack@v3
      with:
        status: success
        text: 'PySpark pipeline deployed to Azure Staging environment'
        webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}
      if: always()

  # Deploy to Azure Production
  deploy-prod:
    name: Deploy to Azure Production
    runs-on: ubuntu-latest
    needs: build
    if: github.ref == 'refs/heads/main' || (github.event_name == 'workflow_dispatch' && github.event.inputs.environment == 'prod')
    environment: azure-production
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Download deployment package
      uses: actions/download-artifact@v3
      with:
        name: deployment-package
        
    - name: Azure Login
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS_PROD }}
        
    - name: Set Azure subscription
      run: |
        az account set --subscription ${{ secrets.AZURE_SUBSCRIPTION_ID_PROD }}
        
    - name: Deploy to Azure Storage
      run: |
        tar -xzf pyspark-pipeline-azure.tar.gz
        
        az storage blob upload-batch \
          --account-name ${{ secrets.AZURE_STORAGE_ACCOUNT_PROD }} \
          --auth-mode key \
          --account-key ${{ secrets.AZURE_STORAGE_KEY_PROD }} \
          --source deployment/ \
          --destination artifacts \
          --overwrite
          
    - name: Trigger Databricks Job (optional)
      run: |
        if [ "${{ secrets.AZURE_DATABRICKS_WORKSPACE_URL_PROD }}" != "" ] && [ "${{ secrets.AZURE_DATABRICKS_TOKEN_PROD }}" != "" ]; then \
          echo "Triggering Databricks job run (prod)" && \
          curl -sS -X POST "${{ secrets.AZURE_DATABRICKS_WORKSPACE_URL_PROD }}/api/2.1/jobs/run-now" \
            -H "Authorization: Bearer ${{ secrets.AZURE_DATABRICKS_TOKEN_PROD }}" \
            -H 'Content-Type: application/json' \
            -d '{"job_id": '${{ secrets.AZURE_DATABRICKS_JOB_ID_PROD }}', "notebook_params": {"CONFIG_PATH": "config-azure-prod.yaml"}}' | cat; \
        else \
          echo "Databricks trigger skipped - secrets not configured"; \
        fi
          
    - name: Wait for pipeline completion
      run: |
        echo "Waiting for pipeline to complete..."
        sleep 600  # Wait 10 minutes
        
    - name: Run comprehensive validation
      run: |
        if [ -f scripts/smoke_validate.py ]; then python scripts/smoke_validate.py; else echo "No production validation"; fi
          
    - name: Notify deployment
      uses: 8398a7/action-slack@v3
      with:
        status: success
        text: 'PySpark pipeline deployed to Azure Production environment'
        webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}
      if: always()

  # Security & Compliance
  security-scan:
    name: Security & Compliance
    runs-on: ubuntu-latest
    needs: build
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'
        
    - name: Upload Trivy scan results
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: 'trivy-results.sarif'
        
    - name: Run Safety check
      run: |
        pip install safety
        safety check --json --output safety-report.json || true
        
    - name: Upload Safety report
      uses: actions/upload-artifact@v3
      with:
        name: security-reports
        path: |
          trivy-results.sarif
          safety-report.json

  # Performance Testing
  performance-test:
    name: Performance Testing
    runs-on: ubuntu-latest
    needs: build
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install pytest-benchmark
        
    - name: Run performance tests
      run: |
        if [ -d tests/performance ]; then pytest tests/performance/ --benchmark-only --benchmark-save=performance-results; else echo "No performance tests"; fi
        
    - name: Upload performance results
      uses: actions/upload-artifact@v3
      with:
        name: performance-results
        path: .benchmarks/

  # Azure Infrastructure Validation
  infrastructure-validation:
    name: Azure Infrastructure Validation
    runs-on: ubuntu-latest
    needs: build
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Azure Login
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS_DEV }}
        
    - name: Install Terraform
      uses: hashicorp/setup-terraform@v2
      with:
        terraform_version: "1.5.0"
        
    - name: Terraform Init
      run: |
        cd infrastructure
        terraform init
        
    - name: Terraform Plan
      run: |
        cd infrastructure
        terraform plan -var-file="azure-variables.tfvars" -out=tfplan
        
    - name: Terraform Validate
      run: |
        cd infrastructure
        terraform validate
        
    - name: Upload Terraform plan
      uses: actions/upload-artifact@v3
      with:
        name: terraform-plan
        path: infrastructure/tfplan

  # Azure Data Quality Validation
  data-quality-validation:
    name: Azure Data Quality Validation
    runs-on: ubuntu-latest
    needs: build
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        
    - name: Run data quality tests
      run: |
        if [ -d tests/data_quality ]; then python -m pytest tests/data_quality/ --junitxml=data-quality-results.xml; else echo "No data quality tests"; fi
        
    - name: Upload data quality results
      uses: actions/upload-artifact@v3
      with:
        name: data-quality-results
        path: data-quality-results.xml
