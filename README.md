# PySpark Data Engineering Project

## ğŸ¯ Project Overview

This project provides a comprehensive PySpark data engineering pipeline with 5 essential data sources for learning and practice.

## ğŸ“Š Data Sources

### 1ï¸âƒ£ HubSpot CRM
- **Contacts**: Customer contact information (25K records)
- **Deals**: Sales opportunities and pipeline (30K records)

### 2ï¸âƒ£ Snowflake Warehouse
- **Customers**: Customer master data (50K records)
- **Orders**: Order transactions (100K records)
- **Products**: Product catalog (10K records)

### 3ï¸âƒ£ Redshift Analytics
- **Customer Behavior**: User behavior analytics (50K records)

### 4ï¸âƒ£ Stream Data
- **Kafka Events**: Real-time event streaming (100K records)

### 5ï¸âƒ£ FX Rates
- **Historical Rates**: Exchange rates (20K records)

## ğŸš€ Getting Started

### Prerequisites
- Python 3.8+
- PySpark 3.5+
- Delta Lake

### Installation
```bash
pip install -r requirements.txt
```

### Running the Pipeline
```bash
python src/pyspark_interview_project/pipeline.py config/config-dev.yaml
```

## ğŸ“ Project Structure

```
â”œâ”€â”€ aws/data_fixed/           # Data sources
â”‚   â”œâ”€â”€ 01_hubspot_crm/       # HubSpot CRM data
â”‚   â”œâ”€â”€ 02_snowflake_warehouse/ # Snowflake warehouse data
â”‚   â”œâ”€â”€ 03_redshift_analytics/ # Redshift analytics data
â”‚   â”œâ”€â”€ 04_stream_data/       # Streaming data
â”‚   â””â”€â”€ 05_fx_rates/          # FX rates data
â”œâ”€â”€ config/                   # Configuration files
â”œâ”€â”€ src/pyspark_interview_project/ # Main pipeline code
â”œâ”€â”€ airflow/dags/             # Airflow DAGs
â””â”€â”€ docs/                     # Documentation
```

## ğŸ¯ Learning Objectives

- **Data Engineering**: ETL pipelines, data quality, transformations
- **Analytics**: Aggregations, window functions, statistical analysis
- **Performance**: Optimization, partitioning, caching strategies
- **Integration**: Multi-source data integration
- **Real-time Processing**: Streaming data and event processing

## ğŸ“š Documentation

- [Simplified Data Sources](docs/SIMPLIFIED_DATA_SOURCES.md)
- [Data Quality Report](docs/DATA_QUALITY_REPORT.md)

## ğŸ”§ Configuration

All configurations are managed in the `config/` directory:
- `default.yaml` - Base configuration
- `aws.yaml` - AWS-specific settings
- `azure.yaml` - Azure-specific settings
- `local.yaml` - Local development settings

## ğŸš€ Ready for PySpark Practice!

This project provides realistic, high-quality data for comprehensive PySpark learning and practice.
