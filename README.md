# PySpark Data Engineering Project

Comprehensive AWS Production ETL Pipeline with Delta Lake

## ğŸ¯ Project Overview

This is a production-ready data engineering project that demonstrates best practices for:
- Multi-source data ingestion (HubSpot, Snowflake, Redshift, Kafka, FX Rates)
- Bronze â†’ Silver â†’ Gold data lakehouse architecture
- Incremental loading with SCD2 support
- Data quality validation
- AWS EMR Serverless deployment
- Delta Lake for ACID transactions

## ğŸ“ Project Structure

```
pyspark_data_engineer_project/
â”œâ”€â”€ config/                      # Configuration files
â”‚   â”œâ”€â”€ local.yaml              # Local development
â”‚   â”œâ”€â”€ config-dev.yaml         # Dev environment
â”‚   â”œâ”€â”€ aws.yaml                # AWS production
â”‚   â””â”€â”€ dq.yaml                 # Data quality config
â”‚
â”œâ”€â”€ src/pyspark_interview_project/
â”‚   â”œâ”€â”€ utils/                   # Core utilities
â”‚   â”œâ”€â”€ extract.py               # Data extraction
â”‚   â”œâ”€â”€ transform.py             # Data transformation
â”‚   â”œâ”€â”€ load.py                  # Data loading
â”‚   â”œâ”€â”€ incremental_loading.py   # SCD2 & CDC
â”‚   â”œâ”€â”€ jobs/                    # EMR job implementations
â”‚   â”œâ”€â”€ dq/                      # Data quality
â”‚   â””â”€â”€ monitoring/              # Monitoring
â”‚
â”œâ”€â”€ jobs/                        # EMR job wrappers
â”œâ”€â”€ aws/
â”‚   â”œâ”€â”€ infra/terraform/        # Infrastructure as code
â”‚   â”œâ”€â”€ scripts/                 # Deployment scripts
â”‚   â””â”€â”€ emr_configs/            # EMR configuration
â”‚
â”œâ”€â”€ tests/                       # Test suite
â”œâ”€â”€ notebooks/                   # Jupyter notebooks
â””â”€â”€ docs/                        # Documentation
```

## ğŸš€ Quick Start

### Local Development

```bash
# Install dependencies
pip install -r requirements.txt

# Run tests
pytest tests/

# Run pipeline locally
python src/pyspark_interview_project/pipeline_core.py config/config-dev.yaml
```

### AWS Deployment

See [AWS_DEPLOYMENT_GUIDE.md](AWS_DEPLOYMENT_GUIDE.md) for complete deployment instructions.

## ğŸ“Š Data Sources

1. **HubSpot CRM** - Contacts and deals
2. **Snowflake** - Orders and customers
3. **Redshift** - Customer behavior analytics
4. **Kafka** - Real-time event streaming
5. **FX Rates** - Exchange rates from vendors

## ğŸ—ï¸ Architecture

- **Bronze Layer**: Raw data ingestion with schema validation
- **Silver Layer**: Cleaned, conformed data with SCD2 support
- **Gold Layer**: Business-ready dimensional models

## ğŸ”§ Key Features

- âœ… **Incremental loading strategies** with watermark-based CDC
- âœ… **SCD2 support** for slowly changing dimensions
- âœ… **Data quality gates** with Great Expectations (critical failure handling)
- âœ… **Multi-format support**: Delta Lake, Apache Iceberg, Parquet
- âœ… **Dual destinations**: S3 (data lake) + Snowflake (analytics)
- âœ… **Real lineage tracking** via OpenLineage
- âœ… **AWS EMR Serverless deployment**
- âœ… **Monitoring and alerting** with CloudWatch

## ğŸ“– Documentation

- **[Getting Started Guide](README_GETTING_STARTED.md)** ğŸŒŸ - **START HERE! Your next steps**
- **[Beginners AWS Guide](BEGINNERS_AWS_DEPLOYMENT_GUIDE.md)** â­ - Step-by-step AWS deployment for novices
- [AWS Deployment Guide](AWS_COMPLETE_DEPLOYMENT_GUIDE.md) - Complete end-to-end AWS deployment
- [Data Sources & Architecture](DATA_SOURCES_AND_ARCHITECTURE.md) - All 6 data sources and architecture
- [P0-P6 Implementation Plan](P0_P6_IMPLEMENTATION_PLAN.md) - Production-ready roadmap
- [AWS Runbook](RUNBOOK_AWS_2025.md) - Operational procedures

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/

# Run specific test suite
pytest tests/test_contracts.py
```

## ğŸ“ Requirements

- Python 3.10+
- PySpark 3.5+
- Delta Lake
- AWS CLI configured
- Terraform 1.0+

## ğŸ“„ License

MIT License

## ğŸ‰ Recent Updates (2025)

### Production-Grade Enhancements Completed

- âœ… **Real Lineage Tracking** - OpenLineage integration with automatic metadata capture
- âœ… **Data Quality Gates** - Great Expectations with critical failure handling
- âœ… **Snowflake Target** - Dual destination loading with MERGE operations
- âœ… **AWS Deployment** - Complete end-to-end deployment guide (see [AWS_COMPLETE_DEPLOYMENT_GUIDE.md](AWS_COMPLETE_DEPLOYMENT_GUIDE.md))
- âœ… **Multi-Source Architecture** - 6 data sources documented (see [DATA_SOURCES_AND_ARCHITECTURE.md](DATA_SOURCES_AND_ARCHITECTURE.md))

**Additional Production Features**
- âœ… Dual destination: S3 (data lake) + Snowflake (analytics)
- âœ… `write_df_to_snowflake()` with MERGE support
- âœ… Idempotent upserts with composite primary keys

**D. Iceberg Toggle**
- âœ… Format flexibility: Delta/Iceberg/Parquet via `config/storage.yaml`
- âœ… Glue catalog integration for Iceberg
- âœ… Transparent to application code

### Quick Start
```bash
# Run complete pipeline locally
python -m pyspark_interview_project.cli \
  --config config/local.yaml \
  --env local \
  --cmd full

# Deploy to AWS
# See AWS_COMPLETE_DEPLOYMENT_GUIDE.md for step-by-step instructions
```
