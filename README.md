# PySpark Data Engineering Project

## ğŸ¯ Project Overview

This project provides a comprehensive PySpark data engineering pipeline with 5 essential data sources for learning and practice.

## ğŸ“Š Data Sources

### 1ï¸âƒ£ HubSpot CRM
- **Contacts**: Customer contact information (25K records)
- **Deals**: Sales opportunities and pipeline (30K records)

### 2ï¸âƒ£ Snowflake Warehouse
- **Customers**: Customer master data (50K records)
- **Orders**: Order transactions (100K records)
- **Products**: Product catalog (10K records)

### 3ï¸âƒ£ Redshift Analytics
- **Customer Behavior**: User behavior analytics (50K records)

### 4ï¸âƒ£ Stream Data
- **Kafka Events**: Real-time event streaming (100K records)

### 5ï¸âƒ£ FX Rates
- **Historical Rates**: Exchange rates (20K records)

## ğŸš€ Getting Started

### Prerequisites
- Python 3.8+
- PySpark 3.5+
- Delta Lake

### Installation
```bash
pip install -r requirements.txt
```

### Running the Pipeline
```bash
python src/pyspark_interview_project/pipeline.py config/config-dev.yaml
```
Project_A/
â”‚
â”œâ”€â”€ README.md                              â† Overview, setup steps, quickstart
â”œâ”€â”€ Makefile                               â† Common shortcuts (run, test, deploy)
â”œâ”€â”€ requirements.txt                       â† Python dependencies for local/dev
â”œâ”€â”€ setup.py                               â† Optional (for packaging jobs)
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ local.yaml                         â† Local testing configs (S3, paths)
â”‚   â”œâ”€â”€ prod.yaml                          â† MWAA/EMR production configs
â”‚   â”œâ”€â”€ dq.yaml                            â† Data quality thresholds / rules
â”‚   â””â”€â”€ logging.conf                       â† Logging format and levels
â”‚
â”œâ”€â”€ dags/                                  â† Airflow DAGs (used by MWAA)
â”‚   â”œâ”€â”€ daily_pipeline_dag.py              â† Master DAG (bronze â†’ silver â†’ gold)
â”‚   â”œâ”€â”€ returns_pipeline_dag.py            â† Returns + reconciliation pipeline
â”‚   â”œâ”€â”€ catalog_and_dq_dag.py              â† Glue registration + DQ checks
â”‚   â””â”€â”€ utils/                             â† Custom Airflow operators/hooks
â”‚
â”œâ”€â”€ aws/                                   â† AWS deployment scripts
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ aws_production_deploy.sh       â† Bootstraps EMR/roles/buckets
â”‚   â”‚   â”œâ”€â”€ emr_submit.sh                  â† Submits EMR Serverless jobs
â”‚   â”‚   â”œâ”€â”€ register_glue_tables.py        â† Glue/Athena table registration
â”‚   â”‚   â”œâ”€â”€ run_ge_checks.py               â† Great Expectations DQ runner
â”‚   â”‚   â”œâ”€â”€ delta_optimize_vacuum.py       â† (Optional) Maintenance job
â”‚   â”‚   â””â”€â”€ teardown.sh                    â† Destroys AWS resources cleanly
â”‚   â”‚
â”‚   â”œâ”€â”€ terraform/                         â† Infrastructure as Code (optional)
â”‚   â”‚   â”œâ”€â”€ main.tf                        â† MWAA, EMR, S3, IAM setup
â”‚   â”‚   â”œâ”€â”€ variables.tf                   â† Parameter definitions
â”‚   â”‚   â””â”€â”€ outputs.tf                     â† Useful resource ARNs/IDs
â”‚   â”‚
â”‚   â””â”€â”€ emr_configs/
â”‚       â”œâ”€â”€ spark-defaults.conf            â† Spark EMR configs
â”‚       â”œâ”€â”€ delta-core.conf                â† Delta Lake specific configs
â”‚       â””â”€â”€ logging.yaml                   â† Spark log4j2 config
â”‚
â”œâ”€â”€ src/                                   â† Core PySpark code
â”‚   â””â”€â”€ pyspark_interview_project/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ utils/
â”‚       â”‚   â”œâ”€â”€ spark_session.py           â† SparkSession builder (Delta-aware)
â”‚       â”‚   â”œâ”€â”€ io_utils.py                â† S3 / Snowflake read-write helpers
â”‚       â”‚   â”œâ”€â”€ path_resolver.py           â† Converts lake:// to s3a:// paths
â”‚       â”‚   â”œâ”€â”€ dq_utils.py                â† Quality check helpers
â”‚       â”‚   â””â”€â”€ metrics.py                 â† Logging + OpenLineage metrics
â”‚       â”‚
â”‚       â”œâ”€â”€ extract/
â”‚       â”‚   â”œâ”€â”€ hubspot_contacts.py        â† Extracts HubSpot contacts CSV
â”‚       â”‚   â”œâ”€â”€ hubspot_companies.py       â† Extracts HubSpot companies CSV
â”‚       â”‚   â”œâ”€â”€ snowflake_orders.py        â† Extract from Snowflake
â”‚       â”‚   â”œâ”€â”€ redshift_behavior.py       â† Extract from Redshift
â”‚       â”‚   â”œâ”€â”€ kafka_orders_stream.py     â† Structured streaming reader
â”‚       â”‚   â””â”€â”€ fx_rates.py                â† Loads FX rate CSV / API
â”‚       â”‚
â”‚       â”œâ”€â”€ transform/
â”‚       â”‚   â”œâ”€â”€ bronze_to_silver.py        â† Cleans & standardizes raw data
â”‚       â”‚   â”œâ”€â”€ silver_to_gold.py          â† Joins across sources for KPIs
â”‚       â”‚   â”œâ”€â”€ enrich_with_fx.py          â† Applies FX normalization
â”‚       â”‚   â”œâ”€â”€ build_customer_segments.py â† Customer segmentation
â”‚       â”‚   â””â”€â”€ build_product_perf.py      â† Product analytics
â”‚       â”‚
â”‚       â”œâ”€â”€ jobs/
â”‚       â”‚   â”œâ”€â”€ pipeline_driver.py         â† Unified entrypoint for DAGs/jobs
â”‚       â”‚   â”œâ”€â”€ run_snowflake_pipeline.py  â† Example standalone runner
â”‚       â”‚   â””â”€â”€ run_redshift_pipeline.py   â† Example Redshift â†’ Gold runner
â”‚       â”‚
â”‚       â”œâ”€â”€ dq/
â”‚       â”‚   â”œâ”€â”€ suites/
â”‚       â”‚   â”‚   â”œâ”€â”€ silver_orders.yml
â”‚       â”‚   â”‚   â”œâ”€â”€ silver_fx_rates.yml
â”‚       â”‚   â”‚   â””â”€â”€ gold_revenue.yml
â”‚       â”‚   â””â”€â”€ dq_runner.py               â† Executes GE suites programmatically
â”‚       â”‚
â”‚       â”œâ”€â”€ pipeline/
â”‚       â”‚   â”œâ”€â”€ run_pipeline.py            â† Orchestrates extract â†’ transform
â”‚       â”‚   â””â”€â”€ orchestration_utils.py     â† Helper utilities for sequencing
â”‚       â”‚
â”‚       â””â”€â”€ monitoring/
â”‚           â”œâ”€â”€ lineage_emitter.py         â† OpenLineage/Marquez integration
â”‚           â”œâ”€â”€ alerts.py                  â† Slack/Email alerts for DQ failures
â”‚           â””â”€â”€ metrics_collector.py       â† Emits Prometheus/CloudWatch metrics
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 00_data_exploration.ipynb          â† Quick validation of sample data
â”‚   â”œâ”€â”€ 01_customer_kpi_analysis.ipynb     â† Gold-layer analytics (PySpark SQL)
â”‚   â”œâ”€â”€ 02_delta_table_validation.ipynb    â† Time travel / vacuum demo
â”‚   â””â”€â”€ 03_streaming_monitoring.ipynb      â† Kafka â†’ Delta checkpoint validation
â”‚
â”œâ”€â”€ data/                                  â† Local-only test data
â”‚   â”œâ”€â”€ hubspot_companies_1000.csv
â”‚   â”œâ”€â”€ hubspot_contacts_1000.csv
â”‚   â”œâ”€â”€ snowflake_orders_100000.csv
â”‚   â”œâ”€â”€ snowflake_customers_50000.csv
â”‚   â”œâ”€â”€ redshift_customer_behavior_50000.csv
â”‚   â”œâ”€â”€ fx_rates_historical_730_days.csv
â”‚   â”œâ”€â”€ stream_kafka_events_100000.csv
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ tests/                                 â† Unit/integration tests
â”‚   â”œâ”€â”€ test_spark_session.py              â† Ensures SparkSession config correct
â”‚   â”œâ”€â”€ test_dq_suites.py                  â† Validates YAML schema and runs dry-run
â”‚   â”œâ”€â”€ test_dag_imports.py                â† Confirms all Airflow DAGs parse
â”‚   â”œâ”€â”€ test_glue_catalog_contract.py      â† Checks S3 paths & DB match config
â”‚   â””â”€â”€ conftest.py
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ guides/
    â”‚   â”œâ”€â”€ README.md                      â† How to run locally + on AWS
    â”‚   â”œâ”€â”€ AWS_COMPLETE_DEPLOYMENT.md     â† Step-by-step MWAA + EMR setup
    â”‚   â”œâ”€â”€ DQ_SETUP.md                    â† GE + expectations instructions
    â”‚   â””â”€â”€ OPENLINEAGE_INTEGRATION.md     â† How to integrate lineage tracking
    â”‚
    â”œâ”€â”€ runbooks/
    â”‚   â”œâ”€â”€ RUNBOOK_AWS_2025.md            â† Operational runbook
    â”‚   â”œâ”€â”€ RUNBOOK_DQ_FAILOVER.md         â† How to handle DQ breaches
    â”‚   â””â”€â”€ RUNBOOK_STREAMING_RESTART.md   â† Restart/Recover Kafka jobs
    â”‚
    â””â”€â”€ architecture/
        â”œâ”€â”€ diagrams/
        â”‚   â”œâ”€â”€ aws_data_lake_architecture.png
        â”‚   â”œâ”€â”€ airflow_mwaa_pipeline.png
        â”‚   â””â”€â”€ delta_lake_layers.png
        â””â”€â”€ architecture_overview.md
## ğŸ“ Project Structure

```
â”œâ”€â”€ aws/data_fixed/           # Data sources
â”‚   â”œâ”€â”€ 01_hubspot_crm/       # HubSpot CRM data
â”‚   â”œâ”€â”€ 02_snowflake_warehouse/ # Snowflake warehouse data
â”‚   â”œâ”€â”€ 03_redshift_analytics/ # Redshift analytics data
â”‚   â”œâ”€â”€ 04_stream_data/       # Streaming data
â”‚   â””â”€â”€ 05_fx_rates/          # FX rates data
â”œâ”€â”€ config/                   # Configuration files
â”œâ”€â”€ src/pyspark_interview_project/ # Main pipeline code
â”œâ”€â”€ airflow/dags/             # Airflow DAGs
â””â”€â”€ docs/                     # Documentation
```

## ğŸ¯ Learning Objectives

- **Data Engineering**: ETL pipelines, data quality, transformations
- **Analytics**: Aggregations, window functions, statistical analysis
- **Performance**: Optimization, partitioning, caching strategies
- **Integration**: Multi-source data integration
- **Real-time Processing**: Streaming data and event processing

## ğŸ“š Documentation

- [Simplified Data Sources](docs/SIMPLIFIED_DATA_SOURCES.md)
- [Data Quality Report](docs/DATA_QUALITY_REPORT.md)

## ğŸ”§ Configuration

All configurations are managed in the `config/` directory:
- `default.yaml` - Base configuration
- `aws.yaml` - AWS-specific settings
- `azure.yaml` - Azure-specific settings
- `local.yaml` - Local development settings

## ğŸš€ Ready for PySpark Practice!

This project provides realistic, high-quality data for comprehensive PySpark learning and practice.
