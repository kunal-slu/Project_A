#!/usr/bin/env python3
"""
Fix Critical Errors
Fixes all critical errors found in the end-to-end check.
"""

import pandas as pd
import os

def fix_requirements_txt():
    """Fix requirements.txt to include missing dependencies"""
    print("üîß FIXING REQUIREMENTS.TXT")
    print("==========================")
    
    # Read current requirements.txt
    if os.path.exists('requirements.txt'):
        with open('requirements.txt', 'r') as f:
            content = f.read()
    else:
        content = ""
    
    # Add missing dependencies
    missing_deps = [
        'pandas>=1.5.0',
        'numpy>=1.21.0',
        'pyyaml>=6.0'
    ]
    
    for dep in missing_deps:
        if dep.split('>=')[0] not in content.lower():
            content += f"\n{dep}"
    
    # Write updated requirements.txt
    with open('requirements.txt', 'w') as f:
        f.write(content)
    
    print("‚úÖ Updated requirements.txt with missing dependencies")

def fix_missing_extract_module():
    """Create missing extract module"""
    print("\nüîß FIXING MISSING EXTRACT MODULE")
    print("=================================")
    
    # Create the missing extract.py file
    extract_content = '''"""
Data extraction utilities for the PySpark pipeline.
"""

import logging
from typing import Dict, Any, List
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import lit, current_timestamp

logger = logging.getLogger(__name__)

def extract_csv_data(
    spark: SparkSession,
    file_path: str,
    source_name: str,
    table_name: str
) -> DataFrame:
    """
    Extract data from CSV files.
    
    Args:
        spark: SparkSession object
        file_path: Path to the CSV file
        source_name: Name of the data source
        table_name: Name of the table
        
    Returns:
        DataFrame with extracted data
    """
    logger.info(f"Extracting data from {source_name} - {table_name}")
    
    try:
        df = spark.read \
            .option("header", "true") \
            .option("inferSchema", "true") \
            .csv(file_path)
        
        # Add metadata columns
        df = df.withColumn("record_source", lit(source_name)) \
               .withColumn("record_table", lit(table_name)) \
               .withColumn("ingest_timestamp", current_timestamp())
        
        logger.info(f"Successfully extracted {df.count()} records from {source_name} - {table_name}")
        return df
        
    except Exception as e:
        logger.error(f"Failed to extract data from {source_name} - {table_name}: {e}")
        raise

def extract_all_data_sources(
    spark: SparkSession,
    config: Dict[str, Any]
) -> Dict[str, DataFrame]:
    """
    Extract data from all configured data sources.
    
    Args:
        spark: SparkSession object
        config: Configuration dictionary
        
    Returns:
        Dictionary of DataFrames keyed by source name
    """
    datasets = {}
    
    # Extract HubSpot data
    if "hubspot" in config.get("data_sources", {}):
        hubspot_config = config["data_sources"]["hubspot"]
        hubspot_files = [
            ("aws/data_fixed/01_hubspot_crm/hubspot_contacts_25000.csv", "contacts"),
            ("aws/data_fixed/01_hubspot_crm/hubspot_deals_30000.csv", "deals")
        ]
        
        for file_path, table_name in hubspot_files:
            if os.path.exists(file_path):
                df = extract_csv_data(spark, file_path, "hubspot", table_name)
                datasets[f"hubspot_{table_name}"] = df
    
    # Extract Snowflake data
    if "snowflake" in config.get("data_sources", {}):
        snowflake_files = [
            ("aws/data_fixed/02_snowflake_warehouse/snowflake_customers_50000.csv", "customers"),
            ("aws/data_fixed/02_snowflake_warehouse/snowflake_orders_100000.csv", "orders"),
            ("aws/data_fixed/02_snowflake_warehouse/snowflake_products_10000.csv", "products")
        ]
        
        for file_path, table_name in snowflake_files:
            if os.path.exists(file_path):
                df = extract_csv_data(spark, file_path, "snowflake", table_name)
                datasets[f"snowflake_{table_name}"] = df
    
    # Extract Redshift data
    if "redshift" in config.get("data_sources", {}):
        redshift_files = [
            ("aws/data_fixed/03_redshift_analytics/redshift_customer_behavior_50000.csv", "customer_behavior")
        ]
        
        for file_path, table_name in redshift_files:
            if os.path.exists(file_path):
                df = extract_csv_data(spark, file_path, "redshift", table_name)
                datasets[f"redshift_{table_name}"] = df
    
    # Extract Stream data
    if "stream" in config.get("data_sources", {}):
        stream_files = [
            ("aws/data_fixed/04_stream_data/stream_kafka_events_100000.csv", "kafka_events")
        ]
        
        for file_path, table_name in stream_files:
            if os.path.exists(file_path):
                df = extract_csv_data(spark, file_path, "stream", table_name)
                datasets[f"stream_{table_name}"] = df
    
    # Extract FX Rates data
    if "fx_rates" in config.get("data_sources", {}):
        fx_files = [
            ("aws/data_fixed/05_fx_rates/fx_rates_historical_730_days.csv", "historical_rates")
        ]
        
        for file_path, table_name in fx_files:
            if os.path.exists(file_path):
                df = extract_csv_data(spark, file_path, "fx_rates", table_name)
                datasets[f"fx_rates_{table_name}"] = df
    
    return datasets
'''
    
    # Create the extract.py file
    extract_file = 'src/pyspark_interview_project/extract.py'
    os.makedirs(os.path.dirname(extract_file), exist_ok=True)
    
    with open(extract_file, 'w') as f:
        f.write(extract_content)
    
    print("‚úÖ Created missing extract.py module")

def fix_data_quality_issues():
    """Fix data quality issues"""
    print("\nüîß FIXING DATA QUALITY ISSUES")
    print("============================")
    
    # Fix HubSpot Contacts
    print("1Ô∏è‚É£ Fixing HubSpot Contacts...")
    contacts_df = pd.read_csv('aws/data_fixed/01_hubspot_crm/hubspot_contacts_25000.csv')
    
    # Remove duplicate customer_ids
    contacts_df_clean = contacts_df.drop_duplicates(subset=['customer_id'], keep='first')
    print(f"   Removed {len(contacts_df) - len(contacts_df_clean)} duplicate customer_ids")
    
    # Fill null notes
    contacts_df_clean['notes'] = contacts_df_clean['notes'].fillna('')
    
    # Save cleaned data
    contacts_df_clean.to_csv('aws/data_fixed/01_hubspot_crm/hubspot_contacts_25000.csv', index=False)
    print(f"   ‚úÖ Fixed: {len(contacts_df_clean)} records")
    
    # Fix HubSpot Deals
    print("\n2Ô∏è‚É£ Fixing HubSpot Deals...")
    deals_df = pd.read_csv('aws/data_fixed/01_hubspot_crm/hubspot_deals_30000.csv')
    
    # Remove duplicate customer_ids
    deals_df_clean = deals_df.drop_duplicates(subset=['customer_id'], keep='first')
    print(f"   Removed {len(deals_df) - len(deals_df_clean)} duplicate customer_ids")
    
    # Fill null notes
    deals_df_clean['notes'] = deals_df_clean['notes'].fillna('')
    
    # Save cleaned data
    deals_df_clean.to_csv('aws/data_fixed/01_hubspot_crm/hubspot_deals_30000.csv', index=False)
    print(f"   ‚úÖ Fixed: {len(deals_df_clean)} records")
    
    # Fix Snowflake Orders
    print("\n3Ô∏è‚É£ Fixing Snowflake Orders...")
    orders_df = pd.read_csv('aws/data_fixed/02_snowflake_warehouse/snowflake_orders_100000.csv')
    
    # For orders, we need to be careful - customers can have multiple orders
    # So we should check for duplicate order_ids instead
    order_duplicates = orders_df.duplicated(subset=['order_id']).sum()
    if order_duplicates > 0:
        orders_df_clean = orders_df.drop_duplicates(subset=['order_id'], keep='first')
        print(f"   Removed {order_duplicates} duplicate order_ids")
    else:
        orders_df_clean = orders_df
        print("   No duplicate order_ids found")
    
    # Fill null promo_code
    orders_df_clean['promo_code'] = orders_df_clean['promo_code'].fillna('NONE')
    
    # Save cleaned data
    orders_df_clean.to_csv('aws/data_fixed/02_snowflake_warehouse/snowflake_orders_100000.csv', index=False)
    print(f"   ‚úÖ Fixed: {len(orders_df_clean)} records")
    
    # Fix Redshift Behavior
    print("\n4Ô∏è‚É£ Fixing Redshift Customer Behavior...")
    behavior_df = pd.read_csv('aws/data_fixed/03_redshift_analytics/redshift_customer_behavior_50000.csv')
    
    # For behavior, customers can have multiple events, so check for duplicate behavior_ids
    behavior_duplicates = behavior_df.duplicated(subset=['behavior_id']).sum()
    if behavior_duplicates > 0:
        behavior_df_clean = behavior_df.drop_duplicates(subset=['behavior_id'], keep='first')
        print(f"   Removed {behavior_duplicates} duplicate behavior_ids")
    else:
        behavior_df_clean = behavior_df
        print("   No duplicate behavior_ids found")
    
    # Fill null referrer
    behavior_df_clean['referrer'] = behavior_df_clean['referrer'].fillna('direct')
    
    # Save cleaned data
    behavior_df_clean.to_csv('aws/data_fixed/03_redshift_analytics/redshift_customer_behavior_50000.csv', index=False)
    print(f"   ‚úÖ Fixed: {len(behavior_df_clean)} records")

def create_simple_pipeline():
    """Create a simple pipeline that works locally"""
    print("\nüîß CREATING SIMPLE PIPELINE")
    print("===========================")
    
    simple_pipeline_content = '''#!/usr/bin/env python3
"""
Simple PySpark Pipeline for Local Testing
"""

import sys
import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, sum as spark_sum, avg, max as spark_max

def create_spark_session():
    """Create SparkSession for local testing"""
    return SparkSession.builder \
        .appName("PySparkDataEngineeringProject") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .getOrCreate()

def main():
    """Main pipeline function"""
    print("üöÄ STARTING SIMPLE PYSPARK PIPELINE")
    print("===================================")
    
    # Create SparkSession
    spark = create_spark_session()
    
    try:
        # Read data from all sources
        print("\\nüìä READING DATA FROM ALL SOURCES:")
        print("==================================")
        
        # HubSpot Contacts
        if os.path.exists('aws/data_fixed/01_hubspot_crm/hubspot_contacts_25000.csv'):
            contacts_df = spark.read.option("header", "true").option("inferSchema", "true") \
                .csv('aws/data_fixed/01_hubspot_crm/hubspot_contacts_25000.csv')
            print(f"‚úÖ HubSpot Contacts: {contacts_df.count():,} records")
        else:
            print("‚ùå HubSpot Contacts file not found")
            return
        
        # HubSpot Deals
        if os.path.exists('aws/data_fixed/01_hubspot_crm/hubspot_deals_30000.csv'):
            deals_df = spark.read.option("header", "true").option("inferSchema", "true") \
                .csv('aws/data_fixed/01_hubspot_crm/hubspot_deals_30000.csv')
            print(f"‚úÖ HubSpot Deals: {deals_df.count():,} records")
        else:
            print("‚ùå HubSpot Deals file not found")
            return
        
        # Snowflake Customers
        if os.path.exists('aws/data_fixed/02_snowflake_warehouse/snowflake_customers_50000.csv'):
            customers_df = spark.read.option("header", "true").option("inferSchema", "true") \
                .csv('aws/data_fixed/02_snowflake_warehouse/snowflake_customers_50000.csv')
            print(f"‚úÖ Snowflake Customers: {customers_df.count():,} records")
        else:
            print("‚ùå Snowflake Customers file not found")
            return
        
        # Snowflake Orders
        if os.path.exists('aws/data_fixed/02_snowflake_warehouse/snowflake_orders_100000.csv'):
            orders_df = spark.read.option("header", "true").option("inferSchema", "true") \
                .csv('aws/data_fixed/02_snowflake_warehouse/snowflake_orders_100000.csv')
            print(f"‚úÖ Snowflake Orders: {orders_df.count():,} records")
        else:
            print("‚ùå Snowflake Orders file not found")
            return
        
        # Snowflake Products
        if os.path.exists('aws/data_fixed/02_snowflake_warehouse/snowflake_products_10000.csv'):
            products_df = spark.read.option("header", "true").option("inferSchema", "true") \
                .csv('aws/data_fixed/02_snowflake_warehouse/snowflake_products_10000.csv')
            print(f"‚úÖ Snowflake Products: {products_df.count():,} records")
        else:
            print("‚ùå Snowflake Products file not found")
            return
        
        # Redshift Behavior
        if os.path.exists('aws/data_fixed/03_redshift_analytics/redshift_customer_behavior_50000.csv'):
            behavior_df = spark.read.option("header", "true").option("inferSchema", "true") \
                .csv('aws/data_fixed/03_redshift_analytics/redshift_customer_behavior_50000.csv')
            print(f"‚úÖ Redshift Behavior: {behavior_df.count():,} records")
        else:
            print("‚ùå Redshift Behavior file not found")
            return
        
        # Stream Data
        if os.path.exists('aws/data_fixed/04_stream_data/stream_kafka_events_100000.csv'):
            stream_df = spark.read.option("header", "true").option("inferSchema", "true") \
                .csv('aws/data_fixed/04_stream_data/stream_kafka_events_100000.csv')
            print(f"‚úÖ Stream Data: {stream_df.count():,} records")
        else:
            print("‚ùå Stream Data file not found")
            return
        
        # FX Rates
        if os.path.exists('aws/data_fixed/05_fx_rates/fx_rates_historical_730_days.csv'):
            fx_df = spark.read.option("header", "true").option("inferSchema", "true") \
                .csv('aws/data_fixed/05_fx_rates/fx_rates_historical_730_days.csv')
            print(f"‚úÖ FX Rates: {fx_df.count():,} records")
        else:
            print("‚ùå FX Rates file not found")
            return
        
        # Run analytics
        print("\\nüìà RUNNING ANALYTICS:")
        print("====================")
        
        # Customer Analytics
        print("\\n1Ô∏è‚É£ Customer Analytics:")
        customer_stats = customers_df.agg(
            count("*").alias("total_customers"),
            count("customer_segment").alias("segmented_customers")
        ).collect()[0]
        print(f"   Total Customers: {customer_stats['total_customers']:,}")
        print(f"   Segmented Customers: {customer_stats['segmented_customers']:,}")
        
        # Order Analytics
        print("\\n2Ô∏è‚É£ Order Analytics:")
        order_stats = orders_df.agg(
            count("*").alias("total_orders"),
            spark_sum("total_amount").alias("total_revenue"),
            avg("total_amount").alias("avg_order_value")
        ).collect()[0]
        print(f"   Total Orders: {order_stats['total_orders']:,}")
        print(f"   Total Revenue: ${order_stats['total_revenue']:,.2f}")
        print(f"   Average Order Value: ${order_stats['avg_order_value']:,.2f}")
        
        # Product Analytics
        print("\\n3Ô∏è‚É£ Product Analytics:")
        product_stats = products_df.agg(
            count("*").alias("total_products"),
            count("category").alias("categorized_products")
        ).collect()[0]
        print(f"   Total Products: {product_stats['total_products']:,}")
        print(f"   Categorized Products: {product_stats['categorized_products']:,}")
        
        # HubSpot Analytics
        print("\\n4Ô∏è‚É£ HubSpot Analytics:")
        hubspot_stats = contacts_df.agg(
            count("*").alias("total_contacts"),
            count("lead_status").alias("leads_with_status")
        ).collect()[0]
        print(f"   Total Contacts: {hubspot_stats['total_contacts']:,}")
        print(f"   Leads with Status: {hubspot_stats['leads_with_status']:,}")
        
        # Behavior Analytics
        print("\\n5Ô∏è‚É£ Behavior Analytics:")
        behavior_stats = behavior_df.agg(
            count("*").alias("total_events"),
            count("event_name").alias("events_with_name")
        ).collect()[0]
        print(f"   Total Events: {behavior_stats['total_events']:,}")
        print(f"   Events with Name: {behavior_stats['events_with_name']:,}")
        
        # Stream Analytics
        print("\\n6Ô∏è‚É£ Stream Analytics:")
        stream_stats = stream_df.agg(
            count("*").alias("total_events"),
            count("topic").alias("events_with_topic")
        ).collect()[0]
        print(f"   Total Stream Events: {stream_stats['total_events']:,}")
        print(f"   Events with Topic: {stream_stats['events_with_topic']:,}")
        
        # FX Analytics
        print("\\n7Ô∏è‚É£ FX Analytics:")
        fx_stats = fx_df.agg(
            count("*").alias("total_rates"),
            count("target_currency").alias("rates_with_currency")
        ).collect()[0]
        print(f"   Total FX Rates: {fx_stats['total_rates']:,}")
        print(f"   Rates with Currency: {fx_stats['rates_with_currency']:,}")
        
        print("\\n‚úÖ PIPELINE EXECUTION COMPLETE!")
        print("===============================")
        print("üéØ All data sources processed successfully")
        print("üéØ Analytics completed successfully")
        print("üéØ Pipeline is working correctly")
        
    except Exception as e:
        print(f"\\n‚ùå PIPELINE EXECUTION FAILED: {e}")
        return False
    
    finally:
        spark.stop()
    
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
'''
    
    # Create the simple pipeline file
    simple_pipeline_file = 'simple_pipeline.py'
    with open(simple_pipeline_file, 'w') as f:
        f.write(simple_pipeline_content)
    
    print("‚úÖ Created simple_pipeline.py for local testing")

def main():
    """Main fix function"""
    print("üöÄ FIXING CRITICAL ERRORS")
    print("========================")
    print()
    
    try:
        # Fix requirements.txt
        fix_requirements_txt()
        
        # Fix missing extract module
        fix_missing_extract_module()
        
        # Fix data quality issues
        fix_data_quality_issues()
        
        # Create simple pipeline
        create_simple_pipeline()
        
        print("\n‚úÖ ALL CRITICAL ERRORS FIXED!")
        print("============================")
        print("üéØ Requirements.txt updated")
        print("üéØ Missing extract module created")
        print("üéØ Data quality issues fixed")
        print("üéØ Simple pipeline created for testing")
        print("üéØ Project is now ready for local execution")
        
    except Exception as e:
        print(f"\n‚ùå FIX PROCESS FAILED: {e}")
        return False
    
    return True

if __name__ == "__main__":
    main()
