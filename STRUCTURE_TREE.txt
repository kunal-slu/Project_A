pyspark_data_engineer_project/
â”‚
â”œâ”€â”€ ğŸ“‹ Configuration Files
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ Makefile
â”‚   â”œâ”€â”€ setup.py
â”‚   â”œâ”€â”€ pyproject.toml
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ requirements-dev.txt
â”‚   â”œâ”€â”€ pytest.ini
â”‚   â””â”€â”€ .env.example
â”‚
â”œâ”€â”€ âš™ï¸ config/                          # Centralized Configuration
â”‚   â”œâ”€â”€ local.yaml
â”‚   â”œâ”€â”€ dev.yaml
â”‚   â”œâ”€â”€ prod.yaml
â”‚   â”œâ”€â”€ dq.yaml
â”‚   â”œâ”€â”€ lineage.yaml
â”‚   â”œâ”€â”€ logging.conf
â”‚   â””â”€â”€ schema_definitions/            # 14 schema files
â”‚       â”œâ”€â”€ crm_accounts.schema.json
â”‚       â”œâ”€â”€ crm_contacts.schema.json
â”‚       â”œâ”€â”€ snowflake_orders.schema.json
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ ğŸª¶ dags/                            # Airflow DAGs
â”‚   â”œâ”€â”€ daily_batch_pipeline_dag.py
â”‚   â”œâ”€â”€ dq_watchdog_dag.py
â”‚   â”œâ”€â”€ salesforce_ingestion_dag.py
â”‚   â”œâ”€â”€ maintenance_dag.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ emr_serverless_operator.py
â”‚       â”œâ”€â”€ sensors.py
â”‚       â””â”€â”€ airflow_helpers.py
â”‚
â”œâ”€â”€ ğŸ§  src/pyspark_interview_project/   # Core Package (120 Python files)
â”‚   â”œâ”€â”€ utils/                          # Utility modules
â”‚   â”œâ”€â”€ contracts/                     # Data contracts
â”‚   â”œâ”€â”€ extract/                        # Extraction modules
â”‚   â”œâ”€â”€ transform/                      # Transformation modules
â”‚   â”œâ”€â”€ pipeline/                       # Pipeline orchestration
â”‚   â”œâ”€â”€ dq/                             # Data quality
â”‚   â”œâ”€â”€ monitoring/                     # Observability
â”‚   â”œâ”€â”€ jobs/                           # Job modules
â”‚   â”œâ”€â”€ io/                             # I/O operations
â”‚   â”œâ”€â”€ lineage/                        # Lineage tracking
â”‚   â””â”€â”€ [other modules]/
â”‚
â”œâ”€â”€ â˜ï¸ aws/                             # AWS Deployment
â”‚   â”œâ”€â”€ jobs/                           # 22 deployment jobs
â”‚   â”‚   â”œâ”€â”€ ingest/                     # 8 ingestion jobs
â”‚   â”‚   â”œâ”€â”€ transform/                  # 3 transformation jobs
â”‚   â”‚   â”œâ”€â”€ analytics/                  # 4 analytics jobs
â”‚   â”‚   â””â”€â”€ maintenance/                # 2 maintenance jobs
â”‚   â”‚
â”‚   â”œâ”€â”€ scripts/                        # Deployment scripts
â”‚   â”‚   â”œâ”€â”€ deployment/
â”‚   â”‚   â”œâ”€â”€ maintenance/
â”‚   â”‚   â””â”€â”€ utilities/
â”‚   â”‚
â”‚   â”œâ”€â”€ terraform/                      # Infrastructure as Code (11 files)
â”‚   â”‚   â”œâ”€â”€ main.tf
â”‚   â”‚   â”œâ”€â”€ variables.tf
â”‚   â”‚   â”œâ”€â”€ outputs.tf
â”‚   â”‚   â”œâ”€â”€ iam.tf
â”‚   â”‚   â”œâ”€â”€ networking.tf
â”‚   â”‚   â”œâ”€â”€ glue_catalog.tf
â”‚   â”‚   â”œâ”€â”€ lake_formation.tf
â”‚   â”‚   â”œâ”€â”€ secrets.tf
â”‚   â”‚   â””â”€â”€ cloudwatch.tf
â”‚   â”‚
â”‚   â”œâ”€â”€ emr_configs/                   # EMR configurations
â”‚   â”‚   â”œâ”€â”€ spark-defaults.conf
â”‚   â”‚   â”œâ”€â”€ delta-core.conf
â”‚   â”‚   â””â”€â”€ logging.yaml
â”‚   â”‚
â”‚   â”œâ”€â”€ config/                         # AWS-specific configs
â”‚   â”œâ”€â”€ dags/                           # AWS-specific DAGs
â”‚   â”œâ”€â”€ data/                           # Sample data
â”‚   â”œâ”€â”€ tests/                          # AWS tests
â”‚   â””â”€â”€ docs/                           # AWS documentation
â”‚
â”œâ”€â”€ ğŸ“Š data/                            # Data Directory
â”‚   â”œâ”€â”€ samples/                        # Sample datasets
â”‚   â”‚   â”œâ”€â”€ crm/                        # 3 CSV files
â”‚   â”‚   â”œâ”€â”€ snowflake/                  # 3 CSV files
â”‚   â”‚   â”œâ”€â”€ redshift/                   # 1 CSV file
â”‚   â”‚   â”œâ”€â”€ fx/                         # 2 CSV files
â”‚   â”‚   â””â”€â”€ kafka/                      # 1 CSV file
â”‚   â”‚
â”‚   â””â”€â”€ lakehouse_delta/                # Delta Lake output
â”‚       â”œâ”€â”€ bronze/
â”‚       â”œâ”€â”€ silver/
â”‚       â””â”€â”€ gold/
â”‚
â”œâ”€â”€ ğŸ“’ notebooks/                      # Jupyter Notebooks
â”‚   â”œâ”€â”€ 01_run_pipeline.py
â”‚   â””â”€â”€ 02_run_standard_pipeline.py
â”‚
â”œâ”€â”€ ğŸ§ª tests/                           # Test Suite (27 files)
â”‚   â”œâ”€â”€ conftest.py
â”‚   â”œâ”€â”€ test_spark_session.py
â”‚   â”œâ”€â”€ test_io_utils.py
â”‚   â”œâ”€â”€ test_dq_runner.py
â”‚   â”œâ”€â”€ test_contracts.py
â”‚   â”œâ”€â”€ test_dag_imports.py
â”‚   â””â”€â”€ [21 more test files]
â”‚
â”œâ”€â”€ ğŸ“š docs/                           # Documentation
â”‚   â”œâ”€â”€ guides/                        # 8 guide files
â”‚   â”œâ”€â”€ runbooks/                      # 9 runbook files
â”‚   â”œâ”€â”€ architecture/                   # Architecture docs
â”‚   â”œâ”€â”€ schema_contracts/             # 3 schema docs
â”‚   â””â”€â”€ [40+ documentation files]
â”‚
â”œâ”€â”€ ğŸ”§ scripts/                        # Utility Scripts
â”‚   â”œâ”€â”€ local/                         # 9 local execution scripts
â”‚   â””â”€â”€ [25+ utility scripts]
â”‚
â””â”€â”€ ğŸ³ docker/                         # Docker configs
    â”œâ”€â”€ docker-compose.yml
    â””â”€â”€ spark-defaults.conf


ğŸ“Š STATISTICS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  Source Code Files:       120 Python files
  AWS Jobs:                22 files
  DAGs:                    7 files
  Config Files:            28 files
  Tests:                    27 files
  Documentation:           60+ files
  Scripts:                  34+ files
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

