# ðŸš€ Azure Quick Start Guide

## âš¡ **5-Minute Azure Setup**

### 1. **Prerequisites Check**
```bash
# Check Azure CLI
az --version

# Login to Azure
az login
```

### 2. **One-Click Deployment**
```bash
# Run automated deployment script
./scripts/azure_deploy.sh
```

### 3. **Access Your Resources**
- **Databricks**: https://[WORKSPACE_URL] (from script output)
- **Storage**: Azure Portal â†’ Storage Accounts â†’ pysparketlstorage
- **Key Vault**: Azure Portal â†’ Key Vaults â†’ pyspark-etl-kv

## ðŸ“‹ **What Gets Created**

| Resource | Name | Purpose |
|----------|------|---------|
| **Resource Group** | `pyspark-etl-rg` | Groups all resources |
| **Storage Account** | `pysparketlstorage` | Data Lake Storage Gen2 |
| **Databricks** | `pyspark-etl-workspace` | Spark compute |
| **Key Vault** | `pyspark-etl-kv` | Secrets management |

### **Storage Containers**
- `lakehouse/` - Bronze/Silver/Gold data
- `input-data/` - Source files
- `output-data/` - Final results
- `backups/` - Disaster recovery

## ðŸŽ¯ **Quick Commands**

### **Upload Data**
```bash
# Upload all data files
az storage blob upload-batch \
  --account-name pysparketlstorage \
  --source data/input_data \
  --destination input-data
```

### **Run ETL Pipeline**
```python
# In Databricks notebook
from pyspark_interview_project import run_pipeline, get_spark_session, load_config_resolved

config = load_config_resolved("/dbfs/pyspark-etl/config/config-azure-dev.yaml")
spark = get_spark_session(config)
run_pipeline(spark, config)
```

### **Check Results**
```bash
# List processed data
az storage blob list \
  --account-name pysparketlstorage \
  --container-name lakehouse \
  --prefix gold/ \
  --output table
```

## ðŸ”§ **Troubleshooting**

### **Common Issues**

| Issue | Solution |
|-------|----------|
| **Authentication Failed** | `az login` |
| **Storage Not Found** | Check resource group exists |
| **Databricks Not Loading** | Wait 5-10 minutes after creation |
| **Data Upload Failed** | Check file paths and permissions |

### **Cost Optimization**
```bash
# Stop Databricks cluster when not in use
az databricks cluster delete \
  --workspace-name pyspark-etl-workspace \
  --cluster-name your-cluster-name
```

## ðŸ“Š **Monitor Your Pipeline**

### **Azure Portal**
1. Go to [Azure Portal](https://portal.azure.com)
2. Navigate to `pyspark-etl-rg`
3. Check:
   - **Storage Account** â†’ Monitoring
   - **Databricks** â†’ Clusters & Jobs
   - **Key Vault** â†’ Access policies

### **Databricks UI**
- **Clusters**: Monitor compute usage
- **Jobs**: View pipeline runs
- **Data**: Browse lakehouse structure
- **SQL**: Query results directly

## ðŸŽ‰ **Success Checklist**

- âœ… Azure resources created
- âœ… Data uploaded to storage
- âœ… Databricks workspace accessible
- âœ… ETL pipeline running
- âœ… Results in gold layer
- âœ… SCD2 validation passing

## ðŸ“ž **Need Help?**

1. **Check logs**: Azure Portal â†’ Monitor â†’ Logs
2. **Databricks support**: Workspace â†’ Help â†’ Support
3. **Documentation**: See `AZURE_DEPLOYMENT_GUIDE.md`
4. **Community**: Azure Data Engineering forums

---

**Your PySpark ETL is now running on Azure!** ðŸš€
