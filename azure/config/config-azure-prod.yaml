# Azure Production Configuration for PySpark Data Engineering Pipeline
# This configuration is optimized for Azure production environments with enterprise-grade settings

# Environment Configuration
environment:
  name: "azure-production"
  region: "East US"
  timezone: "UTC"
  cloud_provider: "azure"
  
# Spark Configuration
spark:
  app_name: "PySpark-Data-Engineering-Azure-Prod"
  master: "yarn"
  deploy_mode: "cluster"
  
  # Performance Tuning
  executor:
    cores: 4
    memory: "8g"
    memory_overhead: "2g"
    instances: 10
    max_instances: 20
    
  driver:
    memory: "4g"
    memory_overhead: "1g"
    
  # Adaptive Query Execution
  sql:
    adaptive:
      enabled: true
      coalesce_partitions:
        enabled: true
        min_partition_num: 1
        max_partition_num: 100
      skew_join:
        enabled: true
        skew_partition_threshold: 256
        skew_partition_factor: 5
      local_shuffle_reader:
        enabled: true
        
  # Dynamic Partition Pruning
  sql:
    dynamic_partition_pruning:
      enabled: true
      reuse_work: true
      
  # Cost-based Optimization
  sql:
    cost_based_optimization:
      enabled: true
      join_reorder:
        enabled: true
        depth: 12
        
  # Serialization
  serializer: "org.apache.spark.serializer.KryoSerializer"
  kryo:
    registration_required: false
    unsafe: true
    
  # Memory Management
  memory:
    fraction: 0.8
    storage_fraction: 0.3
    off_heap:
      enabled: true
      size: "2g"
      
  # Shuffle
  shuffle:
    file:
      buffer: "32k"
    io:
      max_retries: 3
      retry_wait: "5s"
      
  # Network
  network:
    timeout: "800s"
    io:
      max_retries: 3
      retry_wait: "5s"
      
  # Azure Storage Configuration
  hadoop:
    fs:
      wasbs:
        impl: "org.apache.hadoop.fs.azure.NativeAzureFileSystem"
        max_connections: 1000
        max_connections_per_host: 100
        connection_timeout: "200000"
        socket_timeout: "200000"
        retry_policy: "exponential"
        max_retry_attempts: 10
      abfs:
        impl: "org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem"
        max_connections: 1000
        max_connections_per_host: 100
        connection_timeout: "200000"
        socket_timeout: "200000"
        retry_policy: "exponential"
        max_retry_attempts: 10
        
# Delta Lake Configuration
delta:
  # Performance
  auto_optimize:
    optimize_write: true
    auto_compact: true
    
  enable_change_data_feed: true
  
  # Schema Evolution
  schema_evolution:
    mode: "additive"
    allow_incompatible_schema: false
    
  # Data Quality
  data_quality:
    enabled: true
    mode: "strict"
    
  # Retention
  retention:
    log_retention_days: 30
    deleted_file_retention_days: 7
    
  # Optimization
  optimize:
    auto: true
    interval: "1 hour"
    zorder:
      columns: ["order_date", "customer_id", "product_id"]
      
# Azure Data Sources (Data Lake Storage Gen2)
sources:
  customers:
    path: "abfss://bronze@stpysparkdatadlprod.dfs.core.windows.net/customers/"
    format: "delta"
    schema_evolution: true
    
  orders:
    path: "abfss://bronze@stpysparkdatadlprod.dfs.core.windows.net/orders/"
    format: "delta"
    schema_evolution: true
    
  products:
    path: "abfss://bronze@stpysparkdatadlprod.dfs.core.windows.net/products/"
    format: "delta"
    schema_evolution: true
    
  exchange_rates:
    path: "abfss://bronze@stpysparkdatadlprod.dfs.core.windows.net/exchange_rates/"
    format: "delta"
    schema_evolution: true
    
  inventory:
    path: "abfss://bronze@stpysparkdatadlprod.dfs.core.windows.net/inventory/"
    format: "delta"
    schema_evolution: true
    
  returns:
    path: "abfss://bronze@stpysparkdatadlprod.dfs.core.windows.net/returns/"
    format: "delta"
    schema_evolution: true
    
# Azure Data Destinations
destinations:
  bronze:
    base_path: "abfss://bronze@stpysparkdatadlprod.dfs.core.windows.net/"
    format: "delta"
    partition_by: ["year", "month", "day"]
    
  silver:
    base_path: "abfss://silver@stpysparkdatadlprod.dfs.core.windows.net/"
    format: "delta"
    partition_by: ["year", "month", "day"]
    
  gold:
    base_path: "abfss://gold@stpysparkdatadlprod.dfs.core.windows.net/"
    format: "delta"
    partition_by: ["year", "month", "day"]
    
  output:
    base_path: "abfss://output@stpysparkdatadlprod.dfs.core.windows.net/"
    format: "parquet"
    partition_by: ["year", "month", "day"]
    
# Data Quality Configuration
data_quality:
  enabled: true
  strict_mode: true
  
  rules:
    customers:
      - column: "customer_id"
        rule: "not_null"
        severity: "error"
      - column: "email"
        rule: "email_format"
        severity: "error"
      - column: "age"
        rule: "range"
        min: 13
        max: 120
        severity: "warning"
        
    orders:
      - column: "order_id"
        rule: "not_null"
        severity: "error"
      - column: "order_date"
        rule: "date_range"
        min: "2020-01-01"
        max: "current_date"
        severity: "error"
      - column: "total_amount"
        rule: "positive"
        severity: "error"
        
    products:
      - column: "product_id"
        rule: "not_null"
        severity: "error"
      - column: "price"
        rule: "positive"
        severity: "error"
        
  anomaly_detection:
    enabled: true
    methods: ["statistical", "isolation_forest", "dbscan"]
    threshold: 0.95
    
  profiling:
    enabled: true
    sample_size: 10000
    include_histograms: true
    include_correlations: true
    
# Azure Monitoring and Alerting
monitoring:
  enabled: true
  
  metrics:
    spark_metrics: true
    custom_metrics: true
    data_quality_metrics: true
    azure_metrics: true
    
  alerts:
    pipeline_failure: true
    data_quality_violations: true
    performance_degradation: true
    resource_utilization: true
    
  thresholds:
    pipeline_runtime_minutes: 120
    data_quality_violation_rate: 0.01
    executor_memory_utilization: 0.85
    driver_memory_utilization: 0.80
    
  notifications:
    azure_monitor:
      enabled: true
      action_group: "data-engineering-alerts"
    email:
      enabled: true
      recipients: ["data-team@company.com", "oncall@company.com"]
    teams:
      enabled: true
      webhook_url: "${TEAMS_WEBHOOK_URL}"
      
# Azure Security Configuration
security:
  encryption:
    at_rest: true
    in_transit: true
    algorithm: "AES-256"
    
  access_control:
    enabled: true
    method: "azure-ad"
    
  audit:
    enabled: true
    level: "detailed"
    retention_days: 2555
    
  data_masking:
    enabled: true
    sensitive_columns: ["ssn", "credit_card", "phone"]
    method: "hash"
    
# Error Handling and Recovery
error_handling:
  retry:
    max_attempts: 3
    initial_delay: "5s"
    max_delay: "60s"
    backoff_multiplier: 2
    
  dead_letter_queue:
    enabled: true
    path: "abfss://dlq@stpysparkdatadlprod.dfs.core.windows.net/"
    retention_days: 30
    
  circuit_breaker:
    enabled: true
    failure_threshold: 5
    recovery_timeout: "300s"
    
  graceful_degradation:
    enabled: true
    fallback_strategies: ["use_cached_data", "use_historical_data", "skip_optional_steps"]
    
# Performance Optimization
performance:
  caching:
    enabled: true
    strategy: "lru"
    max_memory: "4g"
    
  broadcast_join:
    enabled: true
    threshold: "10m"
    
  skew_handling:
    enabled: true
    method: "salting"
    num_salt_partitions: 10
    
  partitioning:
    strategy: "date_based"
    partition_size_target: "128m"
    max_partitions: 10000
    
  compression:
    format: "snappy"
    level: 1
    
# Azure Logging Configuration
logging:
  level: "INFO"
  format: "json"
  
  appenders:
    console:
      enabled: true
    file:
      enabled: true
      path: "/var/log/pyspark-pipeline/"
      max_size: "100MB"
      max_files: 10
    azure_monitor:
      enabled: true
      workspace_id: "${LOG_ANALYTICS_WORKSPACE_ID}"
      workspace_key: "${LOG_ANALYTICS_WORKSPACE_KEY}"
      
  structured:
    enabled: true
    include: ["timestamp", "level", "logger", "message", "context", "performance_metrics"]
    
# Scheduling and Orchestration
scheduling:
  schedule:
    frequency: "hourly"
    start_time: "00:00"
    timezone: "UTC"
    
  dependencies:
    - "data_ingestion"
    - "data_validation"
    - "data_transformation"
    - "data_quality_check"
    - "data_loading"
    
  sla:
    data_freshness_minutes: 60
    pipeline_runtime_minutes: 120
    data_quality_threshold: 0.99
    
  resources:
    priority: "high"
    preemption: false
    resource_guarantees: true
    
# Azure Data Lineage and Metadata
metadata:
  lineage:
    enabled: true
    granularity: "column_level"
    storage: "azure_purview"
    
  catalog:
    enabled: true
    provider: "azure_purview"
    auto_discovery: true
    
  schema_registry:
    enabled: true
    provider: "azure_schema_registry"
    versioning: true
    
  business_glossary:
    enabled: true
    categories: ["customer_data", "financial_data", "operational_data", "compliance_data"]
    
# Compliance and Governance
compliance:
  data_classification:
    enabled: true
    levels: ["public", "internal", "confidential", "restricted"]
    
  retention:
    enabled: true
    policies:
      customer_data: "7_years"
      financial_data: "7_years"
      operational_data: "3_years"
      logs: "1_year"
      
  privacy:
    gdpr_compliance: true
    data_minimization: true
    right_to_forget: true
    
  audit:
    enabled: true
    frequency: "monthly"
    reports: ["data_access_logs", "data_quality_metrics", "compliance_status", "security_incidents"]
    
# Azure Disaster Recovery
disaster_recovery:
  backup:
    enabled: true
    frequency: "daily"
    retention: "30_days"
    cross_region: true
    
  replication:
    enabled: true
    regions: ["East US 2", "West US 2"]
    sync_mode: "asynchronous"
    
  rto: "4_hours"
  rpo: "1_hour"
  
  failover:
    enabled: true
    automatic: false
    health_check_interval: "5_minutes"
    
# Azure Synapse Analytics Integration (Optional)
synapse:
  enabled: false
  workspace_name: "${SYNAPSE_WORKSPACE_NAME}"
  spark_pool_name: "${SYNAPSE_SPARK_POOL_NAME}"
  
  # Use Synapse Spark instead of HDInsight
  use_synapse_spark: false
  
# Azure Machine Learning Integration (Optional)
ml:
  enabled: false
  workspace_name: "${ML_WORKSPACE_NAME}"
  
  # Model deployment and scoring
  model_registry:
    enabled: false
    name: "${MODEL_REGISTRY_NAME}"
    
# Azure Data Factory Integration (Optional)
data_factory:
  enabled: false
  name: "${DATA_FACTORY_NAME}"
  
  # Pipeline orchestration
  triggers:
    - name: "hourly-trigger"
      type: "schedule"
      frequency: "hour"
      
# Azure Event Hubs Integration (Optional)
event_hubs:
  enabled: false
  namespace: "${EVENT_HUBS_NAMESPACE}"
  
  # Real-time streaming
  consumer_groups:
    - "bronze-layer"
    - "silver-layer"
    - "gold-layer"
