I001 [*] Import block is un-sorted or un-formatted
  --> aws/dags/daily_batch_pipeline_dag.py:9:1
   |
 7 |   If DQ fails at any stage, downstream jobs do not run.
 8 |   """
 9 | / from datetime import datetime, timedelta
10 | | from airflow import DAG
11 | | from airflow.operators.bash import BashOperator
12 | | from airflow.operators.dummy import DummyOperator
13 | | from airflow.sensors.filesystem import FileSensor
   | |_________________________________________________^
14 |
15 |   # Default arguments
   |
help: Organize imports

F401 [*] `airflow.sensors.filesystem.FileSensor` imported but unused
  --> aws/dags/daily_batch_pipeline_dag.py:13:40
   |
11 | from airflow.operators.bash import BashOperator
12 | from airflow.operators.dummy import DummyOperator
13 | from airflow.sensors.filesystem import FileSensor
   |                                        ^^^^^^^^^^
14 |
15 | # Default arguments
   |
help: Remove unused import: `airflow.sensors.filesystem.FileSensor`

I001 [*] Import block is un-sorted or un-formatted
  --> aws/scripts/create_cloudwatch_alarms.py:12:1
   |
10 |   """
11 |
12 | / import sys
13 | | import boto3
14 | | import argparse
15 | | import logging
16 | | from pathlib import Path
   | |________________________^
17 |
18 |   logging.basicConfig(level=logging.INFO)
   |
help: Organize imports

F401 [*] `pathlib.Path` imported but unused
  --> aws/scripts/create_cloudwatch_alarms.py:16:21
   |
14 | import argparse
15 | import logging
16 | from pathlib import Path
   |                     ^^^^
17 |
18 | logging.basicConfig(level=logging.INFO)
   |
help: Remove unused import: `pathlib.Path`

W293 [*] Blank line contains whitespace
  --> aws/scripts/create_cloudwatch_alarms.py:88:1
   |
86 |     parser.add_argument("--region", default="us-east-1", help="AWS region")
87 |     parser.add_argument("--sns-topic", required=True, help="SNS topic ARN for alerts")
88 |     
   | ^^^^
89 |     args = parser.parse_args()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/create_cloudwatch_alarms.py:90:1
   |
89 |     args = parser.parse_args()
90 |     
   | ^^^^
91 |     cloudwatch = boto3.client('cloudwatch', region_name=args.region)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/create_cloudwatch_alarms.py:92:1
   |
91 |     cloudwatch = boto3.client('cloudwatch', region_name=args.region)
92 |     
   | ^^^^
93 |     try:
94 |         # DQ failures alarm
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/create_cloudwatch_alarms.py:96:1
   |
94 |         # DQ failures alarm
95 |         create_dq_alarm(cloudwatch, "etl-dq-critical-failures")
96 |         
   | ^^^^^^^^
97 |         # Latency alarms
98 |         create_latency_alarm(cloudwatch, "etl-latency-bronze", threshold_seconds=7200.0)  # 2 hours
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/create_cloudwatch_alarms.py:101:1
    |
 99 |         create_latency_alarm(cloudwatch, "etl-latency-silver", threshold_seconds=10800.0)  # 3 hours
100 |         create_latency_alarm(cloudwatch, "etl-latency-gold", threshold_seconds=14400.0)  # 4 hours
101 |         
    | ^^^^^^^^
102 |         # Cost alarm
103 |         create_cost_alarm(cloudwatch, "etl-daily-cost-budget", budget_threshold=100.0)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/create_cloudwatch_alarms.py:104:1
    |
102 |         # Cost alarm
103 |         create_cost_alarm(cloudwatch, "etl-daily-cost-budget", budget_threshold=100.0)
104 |         
    | ^^^^^^^^
105 |         logger.info("ðŸŽ‰ All CloudWatch alarms created successfully")
106 |         return 0
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/create_cloudwatch_alarms.py:107:1
    |
105 |         logger.info("ðŸŽ‰ All CloudWatch alarms created successfully")
106 |         return 0
107 |         
    | ^^^^^^^^
108 |     except Exception as e:
109 |         logger.error(f"âŒ Failed to create alarms: {e}", exc_info=True)
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> aws/scripts/maintenance/dr_snapshot_export.py:10:1
   |
 8 |   """
 9 |
10 | / import sys
11 | | import os
12 | | import logging
13 | | import boto3
14 | | from datetime import datetime, timedelta
15 | | from typing import List, Dict, Any
16 | | from pathlib import Path
   | |________________________^
17 |
18 |   sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../..', 'src'))
   |
help: Organize imports

UP035 `typing.List` is deprecated, use `list` instead
  --> aws/scripts/maintenance/dr_snapshot_export.py:15:1
   |
13 | import boto3
14 | from datetime import datetime, timedelta
15 | from typing import List, Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
16 | from pathlib import Path
   |

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> aws/scripts/maintenance/dr_snapshot_export.py:15:1
   |
13 | import boto3
14 | from datetime import datetime, timedelta
15 | from typing import List, Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
16 | from pathlib import Path
   |

F401 [*] `pathlib.Path` imported but unused
  --> aws/scripts/maintenance/dr_snapshot_export.py:16:21
   |
14 | from datetime import datetime, timedelta
15 | from typing import List, Dict, Any
16 | from pathlib import Path
   |                     ^^^^
17 |
18 | sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../..', 'src'))
   |
help: Remove unused import: `pathlib.Path`

W293 [*] Blank line contains whitespace
  --> aws/scripts/maintenance/dr_snapshot_export.py:26:1
   |
24 | class DRSnapshotExporter:
25 |     """Exports data lake snapshots to DR region."""
26 |     
   | ^^^^
27 |     def __init__(self, config: Dict[str, Any]):
28 |         self.config = config
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> aws/scripts/maintenance/dr_snapshot_export.py:27:32
   |
25 |     """Exports data lake snapshots to DR region."""
26 |     
27 |     def __init__(self, config: Dict[str, Any]):
   |                                ^^^^
28 |         self.config = config
29 |         self.primary_region = config.get('aws', {}).get('region', 'us-east-1')
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> aws/scripts/maintenance/dr_snapshot_export.py:31:1
   |
29 |         self.primary_region = config.get('aws', {}).get('region', 'us-east-1')
30 |         self.dr_region = config.get('dr', {}).get('region', 'us-west-2')
31 |         
   | ^^^^^^^^
32 |         # S3 clients
33 |         self.primary_s3 = boto3.client('s3', region_name=self.primary_region)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/maintenance/dr_snapshot_export.py:35:1
   |
33 |         self.primary_s3 = boto3.client('s3', region_name=self.primary_region)
34 |         self.dr_s3 = boto3.client('s3', region_name=self.dr_region)
35 |         
   | ^^^^^^^^
36 |         # Bucket names
37 |         lake_config = config.get('lake', {})
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/maintenance/dr_snapshot_export.py:40:1
   |
38 |         self.primary_bucket = lake_config.get('root', '').replace('s3://', '').split('/')[0]
39 |         self.dr_bucket = config.get('dr', {}).get('bucket', f"{self.primary_bucket}-dr")
40 |         
   | ^^^^^^^^
41 |         logger.info(f"Primary region: {self.primary_region}, bucket: {self.primary_bucket}")
42 |         logger.info(f"DR region: {self.dr_region}, bucket: {self.dr_bucket}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/maintenance/dr_snapshot_export.py:43:1
   |
41 |         logger.info(f"Primary region: {self.primary_region}, bucket: {self.primary_bucket}")
42 |         logger.info(f"DR region: {self.dr_region}, bucket: {self.dr_bucket}")
43 |     
   | ^^^^
44 |     def get_latest_partitions(self, layer: str, table: str, days_back: int = 7) -> List[str]:
45 |         """
   |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
  --> aws/scripts/maintenance/dr_snapshot_export.py:44:84
   |
42 |         logger.info(f"DR region: {self.dr_region}, bucket: {self.dr_bucket}")
43 |     
44 |     def get_latest_partitions(self, layer: str, table: str, days_back: int = 7) -> List[str]:
   |                                                                                    ^^^^
45 |         """
46 |         Get latest partition paths for a table.
   |
help: Replace with `list`

W293 Blank line contains whitespace
  --> aws/scripts/maintenance/dr_snapshot_export.py:47:1
   |
45 |         """
46 |         Get latest partition paths for a table.
47 |         
   | ^^^^^^^^
48 |         Args:
49 |             layer: bronze/silver/gold
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> aws/scripts/maintenance/dr_snapshot_export.py:52:1
   |
50 |             table: Table name
51 |             days_back: How many days back to include
52 |             
   | ^^^^^^^^^^^^
53 |         Returns:
54 |             List of partition paths
   |
help: Remove whitespace from blank line

F841 Local variable `cutoff_date` is assigned to but never used
  --> aws/scripts/maintenance/dr_snapshot_export.py:57:9
   |
55 |         """
56 |         prefix = f"{layer}/{table}/"
57 |         cutoff_date = datetime.utcnow() - timedelta(days=days_back)
   |         ^^^^^^^^^^^
58 |         
59 |         partitions = []
   |
help: Remove assignment to unused variable `cutoff_date`

W293 [*] Blank line contains whitespace
  --> aws/scripts/maintenance/dr_snapshot_export.py:58:1
   |
56 |         prefix = f"{layer}/{table}/"
57 |         cutoff_date = datetime.utcnow() - timedelta(days=days_back)
58 |         
   | ^^^^^^^^
59 |         partitions = []
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/maintenance/dr_snapshot_export.py:60:1
   |
59 |         partitions = []
60 |         
   | ^^^^^^^^
61 |         try:
62 |             paginator = self.primary_s3.get_paginator('list_objects_v2')
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/maintenance/dr_snapshot_export.py:68:1
   |
66 |                 Delimiter='/'
67 |             )
68 |             
   | ^^^^^^^^^^^^
69 |             for page in pages:
70 |                 if 'CommonPrefixes' in page:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/maintenance/dr_snapshot_export.py:76:1
   |
74 |                         if 'ingest_date=' in partition_path or 'order_date=' in partition_path:
75 |                             partitions.append(partition_path)
76 |             
   | ^^^^^^^^^^^^
77 |             logger.info(f"Found {len(partitions)} partitions for {layer}/{table}")
78 |             return partitions
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/maintenance/dr_snapshot_export.py:79:1
   |
77 |             logger.info(f"Found {len(partitions)} partitions for {layer}/{table}")
78 |             return partitions
79 |             
   | ^^^^^^^^^^^^
80 |         except Exception as e:
81 |             logger.error(f"Failed to list partitions: {str(e)}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/maintenance/dr_snapshot_export.py:83:1
   |
81 |             logger.error(f"Failed to list partitions: {str(e)}")
82 |             return []
83 |     
   | ^^^^
84 |     def copy_partition(self, source_key: str, dest_key: str) -> bool:
85 |         """
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> aws/scripts/maintenance/dr_snapshot_export.py:87:1
   |
85 |         """
86 |         Copy partition from primary to DR bucket.
87 |         
   | ^^^^^^^^
88 |         Args:
89 |             source_key: Source S3 key
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> aws/scripts/maintenance/dr_snapshot_export.py:91:1
   |
89 |             source_key: Source S3 key
90 |             dest_key: Destination S3 key
91 |             
   | ^^^^^^^^^^^^
92 |         Returns:
93 |             True if successful
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:100:1
    |
 98 |                 'Key': source_key
 99 |             }
100 |             
    | ^^^^^^^^^^^^
101 |             self.dr_s3.copy_object(
102 |                 CopySource=copy_source,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:106:1
    |
104 |                 Key=dest_key
105 |             )
106 |             
    | ^^^^^^^^^^^^
107 |             logger.debug(f"Copied {source_key} -> {dest_key}")
108 |             return True
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:109:1
    |
107 |             logger.debug(f"Copied {source_key} -> {dest_key}")
108 |             return True
109 |             
    | ^^^^^^^^^^^^
110 |         except Exception as e:
111 |             logger.error(f"Failed to copy {source_key}: {str(e)}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:113:1
    |
111 |             logger.error(f"Failed to copy {source_key}: {str(e)}")
112 |             return False
113 |     
    | ^^^^
114 |     def export_table(self, layer: str, table: str, days_back: int = 7) -> Dict[str, Any]:
115 |         """
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> aws/scripts/maintenance/dr_snapshot_export.py:114:75
    |
112 |             return False
113 |     
114 |     def export_table(self, layer: str, table: str, days_back: int = 7) -> Dict[str, Any]:
    |                                                                           ^^^^
115 |         """
116 |         Export all partitions for a table to DR region.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:117:1
    |
115 |         """
116 |         Export all partitions for a table to DR region.
117 |         
    | ^^^^^^^^
118 |         Args:
119 |             layer: bronze/silver/gold
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:122:1
    |
120 |             table: Table name
121 |             days_back: Days of data to export
122 |             
    | ^^^^^^^^^^^^
123 |         Returns:
124 |             Export statistics
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:127:1
    |
125 |         """
126 |         logger.info(f"Exporting {layer}/{table} to DR region...")
127 |         
    | ^^^^^^^^
128 |         partitions = self.get_latest_partitions(layer, table, days_back)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:129:1
    |
128 |         partitions = self.get_latest_partitions(layer, table, days_back)
129 |         
    | ^^^^^^^^
130 |         stats = {
131 |             "table": f"{layer}/{table}",
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:137:1
    |
135 |             "start_time": datetime.utcnow().isoformat()
136 |         }
137 |         
    | ^^^^^^^^
138 |         for partition in partitions:
139 |             dest_key = partition  # Same key structure in DR bucket
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:140:1
    |
138 |         for partition in partitions:
139 |             dest_key = partition  # Same key structure in DR bucket
140 |             
    | ^^^^^^^^^^^^
141 |             if self.copy_partition(partition, dest_key):
142 |                 stats["partitions_copied"] += 1
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:145:1
    |
143 |             else:
144 |                 stats["partitions_failed"] += 1
145 |         
    | ^^^^^^^^
146 |         stats["end_time"] = datetime.utcnow().isoformat()
147 |         logger.info(f"Export completed: {stats['partitions_copied']}/{stats['partitions_found']} partitions copied")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:148:1
    |
146 |         stats["end_time"] = datetime.utcnow().isoformat()
147 |         logger.info(f"Export completed: {stats['partitions_copied']}/{stats['partitions_found']} partitions copied")
148 |         
    | ^^^^^^^^
149 |         return stats
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:150:1
    |
149 |         return stats
150 |     
    | ^^^^
151 |     def export_all_tables(self, layers: List[str] = None, days_back: int = 7) -> Dict[str, Any]:
152 |         """
    |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
   --> aws/scripts/maintenance/dr_snapshot_export.py:151:41
    |
149 |         return stats
150 |     
151 |     def export_all_tables(self, layers: List[str] = None, days_back: int = 7) -> Dict[str, Any]:
    |                                         ^^^^
152 |         """
153 |         Export all specified layers to DR region.
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> aws/scripts/maintenance/dr_snapshot_export.py:151:82
    |
149 |         return stats
150 |     
151 |     def export_all_tables(self, layers: List[str] = None, days_back: int = 7) -> Dict[str, Any]:
    |                                                                                  ^^^^
152 |         """
153 |         Export all specified layers to DR region.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:154:1
    |
152 |         """
153 |         Export all specified layers to DR region.
154 |         
    | ^^^^^^^^
155 |         Args:
156 |             layers: List of layers to export (default: ['silver', 'gold'])
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:158:1
    |
156 |             layers: List of layers to export (default: ['silver', 'gold'])
157 |             days_back: Days of data to export
158 |             
    | ^^^^^^^^^^^^
159 |         Returns:
160 |             Overall export statistics
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:164:1
    |
162 |         if layers is None:
163 |             layers = ['silver', 'gold']  # Only export cleaned data, not raw bronze
164 |         
    | ^^^^^^^^
165 |         logger.info(f"Starting DR snapshot export for layers: {layers}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:166:1
    |
165 |         logger.info(f"Starting DR snapshot export for layers: {layers}")
166 |         
    | ^^^^^^^^
167 |         # Tables to export (from config or default)
168 |         tables_config = self.config.get('dr', {}).get('tables', {
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:172:1
    |
170 |             'gold': ['fact_sales', 'dim_customer', 'marketing_attribution']
171 |         })
172 |         
    | ^^^^^^^^
173 |         overall_stats = {
174 |             "start_time": datetime.utcnow().isoformat(),
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:180:1
    |
178 |             "table_stats": []
179 |         }
180 |         
    | ^^^^^^^^
181 |         for layer in layers:
182 |             tables = tables_config.get(layer, [])
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:183:1
    |
181 |         for layer in layers:
182 |             tables = tables_config.get(layer, [])
183 |             
    | ^^^^^^^^^^^^
184 |             for table in tables:
185 |                 stats = self.export_table(layer, table, days_back)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:189:1
    |
187 |                 overall_stats["total_partitions_copied"] += stats["partitions_copied"]
188 |                 overall_stats["total_partitions_failed"] += stats["partitions_failed"]
189 |             
    | ^^^^^^^^^^^^
190 |             overall_stats["layers_exported"].append(layer)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:191:1
    |
190 |             overall_stats["layers_exported"].append(layer)
191 |         
    | ^^^^^^^^
192 |         overall_stats["end_time"] = datetime.utcnow().isoformat()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:193:1
    |
192 |         overall_stats["end_time"] = datetime.utcnow().isoformat()
193 |         
    | ^^^^^^^^
194 |         logger.info(f"DR snapshot export completed:")
195 |         logger.info(f"  Total partitions copied: {overall_stats['total_partitions_copied']}")
    |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
   --> aws/scripts/maintenance/dr_snapshot_export.py:194:21
    |
192 |         overall_stats["end_time"] = datetime.utcnow().isoformat()
193 |         
194 |         logger.info(f"DR snapshot export completed:")
    |                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
195 |         logger.info(f"  Total partitions copied: {overall_stats['total_partitions_copied']}")
196 |         logger.info(f"  Total partitions failed: {overall_stats['total_partitions_failed']}")
    |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:197:1
    |
195 |         logger.info(f"  Total partitions copied: {overall_stats['total_partitions_copied']}")
196 |         logger.info(f"  Total partitions failed: {overall_stats['total_partitions_failed']}")
197 |         
    | ^^^^^^^^
198 |         return overall_stats
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:199:1
    |
198 |         return overall_stats
199 |     
    | ^^^^
200 |     def create_dr_metadata(self, export_stats: Dict[str, Any]) -> None:
201 |         """
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> aws/scripts/maintenance/dr_snapshot_export.py:200:48
    |
198 |         return overall_stats
199 |     
200 |     def create_dr_metadata(self, export_stats: Dict[str, Any]) -> None:
    |                                                ^^^^
201 |         """
202 |         Create metadata file in DR bucket documenting the snapshot.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:203:1
    |
201 |         """
202 |         Create metadata file in DR bucket documenting the snapshot.
203 |         
    | ^^^^^^^^
204 |         Args:
205 |             export_stats: Export statistics
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:216:1
    |
214 |             "export_stats": export_stats
215 |         }
216 |         
    | ^^^^^^^^
217 |         metadata_key = "dr_snapshots/latest_snapshot_metadata.json"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:218:1
    |
217 |         metadata_key = "dr_snapshots/latest_snapshot_metadata.json"
218 |         
    | ^^^^^^^^
219 |         try:
220 |             import json
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:227:1
    |
225 |                 ContentType='application/json'
226 |             )
227 |             
    | ^^^^^^^^^^^^
228 |             logger.info(f"Created DR metadata: {metadata_key}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:229:1
    |
228 |             logger.info(f"Created DR metadata: {metadata_key}")
229 |             
    | ^^^^^^^^^^^^
230 |         except Exception as e:
231 |             logger.error(f"Failed to create DR metadata: {str(e)}")
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
   --> aws/scripts/maintenance/dr_snapshot_export.py:236:5
    |
234 |   def main():
235 |       """Main entry point."""
236 | /     import argparse
237 | |     import yaml
    | |_______________^
238 |       
239 |       parser = argparse.ArgumentParser(description="Export data lake snapshot to DR region")
    |
help: Organize imports

W293 [*] Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:238:1
    |
236 |     import argparse
237 |     import yaml
238 |     
    | ^^^^
239 |     parser = argparse.ArgumentParser(description="Export data lake snapshot to DR region")
240 |     parser.add_argument("--config", default="config/prod.yaml", help="Configuration file")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:246:1
    |
244 |                        help="Days of data to export (default: 7)")
245 |     args = parser.parse_args()
246 |     
    | ^^^^
247 |     # Load config
248 |     try:
    |
help: Remove whitespace from blank line

UP015 [*] Unnecessary mode argument
   --> aws/scripts/maintenance/dr_snapshot_export.py:249:32
    |
247 |     # Load config
248 |     try:
249 |         with open(args.config, 'r') as f:
    |                                ^^^
250 |             config = yaml.safe_load(f)
251 |     except Exception as e:
    |
help: Remove mode argument

W293 [*] Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:254:1
    |
252 |         logger.error(f"Failed to load config: {str(e)}")
253 |         sys.exit(1)
254 |     
    | ^^^^
255 |     # Run export
256 |     exporter = DRSnapshotExporter(config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:258:1
    |
256 |     exporter = DRSnapshotExporter(config)
257 |     export_stats = exporter.export_all_tables(args.layers, args.days_back)
258 |     
    | ^^^^
259 |     # Create metadata
260 |     exporter.create_dr_metadata(export_stats)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/maintenance/dr_snapshot_export.py:261:1
    |
259 |     # Create metadata
260 |     exporter.create_dr_metadata(export_stats)
261 |     
    | ^^^^
262 |     # Exit with error if failures
263 |     if export_stats["total_partitions_failed"] > 0:
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> aws/scripts/register_glue_table.py:6:1
   |
 4 |   Creates Glue table metadata pointing to S3 data for querying via Athena/Spark.
 5 |   """
 6 | / import os
 7 | | import logging
 8 | | import boto3
   | |____________^
 9 |
10 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

W293 Blank line contains whitespace
  --> aws/scripts/register_glue_table.py:16:1
   |
14 |     """
15 |     Create Glue table for customer behavior data.
16 |     
   | ^^^^
17 |     Args:
18 |         bucket: S3 bucket name
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/register_glue_table.py:23:1
   |
21 |     """
22 |     glue = boto3.client("glue", region_name=os.getenv("AWS_REGION", "us-east-1"))
23 |     
   | ^^^^
24 |     table_input = {
25 |         "Name": table_name,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/register_glue_table.py:59:1
   |
57 |         }
58 |     }
59 |     
   | ^^^^
60 |     try:
61 |         # Check if table exists
   |
help: Remove whitespace from blank line

F841 Local variable `existing_table` is assigned to but never used
  --> aws/scripts/register_glue_table.py:63:13
   |
61 |         # Check if table exists
62 |         try:
63 |             existing_table = glue.get_table(DatabaseName=database, Name=table_name)
   |             ^^^^^^^^^^^^^^
64 |             logger.info(f"Table {database}.{table_name} already exists, updating...")
   |
help: Remove assignment to unused variable `existing_table`

W293 [*] Blank line contains whitespace
  --> aws/scripts/register_glue_table.py:65:1
   |
63 |             existing_table = glue.get_table(DatabaseName=database, Name=table_name)
64 |             logger.info(f"Table {database}.{table_name} already exists, updating...")
65 |             
   | ^^^^^^^^^^^^
66 |             # Update existing table
67 |             glue.update_table(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/register_glue_table.py:72:1
   |
70 |             )
71 |             logger.info(f"âœ… Updated table: {database}.{table_name}")
72 |             
   | ^^^^^^^^^^^^
73 |         except glue.exceptions.EntityNotFoundException:
74 |             # Create new table
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/register_glue_table.py:80:1
   |
78 |             )
79 |             logger.info(f"âœ… Created table: {database}.{table_name}")
80 |         
   | ^^^^^^^^
81 |     except Exception as e:
82 |         logger.error(f"Failed to create/update Glue table: {e}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/register_glue_table.py:89:1
   |
87 |     """Register Glue tables for all layers."""
88 |     bucket = os.getenv("S3_BUCKET", "my-etl-lake-demo")
89 |     
   | ^^^^
90 |     # Create databases if they don't exist
91 |     glue = boto3.client("glue", region_name=os.getenv("AWS_REGION", "us-east-1"))
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/register_glue_table.py:92:1
   |
90 |     # Create databases if they don't exist
91 |     glue = boto3.client("glue", region_name=os.getenv("AWS_REGION", "us-east-1"))
92 |     
   | ^^^^
93 |     databases = ["bronze", "silver", "gold"]
94 |     for db in databases:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/register_glue_table.py:101:1
    |
 99 |             glue.create_database(DatabaseInput={"Name": db})
100 |             logger.info(f"âœ… Created database: {db}")
101 |     
    | ^^^^
102 |     # Register tables
103 |     tables_config = [
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/register_glue_table.py:108:1
    |
106 |         ("gold", "customer_360", "gold/customer_360/"),
107 |     ]
108 |     
    | ^^^^
109 |     for database, table_name, location in tables_config:
110 |         try:
    |
help: Remove whitespace from blank line

B007 Loop control variable `location` not used within loop body
   --> aws/scripts/register_glue_table.py:109:31
    |
107 |     ]
108 |     
109 |     for database, table_name, location in tables_config:
    |                               ^^^^^^^^
110 |         try:
111 |             create_glue_table(bucket, database, table_name)
    |
help: Rename unused `location` to `_location`

W293 [*] Blank line contains whitespace
   --> aws/scripts/register_glue_table.py:114:1
    |
112 |         except Exception as e:
113 |             logger.error(f"Failed to register {database}.{table_name}: {e}")
114 |     
    | ^^^^
115 |     logger.info("âœ… Glue table registration complete")
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> aws/scripts/run_aws_emr_serverless.py:6:1
   |
 4 |   """
 5 |
 6 | / import argparse
 7 | | import boto3
 8 | | import json
 9 | | import logging
10 | | import os
11 | | import sys
12 | | import time
13 | | import zipfile
14 | | from pathlib import Path
   | |________________________^
15 |
16 |   # Add src to path
   |
help: Organize imports

F401 [*] `json` imported but unused
  --> aws/scripts/run_aws_emr_serverless.py:8:8
   |
 6 | import argparse
 7 | import boto3
 8 | import json
   |        ^^^^
 9 | import logging
10 | import os
   |
help: Remove unused import: `json`

I001 [*] Import block is un-sorted or un-formatted
  --> aws/scripts/run_aws_emr_serverless.py:19:1
   |
17 |   sys.path.insert(0, str(Path(__file__).parent.parent / "src"))
18 |
19 | / from project_a.utils.config import load_conf
20 | | from project_a.utils.logging import setup_json_logging, get_trace_id, log_with_trace
   | |____________________________________________________________________________________^
   |
help: Organize imports

W293 Blank line contains whitespace
  --> aws/scripts/run_aws_emr_serverless.py:26:1
   |
24 |     """
25 |     Create zip package of source code.
26 |     
   | ^^^^
27 |     Args:
28 |         source_dir: Source directory to package
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> aws/scripts/run_aws_emr_serverless.py:30:1
   |
28 |         source_dir: Source directory to package
29 |         output_path: Output zip file path
30 |         
   | ^^^^^^^^
31 |     Returns:
32 |         Path to created zip file
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/run_aws_emr_serverless.py:36:1
   |
34 |     logger = logging.getLogger(__name__)
35 |     logger.info(f"Creating artifact package: {output_path}")
36 |     
   | ^^^^
37 |     with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
38 |         for root, dirs, files in os.walk(source_dir):
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/run_aws_emr_serverless.py:41:1
   |
39 |             # Skip __pycache__ and .git directories
40 |             dirs[:] = [d for d in dirs if d not in ['__pycache__', '.git', '.pytest_cache']]
41 |             
   | ^^^^^^^^^^^^
42 |             for file in files:
43 |                 if file.endswith(('.py', '.yaml', '.yml', '.json')):
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/run_aws_emr_serverless.py:47:1
   |
45 |                     arc_path = os.path.relpath(file_path, source_dir)
46 |                     zipf.write(file_path, arc_path)
47 |     
   | ^^^^
48 |     logger.info(f"Artifact package created: {output_path}")
49 |     return output_path
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> aws/scripts/run_aws_emr_serverless.py:55:1
   |
53 |     """
54 |     Upload file to S3.
55 |     
   | ^^^^
56 |     Args:
57 |         local_path: Local file path
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/run_aws_emr_serverless.py:63:1
   |
61 |     logger = logging.getLogger(__name__)
62 |     logger.info(f"Uploading {local_path} to {s3_path}")
63 |     
   | ^^^^
64 |     s3_client = boto3.client('s3', region_name=aws_region)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/run_aws_emr_serverless.py:65:1
   |
64 |     s3_client = boto3.client('s3', region_name=aws_region)
65 |     
   | ^^^^
66 |     # Parse S3 path
67 |     s3_path = s3_path.replace('s3://', '')
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/run_aws_emr_serverless.py:69:1
   |
67 |     s3_path = s3_path.replace('s3://', '')
68 |     bucket, key = s3_path.split('/', 1)
69 |     
   | ^^^^
70 |     s3_client.upload_file(local_path, bucket, key)
71 |     logger.info(f"Upload completed: s3://{bucket}/{key}")
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> aws/scripts/run_aws_emr_serverless.py:84:1
   |
82 |     """
83 |     Submit job to EMR Serverless.
84 |     
   | ^^^^
85 |     Args:
86 |         application_id: EMR Serverless application ID
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> aws/scripts/run_aws_emr_serverless.py:92:1
   |
90 |         s3_artifacts_path: S3 path to artifacts
91 |         aws_region: AWS region
92 |         
   | ^^^^^^^^
93 |     Returns:
94 |         Job run ID
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/run_aws_emr_serverless.py:98:1
   |
96 |     logger = logging.getLogger(__name__)
97 |     logger.info(f"Submitting EMR Serverless job: {job_name}")
98 |     
   | ^^^^
99 |     emr_client = boto3.client('emr-serverless', region_name=aws_region)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_aws_emr_serverless.py:100:1
    |
 99 |     emr_client = boto3.client('emr-serverless', region_name=aws_region)
100 |     
    | ^^^^
101 |     response = emr_client.start_job_run(
102 |         applicationId=application_id,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_aws_emr_serverless.py:119:1
    |
117 |         }
118 |     )
119 |     
    | ^^^^
120 |     job_run_id = response['jobRunId']
121 |     logger.info(f"Job submitted successfully. Job Run ID: {job_run_id}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_aws_emr_serverless.py:122:1
    |
120 |     job_run_id = response['jobRunId']
121 |     logger.info(f"Job submitted successfully. Job Run ID: {job_run_id}")
122 |     
    | ^^^^
123 |     return job_run_id
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> aws/scripts/run_aws_emr_serverless.py:129:1
    |
127 |     """
128 |     Wait for job completion and return final state.
129 |     
    | ^^^^
130 |     Args:
131 |         job_run_id: Job run ID
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> aws/scripts/run_aws_emr_serverless.py:134:1
    |
132 |         application_id: EMR Serverless application ID
133 |         aws_region: AWS region
134 |         
    | ^^^^^^^^
135 |     Returns:
136 |         Final job state
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_aws_emr_serverless.py:140:1
    |
138 |     logger = logging.getLogger(__name__)
139 |     logger.info(f"Waiting for job completion: {job_run_id}")
140 |     
    | ^^^^
141 |     emr_client = boto3.client('emr-serverless', region_name=aws_region)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_aws_emr_serverless.py:142:1
    |
141 |     emr_client = boto3.client('emr-serverless', region_name=aws_region)
142 |     
    | ^^^^
143 |     while True:
144 |         response = emr_client.get_job_run(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_aws_emr_serverless.py:148:1
    |
146 |             jobRunId=job_run_id
147 |         )
148 |         
    | ^^^^^^^^
149 |         state = response['jobRun']['state']
150 |         logger.info(f"Job state: {state}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_aws_emr_serverless.py:151:1
    |
149 |         state = response['jobRun']['state']
150 |         logger.info(f"Job state: {state}")
151 |         
    | ^^^^^^^^
152 |         if state in ['SUCCESS', 'FAILED', 'CANCELLED']:
153 |             break
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_aws_emr_serverless.py:154:1
    |
152 |         if state in ['SUCCESS', 'FAILED', 'CANCELLED']:
153 |             break
154 |         
    | ^^^^^^^^
155 |         time.sleep(30)  # Wait 30 seconds before checking again
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_aws_emr_serverless.py:156:1
    |
155 |         time.sleep(30)  # Wait 30 seconds before checking again
156 |     
    | ^^^^
157 |     return state
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_aws_emr_serverless.py:165:1
    |
163 |     parser.add_argument("--conf", required=True, help="Configuration file path")
164 |     args = parser.parse_args()
165 |     
    | ^^^^
166 |     # Setup logging
167 |     trace_id = get_trace_id()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_aws_emr_serverless.py:170:1
    |
168 |     logger = setup_json_logging(include_trace_id=True)
169 |     log_with_trace(logger, "info", "Starting AWS EMR Serverless submission", trace_id)
170 |     
    | ^^^^
171 |     try:
172 |         # Load configuration
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_aws_emr_serverless.py:175:1
    |
173 |         config = load_conf(args.conf)
174 |         log_with_trace(logger, "info", f"Configuration loaded: {args.conf}", trace_id)
175 |         
    | ^^^^^^^^
176 |         # Get AWS configuration
177 |         aws_region = config["glue"]["region"]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_aws_emr_serverless.py:180:1
    |
178 |         emr_app_id = config["emr"]["application_id"]
179 |         s3_bucket = os.environ.get("AWS_S3_BUCKET")
180 |         
    | ^^^^^^^^
181 |         if not s3_bucket:
182 |             raise ValueError("AWS_S3_BUCKET environment variable is required")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_aws_emr_serverless.py:183:1
    |
181 |         if not s3_bucket:
182 |             raise ValueError("AWS_S3_BUCKET environment variable is required")
183 |         
    | ^^^^^^^^
184 |         log_with_trace(logger, "info", f"AWS Region: {aws_region}, EMR App: {emr_app_id}", trace_id)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_aws_emr_serverless.py:185:1
    |
184 |         log_with_trace(logger, "info", f"AWS Region: {aws_region}, EMR App: {emr_app_id}", trace_id)
185 |         
    | ^^^^^^^^
186 |         # Create artifact package
187 |         source_dir = str(Path(__file__).parent.parent / "src")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_aws_emr_serverless.py:190:1
    |
188 |         artifact_path = "/tmp/etl-artifacts.zip"
189 |         create_artifact_package(source_dir, artifact_path)
190 |         
    | ^^^^^^^^
191 |         # Upload artifacts to S3
192 |         s3_artifacts_path = f"s3://{s3_bucket}/artifacts"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_aws_emr_serverless.py:194:1
    |
192 |         s3_artifacts_path = f"s3://{s3_bucket}/artifacts"
193 |         upload_to_s3(artifact_path, f"{s3_artifacts_path}/etl-artifacts.zip", aws_region)
194 |         
    | ^^^^^^^^
195 |         # Prepare Spark submit parameters
196 |         spark_submit_params = [
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_aws_emr_serverless.py:203:1
    |
201 |             "--files", f"{s3_artifacts_path}/etl-artifacts.zip"
202 |         ]
203 |         
    | ^^^^^^^^
204 |         # Submit job
205 |         job_name = f"etl-pipeline-{int(time.time())}"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_aws_emr_serverless.py:207:1
    |
205 |         job_name = f"etl-pipeline-{int(time.time())}"
206 |         entry_point = "scripts/run_local_etl.py"
207 |         
    | ^^^^^^^^
208 |         job_run_id = submit_emr_serverless_job(
209 |             emr_app_id,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_aws_emr_serverless.py:216:1
    |
214 |             aws_region
215 |         )
216 |         
    | ^^^^^^^^
217 |         # Wait for completion
218 |         final_state = wait_for_job_completion(job_run_id, emr_app_id, aws_region)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_aws_emr_serverless.py:219:1
    |
217 |         # Wait for completion
218 |         final_state = wait_for_job_completion(job_run_id, emr_app_id, aws_region)
219 |         
    | ^^^^^^^^
220 |         # Print results
221 |         print("\n" + "="*60)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_aws_emr_serverless.py:229:1
    |
227 |         print(f"Region: {aws_region}")
228 |         print("="*60)
229 |         
    | ^^^^^^^^
230 |         if final_state == "SUCCESS":
231 |             log_with_trace(logger, "info", "EMR Serverless job completed successfully", trace_id)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_aws_emr_serverless.py:237:1
    |
235 |             print(f"\nâŒ Job failed with state: {final_state}")
236 |             sys.exit(1)
237 |         
    | ^^^^^^^^
238 |     except Exception as e:
239 |         log_with_trace(logger, "error", f"AWS EMR Serverless submission failed: {str(e)}", trace_id, error=str(e))
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> aws/scripts/run_ge_checks.py:8:1
   |
 6 |   """
 7 |
 8 | / import sys
 9 | | import os
10 | | import argparse
11 | | import logging
12 | | import json
13 | | from pathlib import Path
14 | | from datetime import datetime
   | |_____________________________^
15 |
16 |   # Add project root
   |
help: Organize imports

W293 Blank line contains whitespace
  --> aws/scripts/run_ge_checks.py:38:1
   |
36 |     """
37 |     Run Great Expectations suite and return results.
38 |     
   | ^^^^
39 |     Args:
40 |         suite_name: Name of GE suite (e.g., 'bronze_salesforce_accounts')
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> aws/scripts/run_ge_checks.py:44:1
   |
42 |         critical_only: Only run critical expectations
43 |         config_path: Path to GE config
44 |         
   | ^^^^^^^^
45 |     Returns:
46 |         Dictionary with results
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/run_ge_checks.py:51:1
   |
49 |         logger.error("Great Expectations not available")
50 |         return {"passed": False, "error": "GE not installed"}
51 |     
   | ^^^^
52 |     try:
53 |         # Initialize GE context
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/run_ge_checks.py:58:1
   |
56 |         else:
57 |             context = ge.get_context()
58 |         
   | ^^^^^^^^
59 |         # Load suite
60 |         suite = context.get_expectation_suite(suite_name)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/run_ge_checks.py:61:1
   |
59 |         # Load suite
60 |         suite = context.get_expectation_suite(suite_name)
61 |         
   | ^^^^^^^^
62 |         # Filter critical expectations if needed
63 |         if critical_only:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/run_ge_checks.py:65:1
   |
63 |         if critical_only:
64 |             suite = _filter_critical_expectations(suite)
65 |         
   | ^^^^^^^^
66 |         # Create validator
67 |         validator = context.get_validator(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/run_ge_checks.py:71:1
   |
69 |             expectation_suite=suite
70 |         )
71 |         
   | ^^^^^^^^
72 |         # Run suite
73 |         logger.info(f"Running GE suite: {suite_name}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/run_ge_checks.py:76:1
   |
74 |         logger.info(f"Data asset: {data_asset}")
75 |         logger.info(f"Expectations: {len(suite.expectations)}")
76 |         
   | ^^^^^^^^
77 |         results = validator.validate()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/run_ge_checks.py:78:1
   |
77 |         results = validator.validate()
78 |         
   | ^^^^^^^^
79 |         # Process results
80 |         passed = results.success
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/run_ge_checks.py:82:1
   |
80 |         passed = results.success
81 |         stats = results.statistics
82 |         
   | ^^^^^^^^
83 |         logger.info(f"âœ… Suite {'PASSED' if passed else 'FAILED'}")
84 |         logger.info(f"   Expectations: {stats.get('evaluated_expectations', 0)}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/run_ge_checks.py:87:1
   |
85 |         logger.info(f"   Successful: {stats.get('successful_expectations', 0)}")
86 |         logger.info(f"   Failed: {stats.get('unsuccessful_expectations', 0)}")
87 |         
   | ^^^^^^^^
88 |         return {
89 |             "passed": passed,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/run_ge_checks.py:95:1
   |
93 |             "results": results.to_json_dict()
94 |         }
95 |         
   | ^^^^^^^^
96 |     except Exception as e:
97 |         logger.error(f"Failed to run GE suite: {e}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_ge_checks.py:107:1
    |
105 |     """Filter to only critical expectations."""
106 |     critical_suite = ExpectationSuite(suite_name=suite.expectation_suite_name)
107 |     
    | ^^^^
108 |     for expectation in suite.expectations:
109 |         # Check if expectation is marked as critical
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_ge_checks.py:112:1
    |
110 |         if hasattr(expectation, 'meta') and expectation.meta.get('critical', False):
111 |             critical_suite.add_expectation(expectation)
112 |     
    | ^^^^
113 |     return critical_suite
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_ge_checks.py:119:1
    |
117 |     """Save DQ results to S3 or local file."""
118 |     date_str = datetime.utcnow().strftime("%Y-%m-%d")
119 |     
    | ^^^^
120 |     if output_path.startswith("s3://"):
121 |         # Use boto3 to write
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_ge_checks.py:128:1
    |
126 |         os.makedirs(os.path.dirname(output_path), exist_ok=True)
127 |         output_file = Path(output_path) / f"dq_results_{date_str}.json"
128 |         
    | ^^^^^^^^
129 |         with open(output_file, "w") as f:
130 |             json.dump(results, f, indent=2)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_ge_checks.py:131:1
    |
129 |         with open(output_file, "w") as f:
130 |             json.dump(results, f, indent=2)
131 |         
    | ^^^^^^^^
132 |         logger.info(f"Saved DQ results to: {output_file}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_ge_checks.py:139:1
    |
137 |     if results.get("passed"):
138 |         return
139 |     
    | ^^^^
140 |     logger.error("ðŸš¨ DQ CHECK FAILED - Sending alert")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_ge_checks.py:141:1
    |
140 |     logger.error("ðŸš¨ DQ CHECK FAILED - Sending alert")
141 |     
    | ^^^^
142 |     # Get alerting config
143 |     alerts_config = config.get("monitoring", {}).get("alerts", {})
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_ge_checks.py:144:1
    |
142 |     # Get alerting config
143 |     alerts_config = config.get("monitoring", {}).get("alerts", {})
144 |     
    | ^^^^
145 |     # Send Slack alert (if configured)
146 |     slack_webhook = alerts_config.get("slack_webhook")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_ge_checks.py:150:1
    |
148 |         try:
149 |             import requests
150 |             
    | ^^^^^^^^^^^^
151 |             message = {
152 |                 "text": f"âŒ Data Quality Check Failed",
    |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
   --> aws/scripts/run_ge_checks.py:152:25
    |
151 |             message = {
152 |                 "text": f"âŒ Data Quality Check Failed",
    |                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
153 |                 "blocks": [
154 |                     {
    |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_ge_checks.py:163:1
    |
161 |                 ]
162 |             }
163 |             
    | ^^^^^^^^^^^^
164 |             requests.post(slack_webhook, json=message)
165 |             logger.info("âœ… Sent Slack alert")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_ge_checks.py:168:1
    |
166 |         except Exception as e:
167 |             logger.warning(f"Could not send Slack alert: {e}")
168 |     
    | ^^^^
169 |     # Send email alert (if configured)
170 |     # Implementation would use boto3 SES
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_ge_checks.py:181:1
    |
179 |     parser.add_argument("--fail-on-error", action="store_true", default=True, help="Exit with error code on failure")
180 |     parser.add_argument("--config", help="GE config path")
181 |     
    | ^^^^
182 |     args = parser.parse_args()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_ge_checks.py:183:1
    |
182 |     args = parser.parse_args()
183 |     
    | ^^^^
184 |     logging.basicConfig(level=logging.INFO)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_ge_checks.py:185:1
    |
184 |     logging.basicConfig(level=logging.INFO)
185 |     
    | ^^^^
186 |     # Load config (if needed for alerts)
187 |     config = {}
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_ge_checks.py:188:1
    |
186 |     # Load config (if needed for alerts)
187 |     config = {}
188 |     
    | ^^^^
189 |     # Run suite
190 |     results = run_ge_suite(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_ge_checks.py:196:1
    |
194 |         config_path=args.config
195 |     )
196 |     
    | ^^^^
197 |     # Save results
198 |     save_dq_results(results, args.output)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_ge_checks.py:199:1
    |
197 |     # Save results
198 |     save_dq_results(results, args.output)
199 |     
    | ^^^^
200 |     # Send alert if failed
201 |     if not results.get("passed"):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_ge_checks.py:203:1
    |
201 |     if not results.get("passed"):
202 |         send_dq_alert(results, config)
203 |     
    | ^^^^
204 |     # Exit with error code if failed
205 |     if args.fail_on_error and not results.get("passed"):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/run_ge_checks.py:208:1
    |
206 |         logger.error("âŒ DQ checks failed - exiting with error code")
207 |         sys.exit(1)
208 |     
    | ^^^^
209 |     logger.info("âœ… DQ checks passed")
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> aws/scripts/utilities/emit_lineage_and_metrics.py:13:1
   |
11 |   """
12 |
13 | / import sys
14 | | import logging
15 | | import json
16 | | import uuid
17 | | import time
18 | | from datetime import datetime, timezone
19 | | from typing import Dict, Any, List, Optional
20 | | from dataclasses import dataclass, asdict
21 | | from pyspark.sql import SparkSession
   | |____________________________________^
22 |
23 |   # Add project root to path
   |
help: Organize imports

F401 [*] `json` imported but unused
  --> aws/scripts/utilities/emit_lineage_and_metrics.py:15:8
   |
13 | import sys
14 | import logging
15 | import json
   |        ^^^^
16 | import uuid
17 | import time
   |
help: Remove unused import: `json`

F401 [*] `time` imported but unused
  --> aws/scripts/utilities/emit_lineage_and_metrics.py:17:8
   |
15 | import json
16 | import uuid
17 | import time
   |        ^^^^
18 | from datetime import datetime, timezone
19 | from typing import Dict, Any, List, Optional
   |
help: Remove unused import: `time`

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> aws/scripts/utilities/emit_lineage_and_metrics.py:19:1
   |
17 | import time
18 | from datetime import datetime, timezone
19 | from typing import Dict, Any, List, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
20 | from dataclasses import dataclass, asdict
21 | from pyspark.sql import SparkSession
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> aws/scripts/utilities/emit_lineage_and_metrics.py:19:1
   |
17 | import time
18 | from datetime import datetime, timezone
19 | from typing import Dict, Any, List, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
20 | from dataclasses import dataclass, asdict
21 | from pyspark.sql import SparkSession
   |

F401 [*] `typing.Optional` imported but unused
  --> aws/scripts/utilities/emit_lineage_and_metrics.py:19:37
   |
17 | import time
18 | from datetime import datetime, timezone
19 | from typing import Dict, Any, List, Optional
   |                                     ^^^^^^^^
20 | from dataclasses import dataclass, asdict
21 | from pyspark.sql import SparkSession
   |
help: Remove unused import: `typing.Optional`

I001 [*] Import block is un-sorted or un-formatted
  --> aws/scripts/utilities/emit_lineage_and_metrics.py:26:1
   |
24 |   sys.path.append('/opt/airflow/dags/src')
25 |
26 | / from project_a.utils.spark_session import build_spark
27 | | from project_a.utils.config import load_config
   | |______________________________________________^
28 |
29 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP006 [*] Use `list` instead of `List` for type annotation
  --> aws/scripts/utilities/emit_lineage_and_metrics.py:40:19
   |
38 |     end_time: str
39 |     status: str
40 |     input_tables: List[str]
   |                   ^^^^
41 |     output_tables: List[str]
42 |     row_counts: Dict[str, int]
   |
help: Replace with `list`

UP006 [*] Use `list` instead of `List` for type annotation
  --> aws/scripts/utilities/emit_lineage_and_metrics.py:41:20
   |
39 |     status: str
40 |     input_tables: List[str]
41 |     output_tables: List[str]
   |                    ^^^^
42 |     row_counts: Dict[str, int]
43 |     processing_time_seconds: float
   |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> aws/scripts/utilities/emit_lineage_and_metrics.py:42:17
   |
40 |     input_tables: List[str]
41 |     output_tables: List[str]
42 |     row_counts: Dict[str, int]
   |                 ^^^^
43 |     processing_time_seconds: float
44 |     environment: str
   |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
  --> aws/scripts/utilities/emit_lineage_and_metrics.py:55:20
   |
53 |     job_name: str
54 |     timestamp: str
55 |     source_tables: List[str]
   |                    ^^^^
56 |     target_tables: List[str]
57 |     transformations: List[str]
   |
help: Replace with `list`

UP006 [*] Use `list` instead of `List` for type annotation
  --> aws/scripts/utilities/emit_lineage_and_metrics.py:56:20
   |
54 |     timestamp: str
55 |     source_tables: List[str]
56 |     target_tables: List[str]
   |                    ^^^^
57 |     transformations: List[str]
58 |     data_quality_score: float
   |
help: Replace with `list`

UP006 [*] Use `list` instead of `List` for type annotation
  --> aws/scripts/utilities/emit_lineage_and_metrics.py:57:22
   |
55 |     source_tables: List[str]
56 |     target_tables: List[str]
57 |     transformations: List[str]
   |                      ^^^^
58 |     data_quality_score: float
59 |     environment: str
   |
help: Replace with `list`

W293 [*] Blank line contains whitespace
  --> aws/scripts/utilities/emit_lineage_and_metrics.py:64:1
   |
62 | class LineageEmitter:
63 |     """Handles lineage and metrics emission for ETL operations."""
64 |     
   | ^^^^
65 |     def __init__(self, spark: SparkSession, config: Dict[str, Any]):
66 |         self.spark = spark
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> aws/scripts/utilities/emit_lineage_and_metrics.py:65:53
   |
63 |     """Handles lineage and metrics emission for ETL operations."""
64 |     
65 |     def __init__(self, spark: SparkSession, config: Dict[str, Any]):
   |                                                     ^^^^
66 |         self.spark = spark
67 |         self.config = config
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> aws/scripts/utilities/emit_lineage_and_metrics.py:71:1
   |
69 |         self.namespace = self.lineage_config.get('namespace', 'company-etl')
70 |         self.environment = config.get('environment', 'aws-prod')
71 |         
   | ^^^^^^^^
72 |     def create_run_metadata(self, job_name: str, input_tables: List[str], 
73 |                           output_tables: List[str], start_time: datetime) -> RunMetadata:
   |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
  --> aws/scripts/utilities/emit_lineage_and_metrics.py:72:64
   |
70 |         self.environment = config.get('environment', 'aws-prod')
71 |         
72 |     def create_run_metadata(self, job_name: str, input_tables: List[str], 
   |                                                                ^^^^
73 |                           output_tables: List[str], start_time: datetime) -> RunMetadata:
74 |         """Create run metadata for a pipeline execution."""
   |
help: Replace with `list`

W291 [*] Trailing whitespace
  --> aws/scripts/utilities/emit_lineage_and_metrics.py:72:74
   |
70 |         self.environment = config.get('environment', 'aws-prod')
71 |         
72 |     def create_run_metadata(self, job_name: str, input_tables: List[str], 
   |                                                                          ^
73 |                           output_tables: List[str], start_time: datetime) -> RunMetadata:
74 |         """Create run metadata for a pipeline execution."""
   |
help: Remove trailing whitespace

UP006 [*] Use `list` instead of `List` for type annotation
  --> aws/scripts/utilities/emit_lineage_and_metrics.py:73:42
   |
72 |     def create_run_metadata(self, job_name: str, input_tables: List[str], 
73 |                           output_tables: List[str], start_time: datetime) -> RunMetadata:
   |                                          ^^^^
74 |         """Create run metadata for a pipeline execution."""
   |
help: Replace with `list`

W293 [*] Blank line contains whitespace
  --> aws/scripts/utilities/emit_lineage_and_metrics.py:75:1
   |
73 |                           output_tables: List[str], start_time: datetime) -> RunMetadata:
74 |         """Create run metadata for a pipeline execution."""
75 |         
   | ^^^^^^^^
76 |         run_id = f"{job_name}_{int(start_time.timestamp())}_{str(uuid.uuid4())[:8]}"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/utilities/emit_lineage_and_metrics.py:77:1
   |
76 |         run_id = f"{job_name}_{int(start_time.timestamp())}_{str(uuid.uuid4())[:8]}"
77 |         
   | ^^^^^^^^
78 |         return RunMetadata(
79 |             run_id=run_id,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/utilities/emit_lineage_and_metrics.py:91:1
   |
89 |             version=self.config.get('version', '1.0.0')
90 |         )
91 |     
   | ^^^^
92 |     def emit_lineage_event(self, run_metadata: RunMetadata, 
93 |                           transformations: List[str], 
   |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
  --> aws/scripts/utilities/emit_lineage_and_metrics.py:92:60
   |
90 |         )
91 |     
92 |     def emit_lineage_event(self, run_metadata: RunMetadata, 
   |                                                            ^
93 |                           transformations: List[str], 
94 |                           data_quality_score: float) -> LineageEvent:
   |
help: Remove trailing whitespace

UP006 [*] Use `list` instead of `List` for type annotation
  --> aws/scripts/utilities/emit_lineage_and_metrics.py:93:44
   |
92 |     def emit_lineage_event(self, run_metadata: RunMetadata, 
93 |                           transformations: List[str], 
   |                                            ^^^^
94 |                           data_quality_score: float) -> LineageEvent:
95 |         """Create and emit lineage event."""
   |
help: Replace with `list`

W291 [*] Trailing whitespace
  --> aws/scripts/utilities/emit_lineage_and_metrics.py:93:54
   |
92 |     def emit_lineage_event(self, run_metadata: RunMetadata, 
93 |                           transformations: List[str], 
   |                                                      ^
94 |                           data_quality_score: float) -> LineageEvent:
95 |         """Create and emit lineage event."""
   |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
  --> aws/scripts/utilities/emit_lineage_and_metrics.py:96:1
   |
94 |                           data_quality_score: float) -> LineageEvent:
95 |         """Create and emit lineage event."""
96 |         
   | ^^^^^^^^
97 |         lineage_event = LineageEvent(
98 |             event_id=str(uuid.uuid4()),
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:108:1
    |
106 |             environment=self.environment
107 |         )
108 |         
    | ^^^^^^^^
109 |         # Emit to CloudWatch
110 |         self._emit_to_cloudwatch(lineage_event)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:111:1
    |
109 |         # Emit to CloudWatch
110 |         self._emit_to_cloudwatch(lineage_event)
111 |         
    | ^^^^^^^^
112 |         # Emit to OpenLineage (if configured)
113 |         if self.lineage_config.get('openlineage_enabled', False):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:115:1
    |
113 |         if self.lineage_config.get('openlineage_enabled', False):
114 |             self._emit_to_openlineage(lineage_event)
115 |         
    | ^^^^^^^^
116 |         return lineage_event
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:117:1
    |
116 |         return lineage_event
117 |     
    | ^^^^
118 |     def _emit_to_cloudwatch(self, lineage_event: LineageEvent) -> None:
119 |         """Emit lineage event to CloudWatch."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:120:1
    |
118 |     def _emit_to_cloudwatch(self, lineage_event: LineageEvent) -> None:
119 |         """Emit lineage event to CloudWatch."""
120 |         
    | ^^^^^^^^
121 |         try:
122 |             import boto3
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:123:1
    |
121 |         try:
122 |             import boto3
123 |             
    | ^^^^^^^^^^^^
124 |             cloudwatch = boto3.client('cloudwatch', region_name=self.config['aws']['region'])
125 |             namespace = f"{self.namespace}/lineage"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:126:1
    |
124 |             cloudwatch = boto3.client('cloudwatch', region_name=self.config['aws']['region'])
125 |             namespace = f"{self.namespace}/lineage"
126 |             
    | ^^^^^^^^^^^^
127 |             # Emit custom metrics
128 |             cloudwatch.put_metric_data(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:151:1
    |
149 |                 ]
150 |             )
151 |             
    | ^^^^^^^^^^^^
152 |             # Log detailed lineage information
153 |             logger.info(f"Lineage event emitted to CloudWatch: {lineage_event.event_id}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:154:1
    |
152 |             # Log detailed lineage information
153 |             logger.info(f"Lineage event emitted to CloudWatch: {lineage_event.event_id}")
154 |             
    | ^^^^^^^^^^^^
155 |         except Exception as e:
156 |             logger.warning(f"Failed to emit to CloudWatch: {str(e)}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:157:1
    |
155 |         except Exception as e:
156 |             logger.warning(f"Failed to emit to CloudWatch: {str(e)}")
157 |     
    | ^^^^
158 |     def _emit_to_openlineage(self, lineage_event: LineageEvent) -> None:
159 |         """Emit lineage event to OpenLineage."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:160:1
    |
158 |     def _emit_to_openlineage(self, lineage_event: LineageEvent) -> None:
159 |         """Emit lineage event to OpenLineage."""
160 |         
    | ^^^^^^^^
161 |         try:
162 |             # This would integrate with OpenLineage client
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:165:1
    |
163 |             # For now, we'll log the event structure
164 |             logger.info(f"OpenLineage event: {asdict(lineage_event)}")
165 |             
    | ^^^^^^^^^^^^
166 |         except Exception as e:
167 |             logger.warning(f"Failed to emit to OpenLineage: {str(e)}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:168:1
    |
166 |         except Exception as e:
167 |             logger.warning(f"Failed to emit to OpenLineage: {str(e)}")
168 |     
    | ^^^^
169 |     def log_table_metrics(self, run_metadata: RunMetadata, 
170 |                          table_name: str, row_count: int, 
    |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:169:59
    |
167 |             logger.warning(f"Failed to emit to OpenLineage: {str(e)}")
168 |     
169 |     def log_table_metrics(self, run_metadata: RunMetadata, 
    |                                                           ^
170 |                          table_name: str, row_count: int, 
171 |                          s3_path: str) -> None:
    |
help: Remove trailing whitespace

W291 [*] Trailing whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:170:58
    |
169 |     def log_table_metrics(self, run_metadata: RunMetadata, 
170 |                          table_name: str, row_count: int, 
    |                                                          ^
171 |                          s3_path: str) -> None:
172 |         """Log metrics for a specific table."""
    |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:173:1
    |
171 |                          s3_path: str) -> None:
172 |         """Log metrics for a specific table."""
173 |         
    | ^^^^^^^^
174 |         run_metadata.row_counts[table_name] = row_count
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:175:1
    |
174 |         run_metadata.row_counts[table_name] = row_count
175 |         
    | ^^^^^^^^
176 |         logger.info(f"Table metrics - {table_name}: {row_count} rows, path: {s3_path}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:177:1
    |
176 |         logger.info(f"Table metrics - {table_name}: {row_count} rows, path: {s3_path}")
177 |         
    | ^^^^^^^^
178 |         # Emit table-level metrics to CloudWatch
179 |         try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:181:1
    |
179 |         try:
180 |             import boto3
181 |             
    | ^^^^^^^^^^^^
182 |             cloudwatch = boto3.client('cloudwatch', region_name=self.config['aws']['region'])
183 |             namespace = f"{self.namespace}/tables"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:184:1
    |
182 |             cloudwatch = boto3.client('cloudwatch', region_name=self.config['aws']['region'])
183 |             namespace = f"{self.namespace}/tables"
184 |             
    | ^^^^^^^^^^^^
185 |             cloudwatch.put_metric_data(
186 |                 Namespace=namespace,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:199:1
    |
197 |                 ]
198 |             )
199 |             
    | ^^^^^^^^^^^^
200 |         except Exception as e:
201 |             logger.warning(f"Failed to emit table metrics: {str(e)}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:202:1
    |
200 |         except Exception as e:
201 |             logger.warning(f"Failed to emit table metrics: {str(e)}")
202 |     
    | ^^^^
203 |     def finalize_run_metadata(self, run_metadata: RunMetadata, 
204 |                             status: str, end_time: datetime) -> None:
    |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:203:63
    |
201 |             logger.warning(f"Failed to emit table metrics: {str(e)}")
202 |     
203 |     def finalize_run_metadata(self, run_metadata: RunMetadata, 
    |                                                               ^
204 |                             status: str, end_time: datetime) -> None:
205 |         """Finalize run metadata with completion information."""
    |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:206:1
    |
204 |                             status: str, end_time: datetime) -> None:
205 |         """Finalize run metadata with completion information."""
206 |         
    | ^^^^^^^^
207 |         run_metadata.end_time = end_time.isoformat()
208 |         run_metadata.status = status
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:210:1
    |
208 |         run_metadata.status = status
209 |         run_metadata.processing_time_seconds = (end_time - datetime.fromisoformat(run_metadata.start_time)).total_seconds()
210 |         
    | ^^^^^^^^
211 |         # Emit final run metrics
212 |         try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:214:1
    |
212 |         try:
213 |             import boto3
214 |             
    | ^^^^^^^^^^^^
215 |             cloudwatch = boto3.client('cloudwatch', region_name=self.config['aws']['region'])
216 |             namespace = f"{self.namespace}/runs"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:217:1
    |
215 |             cloudwatch = boto3.client('cloudwatch', region_name=self.config['aws']['region'])
216 |             namespace = f"{self.namespace}/runs"
217 |             
    | ^^^^^^^^^^^^
218 |             cloudwatch.put_metric_data(
219 |                 Namespace=namespace,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:242:1
    |
240 |                 ]
241 |             )
242 |             
    | ^^^^^^^^^^^^
243 |         except Exception as e:
244 |             logger.warning(f"Failed to emit final run metrics: {str(e)}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:245:1
    |
243 |         except Exception as e:
244 |             logger.warning(f"Failed to emit final run metrics: {str(e)}")
245 |         
    | ^^^^^^^^
246 |         logger.info(f"Run {run_metadata.run_id} completed: {status}, "
247 |                    f"processed {sum(run_metadata.row_counts.values())} rows in "
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:251:56
    |
251 | def emit_pipeline_lineage(spark: SparkSession, config: Dict[str, Any],
    |                                                        ^^^^
252 |                          job_name: str, input_tables: List[str], 
253 |                          output_tables: List[str], transformations: List[str],
    |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:252:55
    |
251 | def emit_pipeline_lineage(spark: SparkSession, config: Dict[str, Any],
252 |                          job_name: str, input_tables: List[str], 
    |                                                       ^^^^
253 |                          output_tables: List[str], transformations: List[str],
254 |                          data_quality_score: float = 1.0) -> str:
    |
help: Replace with `list`

W291 [*] Trailing whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:252:65
    |
251 | def emit_pipeline_lineage(spark: SparkSession, config: Dict[str, Any],
252 |                          job_name: str, input_tables: List[str], 
    |                                                                 ^
253 |                          output_tables: List[str], transformations: List[str],
254 |                          data_quality_score: float = 1.0) -> str:
    |
help: Remove trailing whitespace

UP006 [*] Use `list` instead of `List` for type annotation
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:253:41
    |
251 | def emit_pipeline_lineage(spark: SparkSession, config: Dict[str, Any],
252 |                          job_name: str, input_tables: List[str], 
253 |                          output_tables: List[str], transformations: List[str],
    |                                         ^^^^
254 |                          data_quality_score: float = 1.0) -> str:
255 |     """Emit lineage and metrics for a complete pipeline run."""
    |
help: Replace with `list`

UP006 [*] Use `list` instead of `List` for type annotation
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:253:69
    |
251 | def emit_pipeline_lineage(spark: SparkSession, config: Dict[str, Any],
252 |                          job_name: str, input_tables: List[str], 
253 |                          output_tables: List[str], transformations: List[str],
    |                                                                     ^^^^
254 |                          data_quality_score: float = 1.0) -> str:
255 |     """Emit lineage and metrics for a complete pipeline run."""
    |
help: Replace with `list`

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:256:1
    |
254 |                          data_quality_score: float = 1.0) -> str:
255 |     """Emit lineage and metrics for a complete pipeline run."""
256 |     
    | ^^^^
257 |     logger.info(f"Starting lineage emission for job: {job_name}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:258:1
    |
257 |     logger.info(f"Starting lineage emission for job: {job_name}")
258 |     
    | ^^^^
259 |     try:
260 |         # Initialize lineage emitter
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:262:1
    |
260 |         # Initialize lineage emitter
261 |         emitter = LineageEmitter(spark, config)
262 |         
    | ^^^^^^^^
263 |         # Create run metadata
264 |         start_time = datetime.now(timezone.utc)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:268:1
    |
266 |             job_name, input_tables, output_tables, start_time
267 |         )
268 |         
    | ^^^^^^^^
269 |         # Emit lineage event
270 |         lineage_event = emitter.emit_lineage_event(
    |
help: Remove whitespace from blank line

F841 Local variable `lineage_event` is assigned to but never used
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:270:9
    |
269 |         # Emit lineage event
270 |         lineage_event = emitter.emit_lineage_event(
    |         ^^^^^^^^^^^^^
271 |             run_metadata, transformations, data_quality_score
272 |         )
    |
help: Remove assignment to unused variable `lineage_event`

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:273:1
    |
271 |             run_metadata, transformations, data_quality_score
272 |         )
273 |         
    | ^^^^^^^^
274 |         logger.info(f"Lineage emission completed for run: {run_metadata.run_id}")
275 |         return run_metadata.run_id
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:276:1
    |
274 |         logger.info(f"Lineage emission completed for run: {run_metadata.run_id}")
275 |         return run_metadata.run_id
276 |         
    | ^^^^^^^^
277 |     except Exception as e:
278 |         logger.error(f"Failed to emit lineage for job {job_name}: {str(e)}")
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:282:54
    |
282 | def log_table_operation(spark: SparkSession, config: Dict[str, Any],
    |                                                      ^^^^
283 |                        run_id: str, table_name: str, row_count: int, 
284 |                        s3_path: str) -> None:
    |
help: Replace with `dict`

W291 [*] Trailing whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:283:69
    |
282 | def log_table_operation(spark: SparkSession, config: Dict[str, Any],
283 |                        run_id: str, table_name: str, row_count: int, 
    |                                                                     ^
284 |                        s3_path: str) -> None:
285 |     """Log metrics for a specific table operation."""
    |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:286:1
    |
284 |                        s3_path: str) -> None:
285 |     """Log metrics for a specific table operation."""
286 |     
    | ^^^^
287 |     try:
288 |         emitter = LineageEmitter(spark, config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:289:1
    |
287 |     try:
288 |         emitter = LineageEmitter(spark, config)
289 |         
    | ^^^^^^^^
290 |         # Create a minimal run metadata for logging
291 |         run_metadata = RunMetadata(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:304:1
    |
302 |             version=config.get('version', '1.0.0')
303 |         )
304 |         
    | ^^^^^^^^
305 |         emitter.log_table_metrics(run_metadata, table_name, row_count, s3_path)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:306:1
    |
305 |         emitter.log_table_metrics(run_metadata, table_name, row_count, s3_path)
306 |         
    | ^^^^^^^^
307 |     except Exception as e:
308 |         logger.error(f"Failed to log table operation: {str(e)}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:313:1
    |
311 | def main():
312 |     """Main entry point for lineage emission job."""
313 |     
    | ^^^^
314 |     # Initialize Spark
315 |     spark = build_spark("LineageEmissionJob")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:316:1
    |
314 |     # Initialize Spark
315 |     spark = build_spark("LineageEmissionJob")
316 |     
    | ^^^^
317 |     # Load configuration
318 |     config = load_config("aws/config/config-prod.yaml")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:319:1
    |
317 |     # Load configuration
318 |     config = load_config("aws/config/config-prod.yaml")
319 |     
    | ^^^^
320 |     try:
321 |         # Example usage - emit lineage for a complete pipeline
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:326:1
    |
324 |         output_tables = ["silver_orders", "silver_customers"]
325 |         transformations = ["deduplication", "standardization", "validation"]
326 |         
    | ^^^^^^^^
327 |         run_id = emit_pipeline_lineage(
328 |             spark, config, job_name, input_tables, 
    |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:328:51
    |
327 |         run_id = emit_pipeline_lineage(
328 |             spark, config, job_name, input_tables, 
    |                                                   ^
329 |             output_tables, transformations, data_quality_score=0.95
330 |         )
    |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:331:1
    |
329 |             output_tables, transformations, data_quality_score=0.95
330 |         )
331 |         
    | ^^^^^^^^
332 |         logger.info(f"Lineage emission job completed successfully: {run_id}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:333:1
    |
332 |         logger.info(f"Lineage emission job completed successfully: {run_id}")
333 |         
    | ^^^^^^^^
334 |     except Exception as e:
335 |         logger.error(f"Lineage emission job failed: {str(e)}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/emit_lineage_and_metrics.py:337:1
    |
335 |         logger.error(f"Lineage emission job failed: {str(e)}")
336 |         sys.exit(1)
337 |     
    | ^^^^
338 |     finally:
339 |         spark.stop()
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> aws/scripts/utilities/lf_tags_seed.py:7:1
   |
 5 |   """
 6 |
 7 | / import os
 8 | | import sys
 9 | | import logging
10 | | import boto3
11 | | from typing import Dict, List, Any
12 | | from pathlib import Path
   | |________________________^
13 |
14 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> aws/scripts/utilities/lf_tags_seed.py:11:1
   |
 9 | import logging
10 | import boto3
11 | from typing import Dict, List, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
12 | from pathlib import Path
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> aws/scripts/utilities/lf_tags_seed.py:11:1
   |
 9 | import logging
10 | import boto3
11 | from typing import Dict, List, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
12 | from pathlib import Path
   |

F401 [*] `pathlib.Path` imported but unused
  --> aws/scripts/utilities/lf_tags_seed.py:12:21
   |
10 | import boto3
11 | from typing import Dict, List, Any
12 | from pathlib import Path
   |                     ^^^^
13 |
14 | logger = logging.getLogger(__name__)
   |
help: Remove unused import: `pathlib.Path`

UP006 [*] Use `list` instead of `List` for type annotation
  --> aws/scripts/utilities/lf_tags_seed.py:17:37
   |
17 | def create_lf_tags(lf_client, tags: List[Dict[str, str]]) -> None:
   |                                     ^^^^
18 |     """
19 |     Create Lake Formation tags.
   |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> aws/scripts/utilities/lf_tags_seed.py:17:42
   |
17 | def create_lf_tags(lf_client, tags: List[Dict[str, str]]) -> None:
   |                                          ^^^^
18 |     """
19 |     Create Lake Formation tags.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> aws/scripts/utilities/lf_tags_seed.py:20:1
   |
18 |     """
19 |     Create Lake Formation tags.
20 |     
   | ^^^^
21 |     Args:
22 |         lf_client: Lake Formation client
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/utilities/lf_tags_seed.py:26:1
   |
24 |     """
25 |     logger.info("Creating Lake Formation tags")
26 |     
   | ^^^^
27 |     for tag in tags:
28 |         try:
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> aws/scripts/utilities/lf_tags_seed.py:41:80
   |
41 | def attach_tags_to_table(lf_client, database_name: str, table_name: str, tags: Dict[str, str]) -> None:
   |                                                                                ^^^^
42 |     """
43 |     Attach tags to a Glue table.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> aws/scripts/utilities/lf_tags_seed.py:44:1
   |
42 |     """
43 |     Attach tags to a Glue table.
44 |     
   | ^^^^
45 |     Args:
46 |         lf_client: Lake Formation client
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/utilities/lf_tags_seed.py:52:1
   |
50 |     """
51 |     logger.info(f"Attaching tags to table {database_name}.{table_name}")
52 |     
   | ^^^^
53 |     try:
54 |         lf_client.update_lf_tag(
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> aws/scripts/utilities/lf_tags_seed.py:70:99
   |
70 | def attach_tags_to_column(lf_client, database_name: str, table_name: str, column_name: str, tags: Dict[str, str]) -> None:
   |                                                                                                   ^^^^
71 |     """
72 |     Attach tags to a table column.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> aws/scripts/utilities/lf_tags_seed.py:73:1
   |
71 |     """
72 |     Attach tags to a table column.
73 |     
   | ^^^^
74 |     Args:
75 |         lf_client: Lake Formation client
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/utilities/lf_tags_seed.py:82:1
   |
80 |     """
81 |     logger.info(f"Attaching tags to column {database_name}.{table_name}.{column_name}")
82 |     
   | ^^^^
83 |     try:
84 |         lf_client.update_lf_tag(
   |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
   --> aws/scripts/utilities/lf_tags_seed.py:101:65
    |
101 | def create_grant_policy(lf_client, principal: str, permissions: List[str], resource: Dict[str, Any], conditions: Dict[str, Any] = Nonâ€¦
    |                                                                 ^^^^
102 |     """
103 |     Create a Lake Formation grant policy.
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> aws/scripts/utilities/lf_tags_seed.py:101:86
    |
101 | def create_grant_policy(lf_client, principal: str, permissions: List[str], resource: Dict[str, Any], conditions: Dict[str, Any] = Nonâ€¦
    |                                                                                      ^^^^
102 |     """
103 |     Create a Lake Formation grant policy.
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> aws/scripts/utilities/lf_tags_seed.py:101:114
    |
101 | def create_grant_policy(lf_client, principal: str, permissions: List[str], resource: Dict[str, Any], conditions: Dict[str, Any] = Nonâ€¦
    |                                                                                                                  ^^^^
102 |     """
103 |     Create a Lake Formation grant policy.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> aws/scripts/utilities/lf_tags_seed.py:104:1
    |
102 |     """
103 |     Create a Lake Formation grant policy.
104 |     
    | ^^^^
105 |     Args:
106 |         lf_client: Lake Formation client
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/lf_tags_seed.py:113:1
    |
111 |     """
112 |     logger.info(f"Creating grant policy for {principal}")
113 |     
    | ^^^^
114 |     try:
115 |         grant_params = {
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/lf_tags_seed.py:120:1
    |
118 |             'Permissions': permissions
119 |         }
120 |         
    | ^^^^^^^^
121 |         if conditions:
122 |             grant_params['PermissionsWithGrantOption'] = permissions
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/lf_tags_seed.py:124:1
    |
122 |             grant_params['PermissionsWithGrantOption'] = permissions
123 |             grant_params['Conditions'] = conditions
124 |         
    | ^^^^^^^^
125 |         lf_client.grant_permissions(**grant_params)
126 |         logger.info(f"Granted permissions {permissions} to {principal}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/lf_tags_seed.py:135:1
    |
133 |     """Main function to seed Lake Formation tags."""
134 |     import argparse
135 |     
    | ^^^^
136 |     parser = argparse.ArgumentParser(description="Seed Lake Formation tags and policies")
137 |     parser.add_argument("--database", required=True, help="Glue database name")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/lf_tags_seed.py:140:1
    |
138 |     parser.add_argument("--region", default="us-east-1", help="AWS region")
139 |     args = parser.parse_args()
140 |     
    | ^^^^
141 |     # Setup logging
142 |     log_level = os.getenv("LOG_LEVEL", "INFO")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/lf_tags_seed.py:147:1
    |
145 |         format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
146 |     )
147 |     
    | ^^^^
148 |     try:
149 |         # Create Lake Formation client
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/lf_tags_seed.py:151:1
    |
149 |         # Create Lake Formation client
150 |         lf_client = boto3.client('lakeformation', region_name=args.region)
151 |         
    | ^^^^^^^^
152 |         # Define LF-Tags
153 |         lf_tags = [
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/lf_tags_seed.py:167:1
    |
165 |             }
166 |         ]
167 |         
    | ^^^^^^^^
168 |         # Create LF-Tags
169 |         create_lf_tags(lf_client, lf_tags)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/lf_tags_seed.py:170:1
    |
168 |         # Create LF-Tags
169 |         create_lf_tags(lf_client, lf_tags)
170 |         
    | ^^^^^^^^
171 |         # Attach tags to tables
172 |         tables = ["customers", "orders", "products"]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/lf_tags_seed.py:173:1
    |
171 |         # Attach tags to tables
172 |         tables = ["customers", "orders", "products"]
173 |         
    | ^^^^^^^^
174 |         for table in tables:
175 |             # Attach sensitivity tags
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/lf_tags_seed.py:191:1
    |
189 |                     "values": ["internal"]
190 |                 })
191 |             
    | ^^^^^^^^^^^^
192 |             # Attach region tags
193 |             attach_tags_to_table(lf_client, args.database, table, {
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/lf_tags_seed.py:197:1
    |
195 |                 "values": ["NA"]
196 |             })
197 |         
    | ^^^^^^^^
198 |         # Create grant policies
199 |         # Example: EU analysts can only access EU data
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/lf_tags_seed.py:201:1
    |
199 |         # Example: EU analysts can only access EU data
200 |         eu_analyst_principal = "arn:aws:iam::123456789012:role/EUAnalystRole"
201 |         
    | ^^^^^^^^
202 |         create_grant_policy(
203 |             lf_client,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/lf_tags_seed.py:218:1
    |
216 |             }
217 |         )
218 |         
    | ^^^^^^^^
219 |         logger.info("Lake Formation tags and policies created successfully")
220 |         print("âœ… Lake Formation governance setup completed!")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/lf_tags_seed.py:224:1
    |
222 |         print("   - Attached tags to tables and columns")
223 |         print("   - Created row-level access policies")
224 |         
    | ^^^^^^^^
225 |     except Exception as e:
226 |         logger.error(f"Lake Formation setup failed: {e}")
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> aws/scripts/utilities/notify_on_sla_breach.py:9:1
   |
 7 |   """
 8 |
 9 | / import sys
10 | | import os
11 | | import json
12 | | import logging
13 | | from datetime import datetime, timedelta
14 | | from typing import Dict, Any, Optional
   | |______________________________________^
15 |
16 |   sys.path.insert(0, os.path.dirname(__file__).replace('/jobs', '/..'))
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> aws/scripts/utilities/notify_on_sla_breach.py:14:1
   |
12 | import logging
13 | from datetime import datetime, timedelta
14 | from typing import Dict, Any, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
15 |
16 | sys.path.insert(0, os.path.dirname(__file__).replace('/jobs', '/..'))
   |

W293 [*] Blank line contains whitespace
  --> aws/scripts/utilities/notify_on_sla_breach.py:29:1
   |
27 | class SLAChecker:
28 |     """Checks SLA compliance and sends alerts."""
29 |     
   | ^^^^
30 |     def __init__(self, config: Dict[str, Any]):
31 |         self.config = config
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> aws/scripts/utilities/notify_on_sla_breach.py:30:32
   |
28 |     """Checks SLA compliance and sends alerts."""
29 |     
30 |     def __init__(self, config: Dict[str, Any]):
   |                                ^^^^
31 |         self.config = config
32 |         self.cloudwatch = boto3.client('cloudwatch') if boto3 else None
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> aws/scripts/utilities/notify_on_sla_breach.py:36:1
   |
34 |         self.slack_webhook = config.get('alerts', {}).get('slack_webhook')
35 |         self.sns_topic_arn = config.get('alerts', {}).get('sns_topic_arn')
36 |         
   | ^^^^^^^^
37 |         # SLA thresholds from config
38 |         self.sla_config = config.get('slas', {})
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/utilities/notify_on_sla_breach.py:41:1
   |
39 |         self.default_runtime_minutes = self.sla_config.get('default_runtime_minutes', 60)
40 |         self.min_records_threshold = self.sla_config.get('min_records_threshold', 1)
41 |     
   | ^^^^
42 |     def get_job_metrics(self, job_name: str, hours_back: int = 1) -> Dict[str, Any]:
43 |         """
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> aws/scripts/utilities/notify_on_sla_breach.py:42:70
   |
40 |         self.min_records_threshold = self.sla_config.get('min_records_threshold', 1)
41 |     
42 |     def get_job_metrics(self, job_name: str, hours_back: int = 1) -> Dict[str, Any]:
   |                                                                      ^^^^
43 |         """
44 |         Get CloudWatch metrics for a job.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> aws/scripts/utilities/notify_on_sla_breach.py:45:1
   |
43 |         """
44 |         Get CloudWatch metrics for a job.
45 |         
   | ^^^^^^^^
46 |         Args:
47 |             job_name: Name of the job
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> aws/scripts/utilities/notify_on_sla_breach.py:49:1
   |
47 |             job_name: Name of the job
48 |             hours_back: How many hours back to look
49 |             
   | ^^^^^^^^^^^^
50 |         Returns:
51 |             Dictionary with metrics
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/utilities/notify_on_sla_breach.py:56:1
   |
54 |             logger.warning("CloudWatch not available, returning mock metrics")
55 |             return {"runtime_minutes": 0, "records_processed": 0}
56 |         
   | ^^^^^^^^
57 |         end_time = datetime.utcnow()
58 |         start_time = end_time - timedelta(hours=hours_back)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/utilities/notify_on_sla_breach.py:59:1
   |
57 |         end_time = datetime.utcnow()
58 |         start_time = end_time - timedelta(hours=hours_back)
59 |         
   | ^^^^^^^^
60 |         try:
61 |             # Get runtime metric
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/utilities/notify_on_sla_breach.py:71:1
   |
69 |                 Statistics=['Average', 'Maximum']
70 |             )
71 |             
   | ^^^^^^^^^^^^
72 |             # Get records processed metric
73 |             records_response = self.cloudwatch.get_metric_statistics(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/utilities/notify_on_sla_breach.py:82:1
   |
80 |                 Statistics=['Sum']
81 |             )
82 |             
   | ^^^^^^^^^^^^
83 |             runtime_minutes = 0
84 |             if runtime_response.get('Datapoints'):
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/utilities/notify_on_sla_breach.py:87:1
   |
85 |                 max_runtime = max(dp['Maximum'] for dp in runtime_response['Datapoints'])
86 |                 runtime_minutes = max_runtime / 60
87 |             
   | ^^^^^^^^^^^^
88 |             records_processed = 0
89 |             if records_response.get('Datapoints'):
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/utilities/notify_on_sla_breach.py:91:1
   |
89 |             if records_response.get('Datapoints'):
90 |                 records_processed = sum(dp['Sum'] for dp in records_response['Datapoints'])
91 |             
   | ^^^^^^^^^^^^
92 |             return {
93 |                 "runtime_minutes": runtime_minutes,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/utilities/notify_on_sla_breach.py:96:1
   |
94 |                 "records_processed": int(records_processed)
95 |             }
96 |             
   | ^^^^^^^^^^^^
97 |         except ClientError as e:
98 |             logger.error(f"Failed to get CloudWatch metrics: {str(e)}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/notify_on_sla_breach.py:100:1
    |
 98 |             logger.error(f"Failed to get CloudWatch metrics: {str(e)}")
 99 |             return {"runtime_minutes": 0, "records_processed": 0}
100 |     
    | ^^^^
101 |     def check_sla_breach(self, job_name: str) -> Optional[Dict[str, Any]]:
102 |         """
    |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> aws/scripts/utilities/notify_on_sla_breach.py:101:50
    |
 99 |             return {"runtime_minutes": 0, "records_processed": 0}
100 |     
101 |     def check_sla_breach(self, job_name: str) -> Optional[Dict[str, Any]]:
    |                                                  ^^^^^^^^^^^^^^^^^^^^^^^^
102 |         """
103 |         Check if job breached SLA.
    |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> aws/scripts/utilities/notify_on_sla_breach.py:101:59
    |
 99 |             return {"runtime_minutes": 0, "records_processed": 0}
100 |     
101 |     def check_sla_breach(self, job_name: str) -> Optional[Dict[str, Any]]:
    |                                                           ^^^^
102 |         """
103 |         Check if job breached SLA.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> aws/scripts/utilities/notify_on_sla_breach.py:104:1
    |
102 |         """
103 |         Check if job breached SLA.
104 |         
    | ^^^^^^^^
105 |         Args:
106 |             job_name: Name of the job
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> aws/scripts/utilities/notify_on_sla_breach.py:107:1
    |
105 |         Args:
106 |             job_name: Name of the job
107 |             
    | ^^^^^^^^^^^^
108 |         Returns:
109 |             Alert dictionary if breach detected, None otherwise
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/notify_on_sla_breach.py:114:1
    |
112 |         runtime_minutes = metrics.get("runtime_minutes", 0)
113 |         records_processed = metrics.get("records_processed", 0)
114 |         
    | ^^^^^^^^
115 |         # Get job-specific SLA if available
116 |         job_sla = self.sla_config.get('jobs', {}).get(job_name, {})
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/notify_on_sla_breach.py:119:1
    |
117 |         max_runtime_minutes = job_sla.get('runtime_minutes', self.default_runtime_minutes)
118 |         min_records = job_sla.get('min_records', self.min_records_threshold)
119 |         
    | ^^^^^^^^
120 |         breaches = []
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/notify_on_sla_breach.py:121:1
    |
120 |         breaches = []
121 |         
    | ^^^^^^^^
122 |         if runtime_minutes > max_runtime_minutes:
123 |             breaches.append(f"Runtime {runtime_minutes:.1f} min exceeds SLA {max_runtime_minutes} min")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/notify_on_sla_breach.py:124:1
    |
122 |         if runtime_minutes > max_runtime_minutes:
123 |             breaches.append(f"Runtime {runtime_minutes:.1f} min exceeds SLA {max_runtime_minutes} min")
124 |         
    | ^^^^^^^^
125 |         if records_processed < min_records:
126 |             breaches.append(f"Records processed {records_processed} below threshold {min_records}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/notify_on_sla_breach.py:127:1
    |
125 |         if records_processed < min_records:
126 |             breaches.append(f"Records processed {records_processed} below threshold {min_records}")
127 |         
    | ^^^^^^^^
128 |         if breaches:
129 |             return {
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/notify_on_sla_breach.py:139:1
    |
137 |                 "timestamp": datetime.utcnow().isoformat()
138 |             }
139 |         
    | ^^^^^^^^
140 |         return None
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/notify_on_sla_breach.py:141:1
    |
140 |         return None
141 |     
    | ^^^^
142 |     def send_slack_alert(self, alert: Dict[str, Any]) -> bool:
143 |         """Send alert to Slack webhook."""
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> aws/scripts/utilities/notify_on_sla_breach.py:142:39
    |
140 |         return None
141 |     
142 |     def send_slack_alert(self, alert: Dict[str, Any]) -> bool:
    |                                       ^^^^
143 |         """Send alert to Slack webhook."""
144 |         if not self.slack_webhook:
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/notify_on_sla_breach.py:147:1
    |
145 |             logger.warning("Slack webhook not configured")
146 |             return False
147 |         
    | ^^^^^^^^
148 |         if not boto3:
149 |             logger.warning("boto3 not available, skipping Slack alert")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/notify_on_sla_breach.py:151:1
    |
149 |             logger.warning("boto3 not available, skipping Slack alert")
150 |             return False
151 |         
    | ^^^^^^^^
152 |         try:
153 |             import urllib.request
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
   --> aws/scripts/utilities/notify_on_sla_breach.py:153:13
    |
152 |           try:
153 | /             import urllib.request
154 | |             import urllib.parse
    | |_______________________________^
155 |               
156 |               message = {
    |
help: Organize imports

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/notify_on_sla_breach.py:155:1
    |
153 |             import urllib.request
154 |             import urllib.parse
155 |             
    | ^^^^^^^^^^^^
156 |             message = {
157 |                 "text": f"ðŸš¨ SLA Breach Alert: {alert['job_name']}",
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/notify_on_sla_breach.py:188:1
    |
186 |                 ]
187 |             }
188 |             
    | ^^^^^^^^^^^^
189 |             req = urllib.request.Request(
190 |                 self.slack_webhook,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/notify_on_sla_breach.py:194:1
    |
192 |                 headers={'Content-Type': 'application/json'}
193 |             )
194 |             
    | ^^^^^^^^^^^^
195 |             urllib.request.urlopen(req)
196 |             logger.info(f"Sent Slack alert for {alert['job_name']}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/notify_on_sla_breach.py:198:1
    |
196 |             logger.info(f"Sent Slack alert for {alert['job_name']}")
197 |             return True
198 |             
    | ^^^^^^^^^^^^
199 |         except Exception as e:
200 |             logger.error(f"Failed to send Slack alert: {str(e)}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/notify_on_sla_breach.py:202:1
    |
200 |             logger.error(f"Failed to send Slack alert: {str(e)}")
201 |             return False
202 |     
    | ^^^^
203 |     def send_sns_alert(self, alert: Dict[str, Any]) -> bool:
204 |         """Send alert to SNS topic."""
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> aws/scripts/utilities/notify_on_sla_breach.py:203:37
    |
201 |             return False
202 |     
203 |     def send_sns_alert(self, alert: Dict[str, Any]) -> bool:
    |                                     ^^^^
204 |         """Send alert to SNS topic."""
205 |         if not self.sns_topic_arn or not self.sns:
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/notify_on_sla_breach.py:208:1
    |
206 |             logger.warning("SNS not configured")
207 |             return False
208 |         
    | ^^^^^^^^
209 |         try:
210 |             subject = f"SLA Breach: {alert['job_name']}"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/notify_on_sla_breach.py:212:1
    |
210 |             subject = f"SLA Breach: {alert['job_name']}"
211 |             message = json.dumps(alert, indent=2)
212 |             
    | ^^^^^^^^^^^^
213 |             self.sns.publish(
214 |                 TopicArn=self.sns_topic_arn,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/notify_on_sla_breach.py:218:1
    |
216 |                 Message=message
217 |             )
218 |             
    | ^^^^^^^^^^^^
219 |             logger.info(f"Sent SNS alert for {alert['job_name']}")
220 |             return True
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/notify_on_sla_breach.py:221:1
    |
219 |             logger.info(f"Sent SNS alert for {alert['job_name']}")
220 |             return True
221 |             
    | ^^^^^^^^^^^^
222 |         except Exception as e:
223 |             logger.error(f"Failed to send SNS alert: {str(e)}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/notify_on_sla_breach.py:225:1
    |
223 |             logger.error(f"Failed to send SNS alert: {str(e)}")
224 |             return False
225 |     
    | ^^^^
226 |     def check_and_alert(self, job_name: str) -> bool:
227 |         """
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> aws/scripts/utilities/notify_on_sla_breach.py:229:1
    |
227 |         """
228 |         Check SLA for job and send alerts if breach detected.
229 |         
    | ^^^^^^^^
230 |         Args:
231 |             job_name: Name of the job
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> aws/scripts/utilities/notify_on_sla_breach.py:232:1
    |
230 |         Args:
231 |             job_name: Name of the job
232 |             
    | ^^^^^^^^^^^^
233 |         Returns:
234 |             True if breach detected, False otherwise
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/notify_on_sla_breach.py:237:1
    |
235 |         """
236 |         alert = self.check_sla_breach(job_name)
237 |         
    | ^^^^^^^^
238 |         if alert:
239 |             logger.warning(f"SLA breach detected for {job_name}: {alert['breaches']}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/notify_on_sla_breach.py:240:1
    |
238 |         if alert:
239 |             logger.warning(f"SLA breach detected for {job_name}: {alert['breaches']}")
240 |             
    | ^^^^^^^^^^^^
241 |             # Send alerts
242 |             slack_sent = self.send_slack_alert(alert)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/notify_on_sla_breach.py:244:1
    |
242 |             slack_sent = self.send_slack_alert(alert)
243 |             sns_sent = self.send_sns_alert(alert)
244 |             
    | ^^^^^^^^^^^^
245 |             if slack_sent or sns_sent:
246 |                 return True
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/notify_on_sla_breach.py:250:1
    |
248 |                 logger.error("Failed to send any alerts")
249 |                 return True
250 |         
    | ^^^^^^^^
251 |         logger.info(f"No SLA breach for {job_name}")
252 |         return False
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/notify_on_sla_breach.py:258:1
    |
256 |     """Main entry point."""
257 |     import argparse
258 |     
    | ^^^^
259 |     parser = argparse.ArgumentParser(description="Check SLA and send alerts")
260 |     parser.add_argument("--config", required=True, help="Configuration file")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/notify_on_sla_breach.py:263:1
    |
261 |     parser.add_argument("--job-name", required=True, help="Job name to check")
262 |     args = parser.parse_args()
263 |     
    | ^^^^
264 |     # Load config
265 |     try:
    |
help: Remove whitespace from blank line

UP015 [*] Unnecessary mode argument
   --> aws/scripts/utilities/notify_on_sla_breach.py:267:32
    |
265 |     try:
266 |         import yaml
267 |         with open(args.config, 'r') as f:
    |                                ^^^
268 |             config = yaml.safe_load(f)
269 |     except Exception as e:
    |
help: Remove mode argument

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/notify_on_sla_breach.py:272:1
    |
270 |         logger.error(f"Failed to load config: {str(e)}")
271 |         sys.exit(1)
272 |     
    | ^^^^
273 |     checker = SLAChecker(config)
274 |     breach_detected = checker.check_and_alert(args.job_name)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/notify_on_sla_breach.py:275:1
    |
273 |     checker = SLAChecker(config)
274 |     breach_detected = checker.check_and_alert(args.job_name)
275 |     
    | ^^^^
276 |     sys.exit(1 if breach_detected else 0)
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> aws/scripts/utilities/register_glue_tables.py:7:1
   |
 5 |   """
 6 |
 7 | / import os
 8 | | import sys
 9 | | import logging
10 | | import boto3
11 | | from typing import List, Dict, Any
12 | | from pathlib import Path
   | |________________________^
13 |
14 |   # Standardized environment variables for Glue/Athena alignment
   |
help: Organize imports

UP035 `typing.List` is deprecated, use `list` instead
  --> aws/scripts/utilities/register_glue_tables.py:11:1
   |
 9 | import logging
10 | import boto3
11 | from typing import List, Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
12 | from pathlib import Path
   |

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> aws/scripts/utilities/register_glue_tables.py:11:1
   |
 9 | import logging
10 | import boto3
11 | from typing import List, Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
12 | from pathlib import Path
   |

F401 [*] `typing.Dict` imported but unused
  --> aws/scripts/utilities/register_glue_tables.py:11:26
   |
 9 | import logging
10 | import boto3
11 | from typing import List, Dict, Any
   |                          ^^^^
12 | from pathlib import Path
   |
help: Remove unused import

F401 [*] `typing.Any` imported but unused
  --> aws/scripts/utilities/register_glue_tables.py:11:32
   |
 9 | import logging
10 | import boto3
11 | from typing import List, Dict, Any
   |                                ^^^
12 | from pathlib import Path
   |
help: Remove unused import

E402 Module level import not at top of file
  --> aws/scripts/utilities/register_glue_tables.py:22:1
   |
20 | sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))
21 |
22 | from project_a.utils.spark import get_spark_session
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
23 | from project_a.utils.config import load_conf
   |

I001 [*] Import block is un-sorted or un-formatted
  --> aws/scripts/utilities/register_glue_tables.py:22:1
   |
20 |   sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))
21 |
22 | / from project_a.utils.spark import get_spark_session
23 | | from project_a.utils.config import load_conf
   | |____________________________________________^
24 |
25 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

E402 Module level import not at top of file
  --> aws/scripts/utilities/register_glue_tables.py:23:1
   |
22 | from project_a.utils.spark import get_spark_session
23 | from project_a.utils.config import load_conf
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
24 |
25 | logger = logging.getLogger(__name__)
   |

W293 Blank line contains whitespace
  --> aws/scripts/utilities/register_glue_tables.py:37:1
   |
35 |     """
36 |     Register a Delta table in Glue Data Catalog.
37 |     
   | ^^^^
38 |     Args:
39 |         spark: Spark session
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/utilities/register_glue_tables.py:46:1
   |
44 |     """
45 |     logger.info(f"Registering table {database_name}.{table_name} at {s3_location}")
46 |     
   | ^^^^
47 |     # Create external table SQL
48 |     create_table_sql = f"""
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/utilities/register_glue_tables.py:54:1
   |
52 |     TBLPROPERTIES ('table_type'='{table_type}')
53 |     """
54 |     
   | ^^^^
55 |     try:
56 |         spark.sql(create_table_sql)
   |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
  --> aws/scripts/utilities/register_glue_tables.py:68:6
   |
66 |     database_name: str,
67 |     layer: str
68 | ) -> List[str]:
   |      ^^^^
69 |     """
70 |     Scan S3 location and register all Delta tables found.
   |
help: Replace with `list`

W293 Blank line contains whitespace
  --> aws/scripts/utilities/register_glue_tables.py:71:1
   |
69 |     """
70 |     Scan S3 location and register all Delta tables found.
71 |     
   | ^^^^
72 |     Args:
73 |         spark: Spark session
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> aws/scripts/utilities/register_glue_tables.py:77:1
   |
75 |         database_name: Glue database name
76 |         layer: Data layer (bronze, silver, gold)
77 |         
   | ^^^^^^^^
78 |     Returns:
79 |         List of registered table names
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/utilities/register_glue_tables.py:83:1
   |
81 |     registered_tables = []
82 |     s3_prefix = f"{lake_root}/{layer}/"
83 |     
   | ^^^^
84 |     logger.info(f"Scanning for Delta tables in {s3_prefix}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/utilities/register_glue_tables.py:85:1
   |
84 |     logger.info(f"Scanning for Delta tables in {s3_prefix}")
85 |     
   | ^^^^
86 |     # List all directories in the layer using S3 client
87 |     try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/utilities/register_glue_tables.py:91:1
   |
89 |         bucket_name = s3_prefix.replace('s3://', '').split('/')[0]
90 |         prefix = '/'.join(s3_prefix.replace('s3://', '').split('/')[1:])
91 |         
   | ^^^^^^^^
92 |         response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix, Delimiter='/')
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/utilities/register_glue_tables.py:93:1
   |
92 |         response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix, Delimiter='/')
93 |         
   | ^^^^^^^^
94 |         for obj in response.get('CommonPrefixes', []):
95 |             table_name = obj['Prefix'].replace(prefix, '').rstrip('/')
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/utilities/register_glue_tables.py:97:1
   |
95 |             table_name = obj['Prefix'].replace(prefix, '').rstrip('/')
96 |             table_location = f"{s3_prefix}{table_name}"
97 |             
   | ^^^^^^^^^^^^
98 |             # Check if it's a Delta table by looking for _delta_log
99 |             try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/register_glue_tables.py:103:1
    |
101 |                 test_df = spark.read.format("delta").load(table_location)
102 |                 test_df.limit(1).collect()  # Force evaluation
103 |                 
    | ^^^^^^^^^^^^^^^^
104 |                 # Register the table
105 |                 register_delta_table(spark, database_name, table_name, table_location)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/register_glue_tables.py:107:1
    |
105 |                 register_delta_table(spark, database_name, table_name, table_location)
106 |                 registered_tables.append(table_name)
107 |                 
    | ^^^^^^^^^^^^^^^^
108 |             except Exception as e:
109 |                 logger.warning(f"Skipping {table_location} - not a valid Delta table: {e}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/register_glue_tables.py:111:1
    |
109 |                 logger.warning(f"Skipping {table_location} - not a valid Delta table: {e}")
110 |                 continue
111 |                 
    | ^^^^^^^^^^^^^^^^
112 |     except Exception as e:
113 |         logger.error(f"Failed to scan directory {s3_prefix}: {e}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/register_glue_tables.py:115:1
    |
113 |         logger.error(f"Failed to scan directory {s3_prefix}: {e}")
114 |         raise
115 |     
    | ^^^^
116 |     return registered_tables
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> aws/scripts/utilities/register_glue_tables.py:122:1
    |
120 |     """
121 |     Create Glue database if it doesn't exist.
122 |     
    | ^^^^
123 |     Args:
124 |         spark: Spark session
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/register_glue_tables.py:128:1
    |
126 |     """
127 |     logger.info(f"Creating database {database_name} if it doesn't exist")
128 |     
    | ^^^^
129 |     create_db_sql = f"""
130 |     CREATE DATABASE IF NOT EXISTS {database_name}
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/register_glue_tables.py:133:1
    |
131 |     COMMENT 'Data lake {database_name} layer'
132 |     """
133 |     
    | ^^^^
134 |     try:
135 |         spark.sql(create_db_sql)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/register_glue_tables.py:145:1
    |
143 |     """Main function to register Glue tables."""
144 |     import argparse
145 |     
    | ^^^^
146 |     parser = argparse.ArgumentParser(description="Register Delta tables in Glue Data Catalog")
147 |     parser.add_argument("--lake-root", required=True, help="S3 root path of the data lake")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/register_glue_tables.py:153:1
    |
151 |     parser.add_argument("--config", help="Configuration file path")
152 |     args = parser.parse_args()
153 |     
    | ^^^^
154 |     # Setup logging
155 |     log_level = os.getenv("LOG_LEVEL", "INFO")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/register_glue_tables.py:160:1
    |
158 |         format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
159 |     )
160 |     
    | ^^^^
161 |     try:
162 |         # Load configuration if provided
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/register_glue_tables.py:168:1
    |
166 |         else:
167 |             lake_root = args.lake_root
168 |         
    | ^^^^^^^^
169 |         # Create Spark session
170 |         spark = get_spark_session("GlueTableRegistration")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/register_glue_tables.py:171:1
    |
169 |         # Create Spark session
170 |         spark = get_spark_session("GlueTableRegistration")
171 |         
    | ^^^^^^^^
172 |         # Create database if it doesn't exist
173 |         create_database_if_not_exists(spark, args.database)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/register_glue_tables.py:174:1
    |
172 |         # Create database if it doesn't exist
173 |         create_database_if_not_exists(spark, args.database)
174 |         
    | ^^^^^^^^
175 |         if args.table:
176 |             # Register specific table
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/register_glue_tables.py:185:1
    |
183 |                 spark, lake_root, args.database, args.layer
184 |             )
185 |         
    | ^^^^^^^^
186 |         logger.info(f"Successfully registered {len(registered_tables)} tables:")
187 |         for table in registered_tables:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/register_glue_tables.py:189:1
    |
187 |         for table in registered_tables:
188 |             logger.info(f"  - {args.database}.{table}")
189 |         
    | ^^^^^^^^
190 |         print(f"âœ… Registered {len(registered_tables)} tables in {args.database}.{args.layer}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/register_glue_tables.py:191:1
    |
190 |         print(f"âœ… Registered {len(registered_tables)} tables in {args.database}.{args.layer}")
191 |         
    | ^^^^^^^^
192 |     except Exception as e:
193 |         logger.error(f"Table registration failed: {e}")
    |
help: Remove whitespace from blank line

W292 [*] No newline at end of file
   --> aws/scripts/utilities/register_glue_tables.py:198:11
    |
197 | if __name__ == "__main__":
198 |     main()
    |           ^
    |
help: Add trailing newline

I001 [*] Import block is un-sorted or un-formatted
  --> aws/scripts/utilities/run_ge_checks.py:7:1
   |
 5 |   """
 6 |
 7 | / import os
 8 | | import sys
 9 | | import json
10 | | import logging
11 | | import yaml
12 | | from typing import Dict, List, Any
13 | | from pathlib import Path
14 | | from datetime import datetime
   | |_____________________________^
15 |
16 |   # Add src to path
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> aws/scripts/utilities/run_ge_checks.py:12:1
   |
10 | import logging
11 | import yaml
12 | from typing import Dict, List, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
13 | from pathlib import Path
14 | from datetime import datetime
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> aws/scripts/utilities/run_ge_checks.py:12:1
   |
10 | import logging
11 | import yaml
12 | from typing import Dict, List, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
13 | from pathlib import Path
14 | from datetime import datetime
   |

F401 [*] `typing.List` imported but unused
  --> aws/scripts/utilities/run_ge_checks.py:12:26
   |
10 | import logging
11 | import yaml
12 | from typing import Dict, List, Any
   |                          ^^^^
13 | from pathlib import Path
14 | from datetime import datetime
   |
help: Remove unused import: `typing.List`

I001 [*] Import block is un-sorted or un-formatted
  --> aws/scripts/utilities/run_ge_checks.py:19:1
   |
17 |   sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))
18 |
19 | / from project_a.utils.spark import get_spark_session
20 | | from project_a.utils.config import load_conf
   | |____________________________________________^
21 |
22 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> aws/scripts/utilities/run_ge_checks.py:25:48
   |
25 | def load_expectation_suite(suite_path: str) -> Dict[str, Any]:
   |                                                ^^^^
26 |     """
27 |     Load expectation suite from YAML file.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> aws/scripts/utilities/run_ge_checks.py:28:1
   |
26 |     """
27 |     Load expectation suite from YAML file.
28 |     
   | ^^^^
29 |     Args:
30 |         suite_path: Path to the expectation suite YAML file
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> aws/scripts/utilities/run_ge_checks.py:31:1
   |
29 |     Args:
30 |         suite_path: Path to the expectation suite YAML file
31 |         
   | ^^^^^^^^
32 |     Returns:
33 |         Expectation suite dictionary
   |
help: Remove whitespace from blank line

UP015 [*] Unnecessary mode argument
  --> aws/scripts/utilities/run_ge_checks.py:35:27
   |
33 |         Expectation suite dictionary
34 |     """
35 |     with open(suite_path, 'r') as f:
   |                           ^^^
36 |         return yaml.safe_load(f)
   |
help: Remove mode argument

W293 Blank line contains whitespace
  --> aws/scripts/utilities/run_ge_checks.py:42:1
   |
40 |     """
41 |     Run YAML-based data quality policy checks.
42 |     
   | ^^^^
43 |     Args:
44 |         df: Spark DataFrame to check
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> aws/scripts/utilities/run_ge_checks.py:47:1
   |
45 |         policy: YAML policy configuration
46 |         key_cols: Optional list of key columns (if not provided, uses policy.get("unique"))
47 |         
   | ^^^^^^^^
48 |     Returns:
49 |         Results dictionary with check outcomes
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/utilities/run_ge_checks.py:53:1
   |
51 |     # Use key_cols if provided, else try policy.get("unique")
52 |     uniq = key_cols or policy.get("unique") or []
53 |     
   | ^^^^
54 |     results = {
55 |         "timestamp": datetime.now().isoformat(),
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/utilities/run_ge_checks.py:64:1
   |
62 |         }
63 |     }
64 |     
   | ^^^^
65 |     # Run uniqueness checks if specified
66 |     if uniq:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> aws/scripts/utilities/run_ge_checks.py:88:1
   |
86 |                     results["summary"]["critical_failures"] += 1
87 |                 results["summary"]["total_checks"] += 1
88 |     
   | ^^^^
89 |     # Run null checks
90 |     for col in policy.get("not_null", []):
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:110:1
    |
108 |                 results["summary"]["critical_failures"] += 1
109 |             results["summary"]["total_checks"] += 1
110 |     
    | ^^^^
111 |     # Run range checks
112 |     for col, range_config in policy.get("range", {}).items():
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:116:1
    |
114 |             min_val = range_config.get("min")
115 |             max_val = range_config.get("max")
116 |             
    | ^^^^^^^^^^^^
117 |             condition = df[col].isNotNull()
118 |             if min_val is not None:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:122:1
    |
120 |             if max_val is not None:
121 |                 condition = condition & (df[col] <= max_val)
122 |                 
    | ^^^^^^^^^^^^^^^^
123 |             invalid_count = df.filter(~condition).count()
124 |             if invalid_count == 0:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:140:1
    |
138 |                 results["summary"]["failed"] += 1
139 |             results["summary"]["total_checks"] += 1
140 |     
    | ^^^^
141 |     return results
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> aws/scripts/utilities/run_ge_checks.py:147:12
    |
145 |     spark,
146 |     table_location: str,
147 |     suite: Dict[str, Any],
    |            ^^^^
148 |     table_name: str
149 | ) -> Dict[str, Any]:
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> aws/scripts/utilities/run_ge_checks.py:149:6
    |
147 |     suite: Dict[str, Any],
148 |     table_name: str
149 | ) -> Dict[str, Any]:
    |      ^^^^
150 |     """
151 |     Run expectation checks on a table.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:152:1
    |
150 |     """
151 |     Run expectation checks on a table.
152 |     
    | ^^^^
153 |     Args:
154 |         spark: Spark session
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:158:1
    |
156 |         suite: Expectation suite configuration
157 |         table_name: Name of the table being checked
158 |         
    | ^^^^^^^^
159 |     Returns:
160 |         Results dictionary with check outcomes
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:163:1
    |
161 |     """
162 |     logger.info(f"Running DQ checks on {table_name} at {table_location}")
163 |     
    | ^^^^
164 |     # Read the Delta table
165 |     df = spark.read.format("delta").load(table_location)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:166:1
    |
164 |     # Read the Delta table
165 |     df = spark.read.format("delta").load(table_location)
166 |     
    | ^^^^
167 |     results = {
168 |         "table_name": table_name,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:179:1
    |
177 |         }
178 |     }
179 |     
    | ^^^^
180 |     # Run each expectation
181 |     for expectation in suite.get("expectations", []):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:185:1
    |
183 |         results["checks"].append(check_result)
184 |         results["summary"]["total_checks"] += 1
185 |         
    | ^^^^^^^^
186 |         if check_result["passed"]:
187 |             results["summary"]["passed"] += 1
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:192:1
    |
190 |             if check_result.get("severity") == "critical":
191 |                 results["summary"]["critical_failures"] += 1
192 |     
    | ^^^^
193 |     return results
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> aws/scripts/utilities/run_ge_checks.py:196:45
    |
196 | def run_single_expectation(df, expectation: Dict[str, Any], table_name: str) -> Dict[str, Any]:
    |                                             ^^^^
197 |     """
198 |     Run a single expectation check.
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> aws/scripts/utilities/run_ge_checks.py:196:81
    |
196 | def run_single_expectation(df, expectation: Dict[str, Any], table_name: str) -> Dict[str, Any]:
    |                                                                                 ^^^^
197 |     """
198 |     Run a single expectation check.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:199:1
    |
197 |     """
198 |     Run a single expectation check.
199 |     
    | ^^^^
200 |     Args:
201 |         df: Spark DataFrame
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:204:1
    |
202 |         expectation: Expectation configuration
203 |         table_name: Name of the table being checked
204 |         
    | ^^^^^^^^
205 |     Returns:
206 |         Check result dictionary
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:212:1
    |
210 |     meta = expectation.get("meta", {})
211 |     severity = meta.get("severity", "warning")
212 |     
    | ^^^^
213 |     check_result = {
214 |         "expectation_type": expectation_type,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:221:1
    |
219 |         "error": None
220 |     }
221 |     
    | ^^^^
222 |     try:
223 |         if expectation_type == "expect_column_to_exist":
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:229:1
    |
227 |             else:
228 |                 check_result["error"] = f"Column '{column}' does not exist"
229 |                 
    | ^^^^^^^^^^^^^^^^
230 |         elif expectation_type == "expect_column_values_to_not_be_null":
231 |             column = kwargs["column"]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:237:1
    |
235 |             else:
236 |                 check_result["error"] = f"Column '{column}' has {null_count} null values"
237 |                 
    | ^^^^^^^^^^^^^^^^
238 |         elif expectation_type == "expect_column_values_to_be_unique":
239 |             column = kwargs["column"]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:246:1
    |
244 |             else:
245 |                 check_result["error"] = f"Column '{column}' has {total_count - unique_count} duplicate values"
246 |                 
    | ^^^^^^^^^^^^^^^^
247 |         elif expectation_type == "expect_compound_columns_to_be_unique":
248 |             columns = kwargs["column_list"]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:255:1
    |
253 |             else:
254 |                 check_result["error"] = f"Compound columns {columns} have {total_count - unique_count} duplicate values"
255 |                 
    | ^^^^^^^^^^^^^^^^
256 |         elif expectation_type == "expect_column_values_to_be_between":
257 |             column = kwargs["column"]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:260:1
    |
258 |             min_value = kwargs.get("min_value")
259 |             max_value = kwargs.get("max_value")
260 |             
    | ^^^^^^^^^^^^
261 |             # Count values outside range
262 |             condition = df[column].isNotNull()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:267:1
    |
265 |             if max_value is not None:
266 |                 condition = condition & (df[column] <= max_value)
267 |                 
    | ^^^^^^^^^^^^^^^^
268 |             invalid_count = df.filter(~condition).count()
269 |             if invalid_count == 0:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:273:1
    |
271 |             else:
272 |                 check_result["error"] = f"Column '{column}' has {invalid_count} values outside range [{min_value}, {max_value}]"
273 |                 
    | ^^^^^^^^^^^^^^^^
274 |         elif expectation_type == "expect_column_values_to_be_of_type":
275 |             column = kwargs["column"]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:278:1
    |
276 |             expected_type = kwargs["type_"]
277 |             actual_type = str(df.schema[column].dataType)
278 |             
    | ^^^^^^^^^^^^
279 |             # Map Spark types to expected types
280 |             type_mapping = {
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:287:1
    |
285 |                 "timestamp": "TimestampType"
286 |             }
287 |             
    | ^^^^^^^^^^^^
288 |             expected_spark_type = type_mapping.get(expected_type, expected_type)
289 |             if expected_spark_type in actual_type:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:293:1
    |
291 |             else:
292 |                 check_result["error"] = f"Column '{column}' has type {actual_type}, expected {expected_type}"
293 |                 
    | ^^^^^^^^^^^^^^^^
294 |         else:
295 |             check_result["error"] = f"Unsupported expectation type: {expectation_type}"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:296:1
    |
294 |         else:
295 |             check_result["error"] = f"Unsupported expectation type: {expectation_type}"
296 |             
    | ^^^^^^^^^^^^
297 |     except Exception as e:
298 |         check_result["error"] = f"Error running expectation: {str(e)}"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:299:1
    |
297 |     except Exception as e:
298 |         check_result["error"] = f"Error running expectation: {str(e)}"
299 |     
    | ^^^^
300 |     return check_result
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> aws/scripts/utilities/run_ge_checks.py:303:34
    |
303 | def write_results_to_s3(results: Dict[str, Any], s3_path: str) -> None:
    |                                  ^^^^
304 |     """
305 |     Write DQ results to S3.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:306:1
    |
304 |     """
305 |     Write DQ results to S3.
306 |     
    | ^^^^
307 |     Args:
308 |         results: DQ results dictionary
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:312:1
    |
310 |     """
311 |     import boto3
312 |     
    | ^^^^
313 |     s3_client = boto3.client('s3')
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:314:1
    |
313 |     s3_client = boto3.client('s3')
314 |     
    | ^^^^
315 |     # Parse S3 path
316 |     if s3_path.startswith('s3://'):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:318:1
    |
316 |     if s3_path.startswith('s3://'):
317 |         s3_path = s3_path[5:]
318 |     
    | ^^^^
319 |     bucket, key = s3_path.split('/', 1)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:320:1
    |
319 |     bucket, key = s3_path.split('/', 1)
320 |     
    | ^^^^
321 |     # Generate filename with timestamp
322 |     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:325:1
    |
323 |     table_name = results["table_name"]
324 |     filename = f"dq_results_{table_name}_{timestamp}.json"
325 |     
    | ^^^^
326 |     full_key = f"{key}/{filename}"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:327:1
    |
326 |     full_key = f"{key}/{filename}"
327 |     
    | ^^^^
328 |     # Write results as JSON
329 |     results_json = json.dumps(results, indent=2)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:330:1
    |
328 |     # Write results as JSON
329 |     results_json = json.dumps(results, indent=2)
330 |     
    | ^^^^
331 |     try:
332 |         s3_client.put_object(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:347:1
    |
345 |     """Main function to run DQ checks."""
346 |     import argparse
347 |     
    | ^^^^
348 |     parser = argparse.ArgumentParser(description="Run Great Expectations data quality checks")
349 |     parser.add_argument("--lake-root", required=True, help="S3 root path of the data lake")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:356:1
    |
354 |     parser.add_argument("--config", help="Configuration file path")
355 |     args = parser.parse_args()
356 |     
    | ^^^^
357 |     # Setup logging
358 |     log_level = os.getenv("LOG_LEVEL", "INFO")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:363:1
    |
361 |         format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
362 |     )
363 |     
    | ^^^^
364 |     try:
365 |         # Load configuration if provided
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:371:1
    |
369 |         else:
370 |             lake_root = args.lake_root
371 |         
    | ^^^^^^^^
372 |         # Load expectation suite
373 |         suite = load_expectation_suite(args.suite)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:374:1
    |
372 |         # Load expectation suite
373 |         suite = load_expectation_suite(args.suite)
374 |         
    | ^^^^^^^^
375 |         # Create Spark session
376 |         spark = get_spark_session("DataQualityChecks")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:377:1
    |
375 |         # Create Spark session
376 |         spark = get_spark_session("DataQualityChecks")
377 |         
    | ^^^^^^^^
378 |         # Build table location
379 |         table_location = f"{lake_root}/{args.layer}/{args.table}"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:380:1
    |
378 |         # Build table location
379 |         table_location = f"{lake_root}/{args.layer}/{args.table}"
380 |         
    | ^^^^^^^^
381 |         # Run DQ checks
382 |         results = run_expectation_checks(spark, table_location, suite, args.table)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:383:1
    |
381 |         # Run DQ checks
382 |         results = run_expectation_checks(spark, table_location, suite, args.table)
383 |         
    | ^^^^^^^^
384 |         # Write results to S3
385 |         results_path = f"s3://{args.lake_bucket}/gold/quality/{args.layer}"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:387:1
    |
385 |         results_path = f"s3://{args.lake_bucket}/gold/quality/{args.layer}"
386 |         write_results_to_s3(results, results_path)
387 |         
    | ^^^^^^^^
388 |         # Print summary
389 |         summary = results["summary"]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:395:1
    |
393 |         print(f"   Failed: {summary['failed']}")
394 |         print(f"   Critical failures: {summary['critical_failures']}")
395 |         
    | ^^^^^^^^
396 |         # Exit with error code if critical failures
397 |         if summary["critical_failures"] > 0:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> aws/scripts/utilities/run_ge_checks.py:402:1
    |
400 |         else:
401 |             print("âœ… All critical checks passed!")
402 |         
    | ^^^^^^^^
403 |     except Exception as e:
404 |         logger.error(f"DQ checks failed: {e}")
    |
help: Remove whitespace from blank line

W292 [*] No newline at end of file
   --> aws/scripts/utilities/run_ge_checks.py:409:11
    |
408 | if __name__ == "__main__":
409 |     main()
    |           ^
    |
help: Add trailing newline

I001 [*] Import block is un-sorted or un-formatted
 --> jobs/dq/dq_gate.py:6:1
  |
4 |   Fails pipeline on critical violations
5 |   """
6 | / import sys
7 | | import argparse
8 | | from pathlib import Path
  | |________________________^
9 |   sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))
  |
help: Organize imports

I001 [*] Import block is un-sorted or un-formatted
  --> jobs/dq/dq_gate.py:11:1
   |
 9 |   sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))
10 |
11 | / from pyspark.sql import SparkSession
12 | | from project_a.utils.spark_session import build_spark
13 | | from project_a.config_loader import load_config_resolved
14 | | from project_a.dq.gate import DQGate
   | |____________________________________^
   |
help: Organize imports

F401 [*] `pyspark.sql.SparkSession` imported but unused
  --> jobs/dq/dq_gate.py:11:25
   |
 9 | sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))
10 |
11 | from pyspark.sql import SparkSession
   |                         ^^^^^^^^^^^^
12 | from project_a.utils.spark_session import build_spark
13 | from project_a.config_loader import load_config_resolved
   |
help: Remove unused import: `pyspark.sql.SparkSession`

W293 [*] Blank line contains whitespace
  --> jobs/dq/dq_gate.py:23:1
   |
21 |     parser.add_argument("--layer", required=True, help="Layer (bronze/silver/gold)")
22 |     args = parser.parse_args()
23 |     
   | ^^^^
24 |     # Load config
25 |     config_path = Path("config/dev.yaml")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/dq/dq_gate.py:29:1
   |
27 |         config_path = Path("config/prod.yaml")
28 |     config = load_config_resolved(str(config_path))
29 |     
   | ^^^^
30 |     # Build Spark session
31 |     spark = build_spark(config)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/dq/dq_gate.py:32:1
   |
30 |     # Build Spark session
31 |     spark = build_spark(config)
32 |     
   | ^^^^
33 |     try:
34 |         lake_bucket = config["buckets"]["lake"]
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/dq/dq_gate.py:35:1
   |
33 |     try:
34 |         lake_bucket = config["buckets"]["lake"]
35 |         
   | ^^^^^^^^
36 |         # Read table
37 |         table_path = f"s3a://{lake_bucket}/{args.layer}/{args.table}/"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/dq/dq_gate.py:39:1
   |
37 |         table_path = f"s3a://{lake_bucket}/{args.layer}/{args.table}/"
38 |         df = spark.read.format("delta").load(table_path)
39 |         
   | ^^^^^^^^
40 |         # Run DQ gate (pass config)
41 |         gate = DQGate(config)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/dq/dq_gate.py:43:1
   |
41 |         gate = DQGate(config)
42 |         result = gate.check_and_block(spark, df, args.table, layer=args.layer)
43 |         
   | ^^^^^^^^
44 |         print(f"âœ… DQ Gate passed for {args.layer}.{args.table}")
45 |         print(f"  - Critical failures: {result.get('critical_failures', 0)}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/dq/dq_gate.py:48:1
   |
46 |         print(f"  - Warnings: {result.get('warnings', 0)}")
47 |         sys.exit(0)
48 |             
   | ^^^^^^^^^^^^
49 |     except Exception as e:
50 |         print(f"âŒ DQ Gate error: {e}")
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> jobs/dq/run_comprehensive_dq.py:17:1
   |
15 |   10. Performance optimization
16 |   """
17 | / import sys
18 | | import argparse
19 | | import logging
20 | | from pathlib import Path
21 | | from typing import Dict, Any
   | |____________________________^
22 |
23 |   # Add src to path
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> jobs/dq/run_comprehensive_dq.py:21:1
   |
19 | import logging
20 | from pathlib import Path
21 | from typing import Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
22 |
23 | # Add src to path
   |

E402 Module level import not at top of file
  --> jobs/dq/run_comprehensive_dq.py:29:1
   |
27 |     sys.path.insert(0, str(src_path))
28 |
29 | from pyspark.sql import SparkSession
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
30 | from project_a.utils.spark_session import build_spark
31 | from project_a.config_loader import load_config_resolved
   |

I001 [*] Import block is un-sorted or un-formatted
  --> jobs/dq/run_comprehensive_dq.py:29:1
   |
27 |       sys.path.insert(0, str(src_path))
28 |
29 | / from pyspark.sql import SparkSession
30 | | from project_a.utils.spark_session import build_spark
31 | | from project_a.config_loader import load_config_resolved
32 | | from project_a.utils.path_resolver import resolve_data_path
33 | | from project_a.dq.comprehensive_validator import ComprehensiveValidator
34 | | from project_a.schemas.bronze_schemas import (
35 | |     CRM_ACCOUNTS_SCHEMA, CRM_CONTACTS_SCHEMA, CRM_OPPORTUNITIES_SCHEMA,
36 | |     REDSHIFT_BEHAVIOR_SCHEMA, SNOWFLAKE_CUSTOMERS_SCHEMA,
37 | |     SNOWFLAKE_ORDERS_SCHEMA, SNOWFLAKE_PRODUCTS_SCHEMA,
38 | |     KAFKA_EVENTS_SCHEMA, FX_RATES_SCHEMA
39 | | )
   | |_^
40 |
41 |   logging.basicConfig(level=logging.INFO)
   |
help: Organize imports

E402 Module level import not at top of file
  --> jobs/dq/run_comprehensive_dq.py:30:1
   |
29 | from pyspark.sql import SparkSession
30 | from project_a.utils.spark_session import build_spark
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
31 | from project_a.config_loader import load_config_resolved
32 | from project_a.utils.path_resolver import resolve_data_path
   |

E402 Module level import not at top of file
  --> jobs/dq/run_comprehensive_dq.py:31:1
   |
29 | from pyspark.sql import SparkSession
30 | from project_a.utils.spark_session import build_spark
31 | from project_a.config_loader import load_config_resolved
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
32 | from project_a.utils.path_resolver import resolve_data_path
33 | from project_a.dq.comprehensive_validator import ComprehensiveValidator
   |

E402 Module level import not at top of file
  --> jobs/dq/run_comprehensive_dq.py:32:1
   |
30 | from project_a.utils.spark_session import build_spark
31 | from project_a.config_loader import load_config_resolved
32 | from project_a.utils.path_resolver import resolve_data_path
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
33 | from project_a.dq.comprehensive_validator import ComprehensiveValidator
34 | from project_a.schemas.bronze_schemas import (
   |

E402 Module level import not at top of file
  --> jobs/dq/run_comprehensive_dq.py:33:1
   |
31 | from project_a.config_loader import load_config_resolved
32 | from project_a.utils.path_resolver import resolve_data_path
33 | from project_a.dq.comprehensive_validator import ComprehensiveValidator
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
34 | from project_a.schemas.bronze_schemas import (
35 |     CRM_ACCOUNTS_SCHEMA, CRM_CONTACTS_SCHEMA, CRM_OPPORTUNITIES_SCHEMA,
   |

E402 Module level import not at top of file
  --> jobs/dq/run_comprehensive_dq.py:34:1
   |
32 |   from project_a.utils.path_resolver import resolve_data_path
33 |   from project_a.dq.comprehensive_validator import ComprehensiveValidator
34 | / from project_a.schemas.bronze_schemas import (
35 | |     CRM_ACCOUNTS_SCHEMA, CRM_CONTACTS_SCHEMA, CRM_OPPORTUNITIES_SCHEMA,
36 | |     REDSHIFT_BEHAVIOR_SCHEMA, SNOWFLAKE_CUSTOMERS_SCHEMA,
37 | |     SNOWFLAKE_ORDERS_SCHEMA, SNOWFLAKE_PRODUCTS_SCHEMA,
38 | |     KAFKA_EVENTS_SCHEMA, FX_RATES_SCHEMA
39 | | )
   | |_^
40 |
41 |   logging.basicConfig(level=logging.INFO)
   |

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> jobs/dq/run_comprehensive_dq.py:45:53
   |
45 | def load_bronze_tables(spark: SparkSession, config: Dict[str, Any]) -> Dict[str, Any]:
   |                                                     ^^^^
46 |     """Load all bronze tables."""
47 |     bronze_root = resolve_data_path(config, "bronze")
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> jobs/dq/run_comprehensive_dq.py:45:72
   |
45 | def load_bronze_tables(spark: SparkSession, config: Dict[str, Any]) -> Dict[str, Any]:
   |                                                                        ^^^^
46 |     """Load all bronze tables."""
47 |     bronze_root = resolve_data_path(config, "bronze")
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> jobs/dq/run_comprehensive_dq.py:49:1
   |
47 |     bronze_root = resolve_data_path(config, "bronze")
48 |     sources = config.get("sources", {})
49 |     
   | ^^^^
50 |     bronze_data = {}
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/dq/run_comprehensive_dq.py:51:1
   |
50 |     bronze_data = {}
51 |     
   | ^^^^
52 |     # Load CRM
53 |     try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/dq/run_comprehensive_dq.py:60:1
   |
58 |     except Exception as e:
59 |         logger.warning(f"Could not load CRM bronze: {e}")
60 |     
   | ^^^^
61 |     # Load Snowflake
62 |     try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/dq/run_comprehensive_dq.py:69:1
   |
67 |     except Exception as e:
68 |         logger.warning(f"Could not load Snowflake bronze: {e}")
69 |     
   | ^^^^
70 |     # Load Redshift
71 |     try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/dq/run_comprehensive_dq.py:76:1
   |
74 |     except Exception as e:
75 |         logger.warning(f"Could not load Redshift bronze: {e}")
76 |     
   | ^^^^
77 |     # Load Kafka
78 |     try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/dq/run_comprehensive_dq.py:83:1
   |
81 |     except Exception as e:
82 |         logger.warning(f"Could not load Kafka bronze: {e}")
83 |     
   | ^^^^
84 |     # Load FX
85 |     try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/dq/run_comprehensive_dq.py:90:1
   |
88 |     except Exception as e:
89 |         logger.warning(f"Could not load FX bronze: {e}")
90 |     
   | ^^^^
91 |     return bronze_data
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> jobs/dq/run_comprehensive_dq.py:94:53
   |
94 | def load_silver_tables(spark: SparkSession, config: Dict[str, Any]) -> Dict[str, Any]:
   |                                                     ^^^^
95 |     """Load all silver tables."""
96 |     silver_root = resolve_data_path(config, "silver")
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> jobs/dq/run_comprehensive_dq.py:94:72
   |
94 | def load_silver_tables(spark: SparkSession, config: Dict[str, Any]) -> Dict[str, Any]:
   |                                                                        ^^^^
95 |     """Load all silver tables."""
96 |     silver_root = resolve_data_path(config, "silver")
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> jobs/dq/run_comprehensive_dq.py:98:1
    |
 96 |     silver_root = resolve_data_path(config, "silver")
 97 |     tables_config = config.get("tables", {}).get("silver", {})
 98 |     
    | ^^^^
 99 |     silver_data = {}
100 |     table_names = {
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/dq/run_comprehensive_dq.py:108:1
    |
106 |         "order_events": tables_config.get("order_events", "order_events_silver")
107 |     }
108 |     
    | ^^^^
109 |     for key, table_name in table_names.items():
110 |         try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/dq/run_comprehensive_dq.py:114:1
    |
112 |         except Exception as e:
113 |             logger.warning(f"Could not load {table_name}: {e}")
114 |     
    | ^^^^
115 |     return silver_data
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> jobs/dq/run_comprehensive_dq.py:118:51
    |
118 | def load_gold_tables(spark: SparkSession, config: Dict[str, Any]) -> Dict[str, Any]:
    |                                                   ^^^^
119 |     """Load all gold tables."""
120 |     gold_root = resolve_data_path(config, "gold")
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> jobs/dq/run_comprehensive_dq.py:118:70
    |
118 | def load_gold_tables(spark: SparkSession, config: Dict[str, Any]) -> Dict[str, Any]:
    |                                                                      ^^^^
119 |     """Load all gold tables."""
120 |     gold_root = resolve_data_path(config, "gold")
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> jobs/dq/run_comprehensive_dq.py:122:1
    |
120 |     gold_root = resolve_data_path(config, "gold")
121 |     tables_config = config.get("tables", {}).get("gold", {})
122 |     
    | ^^^^
123 |     gold_data = {}
124 |     table_names = {
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/dq/run_comprehensive_dq.py:130:1
    |
128 |         "customer_360": tables_config.get("fact_customer_24m", "customer_360")
129 |     }
130 |     
    | ^^^^
131 |     for key, table_name in table_names.items():
132 |         try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/dq/run_comprehensive_dq.py:136:1
    |
134 |         except Exception as e:
135 |             logger.warning(f"Could not load {table_name}: {e}")
136 |     
    | ^^^^
137 |     return gold_data
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/dq/run_comprehensive_dq.py:148:1
    |
146 |     parser.add_argument("--output", help="Output path for report")
147 |     args = parser.parse_args()
148 |     
    | ^^^^
149 |     # Load config
150 |     if args.config:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/dq/run_comprehensive_dq.py:157:1
    |
155 |             config_path = Path("config/local.yaml")
156 |         config = load_config_resolved(str(config_path))
157 |     
    | ^^^^
158 |     # Build Spark
159 |     spark = build_spark(config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/dq/run_comprehensive_dq.py:160:1
    |
158 |     # Build Spark
159 |     spark = build_spark(config)
160 |     
    | ^^^^
161 |     try:
162 |         validator = ComprehensiveValidator(spark)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/dq/run_comprehensive_dq.py:163:1
    |
161 |     try:
162 |         validator = ComprehensiveValidator(spark)
163 |         
    | ^^^^^^^^
164 |         # Load expected schemas
165 |         expected_schemas = {
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/dq/run_comprehensive_dq.py:176:1
    |
174 |             "fx_rates": FX_RATES_SCHEMA
175 |         }
176 |         
    | ^^^^^^^^
177 |         # Validate based on layer
178 |         if args.layer in ["bronze", "all"]:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/dq/run_comprehensive_dq.py:182:1
    |
180 |             bronze_data = load_bronze_tables(spark, config)
181 |             validator.validate_bronze_layer(bronze_data, expected_schemas)
182 |         
    | ^^^^^^^^
183 |         if args.layer in ["silver", "all"]:
184 |             logger.info("Loading silver tables...")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/dq/run_comprehensive_dq.py:188:1
    |
186 |             bronze_data = load_bronze_tables(spark, config) if args.layer == "silver" else bronze_data
187 |             validator.validate_silver_layer(silver_data, bronze_data)
188 |         
    | ^^^^^^^^
189 |         if args.layer in ["gold", "all"]:
190 |             logger.info("Loading gold tables...")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/dq/run_comprehensive_dq.py:194:1
    |
192 |             silver_data = load_silver_tables(spark, config) if args.layer == "gold" else silver_data
193 |             validator.validate_gold_layer(gold_data, silver_data)
194 |         
    | ^^^^^^^^
195 |         # Generate report
196 |         report = validator.generate_comprehensive_report()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/dq/run_comprehensive_dq.py:198:1
    |
196 |         report = validator.generate_comprehensive_report()
197 |         summary = validator.get_summary()
198 |         
    | ^^^^^^^^
199 |         logger.info("\n" + report)
200 |         logger.info(f"\nSummary: {summary}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/dq/run_comprehensive_dq.py:201:1
    |
199 |         logger.info("\n" + report)
200 |         logger.info(f"\nSummary: {summary}")
201 |         
    | ^^^^^^^^
202 |         # Write report to file if output specified
203 |         if args.output:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/dq/run_comprehensive_dq.py:208:1
    |
206 |             output_path.write_text(report, encoding="utf-8")
207 |             logger.info(f"Report written to {args.output}")
208 |         
    | ^^^^^^^^
209 |         # Exit with error code if any layer failed
210 |         if summary["layers_failed"] > 0:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/dq/run_comprehensive_dq.py:212:1
    |
210 |         if summary["layers_failed"] > 0:
211 |             sys.exit(1)
212 |         
    | ^^^^^^^^
213 |     finally:
214 |         spark.stop()
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> jobs/ingest/_lib/watermark.py:7:1
   |
 5 |   """
 6 |
 7 | / import logging
 8 | | from typing import Optional
 9 | | from datetime import datetime
10 | | from pathlib import Path
   | |________________________^
11 |
12 |   try:
   |
help: Organize imports

F401 [*] `pathlib.Path` imported but unused
  --> jobs/ingest/_lib/watermark.py:10:21
   |
 8 | from typing import Optional
 9 | from datetime import datetime
10 | from pathlib import Path
   |                     ^^^^
11 |
12 | try:
   |
help: Remove unused import: `pathlib.Path`

UP045 [*] Use `X | None` for type annotations
  --> jobs/ingest/_lib/watermark.py:27:6
   |
25 |     config: dict = None,
26 |     use_ssm: bool = False
27 | ) -> Optional[str]:
   |      ^^^^^^^^^^^^^
28 |     """
29 |     Read watermark value from SSM Parameter Store or state store.
   |
help: Convert to `X | None`

W293 Blank line contains whitespace
  --> jobs/ingest/_lib/watermark.py:30:1
   |
28 |     """
29 |     Read watermark value from SSM Parameter Store or state store.
30 |     
   | ^^^^
31 |     Args:
32 |         key: Watermark key (e.g., 'snowflake_orders_max_ts')
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> jobs/ingest/_lib/watermark.py:35:1
   |
33 |         config: Configuration dict
34 |         use_ssm: If True, use SSM; otherwise use state store
35 |         
   | ^^^^^^^^
36 |     Returns:
37 |         ISO8601 timestamp string or None if not found
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/ingest/_lib/watermark.py:57:1
   |
55 |         # Use state store (S3/local)
56 |         from project_a.utils.state_store import get_state_store
57 |         
   | ^^^^^^^^
58 |         state_store = get_state_store(config)
59 |         watermark = state_store.get_watermark(key)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/ingest/_lib/watermark.py:60:1
   |
58 |         state_store = get_state_store(config)
59 |         watermark = state_store.get_watermark(key)
60 |         
   | ^^^^^^^^
61 |         if watermark:
62 |             logger.info(f"âœ… Read watermark from state store: {key} = {watermark}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/ingest/_lib/watermark.py:65:1
   |
63 |         else:
64 |             logger.info(f"No watermark found for {key} (first run)")
65 |         
   | ^^^^^^^^
66 |         return watermark
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> jobs/ingest/_lib/watermark.py:77:1
   |
75 |     """
76 |     Write watermark value to SSM Parameter Store or state store.
77 |     
   | ^^^^
78 |     Args:
79 |         key: Watermark key
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/ingest/_lib/watermark.py:85:1
   |
83 |     """
84 |     iso_value = value.isoformat()
85 |     
   | ^^^^
86 |     if use_ssm and BOTO3_AVAILABLE:
87 |         # Use AWS Systems Manager Parameter Store
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/_lib/watermark.py:100:1
    |
 98 |             logger.warning(f"Failed to write watermark to SSM: {e}, falling back to state store")
 99 |             use_ssm = False
100 |     
    | ^^^^
101 |     if not use_ssm:
102 |         # Use state store (S3/local)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/_lib/watermark.py:104:1
    |
102 |         # Use state store (S3/local)
103 |         from project_a.utils.state_store import get_state_store
104 |         
    | ^^^^^^^^
105 |         state_store = get_state_store(config)
106 |         state_store.set_watermark(key, iso_value)
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> jobs/ingest/fx_json_to_bronze.py:15:1
   |
13 |   - Delta Lake output
14 |   """
15 | / import sys
16 | | import argparse
17 | | import tempfile
18 | | import uuid
19 | | import logging
20 | | from pathlib import Path
21 | | from typing import Dict, Any
22 | | from datetime import datetime
   | |_____________________________^
23 |
24 |   sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> jobs/ingest/fx_json_to_bronze.py:21:1
   |
19 | import logging
20 | from pathlib import Path
21 | from typing import Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
22 | from datetime import datetime
   |

I001 [*] Import block is un-sorted or un-formatted
  --> jobs/ingest/fx_json_to_bronze.py:26:1
   |
24 |   sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))
25 |
26 | / from pyspark.sql import SparkSession, DataFrame
27 | | from pyspark.sql import functions as F
28 | | from pyspark.sql.types import DateType
29 | |
30 | | from project_a.utils.spark_session import build_spark
31 | | from project_a.config_loader import load_config_resolved
32 | | from project_a.extract.fx_json_reader import read_fx_json
33 | | from project_a.utils.error_lanes import ErrorLaneHandler
34 | | from project_a.utils.logging import setup_json_logging, get_trace_id
35 | | from project_a.utils.run_audit import write_run_audit
36 | | from project_a.monitoring.lineage_decorator import lineage_job
37 | | from project_a.monitoring.metrics_collector import emit_rowcount, emit_duration
38 | | import time
   | |___________^
39 |
40 |   logging.basicConfig(level=logging.INFO)
   |
help: Organize imports

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> jobs/ingest/fx_json_to_bronze.py:51:13
   |
49 | def extract_fx_json_to_bronze(
50 |     spark: SparkSession,
51 |     config: Dict[str, Any],
   |             ^^^^
52 |     run_id: str = None
53 | ) -> DataFrame:
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> jobs/ingest/fx_json_to_bronze.py:56:1
   |
54 |     """
55 |     Extract FX rates from JSON format and write to Bronze layer.
56 |     
   | ^^^^
57 |     Args:
58 |         spark: SparkSession instance
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> jobs/ingest/fx_json_to_bronze.py:61:1
   |
59 |         config: Configuration dictionary
60 |         run_id: Optional run identifier (UUID)
61 |         
   | ^^^^^^^^
62 |     Returns:
63 |         DataFrame with validated, enriched FX rates data
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/ingest/fx_json_to_bronze.py:67:1
   |
65 |     if run_id is None:
66 |         run_id = str(uuid.uuid4())
67 |     
   | ^^^^
68 |     logger.info(f"ðŸš€ Starting FX JSON extraction (run_id={run_id})")
69 |     start_time = time.time()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/ingest/fx_json_to_bronze.py:70:1
   |
68 |     logger.info(f"ðŸš€ Starting FX JSON extraction (run_id={run_id})")
69 |     start_time = time.time()
70 |     
   | ^^^^
71 |     try:
72 |         # Get paths from config
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/ingest/fx_json_to_bronze.py:77:1
   |
75 |         bronze_fx_path = fx_cfg.get("bronze_path", "")
76 |         lake_bucket = config.get("buckets", {}).get("lake", "")
77 |         
   | ^^^^^^^^
78 |         if not bronze_fx_path:
79 |             raise ValueError("FX bronze_path not found in config")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/ingest/fx_json_to_bronze.py:80:1
   |
78 |         if not bronze_fx_path:
79 |             raise ValueError("FX bronze_path not found in config")
80 |         
   | ^^^^^^^^
81 |         # Setup error lane handler
82 |         error_handler = ErrorLaneHandler(f"s3://{lake_bucket}")
   |
help: Remove whitespace from blank line

F841 Local variable `error_path` is assigned to but never used
  --> jobs/ingest/fx_json_to_bronze.py:83:9
   |
81 |         # Setup error lane handler
82 |         error_handler = ErrorLaneHandler(f"s3://{lake_bucket}")
83 |         error_path = error_handler.get_error_lane_path("bronze", "fx", run_id)
   |         ^^^^^^^^^^
84 |         
85 |         # Read and normalize JSON using extractor (contract-driven)
   |
help: Remove assignment to unused variable `error_path`

W293 [*] Blank line contains whitespace
  --> jobs/ingest/fx_json_to_bronze.py:84:1
   |
82 |         error_handler = ErrorLaneHandler(f"s3://{lake_bucket}")
83 |         error_path = error_handler.get_error_lane_path("bronze", "fx", run_id)
84 |         
   | ^^^^^^^^
85 |         # Read and normalize JSON using extractor (contract-driven)
86 |         df_raw = read_fx_json(spark, config)
   |
help: Remove whitespace from blank line

F841 Local variable `rows_in` is assigned to but never used
  --> jobs/ingest/fx_json_to_bronze.py:87:9
   |
85 |         # Read and normalize JSON using extractor (contract-driven)
86 |         df_raw = read_fx_json(spark, config)
87 |         rows_in = df_raw.count()
   |         ^^^^^^^
88 |         
89 |         # Apply contract-driven validation:
   |
help: Remove assignment to unused variable `rows_in`

W293 [*] Blank line contains whitespace
  --> jobs/ingest/fx_json_to_bronze.py:88:1
   |
86 |         df_raw = read_fx_json(spark, config)
87 |         rows_in = df_raw.count()
88 |         
   | ^^^^^^^^
89 |         # Apply contract-driven validation:
90 |         # 1. Enforce non-null primary key columns (date, base_ccy, quote_ccy, rate)
   |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
   --> jobs/ingest/fx_json_to_bronze.py:100:39
    |
 98 |             (F.length(F.trim(F.col("base_ccy"))) > 0) &
 99 |             (F.length(F.trim(F.col("quote_ccy"))) > 0) &
100 |             (F.col("rate") > 0.0001) & 
    |                                       ^
101 |             (F.col("rate") < 1000)
102 |         )
    |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
   --> jobs/ingest/fx_json_to_bronze.py:103:1
    |
101 |             (F.col("rate") < 1000)
102 |         )
103 |         
    | ^^^^^^^^
104 |         bad = df_raw.exceptAll(df_clean)
105 |         bad_count = bad.count()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/fx_json_to_bronze.py:106:1
    |
104 |         bad = df_raw.exceptAll(df_clean)
105 |         bad_count = bad.count()
106 |         
    | ^^^^^^^^
107 |         if bad_count > 0:
108 |             logger.warning(f"âš ï¸  Routing {bad_count:,} contract-violating rows to error lane")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/fx_json_to_bronze.py:110:1
    |
108 |             logger.warning(f"âš ï¸  Routing {bad_count:,} contract-violating rows to error lane")
109 |             error_handler.quarantine(bad, "bronze", "fx", run_id, reason="contract_violation")
110 |         
    | ^^^^^^^^
111 |         # Add metadata columns
112 |         run_date = datetime.utcnow().strftime("%Y-%m-%d")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/fx_json_to_bronze.py:118:1
    |
116 |                           .withColumn("_source_system", F.lit("fx_json")) \
117 |                           .withColumn("_format", F.lit("json"))
118 |         
    | ^^^^^^^^
119 |         final_count = df_clean.count()
120 |         logger.info(f"âœ… Cleaned data: {final_count:,} records (routed {bad_count:,} to error lane)")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/fx_json_to_bronze.py:121:1
    |
119 |         final_count = df_clean.count()
120 |         logger.info(f"âœ… Cleaned data: {final_count:,} records (routed {bad_count:,} to error lane)")
121 |         
    | ^^^^^^^^
122 |         # Emit metrics
123 |         duration_ms = (time.time() - start_time) * 1000
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/fx_json_to_bronze.py:129:1
    |
127 |             "format": "json"
128 |         }, config)
129 |         
    | ^^^^^^^^
130 |         emit_duration("latency_seconds", duration_ms / 1000.0, {
131 |             "job": "bronze_fx_json",
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/fx_json_to_bronze.py:134:1
    |
132 |             "stage": "extraction"
133 |         }, config)
134 |         
    | ^^^^^^^^
135 |         logger.info(f"âœ… Extraction complete: {final_count:,} records in {duration_ms:.0f}ms")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/fx_json_to_bronze.py:136:1
    |
135 |         logger.info(f"âœ… Extraction complete: {final_count:,} records in {duration_ms:.0f}ms")
136 |         
    | ^^^^^^^^
137 |         return df_clean
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/fx_json_to_bronze.py:138:1
    |
137 |         return df_clean
138 |         
    | ^^^^^^^^
139 |     except Exception as e:
140 |         logger.error(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/fx_json_to_bronze.py:159:1
    |
157 |     parser.add_argument("--config", help="Config file path (local or S3)")
158 |     args = parser.parse_args()
159 |     
    | ^^^^
160 |     # Setup structured logging
161 |     trace_id = get_trace_id()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/fx_json_to_bronze.py:164:1
    |
162 |     setup_json_logging(level="INFO", include_trace_id=True)
163 |     logger.info(f"Job started (trace_id={trace_id}, env={args.env})")
164 |     
    | ^^^^
165 |     # Load configuration
166 |     if args.config:
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
   --> jobs/ingest/fx_json_to_bronze.py:171:13
    |
169 |           if config_path.startswith("s3://"):
170 |               logger.info(f"Reading config from S3 using Spark: {config_path}")
171 | /             from pyspark.sql import SparkSession
172 | |             import yaml
    | |_______________________^
173 |               
174 |               spark_temp = SparkSession.builder \
    |
help: Organize imports

F401 [*] `yaml` imported but unused
   --> jobs/ingest/fx_json_to_bronze.py:172:20
    |
170 |             logger.info(f"Reading config from S3 using Spark: {config_path}")
171 |             from pyspark.sql import SparkSession
172 |             import yaml
    |                    ^^^^
173 |             
174 |             spark_temp = SparkSession.builder \
    |
help: Remove unused import: `yaml`

W293 [*] Blank line contains whitespace
   --> jobs/ingest/fx_json_to_bronze.py:173:1
    |
171 |             from pyspark.sql import SparkSession
172 |             import yaml
173 |             
    | ^^^^^^^^^^^^
174 |             spark_temp = SparkSession.builder \
175 |                 .appName("config_loader") \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/fx_json_to_bronze.py:178:1
    |
176 |                 .config("spark.sql.adaptive.enabled", "false") \
177 |                 .getOrCreate()
178 |             
    | ^^^^^^^^^^^^
179 |             try:
180 |                 config_lines = spark_temp.sparkContext.textFile(config_path).collect()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/fx_json_to_bronze.py:198:1
    |
196 |             config_path = Path("config/local.yaml")
197 |         config_path = str(config_path)
198 |     
    | ^^^^
199 |     logger.info(f"Loading config from: {config_path}")
200 |     config = load_config_resolved(config_path)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/fx_json_to_bronze.py:201:1
    |
199 |     logger.info(f"Loading config from: {config_path}")
200 |     config = load_config_resolved(config_path)
201 |     
    | ^^^^
202 |     # Ensure environment is set correctly for EMR
203 |     if not config.get("environment"):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/fx_json_to_bronze.py:206:1
    |
204 |         config["environment"] = "emr"
205 |         logger.info("Set environment=emr in config (was missing)")
206 |     
    | ^^^^
207 |     # Generate run ID
208 |     run_id = str(uuid.uuid4())
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/fx_json_to_bronze.py:209:1
    |
207 |     # Generate run ID
208 |     run_id = str(uuid.uuid4())
209 |     
    | ^^^^
210 |     # Build Spark session
211 |     spark = build_spark(config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/fx_json_to_bronze.py:212:1
    |
210 |     # Build Spark session
211 |     spark = build_spark(config)
212 |     
    | ^^^^
213 |     try:
214 |         # Run extraction
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/fx_json_to_bronze.py:216:1
    |
214 |         # Run extraction
215 |         df = extract_fx_json_to_bronze(spark, config, run_id=run_id)
216 |         
    | ^^^^^^^^
217 |         if df.isEmpty():
218 |             logger.warning("âš ï¸  No data to write")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/fx_json_to_bronze.py:220:1
    |
218 |             logger.warning("âš ï¸  No data to write")
219 |             return 0
220 |         
    | ^^^^^^^^
221 |         # Get output path from config
222 |         sources_cfg = config.get("sources", {})
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/fx_json_to_bronze.py:225:1
    |
223 |         fx_cfg = sources_cfg.get("fx", {})
224 |         bronze_fx_path = fx_cfg.get("bronze_path", "")
225 |         
    | ^^^^^^^^
226 |         if not bronze_fx_path:
227 |             bronze_root = config["paths"]["bronze_root"]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/fx_json_to_bronze.py:229:1
    |
227 |             bronze_root = config["paths"]["bronze_root"]
228 |             bronze_fx_path = f"{bronze_root}/fx/delta/"
229 |         
    | ^^^^^^^^
230 |         logger.info(f"ðŸ’¾ Writing to Bronze: {bronze_fx_path}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/fx_json_to_bronze.py:231:1
    |
230 |         logger.info(f"ðŸ’¾ Writing to Bronze: {bronze_fx_path}")
231 |         
    | ^^^^^^^^
232 |         # Write to bronze as Delta with contract-driven partitioning
233 |         # Partition by trade_date for downstream pruning (senior pattern)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/fx_json_to_bronze.py:235:1
    |
233 |         # Partition by trade_date for downstream pruning (senior pattern)
234 |         df_partitioned = df.repartition(F.col("trade_date"))
235 |         
    | ^^^^^^^^
236 |         df_partitioned.write \
237 |             .format("delta") \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/fx_json_to_bronze.py:243:1
    |
241 |             .partitionBy("trade_date", "_run_date") \
242 |             .save(bronze_fx_path)
243 |         
    | ^^^^^^^^
244 |         rows_out = df.count()
245 |         duration_ms = (time.time() - start_time) * 1000
    |
help: Remove whitespace from blank line

F821 Undefined name `start_time`
   --> jobs/ingest/fx_json_to_bronze.py:245:38
    |
244 |         rows_out = df.count()
245 |         duration_ms = (time.time() - start_time) * 1000
    |                                      ^^^^^^^^^^
246 |         
247 |         # Write run audit trail
    |

W293 [*] Blank line contains whitespace
   --> jobs/ingest/fx_json_to_bronze.py:246:1
    |
244 |         rows_out = df.count()
245 |         duration_ms = (time.time() - start_time) * 1000
246 |         
    | ^^^^^^^^
247 |         # Write run audit trail
248 |         lake_bucket = config.get("buckets", {}).get("lake", "")
    |
help: Remove whitespace from blank line

F821 Undefined name `rows_in`
   --> jobs/ingest/fx_json_to_bronze.py:257:29
    |
255 |                     source=fx_cfg.get("raw_path", ""),
256 |                     target=bronze_fx_path,
257 |                     rows_in=rows_in,
    |                             ^^^^^^^
258 |                     rows_out=rows_out,
259 |                     status="SUCCESS",
    |

W293 [*] Blank line contains whitespace
   --> jobs/ingest/fx_json_to_bronze.py:266:1
    |
264 |             except Exception as e:
265 |                 logger.warning(f"âš ï¸  Failed to write run audit: {e}")
266 |         
    | ^^^^^^^^
267 |         logger.info(f"ðŸŽ‰ FX JSON to Bronze job completed: {rows_out:,} records in {duration_ms:.0f}ms")
268 |         return 0
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/fx_json_to_bronze.py:269:1
    |
267 |         logger.info(f"ðŸŽ‰ FX JSON to Bronze job completed: {rows_out:,} records in {duration_ms:.0f}ms")
268 |         return 0
269 |         
    | ^^^^^^^^
270 |     except Exception as e:
271 |         logger.error(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/fx_json_to_bronze.py:281:1
    |
279 |         )
280 |         return 1
281 |         
    | ^^^^^^^^
282 |     finally:
283 |         spark.stop()
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> jobs/ingest/ingest_crm_to_s3.py:7:1
   |
 5 |   """
 6 |
 7 | / import argparse
 8 | | import logging
 9 | | from datetime import datetime
10 | | from typing import Dict, Any, Optional
11 | |
12 | | from pyspark.sql import SparkSession
13 | | from pyspark.sql.functions import lit, current_timestamp, to_date, col
14 | |
15 | | from project_a.utils.spark_session import build_spark
16 | | from project_a.utils.config import load_conf
17 | | from project_a.utils.state_store import get_state_store
18 | | from project_a.monitoring.lineage_decorator import lineage_job
19 | | from project_a.monitoring.metrics_collector import emit_metrics
20 | | import time
   | |___________^
21 |
22 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> jobs/ingest/ingest_crm_to_s3.py:10:1
   |
 8 | import logging
 9 | from datetime import datetime
10 | from typing import Dict, Any, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
11 |
12 | from pyspark.sql import SparkSession
   |

F401 [*] `typing.Optional` imported but unused
  --> jobs/ingest/ingest_crm_to_s3.py:10:31
   |
 8 | import logging
 9 | from datetime import datetime
10 | from typing import Dict, Any, Optional
   |                               ^^^^^^^^
11 |
12 | from pyspark.sql import SparkSession
   |
help: Remove unused import: `typing.Optional`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> jobs/ingest/ingest_crm_to_s3.py:32:13
   |
30 | def ingest_crm_to_s3_raw(
31 |     spark: SparkSession,
32 |     config: Dict[str, Any],
   |             ^^^^
33 |     table: str = "accounts",
34 |     execution_date: str = None,
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> jobs/ingest/ingest_crm_to_s3.py:40:1
   |
38 |     """
39 |     Ingest CRM table to S3 raw zone with incremental support.
40 |     
   | ^^^^
41 |     Args:
42 |         spark: SparkSession
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/ingest/ingest_crm_to_s3.py:49:1
   |
47 |     """
48 |     start_time = time.time()
49 |     
   | ^^^^
50 |     if execution_date is None:
51 |         execution_date = datetime.now().strftime("%Y-%m-%d")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/ingest/ingest_crm_to_s3.py:52:1
   |
50 |     if execution_date is None:
51 |         execution_date = datetime.now().strftime("%Y-%m-%d")
52 |     
   | ^^^^
53 |     state_store = get_state_store(config)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/ingest/ingest_crm_to_s3.py:54:1
   |
53 |     state_store = get_state_store(config)
54 |     
   | ^^^^
55 |     # Get watermark for incremental
56 |     watermark = None
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/ingest/ingest_crm_to_s3.py:62:1
   |
60 |             watermark = datetime.fromisoformat(watermark_str.replace('Z', '+00:00'))
61 |             logger.info(f"Using watermark: {watermark}")
62 |     
   | ^^^^
63 |     # Read CRM data
64 |     if config.get('environment') == 'local':
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/ingest/ingest_crm_to_s3.py:67:1
   |
65 |         csv_path = f"aws/data/crm/{table}.csv"
66 |         df = spark.read.option("header", "true").option("inferSchema", "true").csv(csv_path)
67 |         
   | ^^^^^^^^
68 |         # Filter by watermark if incremental
69 |         if watermark and "last_modified_date" in df.columns:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/ingest/ingest_crm_to_s3.py:77:1
   |
75 |         schema = StructType([])  # Load from config
76 |         df = spark.createDataFrame([], schema)
77 |     
   | ^^^^
78 |     # Add load metadata
79 |     df = df.withColumn("_load_date", to_date(lit(execution_date))) \
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/ingest/ingest_crm_to_s3.py:83:1
   |
81 |            .withColumn("_source", lit("crm")) \
82 |            .withColumn("_table", lit(table))
83 |     
   | ^^^^
84 |     # Write to raw zone
85 |     raw_path = f"s3://my-etl-lake-demo/raw/crm/{table}/load_dt={execution_date}/"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/ingest/ingest_crm_to_s3.py:87:1
   |
85 |     raw_path = f"s3://my-etl-lake-demo/raw/crm/{table}/load_dt={execution_date}/"
86 |     logger.info(f"Writing to raw zone: {raw_path}")
87 |     
   | ^^^^
88 |     df.write \
89 |         .mode("append") \
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/ingest/ingest_crm_to_s3.py:91:1
   |
89 |         .mode("append") \
90 |         .parquet(raw_path)
91 |     
   | ^^^^
92 |     record_count = df.count()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/ingest/ingest_crm_to_s3.py:93:1
   |
92 |     record_count = df.count()
93 |     
   | ^^^^
94 |     # Update watermark
95 |     if record_count > 0 and watermark:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/ingest_crm_to_s3.py:100:1
    |
 98 |         if latest_ts:
 99 |             state_store.set_watermark(f"crm_{table}", latest_ts.isoformat())
100 |     
    | ^^^^
101 |     duration = time.time() - start_time
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/ingest_crm_to_s3.py:102:1
    |
101 |     duration = time.time() - start_time
102 |     
    | ^^^^
103 |     # Emit metrics
104 |     emit_metrics(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/ingest_crm_to_s3.py:112:1
    |
110 |         config=config
111 |     )
112 |     
    | ^^^^
113 |     logger.info(f"âœ… Ingested {record_count:,} records to raw zone")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/ingest_crm_to_s3.py:114:1
    |
113 |     logger.info(f"âœ… Ingested {record_count:,} records to raw zone")
114 |     
    | ^^^^
115 |     return df
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/ingest_crm_to_s3.py:124:1
    |
122 |     parser.add_argument("--full-load", action="store_true", help="Full load (not incremental)")
123 |     parser.add_argument("--config", default="config/prod.yaml", help="Config file")
124 |     
    | ^^^^
125 |     args = parser.parse_args()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/ingest_crm_to_s3.py:126:1
    |
125 |     args = parser.parse_args()
126 |     
    | ^^^^
127 |     config = load_conf(args.config)
128 |     spark = build_spark(app_name=f"ingest_crm_{args.table}", config=config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/ingest_crm_to_s3.py:129:1
    |
127 |     config = load_conf(args.config)
128 |     spark = build_spark(app_name=f"ingest_crm_{args.table}", config=config)
129 |     
    | ^^^^
130 |     try:
131 |         ingest_crm_to_s3_raw(
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> jobs/ingest/ingest_snowflake_to_s3.py:7:1
   |
 5 |   """
 6 |
 7 | / import argparse
 8 | | import logging
 9 | | from datetime import datetime
10 | | from typing import Dict, Any
11 | |
12 | | from pyspark.sql import SparkSession
13 | | from pyspark.sql.functions import lit, current_timestamp, to_date
14 | |
15 | | from project_a.utils.spark_session import build_spark
16 | | from project_a.utils.config import load_conf
17 | | from project_a.utils.secrets import get_snowflake_credentials
18 | | from project_a.monitoring.lineage_decorator import lineage_job
19 | | from project_a.monitoring.metrics_collector import emit_metrics
20 | | import time
21 | | import os
   | |_________^
22 |
23 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> jobs/ingest/ingest_snowflake_to_s3.py:10:1
   |
 8 | import logging
 9 | from datetime import datetime
10 | from typing import Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
11 |
12 | from pyspark.sql import SparkSession
   |

F401 [*] `os` imported but unused
  --> jobs/ingest/ingest_snowflake_to_s3.py:21:8
   |
19 | from project_a.monitoring.metrics_collector import emit_metrics
20 | import time
21 | import os
   |        ^^
22 |
23 | logger = logging.getLogger(__name__)
   |
help: Remove unused import: `os`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> jobs/ingest/ingest_snowflake_to_s3.py:33:13
   |
31 | def ingest_snowflake_to_s3_raw(
32 |     spark: SparkSession,
33 |     config: Dict[str, Any],
   |             ^^^^
34 |     table: str = "orders",
35 |     execution_date: str = None,
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> jobs/ingest/ingest_snowflake_to_s3.py:40:1
   |
38 |     """
39 |     Ingest Snowflake table to S3 raw zone.
40 |     
   | ^^^^
41 |     Args:
42 |         spark: SparkSession
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/ingest/ingest_snowflake_to_s3.py:48:1
   |
46 |     """
47 |     start_time = time.time()
48 |     
   | ^^^^
49 |     if execution_date is None:
50 |         execution_date = datetime.now().strftime("%Y-%m-%d")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/ingest/ingest_snowflake_to_s3.py:51:1
   |
49 |     if execution_date is None:
50 |         execution_date = datetime.now().strftime("%Y-%m-%d")
51 |     
   | ^^^^
52 |     # Get Snowflake credentials
53 |     creds = get_snowflake_credentials(config)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/ingest/ingest_snowflake_to_s3.py:54:1
   |
52 |     # Get Snowflake credentials
53 |     creds = get_snowflake_credentials(config)
54 |     
   | ^^^^
55 |     # Build query (full load for raw zone)
56 |     query = f"SELECT * FROM {table.upper()}"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/ingest/ingest_snowflake_to_s3.py:57:1
   |
55 |     # Build query (full load for raw zone)
56 |     query = f"SELECT * FROM {table.upper()}"
57 |     
   | ^^^^
58 |     logger.info(f"Ingesting Snowflake {table} to S3 raw")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/ingest/ingest_snowflake_to_s3.py:59:1
   |
58 |     logger.info(f"Ingesting Snowflake {table} to S3 raw")
59 |     
   | ^^^^
60 |     # Read from Snowflake
61 |     if config.get('environment') == 'local':
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/ingest/ingest_snowflake_to_s3.py:75:1
   |
73 |             "sfWarehouse": creds.get("warehouse"),
74 |         }
75 |         
   | ^^^^^^^^
76 |         df = spark.read \
77 |             .format("snowflake") \
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/ingest/ingest_snowflake_to_s3.py:81:1
   |
79 |             .option("query", query) \
80 |             .load()
81 |     
   | ^^^^
82 |     # Add load metadata
83 |     df = df.withColumn("_load_date", to_date(lit(execution_date))) \
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/ingest/ingest_snowflake_to_s3.py:87:1
   |
85 |            .withColumn("_source", lit("snowflake")) \
86 |            .withColumn("_table", lit(table))
87 |     
   | ^^^^
88 |     # Write to raw zone
89 |     raw_path = f"s3://my-etl-lake-demo/raw/snowflake/{table}/load_dt={execution_date}/"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/ingest/ingest_snowflake_to_s3.py:91:1
   |
89 |     raw_path = f"s3://my-etl-lake-demo/raw/snowflake/{table}/load_dt={execution_date}/"
90 |     logger.info(f"Writing to raw zone: {raw_path}")
91 |     
   | ^^^^
92 |     df.write \
93 |         .mode("overwrite") \
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/ingest/ingest_snowflake_to_s3.py:95:1
   |
93 |         .mode("overwrite") \
94 |         .parquet(raw_path)
95 |     
   | ^^^^
96 |     record_count = df.count()
97 |     duration = time.time() - start_time
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/ingest_snowflake_to_s3.py:98:1
    |
 96 |     record_count = df.count()
 97 |     duration = time.time() - start_time
 98 |     
    | ^^^^
 99 |     # Emit metrics
100 |     emit_metrics(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/ingest_snowflake_to_s3.py:108:1
    |
106 |         config=config
107 |     )
108 |     
    | ^^^^
109 |     logger.info(f"âœ… Ingested {record_count:,} records to raw zone")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/ingest_snowflake_to_s3.py:110:1
    |
109 |     logger.info(f"âœ… Ingested {record_count:,} records to raw zone")
110 |     
    | ^^^^
111 |     return df
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/ingest_snowflake_to_s3.py:119:1
    |
117 |     parser.add_argument("--execution-date", help="Execution date (YYYY-MM-DD)")
118 |     parser.add_argument("--config", default="config/prod.yaml", help="Config file")
119 |     
    | ^^^^
120 |     args = parser.parse_args()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/ingest_snowflake_to_s3.py:121:1
    |
120 |     args = parser.parse_args()
121 |     
    | ^^^^
122 |     config = load_conf(args.config)
123 |     spark = build_spark(app_name=f"ingest_snowflake_{args.table}", config=config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/ingest_snowflake_to_s3.py:124:1
    |
122 |     config = load_conf(args.config)
123 |     spark = build_spark(app_name=f"ingest_snowflake_{args.table}", config=config)
124 |     
    | ^^^^
125 |     try:
126 |         ingest_snowflake_to_s3_raw(
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> jobs/ingest/snowflake_to_bronze.py:13:1
   |
11 |   """
12 |
13 | / import sys
14 | | import logging
15 | | import uuid
16 | | import argparse
17 | | from pathlib import Path
18 | | from datetime import datetime
19 | | from typing import Dict, Any
   | |____________________________^
20 |
21 |   # Add src to path
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> jobs/ingest/snowflake_to_bronze.py:19:1
   |
17 | from pathlib import Path
18 | from datetime import datetime
19 | from typing import Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
20 |
21 | # Add src to path
   |

I001 [*] Import block is un-sorted or un-formatted
  --> jobs/ingest/snowflake_to_bronze.py:24:1
   |
22 |   sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))
23 |
24 | / from pyspark.sql import SparkSession, DataFrame
25 | | from pyspark.sql import functions as F, types as T
26 | |
27 | | from project_a.utils.spark_session import build_spark
28 | | from project_a.utils.path_resolver import resolve_path
29 | | from project_a.utils.contracts import (
30 | |     load_schema_contract,
31 | |     contract_to_struct_type,
32 | |     align_to_schema,
33 | |     validate_and_quarantine,
34 | |     add_metadata_columns
35 | | )
36 | | from project_a.utils.error_lanes import ErrorLaneHandler, add_row_id
37 | | from project_a.utils.state_store import get_state_store
38 | | from project_a.utils.secrets import get_snowflake_credentials
39 | | from project_a.monitoring.lineage_decorator import lineage_job
40 | | from project_a.monitoring.metrics_collector import emit_rowcount, emit_duration
41 | | from project_a.utils.logging import setup_json_logging, get_trace_id
42 | | from project_a.config_loader import load_config_resolved
43 | |
44 | | import yaml
45 | | import time
   | |___________^
46 |
47 |   logging.basicConfig(level=logging.INFO)
   |
help: Organize imports

F401 [*] `pyspark.sql.types` imported but unused
  --> jobs/ingest/snowflake_to_bronze.py:25:50
   |
24 | from pyspark.sql import SparkSession, DataFrame
25 | from pyspark.sql import functions as F, types as T
   |                                                  ^
26 |
27 | from project_a.utils.spark_session import build_spark
   |
help: Remove unused import: `pyspark.sql.types`

F401 [*] `yaml` imported but unused
  --> jobs/ingest/snowflake_to_bronze.py:44:8
   |
42 | from project_a.config_loader import load_config_resolved
43 |
44 | import yaml
   |        ^^^^
45 | import time
   |
help: Remove unused import: `yaml`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> jobs/ingest/snowflake_to_bronze.py:58:13
   |
56 | def extract_snowflake_orders_production(
57 |     spark: SparkSession,
58 |     config: Dict[str, Any],
   |             ^^^^
59 |     run_id: str = None
60 | ) -> DataFrame:
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> jobs/ingest/snowflake_to_bronze.py:63:1
   |
61 |     """
62 |     Production-grade Snowflake orders extraction with all P0 features.
63 |     
   | ^^^^
64 |     Args:
65 |         spark: SparkSession
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> jobs/ingest/snowflake_to_bronze.py:68:1
   |
66 |         config: Configuration dict
67 |         run_id: Optional run identifier (UUID)
68 |         
   | ^^^^^^^^
69 |     Returns:
70 |         DataFrame with validated, enriched orders data
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/ingest/snowflake_to_bronze.py:74:1
   |
72 |     if run_id is None:
73 |         run_id = str(uuid.uuid4())
74 |     
   | ^^^^
75 |     logger.info(f"ðŸš€ Starting Snowflake orders extraction (run_id={run_id})")
76 |     start_time = time.time()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/ingest/snowflake_to_bronze.py:77:1
   |
75 |     logger.info(f"ðŸš€ Starting Snowflake orders extraction (run_id={run_id})")
76 |     start_time = time.time()
77 |     
   | ^^^^
78 |     # Get source configuration
79 |     source_cfg = config.get("sources", {}).get("snowflake_orders", {})
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/ingest/snowflake_to_bronze.py:83:1
   |
81 |     watermark_col = source_cfg.get("watermark_column", "updated_at")
82 |     watermark_key = source_cfg.get("watermark_state_key", "snowflake_orders_max_ts")
83 |     
   | ^^^^
84 |     try:
85 |         # 1. INCREMENTAL INGESTION: Read watermark
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/ingest/snowflake_to_bronze.py:95:1
   |
93 |             else:
94 |                 logger.info("ðŸ“Œ No watermark found, performing full load")
95 |         
   | ^^^^^^^^
96 |         # 2. EXTRACT: Read from Snowflake, demo files, or local sample
97 |         # Note: demo_mode and demo_path will be passed from main() via config
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:100:1
    |
 98 |         demo_mode = config.get('demo_mode', False)
 99 |         demo_path = config.get('demo_path')
100 |         
    | ^^^^^^^^
101 |         if demo_mode and demo_path:
102 |             # Demo mode: read from CSV files
    |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
   --> jobs/ingest/snowflake_to_bronze.py:112:74
    |
110 |             logger.info(f"âœ… Loaded {df.count()} records from demo files")
111 |         elif config.get('env') == 'dev' and config.get('environment') == 'local':
112 |             sample_path = config.get('paths', {}).get('snowflake_orders', 
    |                                                                          ^
113 |                 "data/samples/snowflake/snowflake_orders_100000.csv")
114 |             df = spark.read.option("header", "true").option("inferSchema", "true").csv(sample_path)
    |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:129:1
    |
127 |                 "sfWarehouse": snowflake_config.get("warehouse"),
128 |             }
129 |             
    | ^^^^^^^^^^^^
130 |             # Build query with watermark
131 |             cond = f"{watermark_col} > '{last_ts.isoformat()}'" if last_ts else "1=1"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:133:1
    |
131 |             cond = f"{watermark_col} > '{last_ts.isoformat()}'" if last_ts else "1=1"
132 |             query = f"SELECT * FROM ORDERS WHERE {cond} ORDER BY {watermark_col}"
133 |             
    | ^^^^^^^^^^^^
134 |             logger.info(f"ðŸ“¥ Executing Snowflake query: WHERE {cond}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:135:1
    |
134 |             logger.info(f"ðŸ“¥ Executing Snowflake query: WHERE {cond}")
135 |             
    | ^^^^^^^^^^^^
136 |             df = spark.read \
137 |                 .format("snowflake") \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:141:1
    |
139 |                 .option("query", query) \
140 |                 .load()
141 |         
    | ^^^^^^^^
142 |         if df.isEmpty():
143 |             logger.warning("âš ï¸  No new records found")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:145:1
    |
143 |             logger.warning("âš ï¸  No new records found")
144 |             return df
145 |         
    | ^^^^^^^^
146 |         # Add row_id for error tracking
147 |         df = add_row_id(df)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:148:1
    |
146 |         # Add row_id for error tracking
147 |         df = add_row_id(df)
148 |         
    | ^^^^^^^^
149 |         # 3. SCHEMA CONTRACT: Load and validate
150 |         contract_path = Path("config/schema_definitions/snowflake_orders_bronze.json")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:156:1
    |
154 |             struct = contract_to_struct_type(contract)
155 |             required_cols = contract.get("required_columns", [])
156 |             
    | ^^^^^^^^^^^^
157 |             # Align to schema
158 |             df, dq = align_to_schema(df, struct, required_cols)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:159:1
    |
157 |             # Align to schema
158 |             df, dq = align_to_schema(df, struct, required_cols)
159 |             
    | ^^^^^^^^^^^^
160 |             # Get lake root for error lanes
161 |             lake_root = config.get("paths", {}).get("bronze", "").replace("lake://", "")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:164:1
    |
162 |             if not lake_root.startswith("s3"):
163 |                 lake_root = resolve_path("lake://bronze", config=config)
164 |             
    | ^^^^^^^^^^^^
165 |             error_handler = ErrorLaneHandler(lake_root)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:166:1
    |
165 |             error_handler = ErrorLaneHandler(lake_root)
166 |             
    | ^^^^^^^^^^^^
167 |             # Validate and quarantine bad rows
168 |             clean_df, quarantined_df, validation_results = validate_and_quarantine(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:174:1
    |
172 |                 error_lane_path=error_handler.get_error_lane_path("bronze", "snowflake_orders", run_id)
173 |             )
174 |             
    | ^^^^^^^^^^^^
175 |             logger.info(f"âœ… Validation: {validation_results['clean_rows']}/{validation_results['total_rows']} rows passed")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:176:1
    |
175 |             logger.info(f"âœ… Validation: {validation_results['clean_rows']}/{validation_results['total_rows']} rows passed")
176 |             
    | ^^^^^^^^^^^^
177 |             if validation_results['quarantined_rows'] > 0:
178 |                 logger.warning(f"âš ï¸  {validation_results['quarantined_rows']} rows quarantined")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:179:1
    |
177 |             if validation_results['quarantined_rows'] > 0:
178 |                 logger.warning(f"âš ï¸  {validation_results['quarantined_rows']} rows quarantined")
179 |             
    | ^^^^^^^^^^^^
180 |             df = clean_df
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:181:1
    |
180 |             df = clean_df
181 |         
    | ^^^^^^^^
182 |         # 4. METADATA COLUMNS: Add run metadata
183 |         run_date = datetime.now().strftime("%Y-%m-%d")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:190:1
    |
188 |             run_date=run_date
189 |         )
190 |         
    | ^^^^^^^^
191 |         # 5. UPDATE WATERMARK: If incremental and records loaded
192 |         if load_type == "incremental" and not df.isEmpty():
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:197:1
    |
195 |                 state_store.set_watermark(watermark_key, max_ts.isoformat())
196 |                 logger.info(f"âœ… Updated watermark: {max_ts.isoformat()}")
197 |         
    | ^^^^^^^^
198 |         # 6. METRICS: Emit metrics
199 |         record_count = df.count()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:201:1
    |
199 |         record_count = df.count()
200 |         duration_ms = (time.time() - start_time) * 1000
201 |         
    | ^^^^^^^^
202 |         emit_rowcount("records_processed_total", record_count, {
203 |             "job": "bronze_snowflake_orders",
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:207:1
    |
205 |             "table": "orders"
206 |         }, config)
207 |         
    | ^^^^^^^^
208 |         emit_rowcount("dq_quarantined_total", validation_results.get('quarantined_rows', 0), {
209 |             "job": "bronze_snowflake_orders",
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:212:1
    |
210 |             "layer": "bronze"
211 |         }, config)
212 |         
    | ^^^^^^^^
213 |         emit_duration("latency_seconds", duration_ms / 1000.0, {
214 |             "job": "bronze_snowflake_orders",
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:217:1
    |
215 |             "stage": "extraction"
216 |         }, config)
217 |         
    | ^^^^^^^^
218 |         logger.info(f"âœ… Extraction complete: {record_count:,} records in {duration_ms:.0f}ms")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:219:1
    |
218 |         logger.info(f"âœ… Extraction complete: {record_count:,} records in {duration_ms:.0f}ms")
219 |         
    | ^^^^^^^^
220 |         return df
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:221:1
    |
220 |         return df
221 |         
    | ^^^^^^^^
222 |     except Exception as e:
223 |         logger.error(f"âŒ Extraction failed: {e}", exc_info=True)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:236:1
    |
234 |     parser.add_argument("--demo-path", help="Path to demo CSV files (S3 or local)")
235 |     args = parser.parse_args()
236 |     
    | ^^^^
237 |     # Setup structured logging
238 |     trace_id = get_trace_id()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:241:1
    |
239 |     setup_json_logging(level="INFO", include_trace_id=True)
240 |     logger.info(f"Job started (trace_id={trace_id}, env={args.env})")
241 |     
    | ^^^^
242 |     # Load configuration
243 |     if args.config:
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
   --> jobs/ingest/snowflake_to_bronze.py:249:13
    |
247 |               logger.info(f"Reading config from S3 using Spark: {config_path}")
248 |               # Use Spark's textFile to read from S3 - no boto3 needed
249 | /             from pyspark.sql import SparkSession
250 | |             import tempfile
251 | |             import yaml
    | |_______________________^
252 |               
253 |               # Create minimal Spark session just for reading config
    |
help: Organize imports

F401 [*] `yaml` imported but unused
   --> jobs/ingest/snowflake_to_bronze.py:251:20
    |
249 |             from pyspark.sql import SparkSession
250 |             import tempfile
251 |             import yaml
    |                    ^^^^
252 |             
253 |             # Create minimal Spark session just for reading config
    |
help: Remove unused import: `yaml`

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:252:1
    |
250 |             import tempfile
251 |             import yaml
252 |             
    | ^^^^^^^^^^^^
253 |             # Create minimal Spark session just for reading config
254 |             spark_temp = SparkSession.builder \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:258:1
    |
256 |                 .config("spark.sql.adaptive.enabled", "false") \
257 |                 .getOrCreate()
258 |             
    | ^^^^^^^^^^^^
259 |             try:
260 |                 # Read config file as text from S3 using Spark
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:263:1
    |
261 |                 config_lines = spark_temp.sparkContext.textFile(config_path).collect()
262 |                 config_content = "\n".join(config_lines)
263 |                 
    | ^^^^^^^^^^^^^^^^
264 |                 # Write to temp file for load_config_resolved
265 |                 tmp_file = tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False, encoding="utf-8")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:283:1
    |
281 |             config_path = Path("config/local.yaml")
282 |         config_path = str(config_path)
283 |     
    | ^^^^
284 |     logger.info(f"Loading config from: {config_path}")
285 |     config = load_config_resolved(config_path)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:286:1
    |
284 |     logger.info(f"Loading config from: {config_path}")
285 |     config = load_config_resolved(config_path)
286 |     
    | ^^^^
287 |     # Ensure environment is set correctly for EMR
288 |     if not config.get("environment"):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:291:1
    |
289 |         config["environment"] = "emr"
290 |         logger.info("Set environment=emr in config (was missing)")
291 |     
    | ^^^^
292 |     # Add demo mode flags to config if provided
293 |     if args.demo and args.demo_path:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:297:1
    |
295 |         config['demo_path'] = args.demo_path
296 |         logger.info(f"Demo mode enabled: {args.demo_path}")
297 |     
    | ^^^^
298 |     # Generate run ID
299 |     run_id = str(uuid.uuid4())
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:300:1
    |
298 |     # Generate run ID
299 |     run_id = str(uuid.uuid4())
300 |     
    | ^^^^
301 |     # Build Spark session
302 |     spark = build_spark(config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:303:1
    |
301 |     # Build Spark session
302 |     spark = build_spark(config)
303 |     
    | ^^^^
304 |     try:
305 |         # Run extraction
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:307:1
    |
305 |         # Run extraction
306 |         df = extract_snowflake_orders_production(spark, config, run_id=run_id)
307 |         
    | ^^^^^^^^
308 |         if df.isEmpty():
309 |             logger.warning("âš ï¸  No data to write")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:311:1
    |
309 |             logger.warning("âš ï¸  No data to write")
310 |             return 0
311 |         
    | ^^^^^^^^
312 |         # Write to bronze
313 |         bronze_path = resolve_path("lake://bronze", "snowflake", "orders", config=config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:315:1
    |
313 |         bronze_path = resolve_path("lake://bronze", "snowflake", "orders", config=config)
314 |         logger.info(f"ðŸ’¾ Writing to: {bronze_path}")
315 |         
    | ^^^^^^^^
316 |         df.write \
317 |             .format("delta") \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:323:1
    |
321 |             .partitionBy("_run_date") \
322 |             .save(bronze_path)
323 |         
    | ^^^^^^^^
324 |         logger.info("ðŸŽ‰ Snowflake to Bronze job completed successfully")
325 |         return 0
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:326:1
    |
324 |         logger.info("ðŸŽ‰ Snowflake to Bronze job completed successfully")
325 |         return 0
326 |         
    | ^^^^^^^^
327 |     except Exception as e:
328 |         logger.error(f"âŒ Job failed: {e}", exc_info=True)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/ingest/snowflake_to_bronze.py:330:1
    |
328 |         logger.error(f"âŒ Job failed: {e}", exc_info=True)
329 |         return 1
330 |         
    | ^^^^^^^^
331 |     finally:
332 |         spark.stop()
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> jobs/run_pipeline.py:17:1
   |
15 |         - fx_to_bronze
16 |         - kafka_events_to_bronze
17 |     
   | ^^^^
18 |     Transformations:
19 |         - bronze_to_silver
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> jobs/run_pipeline.py:21:1
   |
19 |         - bronze_to_silver
20 |         - silver_to_gold
21 |     
   | ^^^^
22 |     Optional:
23 |         - kafka_producer (local simulation)
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> jobs/run_pipeline.py:34:1
   |
32 |   sys.path.insert(0, str(Path(__file__).parent.parent / "src"))
33 |
34 | / from project_a.core.config import ProjectConfig
35 | | from project_a.core.base_job import BaseJob
   | |___________________________________________^
36 |
37 |   logging.basicConfig(
   |
help: Organize imports

F401 [*] `project_a.core.base_job.BaseJob` imported but unused
  --> jobs/run_pipeline.py:35:37
   |
34 | from project_a.core.config import ProjectConfig
35 | from project_a.core.base_job import BaseJob
   |                                     ^^^^^^^
36 |
37 | logging.basicConfig(
   |
help: Remove unused import: `project_a.core.base_job.BaseJob`

W293 [*] Blank line contains whitespace
  --> jobs/run_pipeline.py:50:1
   |
48 |         formatter_class=argparse.RawDescriptionHelpFormatter,
49 |     )
50 |     
   | ^^^^
51 |     parser.add_argument(
52 |         "--job",
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/run_pipeline.py:70:1
   |
68 |         help="Optional run date override (YYYY-MM-DD)",
69 |     )
70 |     
   | ^^^^
71 |     args = parser.parse_args()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/run_pipeline.py:72:1
   |
71 |     args = parser.parse_args()
72 |     
   | ^^^^
73 |     # Load configuration
74 |     try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/run_pipeline.py:80:1
   |
78 |         logger.error(f"âŒ Failed to load configuration: {e}")
79 |         sys.exit(1)
80 |     
   | ^^^^
81 |     # Route to appropriate job
82 |     job_name = args.job.lower()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/run_pipeline.py:83:1
   |
81 |     # Route to appropriate job
82 |     job_name = args.job.lower()
83 |     
   | ^^^^
84 |     try:
85 |         if job_name == "bronze_to_silver":
   |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
   --> jobs/run_pipeline.py:111:25
    |
109 | â€¦
110 | â€¦error(f"âŒ Unknown job: {job_name}")
111 | â€¦info(f"Supported jobs: bronze_to_silver, silver_to_gold, snowflake_to_bronze, redshift_to_bronze, crm_to_bronze, fx_to_bronze, kafka_events_to_bronze, kafka_producer")
    |       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
112 | â€¦t(1)
    |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
   --> jobs/run_pipeline.py:113:1
    |
111 | â€¦         logger.info(f"Supported jobs: bronze_to_silver, silver_to_gold, snowflake_to_bronze, redshift_to_bronze, crm_to_bronze, fx_â€¦
112 | â€¦         sys.exit(1)
113 | â€¦     
    ^^^^^^^^
114 | â€¦     # Execute job
115 | â€¦     logger.info(f"ðŸš€ Starting job: {job_name}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/run_pipeline.py:119:1
    |
117 |         logger.info(f"âœ… Job completed: {job_name}")
118 |         logger.info(f"Result: {result}")
119 |         
    | ^^^^^^^^
120 |     except ImportError as e:
121 |         logger.error(f"âŒ Failed to import job module: {e}")
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> jobs/transform/bronze_to_silver.py:8:1
   |
 6 |   """
 7 |
 8 | / import sys
 9 | | import argparse
10 | | import logging
11 | | import uuid
12 | | import time
13 | | from pathlib import Path
14 | | from typing import Dict, Any
15 | | from datetime import datetime
   | |_____________________________^
16 |
17 |   # Add src to path for local execution
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> jobs/transform/bronze_to_silver.py:14:1
   |
12 | import time
13 | from pathlib import Path
14 | from typing import Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
15 | from datetime import datetime
   |

E402 Module level import not at top of file
  --> jobs/transform/bronze_to_silver.py:23:1
   |
21 |     sys.path.insert(0, str(src_path))
22 |
23 | from pyspark.sql import SparkSession
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
24 |
25 | # Core utilities
   |

I001 [*] Import block is un-sorted or un-formatted
  --> jobs/transform/bronze_to_silver.py:23:1
   |
21 |       sys.path.insert(0, str(src_path))
22 |
23 | / from pyspark.sql import SparkSession
24 | |
25 | | # Core utilities
26 | | from project_a.utils.spark_session import build_spark
27 | | from project_a.pyspark_interview_project.utils.config_loader import load_config_resolved
28 | | from project_a.utils.logging import setup_json_logging, get_trace_id
29 | | from project_a.utils.run_audit import write_run_audit
30 | | from project_a.utils.path_resolver import resolve_data_path
31 | |
32 | | # Monitoring
33 | | from project_a.pyspark_interview_project.monitoring.lineage_decorator import lineage_job
34 | | from project_a.pyspark_interview_project.monitoring.metrics_collector import emit_rowcount, emit_duration
35 | |
36 | | # Bronze loaders
37 | | from project_a.pyspark_interview_project.transform.bronze_loaders import (
38 | |     load_crm_bronze_data,
39 | |     load_snowflake_bronze_data,
40 | |     load_redshift_behavior_bronze_data,
41 | |     load_kafka_bronze_data
42 | | )
43 | |
44 | | # Silver builders
45 | | from project_a.pyspark_interview_project.transform.silver_builders import (
46 | |     build_customers_silver,
47 | |     build_orders_silver,
48 | |     build_products_silver,
49 | |     build_behavior_silver
50 | | )
51 | |
52 | | # FX loader
53 | | from project_a.extract.fx_json_reader import read_fx_rates_from_bronze
54 | |
55 | | # Writer
56 | | from project_a.pyspark_interview_project.io.delta_writer import write_table
   | |___________________________________________________________________________^
57 |
58 |   logging.basicConfig(level=logging.INFO)
   |
help: Organize imports

E402 Module level import not at top of file
  --> jobs/transform/bronze_to_silver.py:26:1
   |
25 | # Core utilities
26 | from project_a.utils.spark_session import build_spark
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
27 | from project_a.pyspark_interview_project.utils.config_loader import load_config_resolved
28 | from project_a.utils.logging import setup_json_logging, get_trace_id
   |

E402 Module level import not at top of file
  --> jobs/transform/bronze_to_silver.py:27:1
   |
25 | # Core utilities
26 | from project_a.utils.spark_session import build_spark
27 | from project_a.pyspark_interview_project.utils.config_loader import load_config_resolved
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
28 | from project_a.utils.logging import setup_json_logging, get_trace_id
29 | from project_a.utils.run_audit import write_run_audit
   |

E402 Module level import not at top of file
  --> jobs/transform/bronze_to_silver.py:28:1
   |
26 | from project_a.utils.spark_session import build_spark
27 | from project_a.pyspark_interview_project.utils.config_loader import load_config_resolved
28 | from project_a.utils.logging import setup_json_logging, get_trace_id
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
29 | from project_a.utils.run_audit import write_run_audit
30 | from project_a.utils.path_resolver import resolve_data_path
   |

E402 Module level import not at top of file
  --> jobs/transform/bronze_to_silver.py:29:1
   |
27 | from project_a.pyspark_interview_project.utils.config_loader import load_config_resolved
28 | from project_a.utils.logging import setup_json_logging, get_trace_id
29 | from project_a.utils.run_audit import write_run_audit
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
30 | from project_a.utils.path_resolver import resolve_data_path
   |

E402 Module level import not at top of file
  --> jobs/transform/bronze_to_silver.py:30:1
   |
28 | from project_a.utils.logging import setup_json_logging, get_trace_id
29 | from project_a.utils.run_audit import write_run_audit
30 | from project_a.utils.path_resolver import resolve_data_path
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
31 |
32 | # Monitoring
   |

E402 Module level import not at top of file
  --> jobs/transform/bronze_to_silver.py:33:1
   |
32 | # Monitoring
33 | from project_a.pyspark_interview_project.monitoring.lineage_decorator import lineage_job
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
34 | from project_a.pyspark_interview_project.monitoring.metrics_collector import emit_rowcount, emit_duration
   |

E402 Module level import not at top of file
  --> jobs/transform/bronze_to_silver.py:34:1
   |
32 | # Monitoring
33 | from project_a.pyspark_interview_project.monitoring.lineage_decorator import lineage_job
34 | from project_a.pyspark_interview_project.monitoring.metrics_collector import emit_rowcount, emit_duration
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
35 |
36 | # Bronze loaders
   |

E402 Module level import not at top of file
  --> jobs/transform/bronze_to_silver.py:37:1
   |
36 |   # Bronze loaders
37 | / from project_a.pyspark_interview_project.transform.bronze_loaders import (
38 | |     load_crm_bronze_data,
39 | |     load_snowflake_bronze_data,
40 | |     load_redshift_behavior_bronze_data,
41 | |     load_kafka_bronze_data
42 | | )
   | |_^
43 |
44 |   # Silver builders
   |

E402 Module level import not at top of file
  --> jobs/transform/bronze_to_silver.py:45:1
   |
44 |   # Silver builders
45 | / from project_a.pyspark_interview_project.transform.silver_builders import (
46 | |     build_customers_silver,
47 | |     build_orders_silver,
48 | |     build_products_silver,
49 | |     build_behavior_silver
50 | | )
   | |_^
51 |
52 |   # FX loader
   |

E402 Module level import not at top of file
  --> jobs/transform/bronze_to_silver.py:53:1
   |
52 | # FX loader
53 | from project_a.extract.fx_json_reader import read_fx_rates_from_bronze
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
54 |
55 | # Writer
   |

E402 Module level import not at top of file
  --> jobs/transform/bronze_to_silver.py:56:1
   |
55 | # Writer
56 | from project_a.pyspark_interview_project.io.delta_writer import write_table
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
57 |
58 | logging.basicConfig(level=logging.INFO)
   |

W293 Blank line contains whitespace
  --> jobs/transform/bronze_to_silver.py:64:1
   |
62 | def build_fx_silver(df_fx):
63 |     """Build FX silver table with explicit schema.
64 |     
   | ^^^^
65 |     Expected schema:
66 |     - trade_date: DateType
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> jobs/transform/bronze_to_silver.py:71:5
   |
69 |       - rate: DoubleType
70 |       """
71 | /     from pyspark.sql import functions as F
72 | |     from pyspark.sql.types import StructType, StructField, DateType, StringType, DoubleType
   | |___________________________________________________________________________________________^
73 |       
74 |       logger.info("ðŸ”§ Building silver.fx_rates...")
   |
help: Organize imports

W293 [*] Blank line contains whitespace
  --> jobs/transform/bronze_to_silver.py:73:1
   |
71 |     from pyspark.sql import functions as F
72 |     from pyspark.sql.types import StructType, StructField, DateType, StringType, DoubleType
73 |     
   | ^^^^
74 |     logger.info("ðŸ”§ Building silver.fx_rates...")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/transform/bronze_to_silver.py:75:1
   |
74 |     logger.info("ðŸ”§ Building silver.fx_rates...")
75 |     
   | ^^^^
76 |     # Define expected schema
77 |     FX_RATES_SCHEMA = StructType([
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/transform/bronze_to_silver.py:83:1
   |
81 |         StructField("rate", DoubleType(), True),
82 |     ])
83 |     
   | ^^^^
84 |     # Check if input is empty
85 |     try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/transform/bronze_to_silver.py:92:1
   |
90 |         # If we can't check emptiness, try to build normally
91 |         pass
92 |     
   | ^^^^
93 |     # Map columns to expected schema
94 |     # FX DataFrame from read_fx_rates_from_bronze has: base_ccy, quote_ccy, rate, date, bid_rate, ask_rate
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/bronze_to_silver.py:102:1
    |
100 |         F.coalesce(F.col("rate"), F.col("exchange_rate"), F.col("fx_rate")).cast("double").alias("rate")
101 |     )
102 |     
    | ^^^^
103 |     # Ensure schema matches exactly
104 |     fx_silver = fx_silver.select(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/bronze_to_silver.py:110:1
    |
108 |         F.col("rate").cast("double").alias("rate")
109 |     )
110 |     
    | ^^^^
111 |     try:
112 |         row_count = fx_silver.count()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/bronze_to_silver.py:116:1
    |
114 |     except Exception:
115 |         pass
116 |     
    | ^^^^
117 |     return fx_silver
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
   --> jobs/transform/bronze_to_silver.py:122:5
    |
120 |   def build_order_events_silver(df_kafka, df_orders):
121 |       """Build order events silver from Kafka data."""
122 | /     from pyspark.sql import functions as F
123 | |     from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType
    | |________________________________________________________________________________________________^
124 |       
125 |       logger.info("ðŸ”§ Building silver.order_events...")
    |
help: Organize imports

W293 [*] Blank line contains whitespace
   --> jobs/transform/bronze_to_silver.py:124:1
    |
122 |     from pyspark.sql import functions as F
123 |     from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType
124 |     
    | ^^^^
125 |     logger.info("ðŸ”§ Building silver.order_events...")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/bronze_to_silver.py:126:1
    |
125 |     logger.info("ðŸ”§ Building silver.order_events...")
126 |     
    | ^^^^
127 |     # Check if Kafka data has direct columns or JSON in value column
128 |     if "order_id" in df_kafka.columns:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/bronze_to_silver.py:149:1
    |
147 |             StructField("channel", StringType(), True),
148 |         ])
149 |         
    | ^^^^^^^^
150 |         order_events = df_kafka.withColumn(
151 |             "parsed_value",
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/bronze_to_silver.py:175:1
    |
173 |         ])
174 |         return df_kafka.sparkSession.createDataFrame([], schema)
175 |     
    | ^^^^
176 |     try:
177 |         logger.info(f"âœ… silver.order_events: {order_events.count():,} rows")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/bronze_to_silver.py:180:1
    |
178 |     except Exception:
179 |         pass
180 |     
    | ^^^^
181 |     return order_events
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> jobs/transform/bronze_to_silver.py:191:13
    |
189 | def bronze_to_silver_complete(
190 |     spark: SparkSession,
191 |     config: Dict[str, Any],
    |             ^^^^
192 |     run_date: str = None
193 | ) -> Dict[str, Any]:
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> jobs/transform/bronze_to_silver.py:193:6
    |
191 |     config: Dict[str, Any],
192 |     run_date: str = None
193 | ) -> Dict[str, Any]:
    |      ^^^^
194 |     """
195 |     Complete bronze to silver transformation.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> jobs/transform/bronze_to_silver.py:196:1
    |
194 |     """
195 |     Complete bronze to silver transformation.
196 |     
    | ^^^^
197 |     Args:
198 |         spark: SparkSession
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> jobs/transform/bronze_to_silver.py:201:1
    |
199 |         config: Configuration dictionary
200 |         run_date: Processing date YYYY-MM-DD
201 |         
    | ^^^^^^^^
202 |     Returns:
203 |         Dictionary with silver DataFrames and metadata
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/bronze_to_silver.py:207:1
    |
205 |     if run_date is None:
206 |         run_date = datetime.utcnow().strftime("%Y-%m-%d")
207 |     
    | ^^^^
208 |     logger.info(f"ðŸš€ Starting bronze to silver transformation (run_date={run_date})")
209 |     start_time = time.time()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/bronze_to_silver.py:210:1
    |
208 |     logger.info(f"ðŸš€ Starting bronze to silver transformation (run_date={run_date})")
209 |     start_time = time.time()
210 |     
    | ^^^^
211 |     # Get paths
212 |     silver_root = resolve_data_path(config, "silver")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/bronze_to_silver.py:222:1
    |
220 |         "order_events": tables_config.get("order_events", "order_events_silver")
221 |     }
222 |     
    | ^^^^
223 |     try:
224 |         # 1. Load bronze data
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/bronze_to_silver.py:248:1
    |
246 |             df_fx = read_fx_rates_from_bronze(spark, bronze_root)
247 |         df_kafka = load_kafka_bronze_data(spark, config)
248 |         
    | ^^^^^^^^
249 |         # 2. Build silver tables
250 |         customers_silver = build_customers_silver(df_customers, df_accounts, df_contacts, df_opps, df_behavior)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/bronze_to_silver.py:256:1
    |
254 |         fx_silver = build_fx_silver(df_fx)
255 |         order_events_silver = build_order_events_silver(df_kafka, orders_silver)
256 |         
    | ^^^^^^^^
257 |         # 3. Write silver tables
258 |         write_table(customers_silver, silver_root, tables["customers"], config, partition_by=["country"])
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/bronze_to_silver.py:264:1
    |
262 |         write_table(fx_silver, silver_root, tables["fx_rates"], config, partition_by=["trade_date"])
263 |         write_table(order_events_silver, silver_root, tables["order_events"], config, partition_by=["event_timestamp"])
264 |         
    | ^^^^^^^^
265 |         # 4. Emit metrics
266 |         duration_ms = (time.time() - start_time) * 1000
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/bronze_to_silver.py:273:1
    |
271 |         except Exception as e:
272 |             logger.warning(f"Could not emit metrics: {e}")
273 |         
    | ^^^^^^^^
274 |         logger.info(f"âœ… Bronze to Silver completed in {duration_ms:.0f}ms")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/bronze_to_silver.py:275:1
    |
274 |         logger.info(f"âœ… Bronze to Silver completed in {duration_ms:.0f}ms")
275 |         
    | ^^^^^^^^
276 |         return {
277 |             "customers": customers_silver,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/bronze_to_silver.py:284:1
    |
282 |             "order_events": order_events_silver
283 |         }
284 |         
    | ^^^^^^^^
285 |     except Exception as e:
286 |         logger.error(f"âŒ Silver transformation failed: {e}", exc_info=True)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/bronze_to_silver.py:296:1
    |
294 |     parser.add_argument("--config", help="Config file path (local or s3://...)")
295 |     args = parser.parse_args()
296 |     
    | ^^^^
297 |     # Setup logging
298 |     trace_id = get_trace_id()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/bronze_to_silver.py:301:1
    |
299 |     setup_json_logging(level="INFO", include_trace_id=True)
300 |     logger.info(f"Job started (trace_id={trace_id}, env={args.env})")
301 |     
    | ^^^^
302 |     # Load config
303 |     if args.config:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/bronze_to_silver.py:312:1
    |
310 |         else:
311 |             config_path = f"config/{env}.yaml"
312 |     
    | ^^^^
313 |     try:
314 |         config = load_config_resolved(config_path, env=args.env)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/bronze_to_silver.py:318:1
    |
316 |         logger.error(f"Failed to load config: {e}")
317 |         raise
318 |     
    | ^^^^
319 |     # Set environment if not set
320 |     if not config.get("environment"):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/bronze_to_silver.py:322:1
    |
320 |     if not config.get("environment"):
321 |         config["environment"] = args.env
322 |     
    | ^^^^
323 |     run_id = str(uuid.uuid4())
324 |     start_time = time.time()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/bronze_to_silver.py:326:1
    |
324 |     start_time = time.time()
325 |     spark = build_spark(app_name="bronze_to_silver", config=config)
326 |     
    | ^^^^
327 |     try:
328 |         run_date = datetime.utcnow().strftime("%Y-%m-%d")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/bronze_to_silver.py:330:1
    |
328 |         run_date = datetime.utcnow().strftime("%Y-%m-%d")
329 |         results = bronze_to_silver_complete(spark, config, run_date=run_date)
330 |         
    | ^^^^^^^^
331 |         duration_ms = (time.time() - start_time) * 1000
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/bronze_to_silver.py:332:1
    |
331 |         duration_ms = (time.time() - start_time) * 1000
332 |         
    | ^^^^^^^^
333 |         logger.info("âœ… Bronze to Silver transformation completed")
334 |         try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/bronze_to_silver.py:343:1
    |
341 |         except Exception as e:
342 |             logger.warning(f"Could not log row counts: {e}")
343 |         
    | ^^^^^^^^
344 |         # Write run audit
345 |         lake_bucket = config.get("buckets", {}).get("lake", "")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/bronze_to_silver.py:363:1
    |
361 |             except Exception as e:
362 |                 logger.warning(f"âš ï¸  Failed to write run audit: {e}")
363 |         
    | ^^^^^^^^
364 |     except Exception as e:
365 |         logger.error(f"âŒ Transformation failed: {e}", exc_info=True)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/bronze_to_silver.py:366:1
    |
364 |     except Exception as e:
365 |         logger.error(f"âŒ Transformation failed: {e}", exc_info=True)
366 |         
    | ^^^^^^^^
367 |         # Write failure audit
368 |         lake_bucket = config.get("buckets", {}).get("lake", "")
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> jobs/transform/raw_to_bronze.py:7:1
   |
 5 |   """
 6 |
 7 | / import argparse
 8 | | import logging
 9 | | from typing import Dict, Any
10 | | from datetime import datetime
11 | |
12 | | from pyspark.sql import SparkSession, DataFrame
13 | | from pyspark.sql.functions import lit, current_timestamp, to_date
14 | |
15 | | from project_a.utils.spark_session import build_spark
16 | | from project_a.utils.config import load_conf
17 | | from project_a.schema_validator import validate_schema
   | |______________________________________________________^
18 |
19 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> jobs/transform/raw_to_bronze.py:9:1
   |
 7 | import argparse
 8 | import logging
 9 | from typing import Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 | from datetime import datetime
   |

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> jobs/transform/raw_to_bronze.py:24:13
   |
22 | def raw_to_bronze(
23 |     spark: SparkSession,
24 |     config: Dict[str, Any],
   |             ^^^^
25 |     source: str,
26 |     table: str,
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> jobs/transform/raw_to_bronze.py:31:1
   |
29 |     """
30 |     Transform raw zone data to bronze layer.
31 |     
   | ^^^^
32 |     Args:
33 |         spark: SparkSession
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> jobs/transform/raw_to_bronze.py:38:1
   |
36 |         table: Table name
37 |         execution_date: Execution date (YYYY-MM-DD)
38 |         
   | ^^^^^^^^
39 |     Returns:
40 |         Bronze DataFrame
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/transform/raw_to_bronze.py:44:1
   |
42 |     if execution_date is None:
43 |         execution_date = datetime.now().strftime("%Y-%m-%d")
44 |     
   | ^^^^
45 |     logger.info(f"Transforming {source}.{table} from raw to bronze")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/transform/raw_to_bronze.py:46:1
   |
45 |     logger.info(f"Transforming {source}.{table} from raw to bronze")
46 |     
   | ^^^^
47 |     # Read from raw zone
48 |     raw_path = f"s3://my-etl-lake-demo/raw/{source}/{table}/load_dt={execution_date}/"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/transform/raw_to_bronze.py:49:1
   |
47 |     # Read from raw zone
48 |     raw_path = f"s3://my-etl-lake-demo/raw/{source}/{table}/load_dt={execution_date}/"
49 |     
   | ^^^^
50 |     try:
51 |         df = spark.read.parquet(raw_path)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/transform/raw_to_bronze.py:55:1
   |
53 |         logger.error(f"Failed to read from raw: {e}")
54 |         raise
55 |     
   | ^^^^
56 |     # Load schema definition
57 |     schema_def_path = f"config/schema_definitions/bronze/{source}_{table}_bronze.json"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/transform/raw_to_bronze.py:58:1
   |
56 |     # Load schema definition
57 |     schema_def_path = f"config/schema_definitions/bronze/{source}_{table}_bronze.json"
58 |     
   | ^^^^
59 |     try:
60 |         import json
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/transform/raw_to_bronze.py:63:1
   |
61 |         with open(schema_def_path) as f:
62 |             schema_def = json.load(f)
63 |         
   | ^^^^^^^^
64 |         # Validate schema
65 |         df, validation_results = validate_schema(df, schema_def, mode="allow_new", config=config)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/transform/raw_to_bronze.py:66:1
   |
64 |         # Validate schema
65 |         df, validation_results = validate_schema(df, schema_def, mode="allow_new", config=config)
66 |         
   | ^^^^^^^^
67 |         if not validation_results.get("passed"):
68 |             logger.warning(f"Schema validation issues: {validation_results}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/transform/raw_to_bronze.py:71:1
   |
69 |     except FileNotFoundError:
70 |         logger.warning(f"Schema definition not found: {schema_def_path}, skipping validation")
71 |     
   | ^^^^
72 |     # Normalize to canonical columns
73 |     # Add standard metadata
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/transform/raw_to_bronze.py:78:1
   |
76 |            .withColumn("ingest_timestamp", current_timestamp()) \
77 |            .withColumn("_proc_date", to_date(lit(execution_date)))
78 |     
   | ^^^^
79 |     # Write to bronze
80 |     bronze_path = f"s3://my-etl-lake-demo/bronze/{source}/{table}/"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/transform/raw_to_bronze.py:82:1
   |
80 |     bronze_path = f"s3://my-etl-lake-demo/bronze/{source}/{table}/"
81 |     logger.info(f"Writing to bronze: {bronze_path}")
82 |     
   | ^^^^
83 |     df.write \
84 |         .format("delta") \
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/transform/raw_to_bronze.py:89:1
   |
87 |         .partitionBy("_proc_date") \
88 |         .save(bronze_path)
89 |     
   | ^^^^
90 |     record_count = df.count()
91 |     logger.info(f"âœ… Transformed {record_count:,} records to bronze")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/transform/raw_to_bronze.py:92:1
   |
90 |     record_count = df.count()
91 |     logger.info(f"âœ… Transformed {record_count:,} records to bronze")
92 |     
   | ^^^^
93 |     return df
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/raw_to_bronze.py:102:1
    |
100 |     parser.add_argument("--execution-date", help="Execution date")
101 |     parser.add_argument("--config", default="config/prod.yaml", help="Config file")
102 |     
    | ^^^^
103 |     args = parser.parse_args()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/raw_to_bronze.py:104:1
    |
103 |     args = parser.parse_args()
104 |     
    | ^^^^
105 |     config = load_conf(args.config)
106 |     spark = build_spark(app_name=f"raw_to_bronze_{args.source}_{args.table}", config=config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/raw_to_bronze.py:107:1
    |
105 |     config = load_conf(args.config)
106 |     spark = build_spark(app_name=f"raw_to_bronze_{args.source}_{args.table}", config=config)
107 |     
    | ^^^^
108 |     try:
109 |         raw_to_bronze(spark, config, args.source, args.table, args.execution_date)
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> jobs/transform/silver_to_gold.py:8:1
   |
 6 |   """
 7 |
 8 | / import sys
 9 | | import argparse
10 | | import logging
11 | | import uuid
12 | | import time
13 | | from pathlib import Path
14 | | from typing import Dict, Any
15 | | from datetime import datetime
   | |_____________________________^
16 |
17 |   # Add src to path for local execution
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> jobs/transform/silver_to_gold.py:14:1
   |
12 | import time
13 | from pathlib import Path
14 | from typing import Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
15 | from datetime import datetime
   |

E402 Module level import not at top of file
  --> jobs/transform/silver_to_gold.py:23:1
   |
21 |     sys.path.insert(0, str(src_path))
22 |
23 | from pyspark.sql import SparkSession, DataFrame
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
24 |
25 | # Core utilities
   |

I001 [*] Import block is un-sorted or un-formatted
  --> jobs/transform/silver_to_gold.py:23:1
   |
21 |       sys.path.insert(0, str(src_path))
22 |
23 | / from pyspark.sql import SparkSession, DataFrame
24 | |
25 | | # Core utilities
26 | | from project_a.utils.spark_session import build_spark
27 | | from project_a.pyspark_interview_project.utils.config_loader import load_config_resolved
28 | | from project_a.utils.logging import setup_json_logging, get_trace_id
29 | | from project_a.utils.run_audit import write_run_audit
30 | | from project_a.utils.path_resolver import resolve_data_path
31 | |
32 | | # Monitoring
33 | | from project_a.pyspark_interview_project.monitoring.lineage_decorator import lineage_job
34 | | from project_a.pyspark_interview_project.monitoring.metrics_collector import emit_rowcount, emit_duration
35 | |
36 | | # Gold builders
37 | | from project_a.pyspark_interview_project.transform.gold_builders import (
38 | |     build_dim_date,
39 | |     build_dim_customer,
40 | |     build_dim_product,
41 | |     build_fact_orders,
42 | |     build_customer_360,
43 | |     build_product_performance
44 | | )
45 | |
46 | | # Writer
47 | | from project_a.pyspark_interview_project.io.delta_writer import write_table, optimize_table
   | |___________________________________________________________________________________________^
48 |
49 |   logging.basicConfig(level=logging.INFO)
   |
help: Organize imports

E402 Module level import not at top of file
  --> jobs/transform/silver_to_gold.py:26:1
   |
25 | # Core utilities
26 | from project_a.utils.spark_session import build_spark
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
27 | from project_a.pyspark_interview_project.utils.config_loader import load_config_resolved
28 | from project_a.utils.logging import setup_json_logging, get_trace_id
   |

E402 Module level import not at top of file
  --> jobs/transform/silver_to_gold.py:27:1
   |
25 | # Core utilities
26 | from project_a.utils.spark_session import build_spark
27 | from project_a.pyspark_interview_project.utils.config_loader import load_config_resolved
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
28 | from project_a.utils.logging import setup_json_logging, get_trace_id
29 | from project_a.utils.run_audit import write_run_audit
   |

E402 Module level import not at top of file
  --> jobs/transform/silver_to_gold.py:28:1
   |
26 | from project_a.utils.spark_session import build_spark
27 | from project_a.pyspark_interview_project.utils.config_loader import load_config_resolved
28 | from project_a.utils.logging import setup_json_logging, get_trace_id
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
29 | from project_a.utils.run_audit import write_run_audit
30 | from project_a.utils.path_resolver import resolve_data_path
   |

E402 Module level import not at top of file
  --> jobs/transform/silver_to_gold.py:29:1
   |
27 | from project_a.pyspark_interview_project.utils.config_loader import load_config_resolved
28 | from project_a.utils.logging import setup_json_logging, get_trace_id
29 | from project_a.utils.run_audit import write_run_audit
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
30 | from project_a.utils.path_resolver import resolve_data_path
   |

E402 Module level import not at top of file
  --> jobs/transform/silver_to_gold.py:30:1
   |
28 | from project_a.utils.logging import setup_json_logging, get_trace_id
29 | from project_a.utils.run_audit import write_run_audit
30 | from project_a.utils.path_resolver import resolve_data_path
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
31 |
32 | # Monitoring
   |

E402 Module level import not at top of file
  --> jobs/transform/silver_to_gold.py:33:1
   |
32 | # Monitoring
33 | from project_a.pyspark_interview_project.monitoring.lineage_decorator import lineage_job
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
34 | from project_a.pyspark_interview_project.monitoring.metrics_collector import emit_rowcount, emit_duration
   |

E402 Module level import not at top of file
  --> jobs/transform/silver_to_gold.py:34:1
   |
32 | # Monitoring
33 | from project_a.pyspark_interview_project.monitoring.lineage_decorator import lineage_job
34 | from project_a.pyspark_interview_project.monitoring.metrics_collector import emit_rowcount, emit_duration
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
35 |
36 | # Gold builders
   |

E402 Module level import not at top of file
  --> jobs/transform/silver_to_gold.py:37:1
   |
36 |   # Gold builders
37 | / from project_a.pyspark_interview_project.transform.gold_builders import (
38 | |     build_dim_date,
39 | |     build_dim_customer,
40 | |     build_dim_product,
41 | |     build_fact_orders,
42 | |     build_customer_360,
43 | |     build_product_performance
44 | | )
   | |_^
45 |
46 |   # Writer
   |

E402 Module level import not at top of file
  --> jobs/transform/silver_to_gold.py:47:1
   |
46 | # Writer
47 | from project_a.pyspark_interview_project.io.delta_writer import write_table, optimize_table
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
48 |
49 | logging.basicConfig(level=logging.INFO)
   |

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> jobs/transform/silver_to_gold.py:57:13
   |
55 |     path: str,
56 |     table_name: str,
57 |     config: Dict[str, Any]
   |             ^^^^
58 | ) -> DataFrame:
59 |     """
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> jobs/transform/silver_to_gold.py:61:1
   |
59 |     """
60 |     Read Silver table with fallback: Parquet (local) â†’ Delta (AWS).
61 |     
   | ^^^^
62 |     Args:
63 |         spark: SparkSession
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> jobs/transform/silver_to_gold.py:67:1
   |
65 |         table_name: Table name for logging
66 |         config: Configuration dictionary
67 |         
   | ^^^^^^^^
68 |     Returns:
69 |         DataFrame or empty DataFrame if not found
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> jobs/transform/silver_to_gold.py:72:1
   |
70 |     """
71 |     write_format = "parquet" if (config.get("environment") or config.get("env", "local")) in ("local", "dev_local") else "delta"
72 |     
   | ^^^^
73 |     try:
74 |         df = spark.read.format(write_format).load(path)
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> jobs/transform/silver_to_gold.py:94:13
   |
92 |             logger.warning(f"{table_name} not found in either format, creating empty DataFrame: {e2}")
93 |             # Return empty DataFrame with minimal schema
94 |             from pyspark.sql.types import StructType, StructField, StringType
   |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
95 |             return spark.createDataFrame([], StructType([StructField("id", StringType(), True)]))
   |
help: Organize imports

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> jobs/transform/silver_to_gold.py:105:13
    |
103 | def silver_to_gold_complete(
104 |     spark: SparkSession,
105 |     config: Dict[str, Any],
    |             ^^^^
106 |     run_date: str = None
107 | ) -> Dict[str, DataFrame]:
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> jobs/transform/silver_to_gold.py:107:6
    |
105 |     config: Dict[str, Any],
106 |     run_date: str = None
107 | ) -> Dict[str, DataFrame]:
    |      ^^^^
108 |     """
109 |     Complete silver to gold transformation with star schema.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> jobs/transform/silver_to_gold.py:110:1
    |
108 |     """
109 |     Complete silver to gold transformation with star schema.
110 |     
    | ^^^^
111 |     Args:
112 |         spark: SparkSession
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> jobs/transform/silver_to_gold.py:115:1
    |
113 |         config: Configuration dictionary
114 |         run_date: Processing date YYYY-MM-DD
115 |         
    | ^^^^^^^^
116 |     Returns:
117 |         Dictionary with gold DataFrames
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/silver_to_gold.py:121:1
    |
119 |     if run_date is None:
120 |         run_date = datetime.utcnow().strftime("%Y-%m-%d")
121 |     
    | ^^^^
122 |     logger.info(f"ðŸš€ Starting silver to gold transformation (run_date={run_date})")
123 |     start_time = time.time()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/silver_to_gold.py:124:1
    |
122 |     logger.info(f"ðŸš€ Starting silver to gold transformation (run_date={run_date})")
123 |     start_time = time.time()
124 |     
    | ^^^^
125 |     # Get paths
126 |     silver_root = resolve_data_path(config, "silver")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/silver_to_gold.py:128:1
    |
126 |     silver_root = resolve_data_path(config, "silver")
127 |     gold_root = resolve_data_path(config, "gold")
128 |     
    | ^^^^
129 |     # Get table names from config
130 |     tables_config = config.get("tables", {})
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/silver_to_gold.py:133:1
    |
131 |     silver_tables = tables_config.get("silver", {})
132 |     gold_tables = tables_config.get("gold", {})
133 |     
    | ^^^^
134 |     silver_table_names = {
135 |         "customers": silver_tables.get("customers", "customers_silver"),
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/silver_to_gold.py:140:1
    |
138 |         "behavior": silver_tables.get("behavior", "customer_behavior_silver"),
139 |     }
140 |     
    | ^^^^
141 |     gold_table_names = {
142 |         "fact_orders": gold_tables.get("fact_orders", "fact_orders"),
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/silver_to_gold.py:149:1
    |
147 |         "product_performance": gold_tables.get("product_performance", "product_performance"),
148 |     }
149 |     
    | ^^^^
150 |     try:
151 |         # 1. Read silver tables
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/silver_to_gold.py:165:1
    |
163 |             spark, f"{silver_root}/{silver_table_names['behavior']}", "behavior_silver", config
164 |         )
165 |         
    | ^^^^^^^^
166 |         # 2. Build dimensions
167 |         logger.info("ðŸ”§ Building dimensions...")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/silver_to_gold.py:171:1
    |
169 |         dim_customer = build_dim_customer(customers_silver, spark)
170 |         dim_product = build_dim_product(products_silver, spark)
171 |         
    | ^^^^^^^^
172 |         # 3. Build fact tables
173 |         logger.info("ðŸ”§ Building fact tables...")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/silver_to_gold.py:175:1
    |
173 |         logger.info("ðŸ”§ Building fact tables...")
174 |         fact_orders = build_fact_orders(orders_silver, dim_customer, dim_product, dim_date, spark)
175 |         
    | ^^^^^^^^
176 |         # 4. Build analytics tables
177 |         logger.info("ðŸ”§ Building analytics tables...")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/silver_to_gold.py:180:1
    |
178 |         customer_360 = build_customer_360(dim_customer, orders_silver, behavior_silver, spark)
179 |         product_performance = build_product_performance(dim_product, orders_silver, spark)
180 |         
    | ^^^^^^^^
181 |         # 5. Write gold tables
182 |         logger.info("ðŸ’¾ Writing gold tables...")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/silver_to_gold.py:189:1
    |
187 |         write_table(customer_360, gold_root, gold_table_names["customer_360"], config, partition_by=["country"])
188 |         write_table(product_performance, gold_root, gold_table_names["product_performance"], config)
189 |         
    | ^^^^^^^^
190 |         # 6. Optimize large fact tables
191 |         logger.info("âš¡ Optimizing gold tables...")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/silver_to_gold.py:193:1
    |
191 |         logger.info("âš¡ Optimizing gold tables...")
192 |         optimize_table(spark, gold_root, gold_table_names["fact_orders"], config, z_order_by=["order_date", "customer_sk"])
193 |         
    | ^^^^^^^^
194 |         # 7. Emit metrics
195 |         duration_ms = (time.time() - start_time) * 1000
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/silver_to_gold.py:202:1
    |
200 |         except Exception as e:
201 |             logger.warning(f"Could not emit metrics: {e}")
202 |         
    | ^^^^^^^^
203 |         logger.info(f"âœ… Silver to Gold completed in {duration_ms:.0f}ms")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/silver_to_gold.py:204:1
    |
203 |         logger.info(f"âœ… Silver to Gold completed in {duration_ms:.0f}ms")
204 |         
    | ^^^^^^^^
205 |         return {
206 |             "fact_orders": fact_orders,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/silver_to_gold.py:213:1
    |
211 |             "product_performance": product_performance
212 |         }
213 |         
    | ^^^^^^^^
214 |     except Exception as e:
215 |         logger.error(f"âŒ Gold transformation failed: {e}", exc_info=True)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/silver_to_gold.py:225:1
    |
223 |     parser.add_argument("--config", help="Config file path (local or s3://...)")
224 |     args = parser.parse_args()
225 |     
    | ^^^^
226 |     # Setup logging
227 |     trace_id = get_trace_id()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/silver_to_gold.py:230:1
    |
228 |     setup_json_logging(level="INFO", include_trace_id=True)
229 |     logger.info(f"Job started (trace_id={trace_id}, env={args.env})")
230 |     
    | ^^^^
231 |     # Load config
232 |     if args.config:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/silver_to_gold.py:240:1
    |
238 |         else:
239 |             config_path = f"config/{env}.yaml"
240 |     
    | ^^^^
241 |     try:
242 |         config = load_config_resolved(config_path, env=args.env)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/silver_to_gold.py:246:1
    |
244 |         logger.error(f"Failed to load config: {e}")
245 |         raise
246 |     
    | ^^^^
247 |     # Set environment if not set
248 |     if not config.get("environment"):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/silver_to_gold.py:250:1
    |
248 |     if not config.get("environment"):
249 |         config["environment"] = args.env
250 |     
    | ^^^^
251 |     run_id = str(uuid.uuid4())
252 |     start_time = time.time()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/silver_to_gold.py:254:1
    |
252 |     start_time = time.time()
253 |     spark = build_spark(app_name="silver_to_gold", config=config)
254 |     
    | ^^^^
255 |     try:
256 |         run_date = datetime.utcnow().strftime("%Y-%m-%d")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/silver_to_gold.py:258:1
    |
256 |         run_date = datetime.utcnow().strftime("%Y-%m-%d")
257 |         results = silver_to_gold_complete(spark, config, run_date=run_date)
258 |         
    | ^^^^^^^^
259 |         duration_ms = (time.time() - start_time) * 1000
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/silver_to_gold.py:260:1
    |
259 |         duration_ms = (time.time() - start_time) * 1000
260 |         
    | ^^^^^^^^
261 |         logger.info("âœ… Silver to Gold transformation completed")
262 |         try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/silver_to_gold.py:271:1
    |
269 |         except Exception as e:
270 |             logger.warning(f"Could not log row counts: {e}")
271 |         
    | ^^^^^^^^
272 |         # Write run audit
273 |         lake_bucket = config.get("buckets", {}).get("lake", "")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/silver_to_gold.py:291:1
    |
289 |             except Exception as e:
290 |                 logger.warning(f"âš ï¸  Failed to write run audit: {e}")
291 |         
    | ^^^^^^^^
292 |     except Exception as e:
293 |         logger.error(f"âŒ Transformation failed: {e}", exc_info=True)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> jobs/transform/silver_to_gold.py:294:1
    |
292 |     except Exception as e:
293 |         logger.error(f"âŒ Transformation failed: {e}", exc_info=True)
294 |         
    | ^^^^^^^^
295 |         # Write failure audit
296 |         lake_bucket = config.get("buckets", {}).get("lake", "")
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> local/scripts/dq/check_data_quality.py:8:1
   |
 6 |   Compares results to ensure consistency.
 7 |   """
 8 | / import sys
 9 | | import os
10 | | from pathlib import Path
   | |________________________^
11 |
12 |   # Add src to path
   |
help: Organize imports

F401 [*] `os` imported but unused
  --> local/scripts/dq/check_data_quality.py:9:8
   |
 7 | """
 8 | import sys
 9 | import os
   |        ^^
10 | from pathlib import Path
   |
help: Remove unused import: `os`

E402 Module level import not at top of file
  --> local/scripts/dq/check_data_quality.py:16:1
   |
14 | sys.path.insert(0, str(PROJECT_ROOT / "src"))
15 |
16 | import logging
   | ^^^^^^^^^^^^^^
17 | from typing import Dict, Any, List
18 | from pyspark.sql import SparkSession, DataFrame
   |

I001 [*] Import block is un-sorted or un-formatted
  --> local/scripts/dq/check_data_quality.py:16:1
   |
14 |   sys.path.insert(0, str(PROJECT_ROOT / "src"))
15 |
16 | / import logging
17 | | from typing import Dict, Any, List
18 | | from pyspark.sql import SparkSession, DataFrame
19 | | from pyspark.sql import functions as F
20 | |
21 | | from project_a.utils.spark_session import build_spark
22 | | from project_a.config_loader import load_config_resolved
23 | | from project_a.utils.logging import setup_json_logging, get_trace_id
   | |____________________________________________________________________^
24 |
25 |   logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
   |
help: Organize imports

E402 Module level import not at top of file
  --> local/scripts/dq/check_data_quality.py:17:1
   |
16 | import logging
17 | from typing import Dict, Any, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
18 | from pyspark.sql import SparkSession, DataFrame
19 | from pyspark.sql import functions as F
   |

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> local/scripts/dq/check_data_quality.py:17:1
   |
16 | import logging
17 | from typing import Dict, Any, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
18 | from pyspark.sql import SparkSession, DataFrame
19 | from pyspark.sql import functions as F
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> local/scripts/dq/check_data_quality.py:17:1
   |
16 | import logging
17 | from typing import Dict, Any, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
18 | from pyspark.sql import SparkSession, DataFrame
19 | from pyspark.sql import functions as F
   |

E402 Module level import not at top of file
  --> local/scripts/dq/check_data_quality.py:18:1
   |
16 | import logging
17 | from typing import Dict, Any, List
18 | from pyspark.sql import SparkSession, DataFrame
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
19 | from pyspark.sql import functions as F
   |

F401 [*] `pyspark.sql.DataFrame` imported but unused
  --> local/scripts/dq/check_data_quality.py:18:39
   |
16 | import logging
17 | from typing import Dict, Any, List
18 | from pyspark.sql import SparkSession, DataFrame
   |                                       ^^^^^^^^^
19 | from pyspark.sql import functions as F
   |
help: Remove unused import: `pyspark.sql.DataFrame`

E402 Module level import not at top of file
  --> local/scripts/dq/check_data_quality.py:19:1
   |
17 | from typing import Dict, Any, List
18 | from pyspark.sql import SparkSession, DataFrame
19 | from pyspark.sql import functions as F
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
20 |
21 | from project_a.utils.spark_session import build_spark
   |

E402 Module level import not at top of file
  --> local/scripts/dq/check_data_quality.py:21:1
   |
19 | from pyspark.sql import functions as F
20 |
21 | from project_a.utils.spark_session import build_spark
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
22 | from project_a.config_loader import load_config_resolved
23 | from project_a.utils.logging import setup_json_logging, get_trace_id
   |

E402 Module level import not at top of file
  --> local/scripts/dq/check_data_quality.py:22:1
   |
21 | from project_a.utils.spark_session import build_spark
22 | from project_a.config_loader import load_config_resolved
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
23 | from project_a.utils.logging import setup_json_logging, get_trace_id
   |

E402 Module level import not at top of file
  --> local/scripts/dq/check_data_quality.py:23:1
   |
21 | from project_a.utils.spark_session import build_spark
22 | from project_a.config_loader import load_config_resolved
23 | from project_a.utils.logging import setup_json_logging, get_trace_id
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
24 |
25 | logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
   |

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> local/scripts/dq/check_data_quality.py:34:6
   |
32 |     table_path: str,
33 |     format_type: str = "parquet"
34 | ) -> Dict[str, Any]:
   |      ^^^^
35 |     """
36 |     Run basic data quality checks on a table.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> local/scripts/dq/check_data_quality.py:37:1
   |
35 |     """
36 |     Run basic data quality checks on a table.
37 |     
   | ^^^^
38 |     Returns:
39 |         Dictionary with quality metrics
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/dq/check_data_quality.py:42:1
   |
40 |     """
41 |     logger.info(f"ðŸ” Checking quality for {table_name} at {table_path}")
42 |     
   | ^^^^
43 |     try:
44 |         # Read table
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/dq/check_data_quality.py:51:1
   |
49 |         else:
50 |             df = spark.read.format(format_type).load(table_path)
51 |         
   | ^^^^^^^^
52 |         # Basic quality metrics
53 |         total_rows = df.count()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/dq/check_data_quality.py:54:1
   |
52 |         # Basic quality metrics
53 |         total_rows = df.count()
54 |         
   | ^^^^^^^^
55 |         # Check for nulls in key columns
56 |         null_counts = {}
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/dq/check_data_quality.py:60:1
   |
58 |             null_count = df.filter(F.col(col_name).isNull()).count()
59 |             null_counts[col_name] = null_count
60 |         
   | ^^^^^^^^
61 |         # Check for duplicates (on all columns)
62 |         distinct_rows = df.distinct().count()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/dq/check_data_quality.py:64:1
   |
62 |         distinct_rows = df.distinct().count()
63 |         duplicate_count = total_rows - distinct_rows
64 |         
   | ^^^^^^^^
65 |         # Check for empty table
66 |         is_empty = total_rows == 0
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/dq/check_data_quality.py:67:1
   |
65 |         # Check for empty table
66 |         is_empty = total_rows == 0
67 |         
   | ^^^^^^^^
68 |         # Schema info
69 |         schema_fields = len(df.schema.fields)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/dq/check_data_quality.py:70:1
   |
68 |         # Schema info
69 |         schema_fields = len(df.schema.fields)
70 |         
   | ^^^^^^^^
71 |         quality_metrics = {
72 |             "table_name": table_name,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/dq/check_data_quality.py:82:1
   |
80 |             "status": "PASS" if not is_empty and duplicate_count == 0 else "WARN"
81 |         }
82 |         
   | ^^^^^^^^
83 |         logger.info(f"  âœ… {table_name}: {total_rows:,} rows, {duplicate_count} duplicates")
84 |         return quality_metrics
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/dq/check_data_quality.py:85:1
   |
83 |         logger.info(f"  âœ… {table_name}: {total_rows:,} rows, {duplicate_count} duplicates")
84 |         return quality_metrics
85 |         
   | ^^^^^^^^
86 |     except Exception as e:
87 |         logger.error(f"  âŒ Failed to check {table_name}: {e}")
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> local/scripts/dq/check_data_quality.py:96:55
   |
96 | def check_bronze_quality(spark: SparkSession, config: Dict[str, Any]) -> List[Dict[str, Any]]:
   |                                                       ^^^^
97 |     """Check quality of bronze layer tables."""
98 |     logger.info("=" * 80)
   |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
  --> local/scripts/dq/check_data_quality.py:96:74
   |
96 | def check_bronze_quality(spark: SparkSession, config: Dict[str, Any]) -> List[Dict[str, Any]]:
   |                                                                          ^^^^
97 |     """Check quality of bronze layer tables."""
98 |     logger.info("=" * 80)
   |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> local/scripts/dq/check_data_quality.py:96:79
   |
96 | def check_bronze_quality(spark: SparkSession, config: Dict[str, Any]) -> List[Dict[str, Any]]:
   |                                                                               ^^^^
97 |     """Check quality of bronze layer tables."""
98 |     logger.info("=" * 80)
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:101:1
    |
 99 |     logger.info("BRONZE LAYER DATA QUALITY CHECK")
100 |     logger.info("=" * 80)
101 |     
    | ^^^^
102 |     bronze_root = config["paths"]["bronze_root"]
103 |     sources = config.get("sources", {})
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:104:1
    |
102 |     bronze_root = config["paths"]["bronze_root"]
103 |     sources = config.get("sources", {})
104 |     
    | ^^^^
105 |     results = []
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:106:1
    |
105 |     results = []
106 |     
    | ^^^^
107 |     # Check CRM data
108 |     if "crm" in sources:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:112:1
    |
110 |         base_path = crm_cfg.get("base_path", f"{bronze_root}/crm")
111 |         files = crm_cfg.get("files", {})
112 |         
    | ^^^^^^^^
113 |         for file_type, filename in files.items():
114 |             file_path = f"{base_path}/{filename}"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:117:1
    |
115 |             result = check_table_quality(spark, f"bronze.crm.{file_type}", file_path, "csv")
116 |             results.append(result)
117 |     
    | ^^^^
118 |     # Check Snowflake data
119 |     if "snowflake" in sources:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:123:1
    |
121 |         base_path = sf_cfg.get("base_path", f"{bronze_root}/snowflakes")
122 |         files = sf_cfg.get("files", {})
123 |         
    | ^^^^^^^^
124 |         for file_type, filename in files.items():
125 |             file_path = f"{base_path}/{filename}"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:128:1
    |
126 |             result = check_table_quality(spark, f"bronze.snowflake.{file_type}", file_path, "csv")
127 |             results.append(result)
128 |     
    | ^^^^
129 |     # Check Redshift data
130 |     if "redshift" in sources:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:134:1
    |
132 |         base_path = rs_cfg.get("base_path", f"{bronze_root}/redshift")
133 |         files = rs_cfg.get("files", {})
134 |         
    | ^^^^^^^^
135 |         for file_type, filename in files.items():
136 |             file_path = f"{base_path}/{filename}"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:139:1
    |
137 |             result = check_table_quality(spark, f"bronze.redshift.{file_type}", file_path, "csv")
138 |             results.append(result)
139 |     
    | ^^^^
140 |     # Check Kafka data
141 |     if "kafka_sim" in sources:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:145:1
    |
143 |         base_path = kafka_cfg.get("base_path", f"{bronze_root}/kafka")
144 |         files = kafka_cfg.get("files", {})
145 |         
    | ^^^^^^^^
146 |         for file_type, filename in files.items():
147 |             file_path = f"{base_path}/{filename}"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:150:1
    |
148 |             result = check_table_quality(spark, f"bronze.kafka.{file_type}", file_path, "csv")
149 |             results.append(result)
150 |     
    | ^^^^
151 |     # Check FX data
152 |     if "fx" in sources:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:156:1
    |
154 |         base_path = fx_cfg.get("base_path", f"{bronze_root}/fx")
155 |         files = fx_cfg.get("files", {})
156 |         
    | ^^^^^^^^
157 |         for file_type, filename in files.items():
158 |             if filename.endswith(".json"):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:162:1
    |
160 |                 result = check_table_quality(spark, f"bronze.fx.{file_type}", file_path, "json")
161 |                 results.append(result)
162 |     
    | ^^^^
163 |     return results
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/scripts/dq/check_data_quality.py:166:55
    |
166 | def check_silver_quality(spark: SparkSession, config: Dict[str, Any]) -> List[Dict[str, Any]]:
    |                                                       ^^^^
167 |     """Check quality of silver layer tables."""
168 |     logger.info("=" * 80)
    |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
   --> local/scripts/dq/check_data_quality.py:166:74
    |
166 | def check_silver_quality(spark: SparkSession, config: Dict[str, Any]) -> List[Dict[str, Any]]:
    |                                                                          ^^^^
167 |     """Check quality of silver layer tables."""
168 |     logger.info("=" * 80)
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/scripts/dq/check_data_quality.py:166:79
    |
166 | def check_silver_quality(spark: SparkSession, config: Dict[str, Any]) -> List[Dict[str, Any]]:
    |                                                                               ^^^^
167 |     """Check quality of silver layer tables."""
168 |     logger.info("=" * 80)
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:171:1
    |
169 |     logger.info("SILVER LAYER DATA QUALITY CHECK")
170 |     logger.info("=" * 80)
171 |     
    | ^^^^
172 |     silver_root = config["paths"]["silver_root"]
173 |     tables = config.get("tables", {}).get("silver", {})
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:174:1
    |
172 |     silver_root = config["paths"]["silver_root"]
173 |     tables = config.get("tables", {}).get("silver", {})
174 |     
    | ^^^^
175 |     results = []
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:176:1
    |
175 |     results = []
176 |     
    | ^^^^
177 |     # Determine format (delta or parquet)
178 |     format_type = "delta" if "delta" in silver_root.lower() or silver_root.startswith("s3://") else "parquet"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:179:1
    |
177 |     # Determine format (delta or parquet)
178 |     format_type = "delta" if "delta" in silver_root.lower() or silver_root.startswith("s3://") else "parquet"
179 |     
    | ^^^^
180 |     for table_key, table_name in tables.items():
181 |         table_path = f"{silver_root}/{table_name}"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:184:1
    |
182 |         result = check_table_quality(spark, f"silver.{table_key}", table_path, format_type)
183 |         results.append(result)
184 |     
    | ^^^^
185 |     return results
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/scripts/dq/check_data_quality.py:188:53
    |
188 | def check_gold_quality(spark: SparkSession, config: Dict[str, Any]) -> List[Dict[str, Any]]:
    |                                                     ^^^^
189 |     """Check quality of gold layer tables."""
190 |     logger.info("=" * 80)
    |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
   --> local/scripts/dq/check_data_quality.py:188:72
    |
188 | def check_gold_quality(spark: SparkSession, config: Dict[str, Any]) -> List[Dict[str, Any]]:
    |                                                                        ^^^^
189 |     """Check quality of gold layer tables."""
190 |     logger.info("=" * 80)
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/scripts/dq/check_data_quality.py:188:77
    |
188 | def check_gold_quality(spark: SparkSession, config: Dict[str, Any]) -> List[Dict[str, Any]]:
    |                                                                             ^^^^
189 |     """Check quality of gold layer tables."""
190 |     logger.info("=" * 80)
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:193:1
    |
191 |     logger.info("GOLD LAYER DATA QUALITY CHECK")
192 |     logger.info("=" * 80)
193 |     
    | ^^^^
194 |     gold_root = config["paths"]["gold_root"]
195 |     tables = config.get("tables", {}).get("gold", {})
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:196:1
    |
194 |     gold_root = config["paths"]["gold_root"]
195 |     tables = config.get("tables", {}).get("gold", {})
196 |     
    | ^^^^
197 |     results = []
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:198:1
    |
197 |     results = []
198 |     
    | ^^^^
199 |     # Determine format (delta or parquet)
200 |     format_type = "delta" if "delta" in gold_root.lower() or gold_root.startswith("s3://") else "parquet"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:201:1
    |
199 |     # Determine format (delta or parquet)
200 |     format_type = "delta" if "delta" in gold_root.lower() or gold_root.startswith("s3://") else "parquet"
201 |     
    | ^^^^
202 |     for table_key, table_name in tables.items():
203 |         table_path = f"{gold_root}/{table_name}"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:206:1
    |
204 |         result = check_table_quality(spark, f"gold.{table_key}", table_path, format_type)
205 |         results.append(result)
206 |     
    | ^^^^
207 |     return results
    |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
   --> local/scripts/dq/check_data_quality.py:210:35
    |
210 | def print_quality_report(results: List[Dict[str, Any]], layer: str):
    |                                   ^^^^
211 |     """Print formatted quality report."""
212 |     logger.info("")
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/scripts/dq/check_data_quality.py:210:40
    |
210 | def print_quality_report(results: List[Dict[str, Any]], layer: str):
    |                                        ^^^^
211 |     """Print formatted quality report."""
212 |     logger.info("")
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:217:1
    |
215 |     logger.info("=" * 80)
216 |     logger.info("")
217 |     
    | ^^^^
218 |     for result in results:
219 |         status_icon = "âœ…" if result.get("status") == "PASS" else "âš ï¸" if result.get("status") == "WARN" else "âŒ"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:222:1
    |
220 |         logger.info(f"{status_icon} {result['table_name']}")
221 |         logger.info(f"   Path: {result['table_path']}")
222 |         
    | ^^^^^^^^
223 |         if result.get("status") == "ERROR":
224 |             logger.info(f"   Error: {result.get('error', 'Unknown error')}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:230:1
    |
228 |             logger.info(f"   Duplicates: {result.get('duplicate_count', 0):,}")
229 |             logger.info(f"   Schema Fields: {result.get('schema_fields', 0)}")
230 |             
    | ^^^^^^^^^^^^
231 |             null_counts = result.get('null_counts', {})
232 |             if null_counts:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:234:1
    |
232 |             if null_counts:
233 |                 logger.info(f"   Nulls: {sum(null_counts.values()):,} total across checked columns")
234 |         
    | ^^^^^^^^
235 |         logger.info("")
    |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
   --> local/scripts/dq/check_data_quality.py:238:41
    |
238 | def compare_local_vs_aws(local_results: List[Dict[str, Any]], aws_results: List[Dict[str, Any]]):
    |                                         ^^^^
239 |     """Compare local and AWS quality results."""
240 |     logger.info("")
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/scripts/dq/check_data_quality.py:238:46
    |
238 | def compare_local_vs_aws(local_results: List[Dict[str, Any]], aws_results: List[Dict[str, Any]]):
    |                                              ^^^^
239 |     """Compare local and AWS quality results."""
240 |     logger.info("")
    |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
   --> local/scripts/dq/check_data_quality.py:238:76
    |
238 | def compare_local_vs_aws(local_results: List[Dict[str, Any]], aws_results: List[Dict[str, Any]]):
    |                                                                            ^^^^
239 |     """Compare local and AWS quality results."""
240 |     logger.info("")
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/scripts/dq/check_data_quality.py:238:81
    |
238 | def compare_local_vs_aws(local_results: List[Dict[str, Any]], aws_results: List[Dict[str, Any]]):
    |                                                                                 ^^^^
239 |     """Compare local and AWS quality results."""
240 |     logger.info("")
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:245:1
    |
243 |     logger.info("=" * 80)
244 |     logger.info("")
245 |     
    | ^^^^
246 |     # Create lookup dictionaries
247 |     local_lookup = {r["table_name"]: r for r in local_results}
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:249:1
    |
247 |     local_lookup = {r["table_name"]: r for r in local_results}
248 |     aws_lookup = {r["table_name"]: r for r in aws_results}
249 |     
    | ^^^^
250 |     all_tables = set(local_lookup.keys()) | set(aws_lookup.keys())
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:251:1
    |
250 |     all_tables = set(local_lookup.keys()) | set(aws_lookup.keys())
251 |     
    | ^^^^
252 |     for table_name in sorted(all_tables):
253 |         local_result = local_lookup.get(table_name)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:255:1
    |
253 |         local_result = local_lookup.get(table_name)
254 |         aws_result = aws_lookup.get(table_name)
255 |         
    | ^^^^^^^^
256 |         logger.info(f"ðŸ“Š {table_name}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:257:1
    |
256 |         logger.info(f"ðŸ“Š {table_name}")
257 |         
    | ^^^^^^^^
258 |         if local_result and aws_result:
259 |             local_rows = local_result.get("total_rows", 0)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:261:1
    |
259 |             local_rows = local_result.get("total_rows", 0)
260 |             aws_rows = aws_result.get("total_rows", 0)
261 |             
    | ^^^^^^^^^^^^
262 |             if local_rows == aws_rows:
263 |                 logger.info(f"   âœ… Row count match: {local_rows:,}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:266:1
    |
264 |             else:
265 |                 logger.info(f"   âš ï¸  Row count mismatch: Local={local_rows:,}, AWS={aws_rows:,}")
266 |             
    | ^^^^^^^^^^^^
267 |             local_dups = local_result.get("duplicate_count", 0)
268 |             aws_dups = aws_result.get("duplicate_count", 0)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:269:1
    |
267 |             local_dups = local_result.get("duplicate_count", 0)
268 |             aws_dups = aws_result.get("duplicate_count", 0)
269 |             
    | ^^^^^^^^^^^^
270 |             if local_dups == aws_dups:
271 |                 logger.info(f"   âœ… Duplicate count match: {local_dups}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:274:1
    |
272 |             else:
273 |                 logger.info(f"   âš ï¸  Duplicate count mismatch: Local={local_dups}, AWS={aws_dups}")
274 |         
    | ^^^^^^^^
275 |         elif local_result:
276 |             logger.info(f"   âš ï¸  Only in local: {local_result.get('total_rows', 0):,} rows")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:279:1
    |
277 |         elif aws_result:
278 |             logger.info(f"   âš ï¸  Only in AWS: {aws_result.get('total_rows', 0):,} rows")
279 |         
    | ^^^^^^^^
280 |         logger.info("")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:286:1
    |
284 |     """Main entry point for DQ checks."""
285 |     import argparse
286 |     
    | ^^^^
287 |     parser = argparse.ArgumentParser(description="Check data quality for local and AWS")
288 |     parser.add_argument("--env", default="local", choices=["local", "aws"], help="Environment to check")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:293:1
    |
291 |     parser.add_argument("--compare", action="store_true", help="Compare local vs AWS results")
292 |     args = parser.parse_args()
293 |     
    | ^^^^
294 |     # Determine config path
295 |     if args.config:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:301:1
    |
299 |     else:
300 |         config_path = str(PROJECT_ROOT / "aws/config/dev.yaml")
301 |     
    | ^^^^
302 |     logger.info("=" * 80)
303 |     logger.info("DATA QUALITY CHECK")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:309:1
    |
307 |     logger.info(f"Layer: {args.layer}")
308 |     logger.info("")
309 |     
    | ^^^^
310 |     # Load config
311 |     config = load_config_resolved(config_path)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:313:1
    |
311 |     config = load_config_resolved(config_path)
312 |     config["environment"] = args.env
313 |     
    | ^^^^
314 |     # Setup logging
315 |     trace_id = get_trace_id()
    |
help: Remove whitespace from blank line

F841 Local variable `trace_id` is assigned to but never used
   --> local/scripts/dq/check_data_quality.py:315:5
    |
314 |     # Setup logging
315 |     trace_id = get_trace_id()
    |     ^^^^^^^^
316 |     setup_json_logging(include_trace_id=True)
    |
help: Remove assignment to unused variable `trace_id`

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:317:1
    |
315 |     trace_id = get_trace_id()
316 |     setup_json_logging(include_trace_id=True)
317 |     
    | ^^^^
318 |     # Build Spark session
319 |     try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:328:1
    |
326 |         logger.error("")
327 |         sys.exit(1)
328 |     
    | ^^^^
329 |     try:
330 |         all_results = []
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:331:1
    |
329 |     try:
330 |         all_results = []
331 |         
    | ^^^^^^^^
332 |         # Check requested layers
333 |         if args.layer in ["bronze", "all"]:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:337:1
    |
335 |             all_results.extend(bronze_results)
336 |             print_quality_report(bronze_results, "bronze")
337 |         
    | ^^^^^^^^
338 |         if args.layer in ["silver", "all"]:
339 |             silver_results = check_silver_quality(spark, config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:342:1
    |
340 |             all_results.extend(silver_results)
341 |             print_quality_report(silver_results, "silver")
342 |         
    | ^^^^^^^^
343 |         if args.layer in ["gold", "all"]:
344 |             gold_results = check_gold_quality(spark, config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:347:1
    |
345 |             all_results.extend(gold_results)
346 |             print_quality_report(gold_results, "gold")
347 |         
    | ^^^^^^^^
348 |         # Summary
349 |         logger.info("")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:353:1
    |
351 |         logger.info("QUALITY SUMMARY")
352 |         logger.info("=" * 80)
353 |         
    | ^^^^^^^^
354 |         pass_count = sum(1 for r in all_results if r.get("status") == "PASS")
355 |         warn_count = sum(1 for r in all_results if r.get("status") == "WARN")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:357:1
    |
355 |         warn_count = sum(1 for r in all_results if r.get("status") == "WARN")
356 |         error_count = sum(1 for r in all_results if r.get("status") == "ERROR")
357 |         
    | ^^^^^^^^
358 |         logger.info(f"âœ… Pass: {pass_count}")
359 |         logger.info(f"âš ï¸  Warn: {warn_count}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:362:1
    |
360 |         logger.info(f"âŒ Error: {error_count}")
361 |         logger.info(f"ðŸ“Š Total Tables: {len(all_results)}")
362 |         
    | ^^^^^^^^
363 |         # Compare if requested
364 |         if args.compare:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_data_quality.py:370:1
    |
368 |             logger.info("  python local/scripts/dq/check_data_quality.py --env aws > aws_dq.txt")
369 |             logger.info("  # Then compare the files")
370 |         
    | ^^^^^^^^
371 |     finally:
372 |         spark.stop()
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> local/scripts/dq/check_s3_data_quality.py:8:1
   |
 6 |   This is useful when local Spark has compatibility issues.
 7 |   """
 8 | / import sys
 9 | | import os
10 | | from pathlib import Path
   | |________________________^
11 |
12 |   # Add src to path
   |
help: Organize imports

F401 [*] `os` imported but unused
  --> local/scripts/dq/check_s3_data_quality.py:9:8
   |
 7 | """
 8 | import sys
 9 | import os
   |        ^^
10 | from pathlib import Path
   |
help: Remove unused import: `os`

E402 Module level import not at top of file
  --> local/scripts/dq/check_s3_data_quality.py:16:1
   |
14 | sys.path.insert(0, str(PROJECT_ROOT / "src"))
15 |
16 | import logging
   | ^^^^^^^^^^^^^^
17 | import boto3
18 | from typing import Dict, Any, List
   |

I001 [*] Import block is un-sorted or un-formatted
  --> local/scripts/dq/check_s3_data_quality.py:16:1
   |
14 |   sys.path.insert(0, str(PROJECT_ROOT / "src"))
15 |
16 | / import logging
17 | | import boto3
18 | | from typing import Dict, Any, List
19 | | from botocore.exceptions import ClientError
20 | |
21 | | from project_a.config_loader import load_config_resolved
   | |________________________________________________________^
22 |
23 |   logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
   |
help: Organize imports

E402 Module level import not at top of file
  --> local/scripts/dq/check_s3_data_quality.py:17:1
   |
16 | import logging
17 | import boto3
   | ^^^^^^^^^^^^
18 | from typing import Dict, Any, List
19 | from botocore.exceptions import ClientError
   |

E402 Module level import not at top of file
  --> local/scripts/dq/check_s3_data_quality.py:18:1
   |
16 | import logging
17 | import boto3
18 | from typing import Dict, Any, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
19 | from botocore.exceptions import ClientError
   |

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> local/scripts/dq/check_s3_data_quality.py:18:1
   |
16 | import logging
17 | import boto3
18 | from typing import Dict, Any, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
19 | from botocore.exceptions import ClientError
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> local/scripts/dq/check_s3_data_quality.py:18:1
   |
16 | import logging
17 | import boto3
18 | from typing import Dict, Any, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
19 | from botocore.exceptions import ClientError
   |

E402 Module level import not at top of file
  --> local/scripts/dq/check_s3_data_quality.py:19:1
   |
17 | import boto3
18 | from typing import Dict, Any, List
19 | from botocore.exceptions import ClientError
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
20 |
21 | from project_a.config_loader import load_config_resolved
   |

E402 Module level import not at top of file
  --> local/scripts/dq/check_s3_data_quality.py:21:1
   |
19 | from botocore.exceptions import ClientError
20 |
21 | from project_a.config_loader import load_config_resolved
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
22 |
23 | logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
   |

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> local/scripts/dq/check_s3_data_quality.py:64:37
   |
64 | def check_bronze_s3_quality(config: Dict[str, Any]) -> List[Dict[str, Any]]:
   |                                     ^^^^
65 |     """Check quality of bronze layer in S3."""
66 |     logger.info("=" * 80)
   |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
  --> local/scripts/dq/check_s3_data_quality.py:64:56
   |
64 | def check_bronze_s3_quality(config: Dict[str, Any]) -> List[Dict[str, Any]]:
   |                                                        ^^^^
65 |     """Check quality of bronze layer in S3."""
66 |     logger.info("=" * 80)
   |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> local/scripts/dq/check_s3_data_quality.py:64:61
   |
64 | def check_bronze_s3_quality(config: Dict[str, Any]) -> List[Dict[str, Any]]:
   |                                                             ^^^^
65 |     """Check quality of bronze layer in S3."""
66 |     logger.info("=" * 80)
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> local/scripts/dq/check_s3_data_quality.py:69:1
   |
67 |     logger.info("BRONZE LAYER S3 DATA QUALITY CHECK")
68 |     logger.info("=" * 80)
69 |     
   | ^^^^
70 |     s3_client = boto3.client('s3', region_name=config.get("aws", {}).get("region", "us-east-1"))
71 |     bronze_root = config["paths"]["bronze_root"]
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/dq/check_s3_data_quality.py:72:1
   |
70 |     s3_client = boto3.client('s3', region_name=config.get("aws", {}).get("region", "us-east-1"))
71 |     bronze_root = config["paths"]["bronze_root"]
72 |     
   | ^^^^
73 |     # Parse S3 path
74 |     if not bronze_root.startswith("s3://"):
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/dq/check_s3_data_quality.py:77:1
   |
75 |         logger.error(f"Bronze root is not an S3 path: {bronze_root}")
76 |         return []
77 |     
   | ^^^^
78 |     bucket = bronze_root.replace("s3://", "").split("/")[0]
79 |     base_prefix = "/".join(bronze_root.replace("s3://", "").split("/")[1:])
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/dq/check_s3_data_quality.py:80:1
   |
78 |     bucket = bronze_root.replace("s3://", "").split("/")[0]
79 |     base_prefix = "/".join(bronze_root.replace("s3://", "").split("/")[1:])
80 |     
   | ^^^^
81 |     sources = config.get("sources", {})
82 |     results = []
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/dq/check_s3_data_quality.py:83:1
   |
81 |     sources = config.get("sources", {})
82 |     results = []
83 |     
   | ^^^^
84 |     # Check CRM data
85 |     if "crm" in sources:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/dq/check_s3_data_quality.py:89:1
   |
87 |         base_path = crm_cfg.get("base_path", f"{bronze_root}/crm")
88 |         files = crm_cfg.get("files", {})
89 |         
   | ^^^^^^^^
90 |         for file_type, filename in files.items():
91 |             if base_path.startswith("s3://"):
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/dq/check_s3_data_quality.py:97:1
   |
95 |                 file_bucket = bucket
96 |                 file_prefix = f"{base_prefix}/crm/{filename}"
97 |             
   | ^^^^^^^^^^^^
98 |             exists = check_s3_path_exists(s3_client, file_bucket, file_prefix)
99 |             size = get_s3_path_size(s3_client, file_bucket, file_prefix) if exists else 0
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:100:1
    |
 98 |             exists = check_s3_path_exists(s3_client, file_bucket, file_prefix)
 99 |             size = get_s3_path_size(s3_client, file_bucket, file_prefix) if exists else 0
100 |             
    | ^^^^^^^^^^^^
101 |             results.append({
102 |                 "table_name": f"bronze.crm.{file_type}",
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:108:1
    |
106 |                 "status": "PASS" if exists and size > 0 else "ERROR"
107 |             })
108 |     
    | ^^^^
109 |     # Check Snowflake data
110 |     if "snowflake" in sources:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:114:1
    |
112 |         base_path = sf_cfg.get("base_path", f"{bronze_root}/snowflakes")
113 |         files = sf_cfg.get("files", {})
114 |         
    | ^^^^^^^^
115 |         for file_type, filename in files.items():
116 |             if base_path.startswith("s3://"):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:122:1
    |
120 |                 file_bucket = bucket
121 |                 file_prefix = f"{base_prefix}/snowflakes/{filename}"
122 |             
    | ^^^^^^^^^^^^
123 |             exists = check_s3_path_exists(s3_client, file_bucket, file_prefix)
124 |             size = get_s3_path_size(s3_client, file_bucket, file_prefix) if exists else 0
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:125:1
    |
123 |             exists = check_s3_path_exists(s3_client, file_bucket, file_prefix)
124 |             size = get_s3_path_size(s3_client, file_bucket, file_prefix) if exists else 0
125 |             
    | ^^^^^^^^^^^^
126 |             results.append({
127 |                 "table_name": f"bronze.snowflake.{file_type}",
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:133:1
    |
131 |                 "status": "PASS" if exists and size > 0 else "ERROR"
132 |             })
133 |     
    | ^^^^
134 |     # Check Redshift data
135 |     if "redshift" in sources:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:139:1
    |
137 |         base_path = rs_cfg.get("base_path", f"{bronze_root}/redshift")
138 |         files = rs_cfg.get("files", {})
139 |         
    | ^^^^^^^^
140 |         for file_type, filename in files.items():
141 |             if base_path.startswith("s3://"):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:147:1
    |
145 |                 file_bucket = bucket
146 |                 file_prefix = f"{base_prefix}/redshift/{filename}"
147 |             
    | ^^^^^^^^^^^^
148 |             exists = check_s3_path_exists(s3_client, file_bucket, file_prefix)
149 |             size = get_s3_path_size(s3_client, file_bucket, file_prefix) if exists else 0
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:150:1
    |
148 |             exists = check_s3_path_exists(s3_client, file_bucket, file_prefix)
149 |             size = get_s3_path_size(s3_client, file_bucket, file_prefix) if exists else 0
150 |             
    | ^^^^^^^^^^^^
151 |             results.append({
152 |                 "table_name": f"bronze.redshift.{file_type}",
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:158:1
    |
156 |                 "status": "PASS" if exists and size > 0 else "ERROR"
157 |             })
158 |     
    | ^^^^
159 |     # Check Kafka data
160 |     if "kafka_sim" in sources:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:164:1
    |
162 |         base_path = kafka_cfg.get("base_path", f"{bronze_root}/kafka")
163 |         files = kafka_cfg.get("files", {})
164 |         
    | ^^^^^^^^
165 |         for file_type, filename in files.items():
166 |             if base_path.startswith("s3://"):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:172:1
    |
170 |                 file_bucket = bucket
171 |                 file_prefix = f"{base_prefix}/kafka/{filename}"
172 |             
    | ^^^^^^^^^^^^
173 |             exists = check_s3_path_exists(s3_client, file_bucket, file_prefix)
174 |             size = get_s3_path_size(s3_client, file_bucket, file_prefix) if exists else 0
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:175:1
    |
173 |             exists = check_s3_path_exists(s3_client, file_bucket, file_prefix)
174 |             size = get_s3_path_size(s3_client, file_bucket, file_prefix) if exists else 0
175 |             
    | ^^^^^^^^^^^^
176 |             results.append({
177 |                 "table_name": f"bronze.kafka.{file_type}",
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:183:1
    |
181 |                 "status": "PASS" if exists and size > 0 else "ERROR"
182 |             })
183 |     
    | ^^^^
184 |     # Check FX data
185 |     if "fx" in sources:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:189:1
    |
187 |         base_path = fx_cfg.get("base_path", f"{bronze_root}/fx")
188 |         files = fx_cfg.get("files", {})
189 |         
    | ^^^^^^^^
190 |         for file_type, filename in files.items():
191 |             if base_path.startswith("s3://"):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:197:1
    |
195 |                 file_bucket = bucket
196 |                 file_prefix = f"{base_prefix}/fx/{filename}"
197 |             
    | ^^^^^^^^^^^^
198 |             exists = check_s3_path_exists(s3_client, file_bucket, file_prefix)
199 |             size = get_s3_path_size(s3_client, file_bucket, file_prefix) if exists else 0
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:200:1
    |
198 |             exists = check_s3_path_exists(s3_client, file_bucket, file_prefix)
199 |             size = get_s3_path_size(s3_client, file_bucket, file_prefix) if exists else 0
200 |             
    | ^^^^^^^^^^^^
201 |             results.append({
202 |                 "table_name": f"bronze.fx.{file_type}",
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:208:1
    |
206 |                 "status": "PASS" if exists and size > 0 else "ERROR"
207 |             })
208 |     
    | ^^^^
209 |     return results
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/scripts/dq/check_s3_data_quality.py:212:37
    |
212 | def check_silver_s3_quality(config: Dict[str, Any]) -> List[Dict[str, Any]]:
    |                                     ^^^^
213 |     """Check quality of silver layer in S3."""
214 |     logger.info("=" * 80)
    |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
   --> local/scripts/dq/check_s3_data_quality.py:212:56
    |
212 | def check_silver_s3_quality(config: Dict[str, Any]) -> List[Dict[str, Any]]:
    |                                                        ^^^^
213 |     """Check quality of silver layer in S3."""
214 |     logger.info("=" * 80)
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/scripts/dq/check_s3_data_quality.py:212:61
    |
212 | def check_silver_s3_quality(config: Dict[str, Any]) -> List[Dict[str, Any]]:
    |                                                             ^^^^
213 |     """Check quality of silver layer in S3."""
214 |     logger.info("=" * 80)
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:217:1
    |
215 |     logger.info("SILVER LAYER S3 DATA QUALITY CHECK")
216 |     logger.info("=" * 80)
217 |     
    | ^^^^
218 |     s3_client = boto3.client('s3', region_name=config.get("aws", {}).get("region", "us-east-1"))
219 |     silver_root = config["paths"]["silver_root"]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:220:1
    |
218 |     s3_client = boto3.client('s3', region_name=config.get("aws", {}).get("region", "us-east-1"))
219 |     silver_root = config["paths"]["silver_root"]
220 |     
    | ^^^^
221 |     # Parse S3 path
222 |     if not silver_root.startswith("s3://"):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:225:1
    |
223 |         logger.error(f"Silver root is not an S3 path: {silver_root}")
224 |         return []
225 |     
    | ^^^^
226 |     bucket = silver_root.replace("s3://", "").split("/")[0]
227 |     base_prefix = "/".join(silver_root.replace("s3://", "").split("/")[1:])
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:228:1
    |
226 |     bucket = silver_root.replace("s3://", "").split("/")[0]
227 |     base_prefix = "/".join(silver_root.replace("s3://", "").split("/")[1:])
228 |     
    | ^^^^
229 |     tables = config.get("tables", {}).get("silver", {})
230 |     results = []
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:231:1
    |
229 |     tables = config.get("tables", {}).get("silver", {})
230 |     results = []
231 |     
    | ^^^^
232 |     for table_key, table_name in tables.items():
233 |         table_prefix = f"{base_prefix}/{table_name}"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:234:1
    |
232 |     for table_key, table_name in tables.items():
233 |         table_prefix = f"{base_prefix}/{table_name}"
234 |         
    | ^^^^^^^^
235 |         exists = check_s3_path_exists(s3_client, bucket, table_prefix)
236 |         object_count = count_s3_objects(s3_client, bucket, table_prefix) if exists else 0
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:238:1
    |
236 |         object_count = count_s3_objects(s3_client, bucket, table_prefix) if exists else 0
237 |         size = get_s3_path_size(s3_client, bucket, table_prefix) if exists else 0
238 |         
    | ^^^^^^^^
239 |         results.append({
240 |             "table_name": f"silver.{table_key}",
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:247:1
    |
245 |             "status": "PASS" if exists and object_count > 0 else "ERROR"
246 |         })
247 |     
    | ^^^^
248 |     return results
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/scripts/dq/check_s3_data_quality.py:251:35
    |
251 | def check_gold_s3_quality(config: Dict[str, Any]) -> List[Dict[str, Any]]:
    |                                   ^^^^
252 |     """Check quality of gold layer in S3."""
253 |     logger.info("=" * 80)
    |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
   --> local/scripts/dq/check_s3_data_quality.py:251:54
    |
251 | def check_gold_s3_quality(config: Dict[str, Any]) -> List[Dict[str, Any]]:
    |                                                      ^^^^
252 |     """Check quality of gold layer in S3."""
253 |     logger.info("=" * 80)
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/scripts/dq/check_s3_data_quality.py:251:59
    |
251 | def check_gold_s3_quality(config: Dict[str, Any]) -> List[Dict[str, Any]]:
    |                                                           ^^^^
252 |     """Check quality of gold layer in S3."""
253 |     logger.info("=" * 80)
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:256:1
    |
254 |     logger.info("GOLD LAYER S3 DATA QUALITY CHECK")
255 |     logger.info("=" * 80)
256 |     
    | ^^^^
257 |     s3_client = boto3.client('s3', region_name=config.get("aws", {}).get("region", "us-east-1"))
258 |     gold_root = config["paths"]["gold_root"]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:259:1
    |
257 |     s3_client = boto3.client('s3', region_name=config.get("aws", {}).get("region", "us-east-1"))
258 |     gold_root = config["paths"]["gold_root"]
259 |     
    | ^^^^
260 |     # Parse S3 path
261 |     if not gold_root.startswith("s3://"):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:264:1
    |
262 |         logger.error(f"Gold root is not an S3 path: {gold_root}")
263 |         return []
264 |     
    | ^^^^
265 |     bucket = gold_root.replace("s3://", "").split("/")[0]
266 |     base_prefix = "/".join(gold_root.replace("s3://", "").split("/")[1:])
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:267:1
    |
265 |     bucket = gold_root.replace("s3://", "").split("/")[0]
266 |     base_prefix = "/".join(gold_root.replace("s3://", "").split("/")[1:])
267 |     
    | ^^^^
268 |     tables = config.get("tables", {}).get("gold", {})
269 |     results = []
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:270:1
    |
268 |     tables = config.get("tables", {}).get("gold", {})
269 |     results = []
270 |     
    | ^^^^
271 |     for table_key, table_name in tables.items():
272 |         table_prefix = f"{base_prefix}/{table_name}"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:273:1
    |
271 |     for table_key, table_name in tables.items():
272 |         table_prefix = f"{base_prefix}/{table_name}"
273 |         
    | ^^^^^^^^
274 |         exists = check_s3_path_exists(s3_client, bucket, table_prefix)
275 |         object_count = count_s3_objects(s3_client, bucket, table_prefix) if exists else 0
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:277:1
    |
275 |         object_count = count_s3_objects(s3_client, bucket, table_prefix) if exists else 0
276 |         size = get_s3_path_size(s3_client, bucket, table_prefix) if exists else 0
277 |         
    | ^^^^^^^^
278 |         results.append({
279 |             "table_name": f"gold.{table_key}",
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:286:1
    |
284 |             "status": "PASS" if exists and object_count > 0 else "ERROR"
285 |         })
286 |     
    | ^^^^
287 |     return results
    |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
   --> local/scripts/dq/check_s3_data_quality.py:290:35
    |
290 | def print_quality_report(results: List[Dict[str, Any]], layer: str):
    |                                   ^^^^
291 |     """Print formatted quality report."""
292 |     logger.info("")
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/scripts/dq/check_s3_data_quality.py:290:40
    |
290 | def print_quality_report(results: List[Dict[str, Any]], layer: str):
    |                                        ^^^^
291 |     """Print formatted quality report."""
292 |     logger.info("")
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:297:1
    |
295 |     logger.info("=" * 80)
296 |     logger.info("")
297 |     
    | ^^^^
298 |     for result in results:
299 |         status_icon = "âœ…" if result.get("status") == "PASS" else "âŒ"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:302:1
    |
300 |         logger.info(f"{status_icon} {result['table_name']}")
301 |         logger.info(f"   Path: {result['table_path']}")
302 |         
    | ^^^^^^^^
303 |         if result.get("status") == "ERROR":
304 |             logger.info(f"   Status: ERROR - File/table not found or empty")
    |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
   --> local/scripts/dq/check_s3_data_quality.py:304:25
    |
303 |         if result.get("status") == "ERROR":
304 |             logger.info(f"   Status: ERROR - File/table not found or empty")
    |                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
305 |         else:
306 |             size_mb = result.get("size_bytes", 0) / (1024 * 1024)
    |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:310:1
    |
308 |             if "object_count" in result:
309 |                 logger.info(f"   Objects: {result['object_count']}")
310 |         
    | ^^^^^^^^
311 |         logger.info("")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:317:1
    |
315 |     """Main entry point for S3 DQ checks."""
316 |     import argparse
317 |     
    | ^^^^
318 |     parser = argparse.ArgumentParser(description="Check S3 data quality (no Spark required)")
319 |     parser.add_argument("--env", default="local", choices=["local", "aws"], help="Environment")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:323:1
    |
321 |     parser.add_argument("--layer", choices=["bronze", "silver", "gold", "all"], default="all")
322 |     args = parser.parse_args()
323 |     
    | ^^^^
324 |     # Determine config path
325 |     if args.config:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:332:1
    |
330 |     else:
331 |         config_path = str(PROJECT_ROOT / "aws/config/dev.yaml")
332 |     
    | ^^^^
333 |     logger.info("=" * 80)
334 |     logger.info("S3 DATA QUALITY CHECK (No Spark Required)")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:340:1
    |
338 |     logger.info(f"Layer: {args.layer}")
339 |     logger.info("")
340 |     
    | ^^^^
341 |     # Load config
342 |     config = load_config_resolved(config_path)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:344:1
    |
342 |     config = load_config_resolved(config_path)
343 |     config["environment"] = args.env
344 |     
    | ^^^^
345 |     all_results = []
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:346:1
    |
345 |     all_results = []
346 |     
    | ^^^^
347 |     # Check requested layers
348 |     if args.layer in ["bronze", "all"]:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:352:1
    |
350 |         all_results.extend(bronze_results)
351 |         print_quality_report(bronze_results, "bronze")
352 |     
    | ^^^^
353 |     if args.layer in ["silver", "all"]:
354 |         silver_results = check_silver_s3_quality(config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:357:1
    |
355 |         all_results.extend(silver_results)
356 |         print_quality_report(silver_results, "silver")
357 |     
    | ^^^^
358 |     if args.layer in ["gold", "all"]:
359 |         gold_results = check_gold_s3_quality(config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:362:1
    |
360 |         all_results.extend(gold_results)
361 |         print_quality_report(gold_results, "gold")
362 |     
    | ^^^^
363 |     # Summary
364 |     logger.info("")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:368:1
    |
366 |     logger.info("QUALITY SUMMARY")
367 |     logger.info("=" * 80)
368 |     
    | ^^^^
369 |     pass_count = sum(1 for r in all_results if r.get("status") == "PASS")
370 |     error_count = sum(1 for r in all_results if r.get("status") == "ERROR")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:371:1
    |
369 |     pass_count = sum(1 for r in all_results if r.get("status") == "PASS")
370 |     error_count = sum(1 for r in all_results if r.get("status") == "ERROR")
371 |     
    | ^^^^
372 |     logger.info(f"âœ… Pass: {pass_count}")
373 |     logger.info(f"âŒ Error: {error_count}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/dq/check_s3_data_quality.py:375:1
    |
373 |     logger.info(f"âŒ Error: {error_count}")
374 |     logger.info(f"ðŸ“Š Total Tables: {len(all_results)}")
375 |     
    | ^^^^
376 |     if error_count > 0:
377 |         logger.warning("")
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> local/scripts/dq/compare_local_aws_dq.py:7:1
   |
 5 |   Runs DQ checks on both environments and compares results.
 6 |   """
 7 | / import sys
 8 | | import subprocess
 9 | | import json
10 | | import logging
11 | | from pathlib import Path
   | |________________________^
12 |
13 |   logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
   |
help: Organize imports

F401 [*] `json` imported but unused
  --> local/scripts/dq/compare_local_aws_dq.py:9:8
   |
 7 | import sys
 8 | import subprocess
 9 | import json
   |        ^^^^
10 | import logging
11 | from pathlib import Path
   |
help: Remove unused import: `json`

W293 [*] Blank line contains whitespace
  --> local/scripts/dq/compare_local_aws_dq.py:22:1
   |
20 |     """Run DQ check and parse results."""
21 |     logger.info(f"Running DQ check for {env} environment...")
22 |     
   | ^^^^
23 |     script_path = PROJECT_ROOT / "local/scripts/dq/check_data_quality.py"
24 |     config_path = PROJECT_ROOT / f"{env}/config/{'local' if env == 'local' else 'dev'}.yaml"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/dq/compare_local_aws_dq.py:25:1
   |
23 |     script_path = PROJECT_ROOT / "local/scripts/dq/check_data_quality.py"
24 |     config_path = PROJECT_ROOT / f"{env}/config/{'local' if env == 'local' else 'dev'}.yaml"
25 |     
   | ^^^^
26 |     cmd = [
27 |         sys.executable,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/dq/compare_local_aws_dq.py:33:1
   |
31 |         "--layer", layer
32 |     ]
33 |     
   | ^^^^
34 |     result = subprocess.run(
35 |         cmd,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/dq/compare_local_aws_dq.py:40:1
   |
38 |         cwd=PROJECT_ROOT
39 |     )
40 |     
   | ^^^^
41 |     if result.returncode != 0:
42 |         logger.error(f"DQ check failed for {env}: {result.stderr}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/dq/compare_local_aws_dq.py:44:1
   |
42 |         logger.error(f"DQ check failed for {env}: {result.stderr}")
43 |         return {}
44 |     
   | ^^^^
45 |     # Parse output (simplified - in production would use structured output)
46 |     return {
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/dq/compare_local_aws_dq.py:61:1
   |
59 |     logger.info("=" * 80)
60 |     logger.info("")
61 |     
   | ^^^^
62 |     # Extract key metrics from output (simplified parsing)
63 |     logger.info("Local Results:")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/dq/compare_local_aws_dq.py:77:1
   |
75 |     """Main entry point."""
76 |     import argparse
77 |     
   | ^^^^
78 |     parser = argparse.ArgumentParser(description="Compare DQ results between local and AWS")
79 |     parser.add_argument("--layer", choices=["bronze", "silver", "gold", "all"], default="all")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/dq/compare_local_aws_dq.py:81:1
   |
79 |     parser.add_argument("--layer", choices=["bronze", "silver", "gold", "all"], default="all")
80 |     args = parser.parse_args()
81 |     
   | ^^^^
82 |     logger.info("=" * 80)
83 |     logger.info("DATA QUALITY COMPARISON: LOCAL vs AWS")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/dq/compare_local_aws_dq.py:86:1
   |
84 |     logger.info("=" * 80)
85 |     logger.info("")
86 |     
   | ^^^^
87 |     # Run local check
88 |     local_result = run_dq_check("local", args.layer)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/dq/compare_local_aws_dq.py:89:1
   |
87 |     # Run local check
88 |     local_result = run_dq_check("local", args.layer)
89 |     
   | ^^^^
90 |     # Run AWS check (if AWS credentials available)
91 |     try:
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> local/scripts/kafka_producer.py:11:1
   |
 9 |       python local/scripts/kafka_producer.py --config local/config/local.yaml
10 |   """
11 | / import sys
12 | | import os
13 | | import time
14 | | import csv
15 | | import json
16 | | import argparse
17 | | import logging
18 | | from pathlib import Path
19 | | from typing import Dict, Any
   | |____________________________^
20 |
21 |   # Add src to path
   |
help: Organize imports

F401 [*] `os` imported but unused
  --> local/scripts/kafka_producer.py:12:8
   |
10 | """
11 | import sys
12 | import os
   |        ^^
13 | import time
14 | import csv
   |
help: Remove unused import: `os`

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> local/scripts/kafka_producer.py:19:1
   |
17 | import logging
18 | from pathlib import Path
19 | from typing import Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
20 |
21 | # Add src to path
   |

E402 Module level import not at top of file
  --> local/scripts/kafka_producer.py:32:1
   |
30 |     sys.exit(1)
31 |
32 | from project_a.config_loader import load_config_resolved
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
33 |
34 | logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
   |

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> local/scripts/kafka_producer.py:38:30
   |
38 | def parse_kafka_csv_row(row: Dict[str, str]) -> Dict[str, Any]:
   |                              ^^^^
39 |     """
40 |     Parse a CSV row from kafka seed file into event payload.
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> local/scripts/kafka_producer.py:38:49
   |
38 | def parse_kafka_csv_row(row: Dict[str, str]) -> Dict[str, Any]:
   |                                                 ^^^^
39 |     """
40 |     Parse a CSV row from kafka seed file into event payload.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> local/scripts/kafka_producer.py:41:1
   |
39 |     """
40 |     Parse a CSV row from kafka seed file into event payload.
41 |     
   | ^^^^
42 |     The CSV has columns: event_id, topic, partition, offset, timestamp, key, value, headers
43 |     We extract the 'value' JSON and send it as the event payload.
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> local/scripts/kafka_producer.py:63:1
   |
61 |     """
62 |     Read CSV seed file and produce events to Kafka.
63 |     
   | ^^^^
64 |     Args:
65 |         seed_file: Path to CSV seed file
   |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
  --> local/scripts/kafka_producer.py:71:17
   |
69 |         max_events: Maximum number of events to produce (None = all)
70 |     """
71 |     logger.info(f"Starting Kafka producer")
   |                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
72 |     logger.info(f"  Seed file: {seed_file}")
73 |     logger.info(f"  Bootstrap servers: {bootstrap_servers}")
   |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
  --> local/scripts/kafka_producer.py:76:1
   |
74 |     logger.info(f"  Topic: {topic}")
75 |     logger.info(f"  Delay: {delay_seconds}s per event")
76 |     
   | ^^^^
77 |     # Create producer
78 |     try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/kafka_producer.py:89:1
   |
87 |         logger.error("   Make sure Kafka is running: docker-compose up kafka")
88 |         raise
89 |     
   | ^^^^
90 |     # Read CSV and produce events
91 |     seed_path = Path(seed_file)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/kafka_producer.py:94:1
   |
92 |     if not seed_path.exists():
93 |         raise FileNotFoundError(f"Seed file not found: {seed_file}")
94 |     
   | ^^^^
95 |     events_sent = 0
96 |     events_failed = 0
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/kafka_producer.py:97:1
   |
95 |     events_sent = 0
96 |     events_failed = 0
97 |     
   | ^^^^
98 |     try:
99 |         with open(seed_path, 'r', encoding='utf-8') as f:
   |
help: Remove whitespace from blank line

UP015 [*] Unnecessary mode argument
   --> local/scripts/kafka_producer.py:99:30
    |
 98 |     try:
 99 |         with open(seed_path, 'r', encoding='utf-8') as f:
    |                              ^^^
100 |             reader = csv.DictReader(f)
    |
help: Remove mode argument

W293 [*] Blank line contains whitespace
   --> local/scripts/kafka_producer.py:101:1
    |
 99 |         with open(seed_path, 'r', encoding='utf-8') as f:
100 |             reader = csv.DictReader(f)
101 |             
    | ^^^^^^^^^^^^
102 |             for row_num, row in enumerate(reader, 1):
103 |                 if max_events and events_sent >= max_events:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/kafka_producer.py:106:1
    |
104 |                     logger.info(f"Reached max_events limit ({max_events})")
105 |                     break
106 |                 
    | ^^^^^^^^^^^^^^^^
107 |                 # Parse event payload
108 |                 event_payload = parse_kafka_csv_row(row)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/kafka_producer.py:112:1
    |
110 |                     events_failed += 1
111 |                     continue
112 |                 
    | ^^^^^^^^^^^^^^^^
113 |                 # Get key if available
114 |                 event_key = row.get('key', None)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/kafka_producer.py:115:1
    |
113 |                 # Get key if available
114 |                 event_key = row.get('key', None)
115 |                 
    | ^^^^^^^^^^^^^^^^
116 |                 try:
117 |                     # Send event
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/kafka_producer.py:123:1
    |
121 |                         key=event_key
122 |                     )
123 |                     
    | ^^^^^^^^^^^^^^^^^^^^
124 |                     # Wait for send to complete (optional, for reliability)
125 |                     record_metadata = future.get(timeout=10)
    |
help: Remove whitespace from blank line

F841 Local variable `record_metadata` is assigned to but never used
   --> local/scripts/kafka_producer.py:125:21
    |
124 |                     # Wait for send to complete (optional, for reliability)
125 |                     record_metadata = future.get(timeout=10)
    |                     ^^^^^^^^^^^^^^^
126 |                     
127 |                     events_sent += 1
    |
help: Remove assignment to unused variable `record_metadata`

W293 [*] Blank line contains whitespace
   --> local/scripts/kafka_producer.py:126:1
    |
124 |                     # Wait for send to complete (optional, for reliability)
125 |                     record_metadata = future.get(timeout=10)
126 |                     
    | ^^^^^^^^^^^^^^^^^^^^
127 |                     events_sent += 1
128 |                     if events_sent % 100 == 0:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/kafka_producer.py:130:1
    |
128 |                     if events_sent % 100 == 0:
129 |                         logger.info(f"Sent {events_sent} events...")
130 |                     
    | ^^^^^^^^^^^^^^^^^^^^
131 |                     # Simulate streaming delay
132 |                     if delay_seconds > 0:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/kafka_producer.py:134:1
    |
132 |                     if delay_seconds > 0:
133 |                         time.sleep(delay_seconds)
134 |                         
    | ^^^^^^^^^^^^^^^^^^^^^^^^
135 |                 except KafkaError as e:
136 |                     logger.error(f"Failed to send event {row_num}: {e}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/kafka_producer.py:139:1
    |
137 |                     events_failed += 1
138 |                     continue
139 |     
    | ^^^^
140 |     except KeyboardInterrupt:
141 |         logger.info("Interrupted by user")
    |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
   --> local/scripts/kafka_producer.py:146:21
    |
144 |         producer.close()
145 |         logger.info("=" * 60)
146 |         logger.info(f"âœ… Producer finished")
    |                     ^^^^^^^^^^^^^^^^^^^^^^^
147 |         logger.info(f"   Events sent: {events_sent}")
148 |         logger.info(f"   Events failed: {events_failed}")
    |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
   --> local/scripts/kafka_producer.py:162:1
    |
160 |     parser.add_argument("--seed-file", help="CSV seed file path (overrides config)")
161 |     args = parser.parse_args()
162 |     
    | ^^^^
163 |     # Load config
164 |     config_path = PROJECT_ROOT / args.config
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/kafka_producer.py:166:1
    |
164 |     config_path = PROJECT_ROOT / args.config
165 |     config = load_config_resolved(str(config_path))
166 |     
    | ^^^^
167 |     # Get Kafka config
168 |     kafka_cfg = config.get("sources", {}).get("kafka", {})
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/kafka_producer.py:169:1
    |
167 |     # Get Kafka config
168 |     kafka_cfg = config.get("sources", {}).get("kafka", {})
169 |     
    | ^^^^
170 |     bootstrap_servers = args.bootstrap_servers or kafka_cfg.get("local_bootstrap_servers", "localhost:9092")
171 |     topic = args.topic or kafka_cfg.get("topic", "orders_events")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/kafka_producer.py:173:1
    |
171 |     topic = args.topic or kafka_cfg.get("topic", "orders_events")
172 |     seed_file = args.seed_file or kafka_cfg.get("seed_file", "")
173 |     
    | ^^^^
174 |     if not seed_file:
175 |         # Fallback to kafka_sim seed file
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/kafka_producer.py:182:1
    |
180 |         if base_path and orders_seed:
181 |             seed_file = f"{base_path}/{orders_seed}"
182 |     
    | ^^^^
183 |     if not seed_file:
184 |         raise ValueError("Kafka seed file not found in config. Set sources.kafka.seed_file or sources.kafka_sim")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/kafka_producer.py:185:1
    |
183 |     if not seed_file:
184 |         raise ValueError("Kafka seed file not found in config. Set sources.kafka.seed_file or sources.kafka_sim")
185 |     
    | ^^^^
186 |     # Resolve relative paths
187 |     if not seed_file.startswith(("s3://", "file://", "/")):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/kafka_producer.py:189:1
    |
187 |     if not seed_file.startswith(("s3://", "file://", "/")):
188 |         seed_file = str(PROJECT_ROOT / seed_file)
189 |     
    | ^^^^
190 |     # Produce events
191 |     produce_events(
    |
help: Remove whitespace from blank line

E402 Module level import not at top of file
  --> local/scripts/quick_check.py:21:1
   |
19 |     sys.path.insert(0, str(src_path))
20 |
21 | from pyspark.sql import SparkSession
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
22 | from pyspark.sql import functions as F
   |

I001 [*] Import block is un-sorted or un-formatted
  --> local/scripts/quick_check.py:21:1
   |
19 |       sys.path.insert(0, str(src_path))
20 |
21 | / from pyspark.sql import SparkSession
22 | | from pyspark.sql import functions as F
23 | |
24 | | from project_a.utils.spark_session import build_spark
25 | | from project_a.pyspark_interview_project.utils.config_loader import load_config_resolved
26 | | from project_a.utils.path_resolver import resolve_data_path
   | |___________________________________________________________^
27 |
28 |   def main():
   |
help: Organize imports

F401 [*] `pyspark.sql.SparkSession` imported but unused
  --> local/scripts/quick_check.py:21:25
   |
19 |     sys.path.insert(0, str(src_path))
20 |
21 | from pyspark.sql import SparkSession
   |                         ^^^^^^^^^^^^
22 | from pyspark.sql import functions as F
   |
help: Remove unused import: `pyspark.sql.SparkSession`

E402 Module level import not at top of file
  --> local/scripts/quick_check.py:22:1
   |
21 | from pyspark.sql import SparkSession
22 | from pyspark.sql import functions as F
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
23 |
24 | from project_a.utils.spark_session import build_spark
   |

E402 Module level import not at top of file
  --> local/scripts/quick_check.py:24:1
   |
22 | from pyspark.sql import functions as F
23 |
24 | from project_a.utils.spark_session import build_spark
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
25 | from project_a.pyspark_interview_project.utils.config_loader import load_config_resolved
26 | from project_a.utils.path_resolver import resolve_data_path
   |

E402 Module level import not at top of file
  --> local/scripts/quick_check.py:25:1
   |
24 | from project_a.utils.spark_session import build_spark
25 | from project_a.pyspark_interview_project.utils.config_loader import load_config_resolved
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
26 | from project_a.utils.path_resolver import resolve_data_path
   |

E402 Module level import not at top of file
  --> local/scripts/quick_check.py:26:1
   |
24 | from project_a.utils.spark_session import build_spark
25 | from project_a.pyspark_interview_project.utils.config_loader import load_config_resolved
26 | from project_a.utils.path_resolver import resolve_data_path
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
27 |
28 | def main():
   |

W293 [*] Blank line contains whitespace
  --> local/scripts/quick_check.py:33:1
   |
31 |     print("ðŸ” QUICK PIPELINE CHECK")
32 |     print("=" * 80)
33 |     
   | ^^^^
34 |     # Load config
35 |     config = load_config_resolved("local/config/local.yaml", env="local")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/quick_check.py:37:1
   |
35 |     config = load_config_resolved("local/config/local.yaml", env="local")
36 |     config["environment"] = "local"
37 |     
   | ^^^^
38 |     # Build Spark
39 |     spark = build_spark(app_name="quick_check", config=config)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/quick_check.py:40:1
   |
38 |     # Build Spark
39 |     spark = build_spark(app_name="quick_check", config=config)
40 |     
   | ^^^^
41 |     try:
42 |         checks_passed = 0
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/quick_check.py:44:1
   |
42 |         checks_passed = 0
43 |         checks_total = 0
44 |         
   | ^^^^^^^^
45 |         # Check 1: Read orders_silver (should be parquet, no delta errors)
46 |         print("\n[Check 1] Reading orders_silver (parquet only, no delta)...")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/quick_check.py:59:1
   |
57 |             else:
58 |                 print(f"  âš ï¸  orders_silver read failed: {e}")
59 |         
   | ^^^^^^^^
60 |         # Check 2: FX rates schema (even if empty)
61 |         print("\n[Check 2] Checking fx_rates_silver schema...")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/quick_check.py:68:1
   |
66 |             schema_fields = [f.name for f in fx_df.schema.fields]
67 |             expected_fields = ["trade_date", "base_ccy", "counter_ccy", "rate"]
68 |             
   | ^^^^^^^^^^^^
69 |             if all(f in schema_fields for f in expected_fields):
70 |                 print(f"  âœ… fx_rates_silver schema correct: {schema_fields}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/quick_check.py:80:1
   |
78 |             else:
79 |                 print(f"  âš ï¸  fx_rates_silver read failed: {e}")
80 |         
   | ^^^^^^^^
81 |         # Check 3: fact_orders.customer_sk not 100% -1
82 |         print("\n[Check 3] Checking fact_orders.customer_sk...")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/quick_check.py:88:1
   |
86 |             fact_df = spark.read.parquet(f"{gold_root}/fact_orders")
87 |             total = fact_df.count()
88 |             
   | ^^^^^^^^^^^^
89 |             if "customer_sk" in fact_df.columns:
90 |                 missing = fact_df.filter(F.col("customer_sk") == -1).count()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/quick_check.py:92:1
   |
90 |                 missing = fact_df.filter(F.col("customer_sk") == -1).count()
91 |                 missing_pct = (missing / total * 100.0) if total > 0 else 0.0
92 |                 
   | ^^^^^^^^^^^^^^^^
93 |                 if missing_pct < 50.0:  # Less than 50% missing is acceptable
94 |                     print(f"  âœ… fact_orders.customer_sk: {missing_pct:.2f}% missing ({missing:,}/{total:,})")
   |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
   --> local/scripts/quick_check.py:99:23
    |
 97 |                     print(f"  âŒ fact_orders.customer_sk: {missing_pct:.2f}% missing (too high!)")
 98 |             else:
 99 |                 print(f"  âš ï¸  customer_sk column not found in fact_orders")
    |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
100 |         except Exception as e:
101 |             print(f"  âš ï¸  fact_orders check failed: {e}")
    |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
   --> local/scripts/quick_check.py:102:1
    |
100 |         except Exception as e:
101 |             print(f"  âš ï¸  fact_orders check failed: {e}")
102 |         
    | ^^^^^^^^
103 |         # Check 4: Critical tables have data
104 |         print("\n[Check 4] Checking critical tables have data...")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/quick_check.py:112:1
    |
110 |             ("dim_customer", f"{gold_root}/dim_customer"),
111 |         ]
112 |         
    | ^^^^^^^^
113 |         for table_name, path in critical_tables:
114 |             checks_total += 1
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/quick_check.py:125:1
    |
123 |             except Exception as e:
124 |                 print(f"  âŒ {table_name}: Read failed - {e}")
125 |         
    | ^^^^^^^^
126 |         # Summary
127 |         print("\n" + "=" * 80)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/quick_check.py:130:1
    |
128 |         print(f"ðŸ“Š CHECK SUMMARY: {checks_passed}/{checks_total} passed")
129 |         print("=" * 80)
130 |         
    | ^^^^^^^^
131 |         if checks_passed == checks_total:
132 |             print("âœ… All checks passed!")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/quick_check.py:137:1
    |
135 |             print("âš ï¸  Some checks failed")
136 |             return 1
137 |             
    | ^^^^^^^^^^^^
138 |     finally:
139 |         spark.stop()
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> local/scripts/regenerate_source_data.py:13:1
   |
11 |   """
12 |
13 | / import csv
14 | | import json
15 | | import random
16 | | import uuid
17 | | from datetime import datetime, timedelta
18 | | from pathlib import Path
19 | | from typing import List, Dict, Any
   | |__________________________________^
20 |
21 |   # Set seed for reproducibility
   |
help: Organize imports

UP035 `typing.List` is deprecated, use `list` instead
  --> local/scripts/regenerate_source_data.py:19:1
   |
17 | from datetime import datetime, timedelta
18 | from pathlib import Path
19 | from typing import List, Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
20 |
21 | # Set seed for reproducibility
   |

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> local/scripts/regenerate_source_data.py:19:1
   |
17 | from datetime import datetime, timedelta
18 | from pathlib import Path
19 | from typing import List, Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
20 |
21 | # Set seed for reproducibility
   |

UP006 [*] Use `list` instead of `List` for type annotation
   --> local/scripts/regenerate_source_data.py:99:57
    |
 99 | def generate_crm_accounts(num_accounts: int = 30153) -> List[Dict[str, Any]]:
    |                                                         ^^^^
100 |     """Generate CRM accounts data aligned with customer IDs."""
101 |     accounts = []
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/scripts/regenerate_source_data.py:99:62
    |
 99 | def generate_crm_accounts(num_accounts: int = 30153) -> List[Dict[str, Any]]:
    |                                                              ^^^^
100 |     """Generate CRM accounts data aligned with customer IDs."""
101 |     accounts = []
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:104:1
    |
102 |     start_date = datetime(2010, 1, 1)
103 |     end_date = datetime.now()
104 |     
    | ^^^^
105 |     for i in range(1, num_accounts + 1):
106 |         customer_id = generate_customer_id(i)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:108:1
    |
106 |         customer_id = generate_customer_id(i)
107 |         created_date = random_date(start_date, end_date)
108 |         
    | ^^^^^^^^
109 |         accounts.append({
110 |             "Id": customer_id,  # Use customer_id format for alignment
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:140:1
    |
138 |             "LastActivityDate": random_date(datetime.fromisoformat(created_date), end_date),
139 |         })
140 |     
    | ^^^^
141 |     return accounts
    |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
   --> local/scripts/regenerate_source_data.py:144:57
    |
144 | def generate_crm_contacts(num_contacts: int = 45000) -> List[Dict[str, Any]]:
    |                                                         ^^^^
145 |     """Generate CRM contacts data."""
146 |     contacts = []
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/scripts/regenerate_source_data.py:144:62
    |
144 | def generate_crm_contacts(num_contacts: int = 45000) -> List[Dict[str, Any]]:
    |                                                              ^^^^
145 |     """Generate CRM contacts data."""
146 |     contacts = []
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:149:1
    |
147 |     start_date = datetime(2010, 1, 1)
148 |     end_date = datetime.now()
149 |     
    | ^^^^
150 |     # Map contacts to accounts (multiple contacts per account)
151 |     account_contact_map = {}
    |
help: Remove whitespace from blank line

F841 Local variable `account_contact_map` is assigned to but never used
   --> local/scripts/regenerate_source_data.py:151:5
    |
150 |     # Map contacts to accounts (multiple contacts per account)
151 |     account_contact_map = {}
    |     ^^^^^^^^^^^^^^^^^^^
152 |     contact_num = 1
    |
help: Remove assignment to unused variable `account_contact_map`

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:153:1
    |
151 |     account_contact_map = {}
152 |     contact_num = 1
153 |     
    | ^^^^
154 |     for account_id in range(1, min(30153, CUSTOMER_ID_END) + 1):
155 |         customer_id = generate_customer_id(account_id)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:157:1
    |
155 |         customer_id = generate_customer_id(account_id)
156 |         num_contacts_for_account = random.randint(1, 3)
157 |         
    | ^^^^^^^^
158 |         for _ in range(num_contacts_for_account):
159 |             if contact_num > num_contacts:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:161:1
    |
159 |             if contact_num > num_contacts:
160 |                 break
161 |             
    | ^^^^^^^^^^^^
162 |             created_date = random_date(start_date, end_date)
163 |             first_name = f"Contact{contact_num}"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:165:1
    |
163 |             first_name = f"Contact{contact_num}"
164 |             last_name = f"Person{contact_num}"
165 |             
    | ^^^^^^^^^^^^
166 |             contacts.append({
167 |                 "Id": generate_contact_id(contact_num),
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:192:1
    |
190 |                 "EngagementScore": random.uniform(0.0, 100.0),
191 |             })
192 |             
    | ^^^^^^^^^^^^
193 |             contact_num += 1
194 |             if contact_num > num_contacts:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:196:1
    |
194 |             if contact_num > num_contacts:
195 |                 break
196 |     
    | ^^^^
197 |     return contacts
    |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
   --> local/scripts/regenerate_source_data.py:200:58
    |
200 | def generate_crm_opportunities(num_opps: int = 20000) -> List[Dict[str, Any]]:
    |                                                          ^^^^
201 |     """Generate CRM opportunities data."""
202 |     opportunities = []
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/scripts/regenerate_source_data.py:200:63
    |
200 | def generate_crm_opportunities(num_opps: int = 20000) -> List[Dict[str, Any]]:
    |                                                               ^^^^
201 |     """Generate CRM opportunities data."""
202 |     opportunities = []
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:205:1
    |
203 |     start_date = datetime(2020, 1, 1)
204 |     end_date = datetime.now()
205 |     
    | ^^^^
206 |     for i in range(1, num_opps + 1):
207 |         # Map to existing accounts
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:214:1
    |
212 |         is_closed = stage in ["CLOSED_WON", "CLOSED_LOST"]
213 |         is_won = stage == "CLOSED_WON"
214 |         
    | ^^^^^^^^
215 |         opportunities.append({
216 |             "Id": generate_opportunity_id(i),
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:239:1
    |
237 |             "Timeline": random.choice(["Q1", "Q2", "Q3", "Q4", "Immediate"]),
238 |         })
239 |     
    | ^^^^
240 |     return opportunities
    |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
   --> local/scripts/regenerate_source_data.py:243:65
    |
243 | def generate_snowflake_customers(num_customers: int = 50000) -> List[Dict[str, Any]]:
    |                                                                 ^^^^
244 |     """Generate Snowflake customers data."""
245 |     customers = []
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/scripts/regenerate_source_data.py:243:70
    |
243 | def generate_snowflake_customers(num_customers: int = 50000) -> List[Dict[str, Any]]:
    |                                                                      ^^^^
244 |     """Generate Snowflake customers data."""
245 |     customers = []
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:248:1
    |
246 |     start_date = datetime(2010, 1, 1)
247 |     end_date = datetime.now()
248 |     
    | ^^^^
249 |     for i in range(CUSTOMER_ID_START, num_customers + 1):
250 |         customer_id = generate_customer_id(i)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:252:1
    |
250 |         customer_id = generate_customer_id(i)
251 |         created_at = random_datetime(start_date, end_date)
252 |         
    | ^^^^^^^^
253 |         customers.append({
254 |             "customer_id": customer_id,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:278:1
    |
276 |             "estimated_clv": random.uniform(0, 150000),
277 |         })
278 |     
    | ^^^^
279 |     return customers
    |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
   --> local/scripts/regenerate_source_data.py:282:63
    |
282 | def generate_snowflake_products(num_products: int = 10000) -> List[Dict[str, Any]]:
    |                                                               ^^^^
283 |     """Generate Snowflake products data."""
284 |     products = []
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/scripts/regenerate_source_data.py:282:68
    |
282 | def generate_snowflake_products(num_products: int = 10000) -> List[Dict[str, Any]]:
    |                                                                    ^^^^
283 |     """Generate Snowflake products data."""
284 |     products = []
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:285:1
    |
283 |     """Generate Snowflake products data."""
284 |     products = []
285 |     
    | ^^^^
286 |     for i in range(PRODUCT_ID_START, num_products):
287 |         product_id = generate_product_id(i)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:288:1
    |
286 |     for i in range(PRODUCT_ID_START, num_products):
287 |         product_id = generate_product_id(i)
288 |         
    | ^^^^^^^^
289 |         products.append({
290 |             "product_id": product_id,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:303:1
    |
301 |             "updated_at": random_date(datetime(2020, 1, 1), datetime.now()),
302 |         })
303 |     
    | ^^^^
304 |     return products
    |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
   --> local/scripts/regenerate_source_data.py:307:60
    |
307 | def generate_snowflake_orders(num_orders: int = 100000) -> List[Dict[str, Any]]:
    |                                                            ^^^^
308 |     """Generate Snowflake orders data."""
309 |     orders = []
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/scripts/regenerate_source_data.py:307:65
    |
307 | def generate_snowflake_orders(num_orders: int = 100000) -> List[Dict[str, Any]]:
    |                                                                 ^^^^
308 |     """Generate Snowflake orders data."""
309 |     orders = []
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:312:1
    |
310 |     start_date = datetime(2023, 1, 1)  # Last ~24 months
311 |     end_date = datetime.now()
312 |     
    | ^^^^
313 |     # Ensure we have valid customer and product IDs
314 |     valid_customer_ids = list(range(CUSTOMER_ID_START, min(50000, CUSTOMER_ID_END) + 1))
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:316:1
    |
314 |     valid_customer_ids = list(range(CUSTOMER_ID_START, min(50000, CUSTOMER_ID_END) + 1))
315 |     valid_product_ids = list(range(PRODUCT_ID_START, PRODUCT_ID_END))
316 |     
    | ^^^^
317 |     for i in range(ORDER_ID_START, num_orders + 1):
318 |         order_id = generate_order_id(i)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:325:1
    |
323 |         quantity = random.randint(1, 10)
324 |         amount_orig = round(random.uniform(20.0, 5000.0), 2)
325 |         
    | ^^^^^^^^
326 |         orders.append({
327 |             "order_id": order_id,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:341:1
    |
339 |             "shipping_country": random.choice(COUNTRIES),
340 |         })
341 |     
    | ^^^^
342 |     return orders
    |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
   --> local/scripts/regenerate_source_data.py:345:61
    |
345 | def generate_redshift_behavior(num_records: int = 50000) -> List[Dict[str, Any]]:
    |                                                             ^^^^
346 |     """Generate Redshift customer behavior data."""
347 |     behavior = []
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/scripts/regenerate_source_data.py:345:66
    |
345 | def generate_redshift_behavior(num_records: int = 50000) -> List[Dict[str, Any]]:
    |                                                                  ^^^^
346 |     """Generate Redshift customer behavior data."""
347 |     behavior = []
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:350:1
    |
348 |     start_date = datetime(2023, 1, 1)
349 |     end_date = datetime.now()
350 |     
    | ^^^^
351 |     valid_customer_ids = list(range(CUSTOMER_ID_START, min(50000, CUSTOMER_ID_END) + 1))
352 |     event_names = ["login", "page_view", "purchase", "add_to_cart", "search", "view_product"]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:353:1
    |
351 |     valid_customer_ids = list(range(CUSTOMER_ID_START, min(50000, CUSTOMER_ID_END) + 1))
352 |     event_names = ["login", "page_view", "purchase", "add_to_cart", "search", "view_product"]
353 |     
    | ^^^^
354 |     for i in range(1, num_records + 1):
355 |         customer_id = generate_customer_id(random.choice(valid_customer_ids))
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:358:1
    |
356 |         event_timestamp = random_datetime(start_date, end_date)
357 |         event_name = random.choice(event_names)
358 |         
    | ^^^^^^^^
359 |         behavior.append({
360 |             "behavior_id": f"BEH-{i:08d}",
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:385:1
    |
383 |             "session_quality": random.choice(["high", "medium", "low"]),
384 |         })
385 |     
    | ^^^^
386 |     return behavior
    |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
   --> local/scripts/regenerate_source_data.py:389:56
    |
389 | def generate_kafka_events(num_events: int = 100000) -> List[Dict[str, Any]]:
    |                                                        ^^^^
390 |     """Generate Kafka order events data."""
391 |     events = []
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/scripts/regenerate_source_data.py:389:61
    |
389 | def generate_kafka_events(num_events: int = 100000) -> List[Dict[str, Any]]:
    |                                                             ^^^^
390 |     """Generate Kafka order events data."""
391 |     events = []
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:394:1
    |
392 |     start_date = datetime(2023, 1, 1)
393 |     end_date = datetime.now()
394 |     
    | ^^^^
395 |     # Get valid order IDs from generated orders
396 |     valid_order_ids = [generate_order_id(i) for i in range(ORDER_ID_START, min(ORDER_ID_END, 100000) + 1)]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:398:1
    |
396 |     valid_order_ids = [generate_order_id(i) for i in range(ORDER_ID_START, min(ORDER_ID_END, 100000) + 1)]
397 |     valid_customer_ids = list(range(CUSTOMER_ID_START, min(50000, CUSTOMER_ID_END) + 1))
398 |     
    | ^^^^
399 |     # Create order lookup for amounts and currencies
400 |     order_lookup = {}
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:407:1
    |
405 |             "currency": random.choice(CURRENCIES),
406 |         }
407 |     
    | ^^^^
408 |     for i in range(1, num_events + 1):
409 |         event_id = generate_event_id()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:412:1
    |
410 |         order_id = random.choice(valid_order_ids)
411 |         customer_id = generate_customer_id(random.choice(valid_customer_ids))
412 |         
    | ^^^^^^^^
413 |         # Get order details
414 |         order_details = order_lookup.get(order_id, {
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:418:1
    |
416 |             "currency": random.choice(CURRENCIES),
417 |         })
418 |         
    | ^^^^^^^^
419 |         event_type = random.choice(EVENT_TYPES)
420 |         event_timestamp = random_datetime(start_date, end_date)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:422:1
    |
420 |         event_timestamp = random_datetime(start_date, end_date)
421 |         channel = random.choice(CHANNELS)
422 |         
    | ^^^^^^^^
423 |         # Create JSON value
424 |         value_json = {
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:438:1
    |
436 |             }
437 |         }
438 |         
    | ^^^^^^^^
439 |         events.append({
440 |             "event_id": event_id,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:453:1
    |
451 |             }),
452 |         })
453 |     
    | ^^^^
454 |     return events
    |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
   --> local/scripts/regenerate_source_data.py:457:51
    |
457 | def generate_fx_rates_csv(num_days: int = 730) -> List[Dict[str, Any]]:
    |                                                   ^^^^
458 |     """Generate FX rates CSV data."""
459 |     rates = []
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/scripts/regenerate_source_data.py:457:56
    |
457 | def generate_fx_rates_csv(num_days: int = 730) -> List[Dict[str, Any]]:
    |                                                        ^^^^
458 |     """Generate FX rates CSV data."""
459 |     rates = []
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:468:1
    |
466 |         ("USD", "CAD", 1.10, 1.40),
467 |     ]
468 |     
    | ^^^^
469 |     for day in range(num_days):
470 |         trade_date = (base_date + timedelta(days=day)).strftime("%Y-%m-%d")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:471:1
    |
469 |     for day in range(num_days):
470 |         trade_date = (base_date + timedelta(days=day)).strftime("%Y-%m-%d")
471 |         
    | ^^^^^^^^
472 |         for base_ccy, quote_ccy, min_rate, max_rate in currency_pairs:
473 |             # Generate realistic rate with slight daily variation
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:478:1
    |
476 |             fx_rate = round(base_rate * (1 + variation), 4)
477 |             fx_rate = max(min_rate, min(max_rate, fx_rate))  # Clamp to range
478 |             
    | ^^^^^^^^^^^^
479 |             rates.append({
480 |                 "trade_date": trade_date,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:485:1
    |
483 |                 "fx_rate": fx_rate,
484 |             })
485 |     
    | ^^^^
486 |     return rates
    |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
   --> local/scripts/regenerate_source_data.py:489:52
    |
489 | def generate_fx_rates_json(num_days: int = 730) -> List[Dict[str, Any]]:
    |                                                    ^^^^
490 |     """Generate FX rates JSON Lines data."""
491 |     rates = []
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/scripts/regenerate_source_data.py:489:57
    |
489 | def generate_fx_rates_json(num_days: int = 730) -> List[Dict[str, Any]]:
    |                                                         ^^^^
490 |     """Generate FX rates JSON Lines data."""
491 |     rates = []
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:500:1
    |
498 |         ("USD", "CAD", 1.10, 1.40),
499 |     ]
500 |     
    | ^^^^
501 |     for day in range(num_days):
502 |         trade_date = (base_date + timedelta(days=day)).strftime("%Y-%m-%d")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:503:1
    |
501 |     for day in range(num_days):
502 |         trade_date = (base_date + timedelta(days=day)).strftime("%Y-%m-%d")
503 |         
    | ^^^^^^^^
504 |         for base_ccy, quote_ccy, min_rate, max_rate in currency_pairs:
505 |             base_rate = (min_rate + max_rate) / 2
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:509:1
    |
507 |             fx_rate = round(base_rate * (1 + variation), 4)
508 |             fx_rate = max(min_rate, min(max_rate, fx_rate))
509 |             
    | ^^^^^^^^^^^^
510 |             rates.append({
511 |                 "date": trade_date,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:519:1
    |
517 |                 "source": "synthetic",
518 |             })
519 |     
    | ^^^^
520 |     return rates
    |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
   --> local/scripts/regenerate_source_data.py:523:37
    |
523 | def write_csv(filepath: Path, data: List[Dict[str, Any]], fieldnames: List[str]):
    |                                     ^^^^
524 |     """Write data to CSV file."""
525 |     filepath.parent.mkdir(parents=True, exist_ok=True)
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/scripts/regenerate_source_data.py:523:42
    |
523 | def write_csv(filepath: Path, data: List[Dict[str, Any]], fieldnames: List[str]):
    |                                          ^^^^
524 |     """Write data to CSV file."""
525 |     filepath.parent.mkdir(parents=True, exist_ok=True)
    |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
   --> local/scripts/regenerate_source_data.py:523:71
    |
523 | def write_csv(filepath: Path, data: List[Dict[str, Any]], fieldnames: List[str]):
    |                                                                       ^^^^
524 |     """Write data to CSV file."""
525 |     filepath.parent.mkdir(parents=True, exist_ok=True)
    |
help: Replace with `list`

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:526:1
    |
524 |     """Write data to CSV file."""
525 |     filepath.parent.mkdir(parents=True, exist_ok=True)
526 |     
    | ^^^^
527 |     with open(filepath, 'w', newline='', encoding='utf-8') as f:
528 |         writer = csv.DictWriter(f, fieldnames=fieldnames)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:531:1
    |
529 |         writer.writeheader()
530 |         writer.writerows(data)
531 |     
    | ^^^^
532 |     print(f"âœ… Wrote {len(data):,} rows to {filepath}")
    |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
   --> local/scripts/regenerate_source_data.py:535:44
    |
535 | def write_json_lines(filepath: Path, data: List[Dict[str, Any]]):
    |                                            ^^^^
536 |     """Write data to JSON Lines file."""
537 |     filepath.parent.mkdir(parents=True, exist_ok=True)
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/scripts/regenerate_source_data.py:535:49
    |
535 | def write_json_lines(filepath: Path, data: List[Dict[str, Any]]):
    |                                                 ^^^^
536 |     """Write data to JSON Lines file."""
537 |     filepath.parent.mkdir(parents=True, exist_ok=True)
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:538:1
    |
536 |     """Write data to JSON Lines file."""
537 |     filepath.parent.mkdir(parents=True, exist_ok=True)
538 |     
    | ^^^^
539 |     with open(filepath, 'w', encoding='utf-8') as f:
540 |         for record in data:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:542:1
    |
540 |         for record in data:
541 |             f.write(json.dumps(record) + '\n')
542 |     
    | ^^^^
543 |     print(f"âœ… Wrote {len(data):,} lines to {filepath}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:550:1
    |
548 |     print("ðŸ”„ Regenerating all source data files...")
549 |     print("=" * 80)
550 |     
    | ^^^^
551 |     # CRM data
552 |     print("\nðŸ“Š Generating CRM data...")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:559:1
    |
557 |         list(accounts[0].keys())
558 |     )
559 |     
    | ^^^^
560 |     contacts = generate_crm_contacts(45000)
561 |     write_csv(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:566:1
    |
564 |         list(contacts[0].keys())
565 |     )
566 |     
    | ^^^^
567 |     opportunities = generate_crm_opportunities(20000)
568 |     write_csv(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:573:1
    |
571 |         list(opportunities[0].keys())
572 |     )
573 |     
    | ^^^^
574 |     # Snowflake data
575 |     print("\nðŸ“Š Generating Snowflake data...")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:582:1
    |
580 |         list(customers[0].keys())
581 |     )
582 |     
    | ^^^^
583 |     products = generate_snowflake_products(10000)
584 |     write_csv(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:589:1
    |
587 |         list(products[0].keys())
588 |     )
589 |     
    | ^^^^
590 |     orders = generate_snowflake_orders(100000)
591 |     write_csv(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:596:1
    |
594 |         list(orders[0].keys())
595 |     )
596 |     
    | ^^^^
597 |     # Redshift data
598 |     print("\nðŸ“Š Generating Redshift behavior data...")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:605:1
    |
603 |         list(behavior[0].keys())
604 |     )
605 |     
    | ^^^^
606 |     # Kafka data
607 |     print("\nðŸ“Š Generating Kafka events data...")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:614:1
    |
612 |         list(kafka_events[0].keys())
613 |     )
614 |     
    | ^^^^
615 |     # FX rates
616 |     print("\nðŸ“Š Generating FX rates data...")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:623:1
    |
621 |         ["trade_date", "base_ccy", "quote_ccy", "fx_rate"]
622 |     )
623 |     
    | ^^^^
624 |     fx_rates_json = generate_fx_rates_json(730)
625 |     write_json_lines(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/regenerate_source_data.py:629:1
    |
627 |         fx_rates_json
628 |     )
629 |     
    | ^^^^
630 |     print("\n" + "=" * 80)
631 |     print("âœ… All source data files regenerated successfully!")
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> local/scripts/run_local_etl_safe.py:8:1
   |
 6 |   If SparkSession fails, provides helpful diagnostics and alternatives.
 7 |   """
 8 | / import sys
 9 | | import os
10 | | import subprocess
11 | | import logging
12 | | from pathlib import Path
   | |________________________^
13 |
14 |   logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
   |
help: Organize imports

W293 [*] Blank line contains whitespace
  --> local/scripts/run_local_etl_safe.py:23:1
   |
21 |     """Check if Spark environment is properly configured."""
22 |     issues = []
23 |     
   | ^^^^
24 |     # Check Java
25 |     try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/run_local_etl_safe.py:42:1
   |
40 |     except subprocess.TimeoutExpired:
41 |         issues.append("âš ï¸  Java check timed out")
42 |     
   | ^^^^
43 |     # Check PySpark
44 |     try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/run_local_etl_safe.py:49:1
   |
47 |     except ImportError:
48 |         issues.append("âŒ PySpark not installed")
49 |     
   | ^^^^
50 |     # Check JAVA_HOME
51 |     if not os.getenv('JAVA_HOME'):
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/run_local_etl_safe.py:53:1
   |
51 |     if not os.getenv('JAVA_HOME'):
52 |         issues.append("âš ï¸  JAVA_HOME not set (may cause issues)")
53 |     
   | ^^^^
54 |     return issues
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/run_local_etl_safe.py:62:1
   |
60 |     logger.info("STEP 1: BRONZE â†’ SILVER")
61 |     logger.info("=" * 80)
62 |     
   | ^^^^
63 |     # Set PYTHONPATH
64 |     env = os.environ.copy()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/run_local_etl_safe.py:66:1
   |
64 |     env = os.environ.copy()
65 |     env["PYTHONPATH"] = str(PROJECT_ROOT / "src") + (":" + env.get("PYTHONPATH", "") if env.get("PYTHONPATH") else "")
66 |     
   | ^^^^
67 |     cmd = [
68 |         sys.executable,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/run_local_etl_safe.py:73:1
   |
71 |         "--config", str(PROJECT_ROOT / config_path),
72 |     ]
73 |     
   | ^^^^
74 |     logger.info(f"Running: {' '.join(cmd)}")
75 |     result = subprocess.run(cmd, cwd=PROJECT_ROOT, env=env, capture_output=True, text=True)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/run_local_etl_safe.py:76:1
   |
74 |     logger.info(f"Running: {' '.join(cmd)}")
75 |     result = subprocess.run(cmd, cwd=PROJECT_ROOT, env=env, capture_output=True, text=True)
76 |     
   | ^^^^
77 |     if result.returncode != 0:
78 |         logger.error(f"âŒ Bronzeâ†’Silver failed with exit code {result.returncode}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/run_local_etl_safe.py:83:1
   |
81 |         logger.error("STDERR:")
82 |         logger.error(result.stderr)
83 |         
   | ^^^^^^^^
84 |         # Check for SparkSession error
85 |         if "Py4JError" in result.stderr or "SparkSession" in result.stderr:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/run_local_etl_safe.py:99:1
    |
 97 |             logger.error("Alternative: Check S3 data directly (no Spark required):")
 98 |             logger.error(f"  python3 {PROJECT_ROOT}/local/scripts/dq/check_s3_data_quality.py --layer all")
 99 |         
    | ^^^^^^^^
100 |         return False
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/run_local_etl_safe.py:101:1
    |
100 |         return False
101 |     
    | ^^^^
102 |     logger.info("âœ… Bronzeâ†’Silver completed successfully")
103 |     return True
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/run_local_etl_safe.py:111:1
    |
109 |     logger.info("STEP 2: SILVER â†’ GOLD")
110 |     logger.info("=" * 80)
111 |     
    | ^^^^
112 |     # Set PYTHONPATH
113 |     env = os.environ.copy()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/run_local_etl_safe.py:115:1
    |
113 |     env = os.environ.copy()
114 |     env["PYTHONPATH"] = str(PROJECT_ROOT / "src") + (":" + env.get("PYTHONPATH", "") if env.get("PYTHONPATH") else "")
115 |     
    | ^^^^
116 |     cmd = [
117 |         sys.executable,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/run_local_etl_safe.py:122:1
    |
120 |         "--config", str(PROJECT_ROOT / config_path),
121 |     ]
122 |     
    | ^^^^
123 |     logger.info(f"Running: {' '.join(cmd)}")
124 |     result = subprocess.run(cmd, cwd=PROJECT_ROOT, env=env, capture_output=True, text=True)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/run_local_etl_safe.py:125:1
    |
123 |     logger.info(f"Running: {' '.join(cmd)}")
124 |     result = subprocess.run(cmd, cwd=PROJECT_ROOT, env=env, capture_output=True, text=True)
125 |     
    | ^^^^
126 |     if result.returncode != 0:
127 |         logger.error(f"âŒ Silverâ†’Gold failed with exit code {result.returncode}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/run_local_etl_safe.py:133:1
    |
131 |         logger.error(result.stderr)
132 |         return False
133 |     
    | ^^^^
134 |     logger.info("âœ… Silverâ†’Gold completed successfully")
135 |     return True
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/run_local_etl_safe.py:143:1
    |
141 |     logger.info("STEP 3: VERIFICATION")
142 |     logger.info("=" * 80)
143 |     
    | ^^^^
144 |     try:
145 |         from pyspark.sql import SparkSession
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
   --> local/scripts/run_local_etl_safe.py:145:9
    |
144 |       try:
145 | /         from pyspark.sql import SparkSession
146 | |         from project_a.utils.spark_session import build_spark
147 | |         from project_a.config_loader import load_config_resolved
    | |________________________________________________________________^
148 |           
149 |           config = load_config_resolved(str(PROJECT_ROOT / config_path))
    |
help: Organize imports

F401 [*] `pyspark.sql.SparkSession` imported but unused
   --> local/scripts/run_local_etl_safe.py:145:33
    |
144 |     try:
145 |         from pyspark.sql import SparkSession
    |                                 ^^^^^^^^^^^^
146 |         from project_a.utils.spark_session import build_spark
147 |         from project_a.config_loader import load_config_resolved
    |
help: Remove unused import: `pyspark.sql.SparkSession`

W293 [*] Blank line contains whitespace
   --> local/scripts/run_local_etl_safe.py:148:1
    |
146 |         from project_a.utils.spark_session import build_spark
147 |         from project_a.config_loader import load_config_resolved
148 |         
    | ^^^^^^^^
149 |         config = load_config_resolved(str(PROJECT_ROOT / config_path))
150 |         spark = build_spark(config=config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/run_local_etl_safe.py:151:1
    |
149 |         config = load_config_resolved(str(PROJECT_ROOT / config_path))
150 |         spark = build_spark(config=config)
151 |         
    | ^^^^^^^^
152 |         try:
153 |             silver_root = config["paths"]["silver_root"]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/run_local_etl_safe.py:155:1
    |
153 |             silver_root = config["paths"]["silver_root"]
154 |             gold_root = config["paths"]["gold_root"]
155 |             
    | ^^^^^^^^^^^^
156 |             # Check Silver tables
157 |             logger.info("\nðŸ“Š Silver Layer:")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/run_local_etl_safe.py:166:1
    |
164 |                 "order_events": f"{silver_root}/order_events_silver",
165 |             }
166 |             
    | ^^^^^^^^^^^^
167 |             for table_name, path in silver_tables.items():
168 |                 try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/run_local_etl_safe.py:178:1
    |
176 |                 except Exception as e:
177 |                     logger.warning(f"  âš ï¸ {table_name}: {e}")
178 |             
    | ^^^^^^^^^^^^
179 |             # Check Gold tables
180 |             logger.info("\nðŸ“Š Gold Layer:")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/run_local_etl_safe.py:189:1
    |
187 |                 "product_performance": f"{gold_root}/product_performance",
188 |             }
189 |             
    | ^^^^^^^^^^^^
190 |             for table_name, path in gold_tables.items():
191 |                 try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/run_local_etl_safe.py:201:1
    |
199 |                 except Exception as e:
200 |                     logger.warning(f"  âš ï¸ {table_name}: {e}")
201 |         
    | ^^^^^^^^
202 |         finally:
203 |             spark.stop()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/run_local_etl_safe.py:204:1
    |
202 |         finally:
203 |             spark.stop()
204 |     
    | ^^^^
205 |     except Exception as e:
206 |         logger.warning(f"âš ï¸  Could not verify outputs with Spark: {e}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/run_local_etl_safe.py:215:1
    |
213 |     """Run full ETL pipeline locally with safety checks."""
214 |     import argparse
215 |     
    | ^^^^
216 |     parser = argparse.ArgumentParser(description="Run full ETL pipeline locally (safe mode)")
217 |     parser.add_argument("--config", default="local/config/local.yaml", help="Config file path")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/run_local_etl_safe.py:223:1
    |
221 |     parser.add_argument("--check-env", action="store_true", help="Check Spark environment before running")
222 |     args = parser.parse_args()
223 |     
    | ^^^^
224 |     logger.info("=" * 80)
225 |     logger.info("ðŸš€ STARTING LOCAL ETL PIPELINE (SAFE MODE)")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/run_local_etl_safe.py:230:1
    |
228 |     logger.info(f"Project Root: {PROJECT_ROOT}")
229 |     logger.info("")
230 |     
    | ^^^^
231 |     # Check environment if requested
232 |     if args.check_env:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/run_local_etl_safe.py:245:1
    |
243 |             logger.info("âœ… Environment looks good!")
244 |             logger.info("")
245 |     
    | ^^^^
246 |     if args.verify_only:
247 |         verify_outputs(args.config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/run_local_etl_safe.py:249:1
    |
247 |         verify_outputs(args.config)
248 |         return
249 |     
    | ^^^^
250 |     # Step 1: Bronze â†’ Silver
251 |     if not args.skip_bronze:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/run_local_etl_safe.py:257:1
    |
255 |     else:
256 |         logger.info("â­ï¸  Skipping Bronzeâ†’Silver step")
257 |     
    | ^^^^
258 |     # Step 2: Silver â†’ Gold
259 |     if not args.skip_gold:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/run_local_etl_safe.py:265:1
    |
263 |     else:
264 |         logger.info("â­ï¸  Skipping Silverâ†’Gold step")
265 |     
    | ^^^^
266 |     # Step 3: Verify outputs
267 |     verify_outputs(args.config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/run_local_etl_safe.py:268:1
    |
266 |     # Step 3: Verify outputs
267 |     verify_outputs(args.config)
268 |     
    | ^^^^
269 |     logger.info("")
270 |     logger.info("=" * 80)
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> local/scripts/show_s3_output_data.py:7:1
   |
 5 |   Uses boto3 and pandas to read and display sample data from S3 Silver and Gold layers.
 6 |   """
 7 | / import sys
 8 | | import boto3
 9 | | import pandas as pd
10 | | import json
11 | | from pathlib import Path
12 | | from io import BytesIO
13 | | from typing import Dict, Any
   | |____________________________^
14 |
15 |   # Add src to path
   |
help: Organize imports

F401 [*] `json` imported but unused
  --> local/scripts/show_s3_output_data.py:10:8
   |
 8 | import boto3
 9 | import pandas as pd
10 | import json
   |        ^^^^
11 | from pathlib import Path
12 | from io import BytesIO
   |
help: Remove unused import: `json`

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> local/scripts/show_s3_output_data.py:13:1
   |
11 | from pathlib import Path
12 | from io import BytesIO
13 | from typing import Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
14 |
15 | # Add src to path
   |

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> local/scripts/show_s3_output_data.py:24:44
   |
22 |     # Fallback: load config directly
23 |     import yaml
24 |     def load_config_resolved(path: str) -> Dict[str, Any]:
   |                                            ^^^^
25 |         with open(path, 'r') as f:
26 |             return yaml.safe_load(f)
   |
help: Replace with `dict`

UP015 [*] Unnecessary mode argument
  --> local/scripts/show_s3_output_data.py:25:25
   |
23 |     import yaml
24 |     def load_config_resolved(path: str) -> Dict[str, Any]:
25 |         with open(path, 'r') as f:
   |                         ^^^
26 |             return yaml.safe_load(f)
   |
help: Remove mode argument

W291 [*] Trailing whitespace
  --> local/scripts/show_s3_output_data.py:42:68
   |
40 |     try:
41 |         response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)
42 |         files = [obj['Key'] for obj in response.get('Contents', []) 
   |                                                                    ^
43 |                  if obj['Key'].endswith('.parquet') or obj['Key'].endswith('.snappy.parquet')]
44 |         return files[:max_files]
   |
help: Remove trailing whitespace

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> local/scripts/show_s3_output_data.py:49:30
   |
47 |         return []
48 |
49 | def show_silver_data(config: Dict[str, Any]):
   |                              ^^^^
50 |     """Show sample data from Silver layer."""
51 |     print("=" * 80)
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> local/scripts/show_s3_output_data.py:54:1
   |
52 |     print("ðŸ“Š SILVER LAYER - SAMPLE DATA")
53 |     print("=" * 80)
54 |     
   | ^^^^
55 |     s3_client = boto3.client('s3', region_name=config.get("aws", {}).get("region", "us-east-1"))
56 |     silver_root = config["paths"]["silver_root"]
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/show_s3_output_data.py:57:1
   |
55 |     s3_client = boto3.client('s3', region_name=config.get("aws", {}).get("region", "us-east-1"))
56 |     silver_root = config["paths"]["silver_root"]
57 |     
   | ^^^^
58 |     # Parse S3 path
59 |     bucket = silver_root.replace("s3://", "").split("/")[0]
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/show_s3_output_data.py:61:1
   |
59 |     bucket = silver_root.replace("s3://", "").split("/")[0]
60 |     base_prefix = "/".join(silver_root.replace("s3://", "").split("/")[1:])
61 |     
   | ^^^^
62 |     tables = {
63 |         "customers_silver": f"{base_prefix}/customers_silver",
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/show_s3_output_data.py:67:1
   |
65 |         "products_silver": f"{base_prefix}/products_silver",
66 |     }
67 |     
   | ^^^^
68 |     for table_name, prefix in tables.items():
69 |         print(f"\nðŸ” {table_name}:")
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> local/scripts/show_s3_output_data.py:83:28
   |
81 |             print("  âš ï¸  No Parquet files found")
82 |
83 | def show_gold_data(config: Dict[str, Any]):
   |                            ^^^^
84 |     """Show sample data from Gold layer."""
85 |     print("\n" + "=" * 80)
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> local/scripts/show_s3_output_data.py:88:1
   |
86 |     print("ðŸ“Š GOLD LAYER - SAMPLE DATA")
87 |     print("=" * 80)
88 |     
   | ^^^^
89 |     s3_client = boto3.client('s3', region_name=config.get("aws", {}).get("region", "us-east-1"))
90 |     gold_root = config["paths"]["gold_root"]
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/show_s3_output_data.py:91:1
   |
89 |     s3_client = boto3.client('s3', region_name=config.get("aws", {}).get("region", "us-east-1"))
90 |     gold_root = config["paths"]["gold_root"]
91 |     
   | ^^^^
92 |     # Parse S3 path
93 |     bucket = gold_root.replace("s3://", "").split("/")[0]
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/show_s3_output_data.py:95:1
   |
93 |     bucket = gold_root.replace("s3://", "").split("/")[0]
94 |     base_prefix = "/".join(gold_root.replace("s3://", "").split("/")[1:])
95 |     
   | ^^^^
96 |     tables = {
97 |         "fact_orders": f"{base_prefix}/fact_orders",
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/show_s3_output_data.py:101:1
    |
 99 |         "dim_product": f"{base_prefix}/dim_product",
100 |     }
101 |     
    | ^^^^
102 |     for table_name, prefix in tables.items():
103 |         print(f"\nðŸ” {table_name}:")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/show_s3_output_data.py:120:1
    |
118 |     """Main entry point."""
119 |     import argparse
120 |     
    | ^^^^
121 |     parser = argparse.ArgumentParser(description="Show sample output data from S3")
122 |     parser.add_argument("--config", default="aws/config/dev.yaml", help="Config file path")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/show_s3_output_data.py:125:1
    |
123 |     parser.add_argument("--layer", choices=["silver", "gold", "all"], default="all")
124 |     args = parser.parse_args()
125 |     
    | ^^^^
126 |     # Handle relative and absolute paths
127 |     if Path(args.config).is_absolute():
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/show_s3_output_data.py:131:1
    |
129 |     else:
130 |         config_path = PROJECT_ROOT / args.config
131 |     
    | ^^^^
132 |     config = load_config_resolved(str(config_path))
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/show_s3_output_data.py:133:1
    |
132 |     config = load_config_resolved(str(config_path))
133 |     
    | ^^^^
134 |     if args.layer in ["silver", "all"]:
135 |         show_silver_data(config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/show_s3_output_data.py:136:1
    |
134 |     if args.layer in ["silver", "all"]:
135 |         show_silver_data(config)
136 |     
    | ^^^^
137 |     if args.layer in ["gold", "all"]:
138 |         show_gold_data(config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/show_s3_output_data.py:139:1
    |
137 |     if args.layer in ["gold", "all"]:
138 |         show_gold_data(config)
139 |     
    | ^^^^
140 |     print("\n" + "=" * 80)
141 |     print("âœ… Data display complete!")
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> local/scripts/validate_source_data.py:11:1
   |
 9 |   4. Schema correctness
10 |   """
11 | / import sys
12 | | import json
13 | | from pathlib import Path
14 | | from typing import Dict, List, Tuple
   | |____________________________________^
15 |
16 |   # Add src to path
   |
help: Organize imports

F401 [*] `json` imported but unused
  --> local/scripts/validate_source_data.py:12:8
   |
10 | """
11 | import sys
12 | import json
   |        ^^^^
13 | from pathlib import Path
14 | from typing import Dict, List, Tuple
   |
help: Remove unused import: `json`

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> local/scripts/validate_source_data.py:14:1
   |
12 | import json
13 | from pathlib import Path
14 | from typing import Dict, List, Tuple
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
15 |
16 | # Add src to path
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> local/scripts/validate_source_data.py:14:1
   |
12 | import json
13 | from pathlib import Path
14 | from typing import Dict, List, Tuple
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
15 |
16 | # Add src to path
   |

UP035 `typing.Tuple` is deprecated, use `tuple` instead
  --> local/scripts/validate_source_data.py:14:1
   |
12 | import json
13 | from pathlib import Path
14 | from typing import Dict, List, Tuple
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
15 |
16 | # Add src to path
   |

F401 [*] `typing.List` imported but unused
  --> local/scripts/validate_source_data.py:14:26
   |
12 | import json
13 | from pathlib import Path
14 | from typing import Dict, List, Tuple
   |                          ^^^^
15 |
16 | # Add src to path
   |
help: Remove unused import: `typing.List`

I001 [*] Import block is un-sorted or un-formatted
  --> local/scripts/validate_source_data.py:19:1
   |
17 |   sys.path.insert(0, str(Path(__file__).parent.parent / "src"))
18 |
19 | / from pyspark.sql import SparkSession
20 | | from pyspark.sql.functions import col, count, countDistinct
21 | | from pyspark.sql.types import StructType
   | |________________________________________^
22 |
23 |   def create_spark() -> SparkSession:
   |
help: Organize imports

F401 [*] `pyspark.sql.functions.count` imported but unused
  --> local/scripts/validate_source_data.py:20:40
   |
19 | from pyspark.sql import SparkSession
20 | from pyspark.sql.functions import col, count, countDistinct
   |                                        ^^^^^
21 | from pyspark.sql.types import StructType
   |
help: Remove unused import

F401 [*] `pyspark.sql.functions.countDistinct` imported but unused
  --> local/scripts/validate_source_data.py:20:47
   |
19 | from pyspark.sql import SparkSession
20 | from pyspark.sql.functions import col, count, countDistinct
   |                                               ^^^^^^^^^^^^^
21 | from pyspark.sql.types import StructType
   |
help: Remove unused import

F401 [*] `pyspark.sql.types.StructType` imported but unused
  --> local/scripts/validate_source_data.py:21:31
   |
19 | from pyspark.sql import SparkSession
20 | from pyspark.sql.functions import col, count, countDistinct
21 | from pyspark.sql.types import StructType
   |                               ^^^^^^^^^^
22 |
23 | def create_spark() -> SparkSession:
   |
help: Remove unused import: `pyspark.sql.types.StructType`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> local/scripts/validate_source_data.py:35:60
   |
35 | def validate_joins(spark: SparkSession, data_root: str) -> Dict[str, Tuple[bool, str]]:
   |                                                            ^^^^
36 |     """
37 |     Validate all foreign key joins across sources.
   |
help: Replace with `dict`

UP006 [*] Use `tuple` instead of `Tuple` for type annotation
  --> local/scripts/validate_source_data.py:35:70
   |
35 | def validate_joins(spark: SparkSession, data_root: str) -> Dict[str, Tuple[bool, str]]:
   |                                                                      ^^^^^
36 |     """
37 |     Validate all foreign key joins across sources.
   |
help: Replace with `tuple`

W293 Blank line contains whitespace
  --> local/scripts/validate_source_data.py:38:1
   |
36 |     """
37 |     Validate all foreign key joins across sources.
38 |     
   | ^^^^
39 |     Returns:
40 |         Dict mapping join_name -> (is_valid, message)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/validate_source_data.py:43:1
   |
41 |     """
42 |     results = {}
43 |     
   | ^^^^
44 |     try:
45 |         # 1. Snowflake: orders.customer_id â†’ customers.customer_id
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/validate_source_data.py:53:1
   |
51 |             f"{data_root}/bronze/snowflakes/snowflake_orders_100000.csv"
52 |         )
53 |         
   | ^^^^^^^^
54 |         customer_ids = customers.select("customer_id").distinct()
55 |         order_customer_ids = orders.select("customer_id").distinct()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/validate_source_data.py:56:1
   |
54 |         customer_ids = customers.select("customer_id").distinct()
55 |         order_customer_ids = orders.select("customer_id").distinct()
56 |         
   | ^^^^^^^^
57 |         # Check for orphaned orders
58 |         orphaned = order_customer_ids.join(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/validate_source_data.py:62:1
   |
60 |         )
61 |         orphaned_count = orphaned.count()
62 |         
   | ^^^^^^^^
63 |         if orphaned_count == 0:
64 |             results["orders_to_customers"] = (True, "âœ… All orders have valid customer_id")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/validate_source_data.py:70:1
   |
68 |                 f"âŒ {orphaned_count} orders have invalid customer_id"
69 |             )
70 |         
   | ^^^^^^^^
71 |         # 2. Snowflake: orders.product_id â†’ products.product_id
72 |         print("ðŸ” Validating: orders.product_id â†’ products.product_id")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/validate_source_data.py:76:1
   |
74 |             f"{data_root}/bronze/snowflakes/snowflake_products_10000.csv"
75 |         )
76 |         
   | ^^^^^^^^
77 |         product_ids = products.select("product_id").distinct()
78 |         order_product_ids = orders.select("product_id").distinct()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/validate_source_data.py:79:1
   |
77 |         product_ids = products.select("product_id").distinct()
78 |         order_product_ids = orders.select("product_id").distinct()
79 |         
   | ^^^^^^^^
80 |         orphaned = order_product_ids.join(
81 |             product_ids, "product_id", "left_anti"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/validate_source_data.py:84:1
   |
82 |         )
83 |         orphaned_count = orphaned.count()
84 |         
   | ^^^^^^^^
85 |         if orphaned_count == 0:
86 |             results["orders_to_products"] = (True, "âœ… All orders have valid product_id")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/scripts/validate_source_data.py:92:1
   |
90 |                 f"âŒ {orphaned_count} orders have invalid product_id"
91 |             )
92 |         
   | ^^^^^^^^
93 |         # 3. CRM: contacts.account_id â†’ accounts.account_id
94 |         print("ðŸ” Validating: contacts.account_id â†’ accounts.account_id")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:101:1
    |
 99 |             f"{data_root}/bronze/crm/contacts.csv"
100 |         )
101 |         
    | ^^^^^^^^
102 |         account_ids = accounts.select("account_id").distinct()
103 |         contact_account_ids = contacts.select("account_id").distinct().filter(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:106:1
    |
104 |             col("account_id").isNotNull()
105 |         )
106 |         
    | ^^^^^^^^
107 |         orphaned = contact_account_ids.join(
108 |             account_ids, "account_id", "left_anti"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:111:1
    |
109 |         )
110 |         orphaned_count = orphaned.count()
111 |         
    | ^^^^^^^^
112 |         if orphaned_count == 0:
113 |             results["contacts_to_accounts"] = (True, "âœ… All contacts have valid account_id")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:119:1
    |
117 |                 f"âš ï¸  {orphaned_count} contacts have invalid account_id (may be intentional)"
118 |             )
119 |         
    | ^^^^^^^^
120 |         # 4. CRM: opportunities.account_id â†’ accounts.account_id
121 |         print("ðŸ” Validating: opportunities.account_id â†’ accounts.account_id")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:125:1
    |
123 |             f"{data_root}/bronze/crm/opportunities.csv"
124 |         )
125 |         
    | ^^^^^^^^
126 |         opp_account_ids = opportunities.select("account_id").distinct().filter(
127 |             col("account_id").isNotNull()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:129:1
    |
127 |             col("account_id").isNotNull()
128 |         )
129 |         
    | ^^^^^^^^
130 |         orphaned = opp_account_ids.join(
131 |             account_ids, "account_id", "left_anti"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:134:1
    |
132 |         )
133 |         orphaned_count = orphaned.count()
134 |         
    | ^^^^^^^^
135 |         if orphaned_count == 0:
136 |             results["opportunities_to_accounts"] = (True, "âœ… All opportunities have valid account_id")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:142:1
    |
140 |                 f"âš ï¸  {orphaned_count} opportunities have invalid account_id"
141 |             )
142 |         
    | ^^^^^^^^
143 |         # 5. Redshift: behavior.customer_id â†’ customers.customer_id
144 |         print("ðŸ” Validating: behavior.customer_id â†’ customers.customer_id")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:148:1
    |
146 |             f"{data_root}/bronze/redshift/redshift_customer_behavior_50000.csv"
147 |         )
148 |         
    | ^^^^^^^^
149 |         behavior_customer_ids = behavior.select("customer_id").distinct()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:150:1
    |
149 |         behavior_customer_ids = behavior.select("customer_id").distinct()
150 |         
    | ^^^^^^^^
151 |         orphaned = behavior_customer_ids.join(
152 |             customer_ids, "customer_id", "left_anti"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:155:1
    |
153 |         )
154 |         orphaned_count = orphaned.count()
155 |         
    | ^^^^^^^^
156 |         if orphaned_count == 0:
157 |             results["behavior_to_customers"] = (True, "âœ… All behavior records have valid customer_id")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:163:1
    |
161 |                 f"âš ï¸  {orphaned_count} behavior records have invalid customer_id (may be intentional for testing)"
162 |             )
163 |         
    | ^^^^^^^^
164 |         # 6. Kafka: events.customer_id â†’ customers.customer_id (if present in JSON)
165 |         print("ðŸ” Validating: kafka events (checking customer_id in metadata)")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:169:1
    |
167 |             f"{data_root}/bronze/kafka/stream_kafka_events_100000.csv"
168 |         )
169 |         
    | ^^^^^^^^
170 |         # Kafka events may have customer_id in metadata JSON
171 |         # This is a soft check - we'll note if customer_id appears
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:177:1
    |
175 |             f"âœ… {kafka_count:,} Kafka events loaded (customer_id may be in JSON metadata)"
176 |         )
177 |         
    | ^^^^^^^^
178 |     except Exception as e:
179 |         results["error"] = (False, f"âŒ Validation error: {e}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:180:1
    |
178 |     except Exception as e:
179 |         results["error"] = (False, f"âŒ Validation error: {e}")
180 |     
    | ^^^^
181 |     return results
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/scripts/validate_source_data.py:184:67
    |
184 | def validate_data_quality(spark: SparkSession, data_root: str) -> Dict[str, Tuple[bool, str]]:
    |                                                                   ^^^^
185 |     """Validate data quality checks."""
186 |     results = {}
    |
help: Replace with `dict`

UP006 [*] Use `tuple` instead of `Tuple` for type annotation
   --> local/scripts/validate_source_data.py:184:77
    |
184 | def validate_data_quality(spark: SparkSession, data_root: str) -> Dict[str, Tuple[bool, str]]:
    |                                                                             ^^^^^
185 |     """Validate data quality checks."""
186 |     results = {}
    |
help: Replace with `tuple`

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:187:1
    |
185 |     """Validate data quality checks."""
186 |     results = {}
187 |     
    | ^^^^
188 |     try:
189 |         # Check for null primary keys
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:191:1
    |
189 |         # Check for null primary keys
190 |         print("ðŸ” Checking for null primary keys...")
191 |         
    | ^^^^^^^^
192 |         customers = spark.read.option("header", "true").csv(
193 |             f"{data_root}/bronze/snowflakes/snowflake_customers_50000.csv"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:200:1
    |
198 |             f"{'âœ…' if null_customer_ids == 0 else 'âŒ'} Customer IDs: {null_customer_ids} nulls"
199 |         )
200 |         
    | ^^^^^^^^
201 |         orders = spark.read.option("header", "true").csv(
202 |             f"{data_root}/bronze/snowflakes/snowflake_orders_100000.csv"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:209:1
    |
207 |             f"{'âœ…' if null_order_ids == 0 else 'âŒ'} Order IDs: {null_order_ids} nulls"
208 |         )
209 |         
    | ^^^^^^^^
210 |         # Check for negative amounts
211 |         negative_amounts = orders.filter(col("amount") < 0).count()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:216:1
    |
214 |             f"{'âœ…' if negative_amounts == 0 else 'âš ï¸ '} Negative amounts: {negative_amounts}"
215 |         )
216 |         
    | ^^^^^^^^
217 |     except Exception as e:
218 |         results["dq_error"] = (False, f"âŒ DQ check error: {e}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:219:1
    |
217 |     except Exception as e:
218 |         results["dq_error"] = (False, f"âŒ DQ check error: {e}")
219 |     
    | ^^^^
220 |     return results
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:226:1
    |
224 |     """Main validation function."""
225 |     import argparse
226 |     
    | ^^^^
227 |     parser = argparse.ArgumentParser(description="Validate source data")
228 |     parser.add_argument(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:234:1
    |
232 |     )
233 |     args = parser.parse_args()
234 |     
    | ^^^^
235 |     print("=" * 60)
236 |     print("SOURCE DATA VALIDATION")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:239:1
    |
237 |     print("=" * 60)
238 |     print()
239 |     
    | ^^^^
240 |     spark = create_spark()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:241:1
    |
240 |     spark = create_spark()
241 |     
    | ^^^^
242 |     try:
243 |         # Validate joins
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:247:1
    |
245 |         print("-" * 60)
246 |         join_results = validate_joins(spark, args.data_root)
247 |         
    | ^^^^^^^^
248 |         for join_name, (is_valid, message) in join_results.items():
249 |             print(f"  {message}")
    |
help: Remove whitespace from blank line

B007 Loop control variable `join_name` not used within loop body
   --> local/scripts/validate_source_data.py:248:13
    |
246 |         join_results = validate_joins(spark, args.data_root)
247 |         
248 |         for join_name, (is_valid, message) in join_results.items():
    |             ^^^^^^^^^
249 |             print(f"  {message}")
    |
help: Rename unused `join_name` to `_join_name`

B007 Loop control variable `is_valid` not used within loop body
   --> local/scripts/validate_source_data.py:248:25
    |
246 |         join_results = validate_joins(spark, args.data_root)
247 |         
248 |         for join_name, (is_valid, message) in join_results.items():
    |                         ^^^^^^^^
249 |             print(f"  {message}")
    |
help: Rename unused `is_valid` to `_is_valid`

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:250:1
    |
248 |         for join_name, (is_valid, message) in join_results.items():
249 |             print(f"  {message}")
250 |         
    | ^^^^^^^^
251 |         print()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:252:1
    |
251 |         print()
252 |         
    | ^^^^^^^^
253 |         # Validate data quality
254 |         print("ðŸ“Š VALIDATING DATA QUALITY")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:257:1
    |
255 |         print("-" * 60)
256 |         dq_results = validate_data_quality(spark, args.data_root)
257 |         
    | ^^^^^^^^
258 |         for check_name, (is_valid, message) in dq_results.items():
259 |             print(f"  {message}")
    |
help: Remove whitespace from blank line

B007 Loop control variable `check_name` not used within loop body
   --> local/scripts/validate_source_data.py:258:13
    |
256 |         dq_results = validate_data_quality(spark, args.data_root)
257 |         
258 |         for check_name, (is_valid, message) in dq_results.items():
    |             ^^^^^^^^^^
259 |             print(f"  {message}")
    |
help: Rename unused `check_name` to `_check_name`

B007 Loop control variable `is_valid` not used within loop body
   --> local/scripts/validate_source_data.py:258:26
    |
256 |         dq_results = validate_data_quality(spark, args.data_root)
257 |         
258 |         for check_name, (is_valid, message) in dq_results.items():
    |                          ^^^^^^^^
259 |             print(f"  {message}")
    |
help: Rename unused `is_valid` to `_is_valid`

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:260:1
    |
258 |         for check_name, (is_valid, message) in dq_results.items():
259 |             print(f"  {message}")
260 |         
    | ^^^^^^^^
261 |         print()
262 |         print("=" * 60)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:263:1
    |
261 |         print()
262 |         print("=" * 60)
263 |         
    | ^^^^^^^^
264 |         # Summary
265 |         all_joins_valid = all(is_valid for is_valid, _ in join_results.values() if "error" not in join_results)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:267:1
    |
265 |         all_joins_valid = all(is_valid for is_valid, _ in join_results.values() if "error" not in join_results)
266 |         all_dq_valid = all(is_valid for is_valid, _ in dq_results.values() if "error" not in dq_results)
267 |         
    | ^^^^^^^^
268 |         if all_joins_valid and all_dq_valid:
269 |             print("âœ… ALL VALIDATIONS PASSED")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/scripts/validate_source_data.py:274:1
    |
272 |             print("âš ï¸  SOME VALIDATIONS FAILED (see details above)")
273 |             return 1
274 |             
    | ^^^^^^^^^^^^
275 |     except Exception as e:
276 |         print(f"âŒ Validation failed: {e}")
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> local/tests/load_test_pipeline.py:8:1
   |
 6 |   """
 7 |
 8 | / import sys
 9 | | import os
10 | | import time
11 | | import argparse
12 | | from pathlib import Path
   | |________________________^
13 |
14 |   sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', 'src'))
   |
help: Organize imports

I001 [*] Import block is un-sorted or un-formatted
  --> local/tests/load_test_pipeline.py:16:1
   |
14 |   sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', 'src'))
15 |
16 | / from pyspark.sql import SparkSession
17 | | from project_a.utils.spark_session import build_spark
18 | | from project_a.utils.config import load_conf
   | |____________________________________________^
   |
help: Organize imports

W293 [*] Blank line contains whitespace
  --> local/tests/load_test_pipeline.py:24:1
   |
22 |     """Generate test data of specified size."""
23 |     from pyspark.sql.functions import lit, rand
24 |     
   | ^^^^
25 |     test_data_dir = Path(f"/tmp/load_test_{record_count}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/load_test_pipeline.py:26:1
   |
25 |     test_data_dir = Path(f"/tmp/load_test_{record_count}")
26 |     
   | ^^^^
27 |     print(f"Generating {record_count:,} test records...")
28 |     df = spark.range(record_count) \
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/load_test_pipeline.py:32:1
   |
30 |         .withColumn("amount", rand() * 1000) \
31 |         .withColumn("order_date", lit("2024-01-15"))
32 |     
   | ^^^^
33 |     df.write.mode("overwrite").parquet(str(test_data_dir))
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/load_test_pipeline.py:34:1
   |
33 |     df.write.mode("overwrite").parquet(str(test_data_dir))
34 |     
   | ^^^^
35 |     actual_count = df.count()
36 |     size_mb = sum(f.stat().st_size for f in test_data_dir.rglob("*") if f.is_file()) / (1024 * 1024)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/load_test_pipeline.py:37:1
   |
35 |     actual_count = df.count()
36 |     size_mb = sum(f.stat().st_size for f in test_data_dir.rglob("*") if f.is_file()) / (1024 * 1024)
37 |     
   | ^^^^
38 |     print(f"âœ… Generated {actual_count:,} records ({size_mb:.2f} MB)")
39 |     return test_data_dir
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/load_test_pipeline.py:45:1
   |
43 |     """Benchmark Bronze ingestion."""
44 |     start = time.time()
45 |     
   | ^^^^
46 |     df = spark.read.parquet(str(test_data))
47 |     df.write.format("delta").mode("overwrite").save(output_path)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/load_test_pipeline.py:48:1
   |
46 |     df = spark.read.parquet(str(test_data))
47 |     df.write.format("delta").mode("overwrite").save(output_path)
48 |     
   | ^^^^
49 |     duration = time.time() - start
50 |     records = df.count()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/load_test_pipeline.py:52:1
   |
50 |     records = df.count()
51 |     throughput = records / duration if duration > 0 else 0
52 |     
   | ^^^^
53 |     return {
54 |         "stage": "bronze_ingestion",
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/load_test_pipeline.py:65:1
   |
63 |     config = load_conf(config_path)
64 |     spark = build_spark(app_name="load_test", config=config)
65 |     
   | ^^^^
66 |     results = []
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/load_test_pipeline.py:67:1
   |
66 |     results = []
67 |     
   | ^^^^
68 |     try:
69 |         for count in record_counts:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/load_test_pipeline.py:73:1
   |
71 |             print(f"Load Test: {count:,} records")
72 |             print(f"{'='*60}")
73 |             
   | ^^^^^^^^^^^^
74 |             # Generate test data
75 |             test_data = generate_test_data(spark, count)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/load_test_pipeline.py:76:1
   |
74 |             # Generate test data
75 |             test_data = generate_test_data(spark, count)
76 |             
   | ^^^^^^^^^^^^
77 |             # Benchmark ingestion
78 |             result = benchmark_ingestion(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/load_test_pipeline.py:85:1
   |
83 |             result["record_count"] = count
84 |             results.append(result)
85 |             
   | ^^^^^^^^^^^^
86 |             # Cleanup
87 |             import shutil
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/load_test_pipeline.py:89:1
   |
87 |             import shutil
88 |             shutil.rmtree(test_data, ignore_errors=True)
89 |             
   | ^^^^^^^^^^^^
90 |             print(f"âœ… Completed: {result['duration_seconds']:.2f}s ({result['throughput_rps']:.0f} records/sec)")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/load_test_pipeline.py:91:1
   |
90 |             print(f"âœ… Completed: {result['duration_seconds']:.2f}s ({result['throughput_rps']:.0f} records/sec)")
91 |     
   | ^^^^
92 |     finally:
93 |         spark.stop()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/load_test_pipeline.py:94:1
   |
92 |     finally:
93 |         spark.stop()
94 |     
   | ^^^^
95 |     # Print summary
96 |     print(f"\n{'='*60}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/load_test_pipeline.py:103:1
    |
101 |     for r in results:
102 |         print(f"{r['record_count']:>12,} | {r['duration_seconds']:>15.2f} | {r['throughput_rps']:>20,.0f}")
103 |     
    | ^^^^
104 |     return results
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/load_test_pipeline.py:112:1
    |
110 |                        help="Record counts to test (e.g., 1000000 10000000)")
111 |     parser.add_argument("--config", default="config/local.yaml", help="Config file")
112 |     
    | ^^^^
113 |     args = parser.parse_args()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/load_test_pipeline.py:114:1
    |
113 |     args = parser.parse_args()
114 |     
    | ^^^^
115 |     results = run_load_test(args.scales, args.config)
    |
help: Remove whitespace from blank line

W291 Trailing whitespace
  --> local/tests/local/crm_etl_pipeline.py:8:29
   |
 6 | 1. Extract CRM data (Accounts, Contacts, Opportunities)
 7 | 2. Transform to Bronze layer
 8 | 3. Transform to Silver layer  
   |                             ^^
 9 | 4. Transform to Gold layer
10 | 5. Generate analytics
   |
help: Remove trailing whitespace

I001 [*] Import block is un-sorted or un-formatted
  --> local/tests/local/crm_etl_pipeline.py:16:1
   |
14 |   """
15 |
16 | / import os
17 | | import sys
18 | | import logging
19 | | import pandas as pd
20 | | from datetime import datetime
21 | | from typing import Dict, Any
   | |____________________________^
22 |
23 |   # Add src to path for imports
   |
help: Organize imports

F401 [*] `pandas` imported but unused
  --> local/tests/local/crm_etl_pipeline.py:19:18
   |
17 | import sys
18 | import logging
19 | import pandas as pd
   |                  ^^
20 | from datetime import datetime
21 | from typing import Dict, Any
   |
help: Remove unused import: `pandas`

F401 [*] `datetime.datetime` imported but unused
  --> local/tests/local/crm_etl_pipeline.py:20:22
   |
18 | import logging
19 | import pandas as pd
20 | from datetime import datetime
   |                      ^^^^^^^^
21 | from typing import Dict, Any
   |
help: Remove unused import: `datetime.datetime`

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> local/tests/local/crm_etl_pipeline.py:21:1
   |
19 | import pandas as pd
20 | from datetime import datetime
21 | from typing import Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
22 |
23 | # Add src to path for imports
   |

I001 [*] Import block is un-sorted or un-formatted
  --> local/tests/local/crm_etl_pipeline.py:26:1
   |
24 |   sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))
25 |
26 | / from project_a.utils.spark_session import build_spark
27 | | from project_a.utils.config import load_conf
28 | | from project_a.extract.crm_accounts import extract_crm_accounts
29 | | from project_a.extract.crm_contacts import extract_crm_contacts
30 | | from project_a.extract.crm_opportunities import extract_crm_opportunities
   | |_________________________________________________________________________^
31 |
32 |   # Configure logging
   |
help: Organize imports

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> local/tests/local/crm_etl_pipeline.py:40:37
   |
40 | def extract_crm_data(spark, config: Dict[str, Any]) -> Dict[str, Any]:
   |                                     ^^^^
41 |     """Extract all CRM data sources."""
42 |     logger.info("ðŸš€ Starting CRM data extraction...")
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> local/tests/local/crm_etl_pipeline.py:40:56
   |
40 | def extract_crm_data(spark, config: Dict[str, Any]) -> Dict[str, Any]:
   |                                                        ^^^^
41 |     """Extract all CRM data sources."""
42 |     logger.info("ðŸš€ Starting CRM data extraction...")
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> local/tests/local/crm_etl_pipeline.py:43:1
   |
41 |     """Extract all CRM data sources."""
42 |     logger.info("ðŸš€ Starting CRM data extraction...")
43 |     
   | ^^^^
44 |     crm_data = {}
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/crm_etl_pipeline.py:45:1
   |
44 |     crm_data = {}
45 |     
   | ^^^^
46 |     try:
47 |         # Extract Accounts
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/crm_etl_pipeline.py:52:1
   |
50 |         crm_data['accounts'] = accounts_df
51 |         logger.info(f"âœ… Extracted {accounts_df.count()} accounts")
52 |         
   | ^^^^^^^^
53 |         # Extract Contacts
54 |         logger.info("ðŸ‘¥ Extracting CRM Contacts...")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/crm_etl_pipeline.py:58:1
   |
56 |         crm_data['contacts'] = contacts_df
57 |         logger.info(f"âœ… Extracted {contacts_df.count()} contacts")
58 |         
   | ^^^^^^^^
59 |         # Extract Opportunities
60 |         logger.info("ðŸ’° Extracting CRM Opportunities...")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/crm_etl_pipeline.py:64:1
   |
62 |         crm_data['opportunities'] = opportunities_df
63 |         logger.info(f"âœ… Extracted {opportunities_df.count()} opportunities")
64 |         
   | ^^^^^^^^
65 |         logger.info("ðŸŽ‰ CRM data extraction completed successfully!")
66 |         return crm_data
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/crm_etl_pipeline.py:67:1
   |
65 |         logger.info("ðŸŽ‰ CRM data extraction completed successfully!")
66 |         return crm_data
67 |         
   | ^^^^^^^^
68 |     except Exception as e:
69 |         logger.error(f"âŒ CRM data extraction failed: {str(e)}")
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> local/tests/local/crm_etl_pipeline.py:73:42
   |
73 | def transform_to_bronze(spark, crm_data: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
   |                                          ^^^^
74 |     """Transform CRM data to Bronze layer."""
75 |     logger.info("ðŸ”„ Transforming to Bronze layer...")
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> local/tests/local/crm_etl_pipeline.py:73:66
   |
73 | def transform_to_bronze(spark, crm_data: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
   |                                                                  ^^^^
74 |     """Transform CRM data to Bronze layer."""
75 |     logger.info("ðŸ”„ Transforming to Bronze layer...")
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> local/tests/local/crm_etl_pipeline.py:73:85
   |
73 | def transform_to_bronze(spark, crm_data: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
   |                                                                                     ^^^^
74 |     """Transform CRM data to Bronze layer."""
75 |     logger.info("ðŸ”„ Transforming to Bronze layer...")
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> local/tests/local/crm_etl_pipeline.py:76:1
   |
74 |     """Transform CRM data to Bronze layer."""
75 |     logger.info("ðŸ”„ Transforming to Bronze layer...")
76 |     
   | ^^^^
77 |     bronze_data = {}
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/crm_etl_pipeline.py:78:1
   |
77 |     bronze_data = {}
78 |     
   | ^^^^
79 |     try:
80 |         # Create Bronze layer directory
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/crm_etl_pipeline.py:83:1
   |
81 |         bronze_path = config.get('lake', {}).get('bronze_path', 'data/lakehouse_delta/bronze')
82 |         os.makedirs(f"{bronze_path}/crm", exist_ok=True)
83 |         
   | ^^^^^^^^
84 |         for table_name, df in crm_data.items():
85 |             # Write to Bronze layer as Parquet
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/crm_etl_pipeline.py:90:1
   |
88 |             bronze_data[table_name] = bronze_table_path
89 |             logger.info(f"âœ… Bronze layer: {table_name} â†’ {bronze_table_path}")
90 |         
   | ^^^^^^^^
91 |         logger.info("ðŸŽ‰ Bronze layer transformation completed!")
92 |         return bronze_data
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/crm_etl_pipeline.py:93:1
   |
91 |         logger.info("ðŸŽ‰ Bronze layer transformation completed!")
92 |         return bronze_data
93 |         
   | ^^^^^^^^
94 |     except Exception as e:
95 |         logger.error(f"âŒ Bronze layer transformation failed: {str(e)}")
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/tests/local/crm_etl_pipeline.py:99:45
    |
 99 | def transform_to_silver(spark, bronze_data: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
    |                                             ^^^^
100 |     """Transform Bronze data to Silver layer."""
101 |     logger.info("ðŸ”„ Transforming to Silver layer...")
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/tests/local/crm_etl_pipeline.py:99:69
    |
 99 | def transform_to_silver(spark, bronze_data: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
    |                                                                     ^^^^
100 |     """Transform Bronze data to Silver layer."""
101 |     logger.info("ðŸ”„ Transforming to Silver layer...")
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/tests/local/crm_etl_pipeline.py:99:88
    |
 99 | def transform_to_silver(spark, bronze_data: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
    |                                                                                        ^^^^
100 |     """Transform Bronze data to Silver layer."""
101 |     logger.info("ðŸ”„ Transforming to Silver layer...")
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:102:1
    |
100 |     """Transform Bronze data to Silver layer."""
101 |     logger.info("ðŸ”„ Transforming to Silver layer...")
102 |     
    | ^^^^
103 |     silver_data = {}
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:104:1
    |
103 |     silver_data = {}
104 |     
    | ^^^^
105 |     try:
106 |         # Create Silver layer directory
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:109:1
    |
107 |         silver_path = config.get('lake', {}).get('silver_path', 'data/lakehouse_delta/silver')
108 |         os.makedirs(f"{silver_path}/crm", exist_ok=True)
109 |         
    | ^^^^^^^^
110 |         # Transform Accounts to Silver
111 |         accounts_df = spark.read.parquet(bronze_data['accounts'])
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:121:1
    |
119 |         if hasattr(spark, '_jvm') and spark._jvm is not None:
120 |             silver_accounts = silver_accounts.filter(accounts_df['AccountStatus'] == 'Active')
121 |         
    | ^^^^^^^^
122 |         silver_accounts_path = f"{silver_path}/crm/dim_accounts"
123 |         silver_accounts.write.mode("overwrite").parquet(silver_accounts_path)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:126:1
    |
124 |         silver_data['dim_accounts'] = silver_accounts_path
125 |         logger.info(f"âœ… Silver layer: dim_accounts â†’ {silver_accounts_path}")
126 |         
    | ^^^^^^^^
127 |         # Transform Contacts to Silver
128 |         contacts_df = spark.read.parquet(bronze_data['contacts'])
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:137:1
    |
135 |         if hasattr(spark, '_jvm') and spark._jvm is not None:
136 |             silver_contacts = silver_contacts.filter(contacts_df['Email'].isNotNull())
137 |         
    | ^^^^^^^^
138 |         silver_contacts_path = f"{silver_path}/crm/dim_contacts"
139 |         silver_contacts.write.mode("overwrite").parquet(silver_contacts_path)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:142:1
    |
140 |         silver_data['dim_contacts'] = silver_contacts_path
141 |         logger.info(f"âœ… Silver layer: dim_contacts â†’ {silver_contacts_path}")
142 |         
    | ^^^^^^^^
143 |         # Transform Opportunities to Silver
144 |         opportunities_df = spark.read.parquet(bronze_data['opportunities'])
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:153:1
    |
151 |         if hasattr(spark, '_jvm') and spark._jvm is not None:
152 |             silver_opportunities = silver_opportunities.filter(opportunities_df['Amount'] > 0)
153 |         
    | ^^^^^^^^
154 |         silver_opportunities_path = f"{silver_path}/crm/fact_opportunities"
155 |         silver_opportunities.write.mode("overwrite").parquet(silver_opportunities_path)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:158:1
    |
156 |         silver_data['fact_opportunities'] = silver_opportunities_path
157 |         logger.info(f"âœ… Silver layer: fact_opportunities â†’ {silver_opportunities_path}")
158 |         
    | ^^^^^^^^
159 |         logger.info("ðŸŽ‰ Silver layer transformation completed!")
160 |         return silver_data
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:161:1
    |
159 |         logger.info("ðŸŽ‰ Silver layer transformation completed!")
160 |         return silver_data
161 |         
    | ^^^^^^^^
162 |     except Exception as e:
163 |         logger.error(f"âŒ Silver layer transformation failed: {str(e)}")
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/tests/local/crm_etl_pipeline.py:167:43
    |
167 | def transform_to_gold(spark, silver_data: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
    |                                           ^^^^
168 |     """Transform Silver data to Gold layer."""
169 |     logger.info("ðŸ”„ Transforming to Gold layer...")
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/tests/local/crm_etl_pipeline.py:167:67
    |
167 | def transform_to_gold(spark, silver_data: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
    |                                                                   ^^^^
168 |     """Transform Silver data to Gold layer."""
169 |     logger.info("ðŸ”„ Transforming to Gold layer...")
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/tests/local/crm_etl_pipeline.py:167:86
    |
167 | def transform_to_gold(spark, silver_data: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
    |                                                                                      ^^^^
168 |     """Transform Silver data to Gold layer."""
169 |     logger.info("ðŸ”„ Transforming to Gold layer...")
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:170:1
    |
168 |     """Transform Silver data to Gold layer."""
169 |     logger.info("ðŸ”„ Transforming to Gold layer...")
170 |     
    | ^^^^
171 |     gold_data = {}
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:172:1
    |
171 |     gold_data = {}
172 |     
    | ^^^^
173 |     try:
174 |         # Create Gold layer directory
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:177:1
    |
175 |         gold_path = config.get('lake', {}).get('gold_path', 'data/lakehouse_delta/gold')
176 |         os.makedirs(f"{gold_path}/crm", exist_ok=True)
177 |         
    | ^^^^^^^^
178 |         # Load Silver data
179 |         dim_accounts = spark.read.parquet(silver_data['dim_accounts'])
    |
help: Remove whitespace from blank line

F841 Local variable `dim_contacts` is assigned to but never used
   --> local/tests/local/crm_etl_pipeline.py:180:9
    |
178 |         # Load Silver data
179 |         dim_accounts = spark.read.parquet(silver_data['dim_accounts'])
180 |         dim_contacts = spark.read.parquet(silver_data['dim_contacts'])
    |         ^^^^^^^^^^^^
181 |         fact_opportunities = spark.read.parquet(silver_data['fact_opportunities'])
    |
help: Remove assignment to unused variable `dim_contacts`

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:182:1
    |
180 |         dim_contacts = spark.read.parquet(silver_data['dim_contacts'])
181 |         fact_opportunities = spark.read.parquet(silver_data['fact_opportunities'])
182 |         
    | ^^^^^^^^
183 |         # Create Gold analytics tables
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:184:1
    |
183 |         # Create Gold analytics tables
184 |         
    | ^^^^^^^^
185 |         # 1. Revenue by Industry
186 |         if hasattr(spark, '_jvm') and spark._jvm is not None:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:196:1
    |
194 |             # Mock DataFrame for testing
195 |             revenue_by_industry = fact_opportunities
196 |         
    | ^^^^^^^^
197 |         revenue_by_industry_path = f"{gold_path}/crm/revenue_by_industry"
198 |         revenue_by_industry.write.mode("overwrite").parquet(revenue_by_industry_path)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:201:1
    |
199 |         gold_data['revenue_by_industry'] = revenue_by_industry_path
200 |         logger.info(f"âœ… Gold layer: revenue_by_industry â†’ {revenue_by_industry_path}")
201 |         
    | ^^^^^^^^
202 |         # 2. Revenue by Geography
203 |         if hasattr(spark, '_jvm') and spark._jvm is not None:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:213:1
    |
211 |             # Mock DataFrame for testing
212 |             revenue_by_geography = fact_opportunities
213 |         
    | ^^^^^^^^
214 |         revenue_by_geography_path = f"{gold_path}/crm/revenue_by_geography"
215 |         revenue_by_geography.write.mode("overwrite").parquet(revenue_by_geography_path)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:218:1
    |
216 |         gold_data['revenue_by_geography'] = revenue_by_geography_path
217 |         logger.info(f"âœ… Gold layer: revenue_by_geography â†’ {revenue_by_geography_path}")
218 |         
    | ^^^^^^^^
219 |         # 3. Customer Segmentation
220 |         if hasattr(spark, '_jvm') and spark._jvm is not None:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:230:1
    |
228 |             # Mock DataFrame for testing
229 |             customer_segmentation = fact_opportunities
230 |         
    | ^^^^^^^^
231 |         customer_segmentation_path = f"{gold_path}/crm/customer_segmentation"
232 |         customer_segmentation.write.mode("overwrite").parquet(customer_segmentation_path)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:235:1
    |
233 |         gold_data['customer_segmentation'] = customer_segmentation_path
234 |         logger.info(f"âœ… Gold layer: customer_segmentation â†’ {customer_segmentation_path}")
235 |         
    | ^^^^^^^^
236 |         logger.info("ðŸŽ‰ Gold layer transformation completed!")
237 |         return gold_data
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:238:1
    |
236 |         logger.info("ðŸŽ‰ Gold layer transformation completed!")
237 |         return gold_data
238 |         
    | ^^^^^^^^
239 |     except Exception as e:
240 |         logger.error(f"âŒ Gold layer transformation failed: {str(e)}")
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/tests/local/crm_etl_pipeline.py:244:49
    |
244 | def generate_analytics_report(spark, gold_data: Dict[str, Any]) -> None:
    |                                                 ^^^^
245 |     """Generate analytics report from Gold layer."""
246 |     logger.info("ðŸ“Š Generating analytics report...")
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:247:1
    |
245 |     """Generate analytics report from Gold layer."""
246 |     logger.info("ðŸ“Š Generating analytics report...")
247 |     
    | ^^^^
248 |     try:
249 |         # Load Gold data
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:253:1
    |
251 |         revenue_by_geography = spark.read.parquet(gold_data['revenue_by_geography'])
252 |         customer_segmentation = spark.read.parquet(gold_data['customer_segmentation'])
253 |         
    | ^^^^^^^^
254 |         logger.info("\\nðŸŽ¯ CRM ANALYTICS REPORT")
255 |         logger.info("=" * 50)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:256:1
    |
254 |         logger.info("\\nðŸŽ¯ CRM ANALYTICS REPORT")
255 |         logger.info("=" * 50)
256 |         
    | ^^^^^^^^
257 |         # Revenue by Industry
258 |         logger.info("\\nðŸ“Š REVENUE BY INDUSTRY:")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:264:1
    |
262 |         else:
263 |             logger.info("   (Mock mode - skipping detailed analytics)")
264 |         
    | ^^^^^^^^
265 |         # Revenue by Geography
266 |         logger.info("\\nðŸŒ REVENUE BY GEOGRAPHY:")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:272:1
    |
270 |         else:
271 |             logger.info("   (Mock mode - skipping detailed analytics)")
272 |         
    | ^^^^^^^^
273 |         # Customer Segmentation
274 |         logger.info("\\nðŸ‘¥ CUSTOMER SEGMENTATION:")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:280:1
    |
278 |         else:
279 |             logger.info("   (Mock mode - skipping detailed analytics)")
280 |         
    | ^^^^^^^^
281 |         logger.info("\\nðŸŽ‰ Analytics report generated successfully!")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:282:1
    |
281 |         logger.info("\\nðŸŽ‰ Analytics report generated successfully!")
282 |         
    | ^^^^^^^^
283 |     except Exception as e:
284 |         logger.error(f"âŒ Analytics report generation failed: {str(e)}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:295:1
    |
293 |         logger.info("ðŸ“Š Local CRM simulation with Salesforce-compatible data")
294 |         logger.info("ðŸŽ¯ Goal: Bronze â†’ Silver â†’ Gold transformations")
295 |         
    | ^^^^^^^^
296 |         # Load configuration
297 |         config_path = 'config/dev.yaml'
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:300:1
    |
298 |         config = load_conf(config_path)
299 |         config['environment'] = 'local'  # Force local mode
300 |         
    | ^^^^^^^^
301 |         logger.info(f"âš™ï¸ Using config: {config_path}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:302:1
    |
301 |         logger.info(f"âš™ï¸ Using config: {config_path}")
302 |         
    | ^^^^^^^^
303 |         # Build Spark session
304 |         spark = build_spark(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:308:1
    |
306 |             config=config
307 |         )
308 |         
    | ^^^^^^^^
309 |         # Run ETL pipeline
310 |         logger.info("\\nðŸ”„ PHASE 1: EXTRACT")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:312:1
    |
310 |         logger.info("\\nðŸ”„ PHASE 1: EXTRACT")
311 |         crm_data = extract_crm_data(spark, config)
312 |         
    | ^^^^^^^^
313 |         logger.info("\\nðŸ”„ PHASE 2: BRONZE TRANSFORMATION")
314 |         bronze_data = transform_to_bronze(spark, crm_data, config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:315:1
    |
313 |         logger.info("\\nðŸ”„ PHASE 2: BRONZE TRANSFORMATION")
314 |         bronze_data = transform_to_bronze(spark, crm_data, config)
315 |         
    | ^^^^^^^^
316 |         logger.info("\\nðŸ”„ PHASE 3: SILVER TRANSFORMATION")
317 |         silver_data = transform_to_silver(spark, bronze_data, config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:318:1
    |
316 |         logger.info("\\nðŸ”„ PHASE 3: SILVER TRANSFORMATION")
317 |         silver_data = transform_to_silver(spark, bronze_data, config)
318 |         
    | ^^^^^^^^
319 |         logger.info("\\nðŸ”„ PHASE 4: GOLD TRANSFORMATION")
320 |         gold_data = transform_to_gold(spark, silver_data, config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:321:1
    |
319 |         logger.info("\\nðŸ”„ PHASE 4: GOLD TRANSFORMATION")
320 |         gold_data = transform_to_gold(spark, silver_data, config)
321 |         
    | ^^^^^^^^
322 |         logger.info("\\nðŸ“Š PHASE 5: ANALYTICS")
323 |         generate_analytics_report(spark, gold_data)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:324:1
    |
322 |         logger.info("\\nðŸ“Š PHASE 5: ANALYTICS")
323 |         generate_analytics_report(spark, gold_data)
324 |         
    | ^^^^^^^^
325 |         # Summary
326 |         logger.info("\\nðŸŽ‰ CRM ETL PIPELINE COMPLETED SUCCESSFULLY!")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:332:1
    |
330 |         logger.info("âœ… Analytics generated")
331 |         logger.info("âœ… Ready for production deployment")
332 |         
    | ^^^^^^^^
333 |         return True
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:334:1
    |
333 |         return True
334 |         
    | ^^^^^^^^
335 |     except Exception as e:
336 |         logger.error(f"ðŸ’¥ CRM ETL pipeline failed: {str(e)}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/crm_etl_pipeline.py:338:1
    |
336 |         logger.error(f"ðŸ’¥ CRM ETL pipeline failed: {str(e)}")
337 |         return False
338 |     
    | ^^^^
339 |     finally:
340 |         # Clean up Spark session
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> local/tests/local/run_etl_end_to_end.py:12:1
   |
10 |   """
11 |
12 | / import sys
13 | | import os
14 | | import logging
15 | | from pathlib import Path
16 | | from datetime import datetime
   | |_____________________________^
17 |
18 |   # Add paths
   |
help: Organize imports

F401 [*] `os` imported but unused
  --> local/tests/local/run_etl_end_to_end.py:13:8
   |
12 | import sys
13 | import os
   |        ^^
14 | import logging
15 | from pathlib import Path
   |
help: Remove unused import: `os`

E402 Module level import not at top of file
  --> local/tests/local/run_etl_end_to_end.py:23:1
   |
21 | sys.path.insert(0, str(project_root))  # For jobs/
22 |
23 | from pyspark.sql import SparkSession
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
24 | from project_a.utils.spark_session import build_spark
25 | from project_a.utils.config import load_conf
   |

I001 [*] Import block is un-sorted or un-formatted
  --> local/tests/local/run_etl_end_to_end.py:23:1
   |
21 |   sys.path.insert(0, str(project_root))  # For jobs/
22 |
23 | / from pyspark.sql import SparkSession
24 | | from project_a.utils.spark_session import build_spark
25 | | from project_a.utils.config import load_conf
26 | | from project_a.utils.state_store import get_state_store
   | |_______________________________________________________^
27 |
28 |   # Import jobs (handle missing gracefully)
   |
help: Organize imports

E402 Module level import not at top of file
  --> local/tests/local/run_etl_end_to_end.py:24:1
   |
23 | from pyspark.sql import SparkSession
24 | from project_a.utils.spark_session import build_spark
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
25 | from project_a.utils.config import load_conf
26 | from project_a.utils.state_store import get_state_store
   |

E402 Module level import not at top of file
  --> local/tests/local/run_etl_end_to_end.py:25:1
   |
23 | from pyspark.sql import SparkSession
24 | from project_a.utils.spark_session import build_spark
25 | from project_a.utils.config import load_conf
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
26 | from project_a.utils.state_store import get_state_store
   |

E402 Module level import not at top of file
  --> local/tests/local/run_etl_end_to_end.py:26:1
   |
24 | from project_a.utils.spark_session import build_spark
25 | from project_a.utils.config import load_conf
26 | from project_a.utils.state_store import get_state_store
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
27 |
28 | # Import jobs (handle missing gracefully)
   |

F401 [*] `project_a.utils.state_store.get_state_store` imported but unused
  --> local/tests/local/run_etl_end_to_end.py:26:41
   |
24 | from project_a.utils.spark_session import build_spark
25 | from project_a.utils.config import load_conf
26 | from project_a.utils.state_store import get_state_store
   |                                         ^^^^^^^^^^^^^^^
27 |
28 | # Import jobs (handle missing gracefully)
   |
help: Remove unused import: `project_a.utils.state_store.get_state_store`

F821 Undefined name `logger`
  --> local/tests/local/run_etl_end_to_end.py:32:5
   |
30 |     from jobs.ingest.ingest_snowflake_to_s3 import ingest_snowflake_to_s3_raw
31 | except ImportError:
32 |     logger.warning("jobs.ingest not available, will use extractors directly")
   |     ^^^^^^
33 |     ingest_snowflake_to_s3_raw = None
   |

E402 Module level import not at top of file
  --> local/tests/local/run_etl_end_to_end.py:45:1
   |
43 |     raw_to_bronze = None
44 |
45 | from jobs.bronze_to_silver_behavior import transform_bronze_to_silver_behavior
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
46 | from jobs.silver_build_customer_360 import build_customer_360
47 | from jobs.silver_build_product_perf import build_product_perf_daily
   |

E402 Module level import not at top of file
  --> local/tests/local/run_etl_end_to_end.py:46:1
   |
45 | from jobs.bronze_to_silver_behavior import transform_bronze_to_silver_behavior
46 | from jobs.silver_build_customer_360 import build_customer_360
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
47 | from jobs.silver_build_product_perf import build_product_perf_daily
   |

E402 Module level import not at top of file
  --> local/tests/local/run_etl_end_to_end.py:47:1
   |
45 | from jobs.bronze_to_silver_behavior import transform_bronze_to_silver_behavior
46 | from jobs.silver_build_customer_360 import build_customer_360
47 | from jobs.silver_build_product_perf import build_product_perf_daily
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
48 |
49 | logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
   |

W293 [*] Blank line contains whitespace
  --> local/tests/local/run_etl_end_to_end.py:59:1
   |
57 |             logger.warning(f"Skipping S3 check for {description}: {path}")
58 |             return True
59 |         
   | ^^^^^^^^
60 |         # Local path check
61 |         from pathlib import Path
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/run_etl_end_to_end.py:82:1
   |
80 |     logger.info("STARTING END-TO-END ETL PIPELINE TEST")
81 |     logger.info("=" * 80)
82 |     
   | ^^^^
83 |     config = load_conf(config_path)
84 |     execution_date = datetime.now().strftime("%Y-%m-%d")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/run_etl_end_to_end.py:85:1
   |
83 |     config = load_conf(config_path)
84 |     execution_date = datetime.now().strftime("%Y-%m-%d")
85 |     
   | ^^^^
86 |     spark = build_spark(app_name="etl_end_to_end_test", config=config)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/run_etl_end_to_end.py:87:1
   |
86 |     spark = build_spark(app_name="etl_end_to_end_test", config=config)
87 |     
   | ^^^^
88 |     try:
89 |         # Step 1: Ingest to Raw (skip if ingest jobs not available)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/run_etl_end_to_end.py:93:1
   |
91 |         logger.info("STEP 1: INGESTION TO RAW ZONE")
92 |         logger.info("=" * 80)
93 |         
   | ^^^^^^^^
94 |         if ingest_snowflake_to_s3_raw:
95 |             logger.info("\nðŸ“¥ Ingesting Snowflake orders...")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_etl_end_to_end.py:103:1
    |
101 |         else:
102 |             logger.info("âš ï¸ Skipping raw ingestion (jobs not available, using existing bronze)")
103 |         
    | ^^^^^^^^
104 |         if ingest_crm_to_s3_raw:
105 |             logger.info("\nðŸ“¥ Ingesting CRM accounts...")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_etl_end_to_end.py:111:1
    |
109 |             except Exception as e:
110 |                 logger.warning(f"âš ï¸ CRM ingestion failed: {e}")
111 |         
    | ^^^^^^^^
112 |         # Step 2: Raw â†’ Bronze (skip if transform not available)
113 |         logger.info("\n" + "=" * 80)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_etl_end_to_end.py:116:1
    |
114 |         logger.info("STEP 2: TRANSFORM RAW â†’ BRONZE")
115 |         logger.info("=" * 80)
116 |         
    | ^^^^^^^^
117 |         if raw_to_bronze:
118 |             logger.info("\nðŸ”„ Transforming snowflake.orders raw â†’ bronze...")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_etl_end_to_end.py:126:1
    |
124 |         else:
125 |             logger.info("âš ï¸ Skipping rawâ†’bronze (using existing bronze data)")
126 |         
    | ^^^^^^^^
127 |         # Step 3: Bronze â†’ Silver
128 |         logger.info("\n" + "=" * 80)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_etl_end_to_end.py:131:1
    |
129 |         logger.info("STEP 3: TRANSFORM BRONZE â†’ SILVER")
130 |         logger.info("=" * 80)
131 |         
    | ^^^^^^^^
132 |         # First, check if we need to create bronze data
133 |         bronze_path = config.get("data_lake", {}).get("bronze_path", "data/lakehouse_delta/bronze")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_etl_end_to_end.py:135:1
    |
133 |         bronze_path = config.get("data_lake", {}).get("bronze_path", "data/lakehouse_delta/bronze")
134 |         behavior_bronze = f"{bronze_path}/redshift/behavior"
135 |         
    | ^^^^^^^^
136 |         try:
137 |             spark.read.format("delta").load(behavior_bronze).count()
    |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
   --> local/tests/local/run_etl_end_to_end.py:138:25
    |
136 |         try:
137 |             spark.read.format("delta").load(behavior_bronze).count()
138 |             logger.info(f"âœ… Bronze behavior data exists")
    |                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
139 |         except Exception:
140 |             logger.info("âš ï¸ Bronze behavior not found, trying to extract...")
    |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_etl_end_to_end.py:149:1
    |
147 |             except Exception as e:
148 |                 logger.warning(f"âš ï¸ Could not create bronze behavior: {e}")
149 |         
    | ^^^^^^^^
150 |         logger.info("\nðŸ”„ Transforming behavior bronze â†’ silver...")
151 |         try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_etl_end_to_end.py:158:1
    |
156 |             import traceback
157 |             traceback.print_exc()
158 |         
    | ^^^^^^^^
159 |         # Step 4: Silver â†’ Gold
160 |         logger.info("\n" + "=" * 80)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_etl_end_to_end.py:163:1
    |
161 |         logger.info("STEP 4: TRANSFORM SILVER â†’ GOLD")
162 |         logger.info("=" * 80)
163 |         
    | ^^^^^^^^
164 |         logger.info("\nðŸ”„ Building customer_360 gold table...")
165 |         try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_etl_end_to_end.py:170:1
    |
168 |         except Exception as e:
169 |             logger.error(f"âš ï¸ Customer 360 build failed: {e}")
170 |         
    | ^^^^^^^^
171 |         logger.info("\nðŸ”„ Building product_perf_daily gold table...")
172 |         try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_etl_end_to_end.py:177:1
    |
175 |         except Exception as e:
176 |             logger.error(f"âš ï¸ Product perf build failed: {e}")
177 |         
    | ^^^^^^^^
178 |         # Step 5: Verification
179 |         logger.info("\n" + "=" * 80)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_etl_end_to_end.py:182:1
    |
180 |         logger.info("STEP 5: VERIFICATION")
181 |         logger.info("=" * 80)
182 |         
    | ^^^^^^^^
183 |         bronze_path = config.get("data_lake", {}).get("bronze_path", "data/lakehouse_delta/bronze")
184 |         silver_path = config.get("data_lake", {}).get("silver_path", "data/lakehouse_delta/silver")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_etl_end_to_end.py:186:1
    |
184 |         silver_path = config.get("data_lake", {}).get("silver_path", "data/lakehouse_delta/silver")
185 |         gold_path = config.get("data_lake", {}).get("gold_path", "data/lakehouse_delta/gold")
186 |         
    | ^^^^^^^^
187 |         # Check Bronze
188 |         logger.info("\nðŸ“Š Checking Bronze layer...")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_etl_end_to_end.py:191:1
    |
189 |         check_data_exists(spark, f"{bronze_path}/snowflake/orders", "Bronze: snowflake_orders")
190 |         check_data_exists(spark, f"{bronze_path}/redshift/behavior", "Bronze: redshift_behavior")
191 |         
    | ^^^^^^^^
192 |         # Check Silver
193 |         logger.info("\nðŸ“Š Checking Silver layer...")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_etl_end_to_end.py:197:1
    |
195 |         check_data_exists(spark, f"{silver_path}/crm/contacts", "Silver: crm_contacts")
196 |         check_data_exists(spark, f"{silver_path}/snowflake/orders", "Silver: snowflake_orders")
197 |         
    | ^^^^^^^^
198 |         # Check Gold
199 |         logger.info("\nðŸ“Š Checking Gold layer...")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_etl_end_to_end.py:202:1
    |
200 |         check_data_exists(spark, f"{gold_path}/customer_360", "Gold: customer_360")
201 |         check_data_exists(spark, f"{gold_path}/product_perf_daily", "Gold: product_perf_daily")
202 |         
    | ^^^^^^^^
203 |         logger.info("\n" + "=" * 80)
204 |         logger.info("âœ… END-TO-END ETL PIPELINE TEST COMPLETE")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_etl_end_to_end.py:206:1
    |
204 |         logger.info("âœ… END-TO-END ETL PIPELINE TEST COMPLETE")
205 |         logger.info("=" * 80)
206 |         
    | ^^^^^^^^
207 |     except Exception as e:
208 |         logger.error(f"\nâŒ Pipeline failed: {e}", exc_info=True)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_etl_end_to_end.py:216:1
    |
214 | if __name__ == "__main__":
215 |     import argparse
216 |     
    | ^^^^
217 |     parser = argparse.ArgumentParser(description="Run end-to-end ETL pipeline test")
218 |     parser.add_argument("--config", default="config/local.yaml", help="Config file path")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_etl_end_to_end.py:220:1
    |
218 |     parser.add_argument("--config", default="config/local.yaml", help="Config file path")
219 |     args = parser.parse_args()
220 |     
    | ^^^^
221 |     run_etl_pipeline(args.config)
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> local/tests/local/run_pipeline.py:15:1
   |
13 |   """
14 |
15 | / import sys
16 | | import os
17 | | import logging
18 | | import pandas as pd
19 | | import subprocess
20 | | from datetime import datetime
21 | | from pathlib import Path
22 | | from typing import Dict, Any, List, Tuple
   | |_________________________________________^
23 |
24 |   # Add src to path
   |
help: Organize imports

F401 [*] `pandas` imported but unused
  --> local/tests/local/run_pipeline.py:18:18
   |
16 | import os
17 | import logging
18 | import pandas as pd
   |                  ^^
19 | import subprocess
20 | from datetime import datetime
   |
help: Remove unused import: `pandas`

F401 [*] `pathlib.Path` imported but unused
  --> local/tests/local/run_pipeline.py:21:21
   |
19 | import subprocess
20 | from datetime import datetime
21 | from pathlib import Path
   |                     ^^^^
22 | from typing import Dict, Any, List, Tuple
   |
help: Remove unused import: `pathlib.Path`

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> local/tests/local/run_pipeline.py:22:1
   |
20 | from datetime import datetime
21 | from pathlib import Path
22 | from typing import Dict, Any, List, Tuple
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
23 |
24 | # Add src to path
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> local/tests/local/run_pipeline.py:22:1
   |
20 | from datetime import datetime
21 | from pathlib import Path
22 | from typing import Dict, Any, List, Tuple
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
23 |
24 | # Add src to path
   |

UP035 `typing.Tuple` is deprecated, use `tuple` instead
  --> local/tests/local/run_pipeline.py:22:1
   |
20 | from datetime import datetime
21 | from pathlib import Path
22 | from typing import Dict, Any, List, Tuple
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
23 |
24 | # Add src to path
   |

F401 [*] `typing.Any` imported but unused
  --> local/tests/local/run_pipeline.py:22:26
   |
20 | from datetime import datetime
21 | from pathlib import Path
22 | from typing import Dict, Any, List, Tuple
   |                          ^^^
23 |
24 | # Add src to path
   |
help: Remove unused import

F401 [*] `typing.Tuple` imported but unused
  --> local/tests/local/run_pipeline.py:22:37
   |
20 | from datetime import datetime
21 | from pathlib import Path
22 | from typing import Dict, Any, List, Tuple
   |                                     ^^^^^
23 |
24 | # Add src to path
   |
help: Remove unused import

I001 [*] Import block is un-sorted or un-formatted
  --> local/tests/local/run_pipeline.py:27:1
   |
25 |   sys.path.append(os.path.join(os.path.dirname(__file__), '../../src'))
26 |
27 | / from project_a.utils.config import load_conf
28 | | from project_a.utils.spark_session import build_spark
29 | | from project_a.utils.metrics import track_job_start, track_job_complete, track_records_processed
   | |________________________________________________________________________________________________^
30 |
31 |   # Configure logging
   |
help: Organize imports

F401 [*] `project_a.utils.spark_session.build_spark` imported but unused
  --> local/tests/local/run_pipeline.py:28:43
   |
27 | from project_a.utils.config import load_conf
28 | from project_a.utils.spark_session import build_spark
   |                                           ^^^^^^^^^^^
29 | from project_a.utils.metrics import track_job_start, track_job_complete, track_records_processed
   |
help: Remove unused import: `project_a.utils.spark_session.build_spark`

W293 [*] Blank line contains whitespace
  --> local/tests/local/run_pipeline.py:41:1
   |
39 | class PipelineOrchestrator:
40 |     """Main pipeline orchestrator class."""
41 |     
   | ^^^^
42 |     def __init__(self, config_path: str):
43 |         """Initialize the pipeline orchestrator."""
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/run_pipeline.py:47:1
   |
45 |         self.job_id = track_job_start("enterprise_etl_pipeline", self.config)
46 |         self.results = {}
47 |         
   | ^^^^^^^^
48 |     def run_ingest_job(self, job_name: str, job_script: str, args: List[str] = None) -> bool:
49 |         """
   |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
  --> local/tests/local/run_pipeline.py:48:68
   |
46 |         self.results = {}
47 |         
48 |     def run_ingest_job(self, job_name: str, job_script: str, args: List[str] = None) -> bool:
   |                                                                    ^^^^
49 |         """
50 |         Run an individual ingest job.
   |
help: Replace with `list`

W293 Blank line contains whitespace
  --> local/tests/local/run_pipeline.py:51:1
   |
49 |         """
50 |         Run an individual ingest job.
51 |         
   | ^^^^^^^^
52 |         Args:
53 |             job_name: Name of the job
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> local/tests/local/run_pipeline.py:56:1
   |
54 |             job_script: Path to the job script
55 |             args: Additional arguments
56 |             
   | ^^^^^^^^^^^^
57 |         Returns:
58 |             True if successful, False otherwise
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/run_pipeline.py:61:1
   |
59 |         """
60 |         logger.info(f"ðŸš€ Running {job_name}...")
61 |         
   | ^^^^^^^^
62 |         try:
63 |             # Build command
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/run_pipeline.py:67:1
   |
65 |             if args:
66 |                 cmd.extend(args)
67 |             
   | ^^^^^^^^^^^^
68 |             # Run job
69 |             result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/run_pipeline.py:70:1
   |
68 |             # Run job
69 |             result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
70 |             
   | ^^^^^^^^^^^^
71 |             if result.returncode == 0:
72 |                 logger.info(f"âœ… {job_name} completed successfully")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/run_pipeline.py:79:1
   |
77 |                 self.results[job_name] = "FAILED"
78 |                 return False
79 |                 
   | ^^^^^^^^^^^^^^^^
80 |         except subprocess.TimeoutExpired:
81 |             logger.error(f"â° {job_name} timed out")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/run_pipeline.py:88:1
   |
86 |             self.results[job_name] = "EXCEPTION"
87 |             return False
88 |     
   | ^^^^
89 |     def run_bronze_ingestion(self) -> Dict[str, str]:
90 |         """
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> local/tests/local/run_pipeline.py:89:39
   |
87 |             return False
88 |     
89 |     def run_bronze_ingestion(self) -> Dict[str, str]:
   |                                       ^^^^
90 |         """
91 |         Run bronze ingestion for all data sources.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> local/tests/local/run_pipeline.py:92:1
   |
90 |         """
91 |         Run bronze ingestion for all data sources.
92 |         
   | ^^^^^^^^
93 |         Returns:
94 |             Dictionary with ingestion results
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:98:1
    |
 96 |         logger.info("ðŸš€ Starting Bronze Ingestion Phase")
 97 |         logger.info("=" * 60)
 98 |         
    | ^^^^^^^^
 99 |         # Define all ingest jobs
100 |         ingest_jobs = [
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:108:1
    |
106 |             ("FX Rates", "aws/jobs/fx_rates_ingest.py"),
107 |         ]
108 |         
    | ^^^^^^^^
109 |         # Run each ingest job
110 |         for job_name, job_script in ingest_jobs:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:114:1
    |
112 |             if success:
113 |                 track_records_processed(self.job_id, job_name.lower().replace(' ', '_'), 0)  # Placeholder count
114 |         
    | ^^^^^^^^
115 |         return self.results
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:116:1
    |
115 |         return self.results
116 |     
    | ^^^^
117 |     def run_data_quality_checks(self) -> bool:
118 |         """
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:120:1
    |
118 |         """
119 |         Run data quality checks on bronze layer.
120 |         
    | ^^^^^^^^
121 |         Returns:
122 |             True if all checks pass, False otherwise
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:126:1
    |
124 |         logger.info("ðŸ” Running Bronze Data Quality Checks")
125 |         logger.info("=" * 60)
126 |         
    | ^^^^^^^^
127 |         try:
128 |             success = self.run_ingest_job("Bronze DQ Check", "aws/jobs/dq_check_bronze.py")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:133:1
    |
131 |             logger.error(f"âŒ Bronze DQ checks failed: {str(e)}")
132 |             return False
133 |     
    | ^^^^
134 |     def run_silver_transformations(self) -> bool:
135 |         """
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:137:1
    |
135 |         """
136 |         Run silver transformations.
137 |         
    | ^^^^^^^^
138 |         Returns:
139 |             True if successful, False otherwise
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:143:1
    |
141 |         logger.info("ðŸ”„ Starting Silver Transformation Phase")
142 |         logger.info("=" * 60)
143 |         
    | ^^^^^^^^
144 |         try:
145 |             # Run silver transformation jobs
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:149:1
    |
147 |                 ("Snowflake Silver Merge", "aws/jobs/snowflake_bronze_to_silver_merge.py"),
148 |             ]
149 |             
    | ^^^^^^^^^^^^
150 |             all_success = True
151 |             for job_name, job_script in silver_jobs:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:155:1
    |
153 |                 if not success:
154 |                     all_success = False
155 |             
    | ^^^^^^^^^^^^
156 |             return all_success
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:157:1
    |
156 |             return all_success
157 |             
    | ^^^^^^^^^^^^
158 |         except Exception as e:
159 |             logger.error(f"âŒ Silver transformations failed: {str(e)}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:161:1
    |
159 |             logger.error(f"âŒ Silver transformations failed: {str(e)}")
160 |             return False
161 |     
    | ^^^^
162 |     def run_gold_analytics(self) -> bool:
163 |         """
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:165:1
    |
163 |         """
164 |         Run gold analytics and dimensional modeling.
165 |         
    | ^^^^^^^^
166 |         Returns:
167 |             True if successful, False otherwise
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:171:1
    |
169 |         logger.info("ðŸ“ˆ Starting Gold Analytics Phase")
170 |         logger.info("=" * 60)
171 |         
    | ^^^^^^^^
172 |         try:
173 |             # Run gold analytics jobs
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:180:1
    |
178 |                 ("SCD2 Customer Dimension", "aws/jobs/update_customer_dimension_scd2.py"),
179 |             ]
180 |             
    | ^^^^^^^^^^^^
181 |             all_success = True
182 |             for job_name, job_script in gold_jobs:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:186:1
    |
184 |                 if not success:
185 |                     all_success = False
186 |             
    | ^^^^^^^^^^^^
187 |             return all_success
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:188:1
    |
187 |             return all_success
188 |             
    | ^^^^^^^^^^^^
189 |         except Exception as e:
190 |             logger.error(f"âŒ Gold analytics failed: {str(e)}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:192:1
    |
190 |             logger.error(f"âŒ Gold analytics failed: {str(e)}")
191 |             return False
192 |     
    | ^^^^
193 |     def run_final_quality_checks(self) -> bool:
194 |         """
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:196:1
    |
194 |         """
195 |         Run final data quality checks on silver and gold layers.
196 |         
    | ^^^^^^^^
197 |         Returns:
198 |             True if all checks pass, False otherwise
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:202:1
    |
200 |         logger.info("ðŸ” Running Final Data Quality Checks")
201 |         logger.info("=" * 60)
202 |         
    | ^^^^^^^^
203 |         try:
204 |             success = self.run_ingest_job("Silver DQ Check", "aws/jobs/dq_check_silver.py")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:209:1
    |
207 |             logger.error(f"âŒ Final DQ checks failed: {str(e)}")
208 |             return False
209 |     
    | ^^^^
210 |     def emit_lineage_and_metrics(self) -> bool:
211 |         """
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:213:1
    |
211 |         """
212 |         Emit lineage and metrics for the pipeline run.
213 |         
    | ^^^^^^^^
214 |         Returns:
215 |             True if successful, False otherwise
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:219:1
    |
217 |         logger.info("ðŸ“Š Emitting Lineage and Metrics")
218 |         logger.info("=" * 60)
219 |         
    | ^^^^^^^^
220 |         try:
221 |             success = self.run_ingest_job("Lineage and Metrics", "aws/jobs/emit_lineage_and_metrics.py")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:226:1
    |
224 |             logger.error(f"âŒ Lineage and metrics emission failed: {str(e)}")
225 |             return False
226 |     
    | ^^^^
227 |     def run_complete_pipeline(self) -> bool:
228 |         """
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:230:1
    |
228 |         """
229 |         Run the complete end-to-end pipeline.
230 |         
    | ^^^^^^^^
231 |         Returns:
232 |             True if successful, False otherwise
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:237:1
    |
235 |         logger.info("=" * 70)
236 |         logger.info(f"â° Started at: {datetime.now()}")
237 |         
    | ^^^^^^^^
238 |         try:
239 |             # Phase 1: Bronze Ingestion
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:241:1
    |
239 |             # Phase 1: Bronze Ingestion
240 |             bronze_results = self.run_bronze_ingestion()
241 |             
    | ^^^^^^^^^^^^
242 |             # Phase 2: Bronze DQ Checks
243 |             bronze_dq_success = self.run_data_quality_checks()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:244:1
    |
242 |             # Phase 2: Bronze DQ Checks
243 |             bronze_dq_success = self.run_data_quality_checks()
244 |             
    | ^^^^^^^^^^^^
245 |             # Phase 3: Silver Transformations
246 |             silver_success = self.run_silver_transformations()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:247:1
    |
245 |             # Phase 3: Silver Transformations
246 |             silver_success = self.run_silver_transformations()
247 |             
    | ^^^^^^^^^^^^
248 |             # Phase 4: Gold Analytics
249 |             gold_success = self.run_gold_analytics()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:250:1
    |
248 |             # Phase 4: Gold Analytics
249 |             gold_success = self.run_gold_analytics()
250 |             
    | ^^^^^^^^^^^^
251 |             # Phase 5: Final DQ Checks
252 |             final_dq_success = self.run_final_quality_checks()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:253:1
    |
251 |             # Phase 5: Final DQ Checks
252 |             final_dq_success = self.run_final_quality_checks()
253 |             
    | ^^^^^^^^^^^^
254 |             # Phase 6: Lineage and Metrics
255 |             lineage_success = self.emit_lineage_and_metrics()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:256:1
    |
254 |             # Phase 6: Lineage and Metrics
255 |             lineage_success = self.emit_lineage_and_metrics()
256 |             
    | ^^^^^^^^^^^^
257 |             # Summary
258 |             self.print_pipeline_summary(bronze_results, bronze_dq_success, silver_success, 
    |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
   --> local/tests/local/run_pipeline.py:258:91
    |
257 |             # Summary
258 |             self.print_pipeline_summary(bronze_results, bronze_dq_success, silver_success, 
    |                                                                                           ^
259 |                                       gold_success, final_dq_success, lineage_success)
    |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:260:1
    |
258 |             self.print_pipeline_summary(bronze_results, bronze_dq_success, silver_success, 
259 |                                       gold_success, final_dq_success, lineage_success)
260 |             
    | ^^^^^^^^^^^^
261 |             overall_success = (all(result == "SUCCESS" for result in bronze_results.values()) and
262 |                              bronze_dq_success and silver_success and gold_success and
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:264:1
    |
262 |                              bronze_dq_success and silver_success and gold_success and
263 |                              final_dq_success and lineage_success)
264 |             
    | ^^^^^^^^^^^^
265 |             track_job_complete(self.job_id, "SUCCESS" if overall_success else "FAILED")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:266:1
    |
265 |             track_job_complete(self.job_id, "SUCCESS" if overall_success else "FAILED")
266 |             
    | ^^^^^^^^^^^^
267 |             return overall_success
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:268:1
    |
267 |             return overall_success
268 |             
    | ^^^^^^^^^^^^
269 |         except Exception as e:
270 |             logger.error(f"ðŸ’¥ Pipeline execution failed: {str(e)}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:273:1
    |
271 |             track_job_complete(self.job_id, "FAILED", error_msg=str(e))
272 |             return False
273 |     
    | ^^^^
274 |     def print_pipeline_summary(self, bronze_results: Dict[str, str], bronze_dq_success: bool,
275 |                              silver_success: bool, gold_success: bool, final_dq_success: bool,
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> local/tests/local/run_pipeline.py:274:54
    |
272 |             return False
273 |     
274 |     def print_pipeline_summary(self, bronze_results: Dict[str, str], bronze_dq_success: bool,
    |                                                      ^^^^
275 |                              silver_success: bool, gold_success: bool, final_dq_success: bool,
276 |                              lineage_success: bool) -> None:
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:280:1
    |
278 |         logger.info("\nðŸŽ¯ PIPELINE EXECUTION SUMMARY")
279 |         logger.info("=" * 50)
280 |         
    | ^^^^^^^^
281 |         bronze_success_count = sum(1 for result in bronze_results.values() if result == "SUCCESS")
282 |         total_bronze_jobs = len(bronze_results)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:283:1
    |
281 |         bronze_success_count = sum(1 for result in bronze_results.values() if result == "SUCCESS")
282 |         total_bronze_jobs = len(bronze_results)
283 |         
    | ^^^^^^^^
284 |         logger.info(f"ðŸ“Š Bronze Ingestion: {bronze_success_count}/{total_bronze_jobs} successful")
285 |         logger.info(f"ðŸ” Bronze DQ Checks: {'SUCCESS' if bronze_dq_success else 'FAILED'}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:290:1
    |
288 |         logger.info(f"ðŸ” Final DQ Checks: {'SUCCESS' if final_dq_success else 'FAILED'}")
289 |         logger.info(f"ðŸ“Š Lineage & Metrics: {'SUCCESS' if lineage_success else 'FAILED'}")
290 |         
    | ^^^^^^^^
291 |         overall_success = (bronze_success_count == total_bronze_jobs and bronze_dq_success and
292 |                          silver_success and gold_success and final_dq_success and lineage_success)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:293:1
    |
291 |         overall_success = (bronze_success_count == total_bronze_jobs and bronze_dq_success and
292 |                          silver_success and gold_success and final_dq_success and lineage_success)
293 |         
    | ^^^^^^^^
294 |         logger.info(f"\nðŸŽ‰ Pipeline {'completed successfully' if overall_success else 'completed with errors'}!")
295 |         logger.info(f"â° Finished at: {datetime.now()}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:301:1
    |
299 |     """Main pipeline execution."""
300 |     import argparse
301 |     
    | ^^^^
302 |     parser = argparse.ArgumentParser(description="Enterprise ETL Pipeline Orchestrator")
303 |     parser.add_argument("--config", default="config/dev.yaml", help="Configuration file path")
    |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
   --> local/tests/local/run_pipeline.py:304:80
    |
302 |     parser = argparse.ArgumentParser(description="Enterprise ETL Pipeline Orchestrator")
303 |     parser.add_argument("--config", default="config/dev.yaml", help="Configuration file path")
304 |     parser.add_argument("--phase", choices=["bronze", "silver", "gold", "all"], 
    |                                                                                ^
305 |                        default="all", help="Pipeline phase to run")
306 |     args = parser.parse_args()
    |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:307:1
    |
305 |                        default="all", help="Pipeline phase to run")
306 |     args = parser.parse_args()
307 |     
    | ^^^^
308 |     try:
309 |         # Initialize orchestrator
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:311:1
    |
309 |         # Initialize orchestrator
310 |         orchestrator = PipelineOrchestrator(args.config)
311 |         
    | ^^^^^^^^
312 |         if args.phase == "all":
313 |             success = orchestrator.run_complete_pipeline()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:324:1
    |
322 |             logger.error(f"Unknown phase: {args.phase}")
323 |             success = False
324 |         
    | ^^^^^^^^
325 |         return success
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/run_pipeline.py:326:1
    |
325 |         return success
326 |         
    | ^^^^^^^^
327 |     except Exception as e:
328 |         logger.error(f"ðŸ’¥ Pipeline execution failed: {str(e)}")
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
 --> local/tests/local/test_dq_lineage_integration.py:4:1
  |
2 |   Test script to verify GE and Lineage integration in transforms.
3 |   """
4 | / import sys
5 | | import logging
6 | | from pathlib import Path
  | |________________________^
7 |
8 |   # Add project root to path
  |
help: Organize imports

I001 [*] Import block is un-sorted or un-formatted
  --> local/tests/local/test_dq_lineage_integration.py:11:1
   |
 9 |   sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))
10 |
11 | / from pyspark.sql import SparkSession
12 | | from project_a.utils.spark_session import build_spark
13 | | from project_a.utils.config import load_conf
14 | | from project_a.transform.bronze_to_silver import transform_bronze_to_silver
15 | | from project_a.transform.silver_to_gold import transform_silver_to_gold
16 | | from project_a.dq.ge_runner import GERunner
   | |___________________________________________^
17 |
18 |   logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
   |
help: Organize imports

F401 [*] `pyspark.sql.SparkSession` imported but unused
  --> local/tests/local/test_dq_lineage_integration.py:11:25
   |
 9 | sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))
10 |
11 | from pyspark.sql import SparkSession
   |                         ^^^^^^^^^^^^
12 | from project_a.utils.spark_session import build_spark
13 | from project_a.utils.config import load_conf
   |
help: Remove unused import: `pyspark.sql.SparkSession`

F401 [*] `project_a.dq.ge_runner.GERunner` imported but unused
  --> local/tests/local/test_dq_lineage_integration.py:16:36
   |
14 | from project_a.transform.bronze_to_silver import transform_bronze_to_silver
15 | from project_a.transform.silver_to_gold import transform_silver_to_gold
16 | from project_a.dq.ge_runner import GERunner
   |                                    ^^^^^^^^
17 |
18 | logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
   |
help: Remove unused import: `project_a.dq.ge_runner.GERunner`

W293 [*] Blank line contains whitespace
  --> local/tests/local/test_dq_lineage_integration.py:25:1
   |
23 |     """Test GE integration in transforms."""
24 |     logger.info("ðŸ§ª Testing GE + Lineage Integration")
25 |     
   | ^^^^
26 |     # Load config
27 |     config = load_conf("config/local.yaml")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/test_dq_lineage_integration.py:28:1
   |
26 |     # Load config
27 |     config = load_conf("config/local.yaml")
28 |     
   | ^^^^
29 |     # Build Spark session
30 |     spark = build_spark("test_dq_lineage", config)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/test_dq_lineage_integration.py:31:1
   |
29 |     # Build Spark session
30 |     spark = build_spark("test_dq_lineage", config)
31 |     
   | ^^^^
32 |     # Create sample DataFrame
33 |     from pyspark.sql.types import StructType, StructField, StringType, IntegerType
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> local/tests/local/test_dq_lineage_integration.py:33:5
   |
32 |     # Create sample DataFrame
33 |     from pyspark.sql.types import StructType, StructField, StringType, IntegerType
   |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
34 |     schema = StructType([
35 |         StructField("order_id", IntegerType(), True),
   |
help: Organize imports

W293 [*] Blank line contains whitespace
  --> local/tests/local/test_dq_lineage_integration.py:40:1
   |
38 |         StructField("total_amount", IntegerType(), True)
39 |     ])
40 |     
   | ^^^^
41 |     data = [
42 |         (1, 100, "Product A", 100),
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/test_dq_lineage_integration.py:48:1
   |
46 |         (5, 300, "Product B", 250)
47 |     ]
48 |     
   | ^^^^
49 |     df = spark.createDataFrame(data, schema)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/test_dq_lineage_integration.py:50:1
   |
49 |     df = spark.createDataFrame(data, schema)
50 |     
   | ^^^^
51 |     try:
52 |         # Test bronze to silver with GE
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/test_dq_lineage_integration.py:69:1
   |
67 |         except Exception as e:
68 |             logger.warning(f"âš ï¸ GE integration skipped: {e}")
69 |         
   | ^^^^^^^^
70 |         # Test silver to gold with GE
71 |         logger.info("\nðŸ“Š Testing silver_to_gold with GE integration...")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/test_dq_lineage_integration.py:84:1
   |
82 |         except Exception as e:
83 |             logger.warning(f"âš ï¸ GE integration skipped: {e}")
84 |         
   | ^^^^^^^^
85 |         logger.info("\nâœ… GE + Lineage integration test complete!")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/test_dq_lineage_integration.py:86:1
   |
85 |         logger.info("\nâœ… GE + Lineage integration test complete!")
86 |         
   | ^^^^^^^^
87 |     except Exception as e:
88 |         logger.error(f"âŒ Test failed: {e}", exc_info=True)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/test_dq_lineage_integration.py:90:1
   |
88 |         logger.error(f"âŒ Test failed: {e}", exc_info=True)
89 |         raise
90 |     
   | ^^^^
91 |     finally:
92 |         spark.stop()
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> local/tests/local/test_etl_pipeline.py:12:1
   |
10 |   """
11 |
12 | / import sys
13 | | import logging
14 | | from pathlib import Path
15 | | from datetime import datetime
   | |_____________________________^
16 |
17 |   # Add paths
   |
help: Organize imports

F401 [*] `datetime.datetime` imported but unused
  --> local/tests/local/test_etl_pipeline.py:15:22
   |
13 | import logging
14 | from pathlib import Path
15 | from datetime import datetime
   |                      ^^^^^^^^
16 |
17 | # Add paths
   |
help: Remove unused import: `datetime.datetime`

E402 Module level import not at top of file
  --> local/tests/local/test_etl_pipeline.py:22:1
   |
20 | sys.path.insert(0, str(project_root))
21 |
22 | from pyspark.sql import SparkSession
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
23 | from project_a.utils.spark_session import build_spark
24 | from project_a.utils.config import load_conf
   |

I001 [*] Import block is un-sorted or un-formatted
  --> local/tests/local/test_etl_pipeline.py:22:1
   |
20 |   sys.path.insert(0, str(project_root))
21 |
22 | / from pyspark.sql import SparkSession
23 | | from project_a.utils.spark_session import build_spark
24 | | from project_a.utils.config import load_conf
   | |____________________________________________^
25 |
26 |   logging.basicConfig(
   |
help: Organize imports

E402 Module level import not at top of file
  --> local/tests/local/test_etl_pipeline.py:23:1
   |
22 | from pyspark.sql import SparkSession
23 | from project_a.utils.spark_session import build_spark
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
24 | from project_a.utils.config import load_conf
   |

E402 Module level import not at top of file
  --> local/tests/local/test_etl_pipeline.py:24:1
   |
22 | from pyspark.sql import SparkSession
23 | from project_a.utils.spark_session import build_spark
24 | from project_a.utils.config import load_conf
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
25 |
26 | logging.basicConfig(
   |

W293 [*] Blank line contains whitespace
  --> local/tests/local/test_etl_pipeline.py:37:1
   |
35 |     bronze_path = config.get("data_lake", {}).get("bronze_path", "data/lakehouse_delta/bronze")
36 |     behavior_path = f"{bronze_path}/redshift/behavior"
37 |     
   | ^^^^
38 |     try:
39 |         df = spark.read.format("delta").load(behavior_path)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/test_etl_pipeline.py:46:1
   |
44 |     except Exception:
45 |         pass
46 |     
   | ^^^^
47 |     # Try to create from extractor
48 |     logger.info("Creating bronze behavior data from extractor...")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/test_etl_pipeline.py:65:1
   |
63 |     logger.info("STEP 1: BRONZE â†’ SILVER")
64 |     logger.info("="*60)
65 |     
   | ^^^^
66 |     try:
67 |         from jobs.bronze_to_silver_behavior import transform_bronze_to_silver_behavior
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/test_etl_pipeline.py:83:1
   |
81 |     logger.info("STEP 2: SILVER â†’ GOLD")
82 |     logger.info("="*60)
83 |     
   | ^^^^
84 |     results = {}
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/test_etl_pipeline.py:85:1
   |
84 |     results = {}
85 |     
   | ^^^^
86 |     # Customer 360
87 |     logger.info("\nBuilding customer_360...")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/test_etl_pipeline.py:96:1
   |
94 |         logger.error(f"âŒ Customer 360: FAILED - {e}")
95 |         results['customer_360'] = False
96 |     
   | ^^^^
97 |     # Product perf
98 |     logger.info("\nBuilding product_perf_daily...")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/test_etl_pipeline.py:107:1
    |
105 |         logger.error(f"âŒ Product perf: FAILED - {e}")
106 |         results['product_perf'] = False
107 |     
    | ^^^^
108 |     return results
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/test_etl_pipeline.py:116:1
    |
114 |     logger.info("STEP 3: VERIFICATION")
115 |     logger.info("="*60)
116 |     
    | ^^^^
117 |     bronze_path = config.get("data_lake", {}).get("bronze_path", "data/lakehouse_delta/bronze")
118 |     silver_path = config.get("data_lake", {}).get("silver_path", "data/lakehouse_delta/silver")
    |
help: Remove whitespace from blank line

F841 Local variable `bronze_path` is assigned to but never used
   --> local/tests/local/test_etl_pipeline.py:117:5
    |
115 |     logger.info("="*60)
116 |     
117 |     bronze_path = config.get("data_lake", {}).get("bronze_path", "data/lakehouse_delta/bronze")
    |     ^^^^^^^^^^^
118 |     silver_path = config.get("data_lake", {}).get("silver_path", "data/lakehouse_delta/silver")
119 |     gold_path = config.get("data_lake", {}).get("gold_path", "data/lakehouse_delta/gold")
    |
help: Remove assignment to unused variable `bronze_path`

W293 [*] Blank line contains whitespace
   --> local/tests/local/test_etl_pipeline.py:120:1
    |
118 |     silver_path = config.get("data_lake", {}).get("silver_path", "data/lakehouse_delta/silver")
119 |     gold_path = config.get("data_lake", {}).get("gold_path", "data/lakehouse_delta/gold")
120 |     
    | ^^^^
121 |     checks = {}
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/test_etl_pipeline.py:122:1
    |
121 |     checks = {}
122 |     
    | ^^^^
123 |     # Check Silver
124 |     try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/test_etl_pipeline.py:132:1
    |
130 |         logger.warning(f"âš ï¸ Silver behavior: {e}")
131 |         checks['silver_behavior'] = 0
132 |     
    | ^^^^
133 |     # Check Gold
134 |     try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/test_etl_pipeline.py:142:1
    |
140 |         logger.warning(f"âš ï¸ Gold customer_360: {e}")
141 |         checks['gold_customer_360'] = 0
142 |     
    | ^^^^
143 |     try:
144 |         df = spark.read.format("delta").load(f"{gold_path}/product_perf_daily")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/test_etl_pipeline.py:151:1
    |
149 |         logger.warning(f"âš ï¸ Gold product_perf_daily: {e}")
150 |         checks['gold_product_perf'] = 0
151 |     
    | ^^^^
152 |     return checks
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/test_etl_pipeline.py:160:1
    |
158 |     logger.info("END-TO-END ETL PIPELINE TEST")
159 |     logger.info("="*80)
160 |     
    | ^^^^
161 |     config = load_conf("config/local.yaml")
162 |     spark = build_spark(app_name="etl_end_to_end", config=config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/test_etl_pipeline.py:163:1
    |
161 |     config = load_conf("config/local.yaml")
162 |     spark = build_spark(app_name="etl_end_to_end", config=config)
163 |     
    | ^^^^
164 |     try:
165 |         # Ensure bronze data exists
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/test_etl_pipeline.py:168:1
    |
166 |         logger.info("\nPreparing bronze data...")
167 |         ensure_bronze_data(spark, config)
168 |         
    | ^^^^^^^^
169 |         # Run transformations
170 |         bronze_silver_ok = run_bronze_to_silver(spark, config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/test_etl_pipeline.py:172:1
    |
170 |         bronze_silver_ok = run_bronze_to_silver(spark, config)
171 |         gold_results = run_silver_to_gold(spark, config)
172 |         
    | ^^^^^^^^
173 |         # Verify
174 |         checks = verify_results(spark, config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/test_etl_pipeline.py:175:1
    |
173 |         # Verify
174 |         checks = verify_results(spark, config)
175 |         
    | ^^^^^^^^
176 |         # Summary
177 |         logger.info("\n" + "="*80)
    |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
   --> local/tests/local/test_etl_pipeline.py:183:21
    |
181 |         logger.info(f"Customer 360: {'âœ… PASSED' if gold_results.get('customer_360') else 'âŒ FAILED'}")
182 |         logger.info(f"Product Perf: {'âœ… PASSED' if gold_results.get('product_perf') else 'âŒ FAILED'}")
183 |         logger.info(f"\nOutput Record Counts:")
    |                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
184 |         logger.info(f"  Silver behavior: {checks.get('silver_behavior', 0):,}")
185 |         logger.info(f"  Gold customer_360: {checks.get('gold_customer_360', 0):,}")
    |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
   --> local/tests/local/test_etl_pipeline.py:187:1
    |
185 |         logger.info(f"  Gold customer_360: {checks.get('gold_customer_360', 0):,}")
186 |         logger.info(f"  Gold product_perf: {checks.get('gold_product_perf', 0):,}")
187 |         
    | ^^^^^^^^
188 |         if bronze_silver_ok and any(gold_results.values()) and any(checks.values()):
189 |             logger.info("\nðŸŽ‰ END-TO-END TEST: PASSED")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/test_etl_pipeline.py:194:1
    |
192 |             logger.info("\nâš ï¸ END-TO-END TEST: PARTIAL SUCCESS")
193 |             return 0  # Return 0 to not fail CI
194 |         
    | ^^^^^^^^
195 |     except Exception as e:
196 |         logger.error(f"\nâŒ Test failed with exception: {e}", exc_info=True)
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> local/tests/local/test_etl_simple.py:5:1
   |
 3 |   """
 4 |
 5 | / import sys
 6 | | import logging
 7 | | from pathlib import Path
 8 | | from datetime import datetime
   | |_____________________________^
 9 |
10 |   # Add paths
   |
help: Organize imports

F401 [*] `datetime.datetime` imported but unused
  --> local/tests/local/test_etl_simple.py:8:22
   |
 6 | import logging
 7 | from pathlib import Path
 8 | from datetime import datetime
   |                      ^^^^^^^^
 9 |
10 | # Add paths
   |
help: Remove unused import: `datetime.datetime`

E402 Module level import not at top of file
  --> local/tests/local/test_etl_simple.py:15:1
   |
13 | sys.path.insert(0, str(project_root))
14 |
15 | from project_a.utils.spark_session import build_spark
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
16 | from project_a.utils.config import load_conf
   |

I001 [*] Import block is un-sorted or un-formatted
  --> local/tests/local/test_etl_simple.py:15:1
   |
13 |   sys.path.insert(0, str(project_root))
14 |
15 | / from project_a.utils.spark_session import build_spark
16 | | from project_a.utils.config import load_conf
   | |____________________________________________^
17 |
18 |   logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
   |
help: Organize imports

E402 Module level import not at top of file
  --> local/tests/local/test_etl_simple.py:16:1
   |
15 | from project_a.utils.spark_session import build_spark
16 | from project_a.utils.config import load_conf
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
17 |
18 | logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
   |

W293 [*] Blank line contains whitespace
  --> local/tests/local/test_etl_simple.py:38:1
   |
36 |     """Test silver â†’ gold transformation."""
37 |     logger.info("Testing Silver â†’ Gold transformation...")
38 |     
   | ^^^^
39 |     results = {}
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/test_etl_simple.py:40:1
   |
39 |     results = {}
40 |     
   | ^^^^
41 |     # Test customer_360
42 |     try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/test_etl_simple.py:50:1
   |
48 |         logger.error(f"âŒ Customer 360 build: FAILED - {e}")
49 |         results['customer_360'] = False
50 |     
   | ^^^^
51 |     # Test product_perf
52 |     try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/test_etl_simple.py:60:1
   |
58 |         logger.error(f"âŒ Product perf build: FAILED - {e}")
59 |         results['product_perf'] = False
60 |     
   | ^^^^
61 |     return all(results.values())
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/test_etl_simple.py:67:1
   |
65 |     """Verify outputs exist."""
66 |     logger.info("Verifying outputs...")
67 |     
   | ^^^^
68 |     bronze_path = config.get("data_lake", {}).get("bronze_path", "data/lakehouse_delta/bronze")
69 |     silver_path = config.get("data_lake", {}).get("silver_path", "data/lakehouse_delta/silver")
   |
help: Remove whitespace from blank line

F841 Local variable `bronze_path` is assigned to but never used
  --> local/tests/local/test_etl_simple.py:68:5
   |
66 |     logger.info("Verifying outputs...")
67 |     
68 |     bronze_path = config.get("data_lake", {}).get("bronze_path", "data/lakehouse_delta/bronze")
   |     ^^^^^^^^^^^
69 |     silver_path = config.get("data_lake", {}).get("silver_path", "data/lakehouse_delta/silver")
70 |     gold_path = config.get("data_lake", {}).get("gold_path", "data/lakehouse_delta/gold")
   |
help: Remove assignment to unused variable `bronze_path`

W293 [*] Blank line contains whitespace
  --> local/tests/local/test_etl_simple.py:71:1
   |
69 |     silver_path = config.get("data_lake", {}).get("silver_path", "data/lakehouse_delta/silver")
70 |     gold_path = config.get("data_lake", {}).get("gold_path", "data/lakehouse_delta/gold")
71 |     
   | ^^^^
72 |     checks = []
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/test_etl_simple.py:73:1
   |
72 |     checks = []
73 |     
   | ^^^^
74 |     # Check Silver
75 |     try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/test_etl_simple.py:83:1
   |
81 |         logger.warning(f"âš ï¸ Silver behavior not found: {e}")
82 |         checks.append(False)
83 |     
   | ^^^^
84 |     # Check Gold
85 |     try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/test_etl_simple.py:93:1
   |
91 |         logger.warning(f"âš ï¸ Gold customer_360 not found: {e}")
92 |         checks.append(False)
93 |     
   | ^^^^
94 |     try:
95 |         df = spark.read.format("delta").load(f"{gold_path}/product_perf_daily")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/test_etl_simple.py:102:1
    |
100 |         logger.warning(f"âš ï¸ Gold product_perf_daily not found: {e}")
101 |         checks.append(False)
102 |     
    | ^^^^
103 |     return any(checks)  # At least one should exist
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/test_etl_simple.py:110:1
    |
108 |     logger.info("END-TO-END ETL TEST")
109 |     logger.info("=" * 80)
110 |     
    | ^^^^
111 |     config = load_conf("config/local.yaml")
112 |     spark = build_spark(app_name="etl_test", config=config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/test_etl_simple.py:113:1
    |
111 |     config = load_conf("config/local.yaml")
112 |     spark = build_spark(app_name="etl_test", config=config)
113 |     
    | ^^^^
114 |     try:
115 |         # Test 1: Bronze â†’ Silver
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/test_etl_simple.py:117:1
    |
115 |         # Test 1: Bronze â†’ Silver
116 |         bronze_silver_ok = test_bronze_to_silver(spark, config)
117 |         
    | ^^^^^^^^
118 |         # Test 2: Silver â†’ Gold
119 |         silver_gold_ok = test_silver_to_gold(spark, config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/test_etl_simple.py:120:1
    |
118 |         # Test 2: Silver â†’ Gold
119 |         silver_gold_ok = test_silver_to_gold(spark, config)
120 |         
    | ^^^^^^^^
121 |         # Test 3: Verify outputs
122 |         outputs_ok = verify_outputs(spark, config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/test_etl_simple.py:123:1
    |
121 |         # Test 3: Verify outputs
122 |         outputs_ok = verify_outputs(spark, config)
123 |         
    | ^^^^^^^^
124 |         # Summary
125 |         logger.info("\n" + "=" * 80)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/test_etl_simple.py:131:1
    |
129 |         logger.info(f"Silver â†’ Gold: {'âœ… PASSED' if silver_gold_ok else 'âŒ FAILED'}")
130 |         logger.info(f"Outputs Verified: {'âœ… PASSED' if outputs_ok else 'âš ï¸ PARTIAL'}")
131 |         
    | ^^^^^^^^
132 |         if bronze_silver_ok and silver_gold_ok:
133 |             logger.info("\nðŸŽ‰ END-TO-END TEST: PASSED")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/test_etl_simple.py:138:1
    |
136 |             logger.info("\nâŒ END-TO-END TEST: FAILED")
137 |             return 1
138 |             
    | ^^^^^^^^^^^^
139 |     except Exception as e:
140 |         logger.error(f"âŒ Test failed: {e}", exc_info=True)
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> local/tests/local/verify_enterprise_setup.py:17:1
   |
15 |   """
16 |
17 | / import os
18 | | import sys
19 | | import logging
20 | | import pandas as pd
21 | | from pathlib import Path
22 | | from datetime import datetime
   | |_____________________________^
23 |
24 |   # Configure logging
   |
help: Organize imports

W293 [*] Blank line contains whitespace
  --> local/tests/local/verify_enterprise_setup.py:36:1
   |
34 |     logger.info("ðŸ” Verifying Config Files")
35 |     logger.info("=" * 40)
36 |     
   | ^^^^
37 |     config_files = [
38 |         "config/dev.yaml",
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/verify_enterprise_setup.py:41:1
   |
39 |         "config/prod.yaml"
40 |     ]
41 |     
   | ^^^^
42 |     for config_file in config_files:
43 |         if os.path.exists(config_file):
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/verify_enterprise_setup.py:45:1
   |
43 |         if os.path.exists(config_file):
44 |             logger.info(f"âœ… {config_file} exists")
45 |             
   | ^^^^^^^^^^^^
46 |             # Check for required sections
47 |             with open(config_file, 'r') as f:
   |
help: Remove whitespace from blank line

UP015 [*] Unnecessary mode argument
  --> local/tests/local/verify_enterprise_setup.py:47:36
   |
46 |             # Check for required sections
47 |             with open(config_file, 'r') as f:
   |                                    ^^^
48 |                 content = f.read()
49 |                 if 'paths:' in content and 'bronze:' in content:
   |
help: Remove mode argument

F541 [*] f-string without any placeholders
  --> local/tests/local/verify_enterprise_setup.py:50:33
   |
48 |                 content = f.read()
49 |                 if 'paths:' in content and 'bronze:' in content:
50 |                     logger.info(f"   âœ… Contains path mappings")
   |                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
51 |                 else:
52 |                     logger.warning(f"   âš ï¸ Missing path mappings")
   |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
  --> local/tests/local/verify_enterprise_setup.py:52:36
   |
50 |                     logger.info(f"   âœ… Contains path mappings")
51 |                 else:
52 |                     logger.warning(f"   âš ï¸ Missing path mappings")
   |                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
53 |         else:
54 |             logger.error(f"âŒ {config_file} missing")
   |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
  --> local/tests/local/verify_enterprise_setup.py:55:1
   |
53 |         else:
54 |             logger.error(f"âŒ {config_file} missing")
55 |     
   | ^^^^
56 |     return True
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/verify_enterprise_setup.py:63:1
   |
61 |     logger.info("\\nðŸ” Verifying Bronze Ingest Scripts")
62 |     logger.info("=" * 40)
63 |     
   | ^^^^
64 |     bronze_scripts = [
65 |         "aws/jobs/crm_accounts_ingest.py",
   |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
  --> local/tests/local/verify_enterprise_setup.py:66:43
   |
64 |     bronze_scripts = [
65 |         "aws/jobs/crm_accounts_ingest.py",
66 |         "aws/jobs/crm_contacts_ingest.py", 
   |                                           ^
67 |         "aws/jobs/crm_opportunities_ingest.py"
68 |     ]
   |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
  --> local/tests/local/verify_enterprise_setup.py:69:1
   |
67 |         "aws/jobs/crm_opportunities_ingest.py"
68 |     ]
69 |     
   | ^^^^
70 |     for script in bronze_scripts:
71 |         if os.path.exists(script):
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/verify_enterprise_setup.py:73:1
   |
71 |         if os.path.exists(script):
72 |             logger.info(f"âœ… {script} exists")
73 |             
   | ^^^^^^^^^^^^
74 |             # Check for Delta Lake writes
75 |             with open(script, 'r') as f:
   |
help: Remove whitespace from blank line

UP015 [*] Unnecessary mode argument
  --> local/tests/local/verify_enterprise_setup.py:75:31
   |
74 |             # Check for Delta Lake writes
75 |             with open(script, 'r') as f:
   |                               ^^^
76 |                 content = f.read()
77 |                 if 'format("delta")' in content and '_ingestion_ts' in content:
   |
help: Remove mode argument

F541 [*] f-string without any placeholders
  --> local/tests/local/verify_enterprise_setup.py:78:33
   |
76 |                 content = f.read()
77 |                 if 'format("delta")' in content and '_ingestion_ts' in content:
78 |                     logger.info(f"   âœ… Writes to Delta Lake with metadata")
   |                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
79 |                 else:
80 |                     logger.warning(f"   âš ï¸ Missing Delta Lake writes or metadata")
   |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
  --> local/tests/local/verify_enterprise_setup.py:80:36
   |
78 |                     logger.info(f"   âœ… Writes to Delta Lake with metadata")
79 |                 else:
80 |                     logger.warning(f"   âš ï¸ Missing Delta Lake writes or metadata")
   |                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
81 |         else:
82 |             logger.error(f"âŒ {script} missing")
   |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
  --> local/tests/local/verify_enterprise_setup.py:83:1
   |
81 |         else:
82 |             logger.error(f"âŒ {script} missing")
83 |     
   | ^^^^
84 |     return True
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/verify_enterprise_setup.py:91:1
   |
89 |     logger.info("\\nðŸ” Verifying Pipeline Driver")
90 |     logger.info("=" * 40)
91 |     
   | ^^^^
92 |     driver_file = "scripts/local/run_pipeline.py"
93 |     if os.path.exists(driver_file):
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> local/tests/local/verify_enterprise_setup.py:95:1
   |
93 |     if os.path.exists(driver_file):
94 |         logger.info(f"âœ… {driver_file} exists")
95 |         
   | ^^^^^^^^
96 |         with open(driver_file, 'r') as f:
97 |             content = f.read()
   |
help: Remove whitespace from blank line

UP015 [*] Unnecessary mode argument
  --> local/tests/local/verify_enterprise_setup.py:96:32
   |
94 |         logger.info(f"âœ… {driver_file} exists")
95 |         
96 |         with open(driver_file, 'r') as f:
   |                                ^^^
97 |             content = f.read()
   |
help: Remove mode argument

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:98:1
    |
 96 |         with open(driver_file, 'r') as f:
 97 |             content = f.read()
 98 |             
    | ^^^^^^^^^^^^
 99 |             required_functions = [
100 |                 'run_bronze_ingestion',
    |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
   --> local/tests/local/verify_enterprise_setup.py:101:46
    |
 99 |             required_functions = [
100 |                 'run_bronze_ingestion',
101 |                 'run_silver_transformations', 
    |                                              ^
102 |                 'run_gold_analytics',
103 |                 'run_complete_pipeline'
    |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:105:1
    |
103 |                 'run_complete_pipeline'
104 |             ]
105 |             
    | ^^^^^^^^^^^^
106 |             for func in required_functions:
107 |                 if func in content:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:113:1
    |
111 |     else:
112 |         logger.error(f"âŒ {driver_file} missing")
113 |     
    | ^^^^
114 |     return True
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:121:1
    |
119 |     logger.info("\\nðŸ” Verifying Data Quality Enforcement")
120 |     logger.info("=" * 40)
121 |     
    | ^^^^
122 |     dq_files = [
123 |         "src/project_a/dq/runner.py",
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:126:1
    |
124 |         "config/dq.yaml"
125 |     ]
126 |     
    | ^^^^
127 |     for dq_file in dq_files:
128 |         if os.path.exists(dq_file):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:130:1
    |
128 |         if os.path.exists(dq_file):
129 |             logger.info(f"âœ… {dq_file} exists")
130 |             
    | ^^^^^^^^^^^^
131 |             if dq_file.endswith('runner.py'):
132 |                 with open(dq_file, 'r') as f:
    |
help: Remove whitespace from blank line

UP015 [*] Unnecessary mode argument
   --> local/tests/local/verify_enterprise_setup.py:132:36
    |
131 |             if dq_file.endswith('runner.py'):
132 |                 with open(dq_file, 'r') as f:
    |                                    ^^^
133 |                     content = f.read()
134 |                     if 'run_suite' in content and 'DQResult' in content:
    |
help: Remove mode argument

F541 [*] f-string without any placeholders
   --> local/tests/local/verify_enterprise_setup.py:135:37
    |
133 |                     content = f.read()
134 |                     if 'run_suite' in content and 'DQResult' in content:
135 |                         logger.info(f"   âœ… Contains DQ suite execution")
    |                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
136 |                     else:
137 |                         logger.warning(f"   âš ï¸ Missing DQ suite execution")
    |
help: Remove extraneous `f` prefix

F541 [*] f-string without any placeholders
   --> local/tests/local/verify_enterprise_setup.py:137:40
    |
135 |                         logger.info(f"   âœ… Contains DQ suite execution")
136 |                     else:
137 |                         logger.warning(f"   âš ï¸ Missing DQ suite execution")
    |                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
138 |         else:
139 |             logger.error(f"âŒ {dq_file} missing")
    |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:140:1
    |
138 |         else:
139 |             logger.error(f"âŒ {dq_file} missing")
140 |     
    | ^^^^
141 |     return True
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:148:1
    |
146 |     logger.info("\\nðŸ” Verifying AWS Infrastructure")
147 |     logger.info("=" * 40)
148 |     
    | ^^^^
149 |     # Terraform files
150 |     terraform_files = [
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:157:1
    |
155 |         "aws/terraform/variables.tf"
156 |     ]
157 |     
    | ^^^^
158 |     logger.info("ðŸ“ Terraform Files:")
159 |     for tf_file in terraform_files:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:164:1
    |
162 |         else:
163 |             logger.error(f"   âŒ {tf_file} missing")
164 |     
    | ^^^^
165 |     # Scripts
166 |     script_files = [
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:172:1
    |
170 |         "aws/scripts/teardown.sh"
171 |     ]
172 |     
    | ^^^^
173 |     logger.info("\\nðŸ“ Scripts:")
174 |     for script in script_files:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:179:1
    |
177 |         else:
178 |             logger.error(f"   âŒ {script} missing")
179 |     
    | ^^^^
180 |     # EMR configs
181 |     emr_configs = [
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:186:1
    |
184 |         "aws/emr_configs/logging.yaml"
185 |     ]
186 |     
    | ^^^^
187 |     logger.info("\\nðŸ“ EMR Configs:")
188 |     for config in emr_configs:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:193:1
    |
191 |         else:
192 |             logger.error(f"   âŒ {config} missing")
193 |     
    | ^^^^
194 |     return True
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:201:1
    |
199 |     logger.info("\\nðŸ” Verifying Schema Documentation")
200 |     logger.info("=" * 40)
201 |     
    | ^^^^
202 |     schema_docs = [
203 |         "docs/schema_contracts/crm_data_schema.md",
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:207:1
    |
205 |         "docs/schema_contracts/redshift_schema.md"
206 |     ]
207 |     
    | ^^^^
208 |     for doc in schema_docs:
209 |         if os.path.exists(doc):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:211:1
    |
209 |         if os.path.exists(doc):
210 |             logger.info(f"âœ… {doc} exists")
211 |             
    | ^^^^^^^^^^^^
212 |             # Check for required sections
213 |             with open(doc, 'r') as f:
    |
help: Remove whitespace from blank line

UP015 [*] Unnecessary mode argument
   --> local/tests/local/verify_enterprise_setup.py:213:28
    |
212 |             # Check for required sections
213 |             with open(doc, 'r') as f:
    |                            ^^^
214 |                 content = f.read()
215 |                 required_sections = [
    |
help: Remove mode argument

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:220:1
    |
218 |                     'Compliance Notes'
219 |                 ]
220 |                 
    | ^^^^^^^^^^^^^^^^
221 |                 for section in required_sections:
222 |                     if section in content:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:228:1
    |
226 |         else:
227 |             logger.error(f"âŒ {doc} missing")
228 |     
    | ^^^^
229 |     return True
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:236:1
    |
234 |     logger.info("\\nðŸ” Verifying Delta Lake Outputs")
235 |     logger.info("=" * 40)
236 |     
    | ^^^^
237 |     delta_path = Path("data/lakehouse_delta")
238 |     if not delta_path.exists():
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:241:1
    |
239 |         logger.error("âŒ Delta Lake directory not found")
240 |         return False
241 |     
    | ^^^^
242 |     layers = ['bronze', 'silver', 'gold']
243 |     total_files = 0
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:245:1
    |
243 |     total_files = 0
244 |     total_records = 0
245 |     
    | ^^^^
246 |     for layer in layers:
247 |         layer_path = delta_path / layer
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:250:1
    |
248 |         if layer_path.exists():
249 |             logger.info(f"ðŸ“Š {layer.upper()} Layer:")
250 |             
    | ^^^^^^^^^^^^
251 |             for root, dirs, files in os.walk(layer_path):
252 |                 for file in files:
    |
help: Remove whitespace from blank line

B007 Loop control variable `dirs` not used within loop body
   --> local/tests/local/verify_enterprise_setup.py:251:23
    |
249 |             logger.info(f"ðŸ“Š {layer.upper()} Layer:")
250 |             
251 |             for root, dirs, files in os.walk(layer_path):
    |                       ^^^^
252 |                 for file in files:
253 |                     if file.endswith('.parquet'):
    |
help: Rename unused `dirs` to `_dirs`

F841 [*] Local variable `e` is assigned to but never used
   --> local/tests/local/verify_enterprise_setup.py:261:45
    |
259 |                             total_files += 1
260 |                             total_records += len(df)
261 |                         except Exception as e:
    |                                             ^
262 |                             logger.info(f"   ðŸ“„ {file}: (parquet file)")
263 |         else:
    |
help: Remove assignment to unused variable `e`

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:265:1
    |
263 |         else:
264 |             logger.warning(f"âš ï¸ {layer.upper()} layer not found")
265 |     
    | ^^^^
266 |     logger.info(f"\\nðŸ“ˆ Summary: {total_files} files, {total_records:,} total records")
267 |     return total_files > 0
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:275:1
    |
273 |     logger.info("=" * 60)
274 |     logger.info(f"â° Verification started at: {datetime.now()}")
275 |     
    | ^^^^
276 |     verification_results = []
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:277:1
    |
276 |     verification_results = []
277 |     
    | ^^^^
278 |     # Run all verifications
279 |     verification_results.append(("Config Files", verify_config_files()))
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:286:1
    |
284 |     verification_results.append(("Schema Documentation", verify_schema_documentation()))
285 |     verification_results.append(("Delta Lake Outputs", verify_delta_lake_outputs()))
286 |     
    | ^^^^
287 |     # Summary
288 |     logger.info("\\nðŸŽ¯ VERIFICATION SUMMARY")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:290:1
    |
288 |     logger.info("\\nðŸŽ¯ VERIFICATION SUMMARY")
289 |     logger.info("=" * 50)
290 |     
    | ^^^^
291 |     passed_count = 0
292 |     for component, passed in verification_results:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:297:1
    |
295 |         if passed:
296 |             passed_count += 1
297 |     
    | ^^^^
298 |     total_components = len(verification_results)
299 |     success_rate = (passed_count / total_components) * 100
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:300:1
    |
298 |     total_components = len(verification_results)
299 |     success_rate = (passed_count / total_components) * 100
300 |     
    | ^^^^
301 |     logger.info(f"\\nðŸ“Š Overall: {passed_count}/{total_components} components verified ({success_rate:.1f}%)")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:302:1
    |
301 |     logger.info(f"\\nðŸ“Š Overall: {passed_count}/{total_components} components verified ({success_rate:.1f}%)")
302 |     
    | ^^^^
303 |     if success_rate >= 85:
304 |         logger.info("\\nðŸŽ‰ ENTERPRISE-GRADE VERIFICATION PASSED!")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:308:1
    |
306 |     else:
307 |         logger.warning("\\nâš ï¸ Some components need attention for enterprise-grade compliance.")
308 |     
    | ^^^^
309 |     logger.info(f"\\nâ° Verification completed at: {datetime.now()}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> local/tests/local/verify_enterprise_setup.py:310:1
    |
309 |     logger.info(f"\\nâ° Verification completed at: {datetime.now()}")
310 |     
    | ^^^^
311 |     return success_rate >= 85
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
 --> local/tests/smoke_validate.py:2:1
  |
1 |   #!/usr/bin/env python3
2 | / import os
3 | | import sys
4 | | import json
  | |___________^
5 |
6 |   ROOT = os.path.dirname(os.path.dirname(__file__))
  |
help: Organize imports

E402 Module level import not at top of file
  --> local/tests/smoke_validate.py:9:1
   |
 7 | sys.path.insert(0, os.path.join(ROOT, 'src'))
 8 |
 9 | from project_a.utils import load_config, get_spark_session
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 | from project_a.load import read_delta
   |

I001 [*] Import block is un-sorted or un-formatted
  --> local/tests/smoke_validate.py:9:1
   |
 7 |   sys.path.insert(0, os.path.join(ROOT, 'src'))
 8 |
 9 | / from project_a.utils import load_config, get_spark_session
10 | | from project_a.load import read_delta
   | |_____________________________________^
   |
help: Organize imports

E402 Module level import not at top of file
  --> local/tests/smoke_validate.py:10:1
   |
 9 | from project_a.utils import load_config, get_spark_session
10 | from project_a.load import read_delta
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   |

I001 [*] Import block is un-sorted or un-formatted
  --> scripts/convert_fx_csv_to_json.py:8:1
   |
 6 |   the standard format for streaming JSON data ingestion.
 7 |   """
 8 | / import pandas as pd
 9 | | import json
10 | | import sys
11 | | from pathlib import Path
   | |________________________^
12 |
13 |   def convert_csv_to_json_lines(csv_path: str, output_path: str) -> None:
   |
help: Organize imports

W293 Blank line contains whitespace
  --> scripts/convert_fx_csv_to_json.py:16:1
   |
14 |     """
15 |     Convert CSV file to JSON Lines format.
16 |     
   | ^^^^
17 |     Args:
18 |         csv_path: Path to input CSV file
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/convert_fx_csv_to_json.py:23:1
   |
21 |     print(f"ðŸ“¥ Reading CSV from: {csv_path}")
22 |     df = pd.read_csv(csv_path)
23 |     
   | ^^^^
24 |     print(f"ðŸ“Š Found {len(df):,} records")
25 |     print(f"ðŸ“‹ Columns: {', '.join(df.columns)}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/convert_fx_csv_to_json.py:26:1
   |
24 |     print(f"ðŸ“Š Found {len(df):,} records")
25 |     print(f"ðŸ“‹ Columns: {', '.join(df.columns)}")
26 |     
   | ^^^^
27 |     # Convert to row-oriented JSON (one rate per line)
28 |     # Map CSV columns to JSON structure
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/convert_fx_csv_to_json.py:55:1
   |
53 |             # Generic format - use first columns
54 |             record = row.to_dict()
55 |         
   | ^^^^^^^^
56 |         records.append(record)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/convert_fx_csv_to_json.py:57:1
   |
56 |         records.append(record)
57 |     
   | ^^^^
58 |     # Write as JSON Lines (one JSON object per line)
59 |     print(f"ðŸ’¾ Writing JSON Lines to: {output_path}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/convert_fx_csv_to_json.py:63:1
   |
61 |         for record in records:
62 |             f.write(json.dumps(record) + "\n")
63 |     
   | ^^^^
64 |     print(f"âœ… Converted {len(records):,} records to JSON Lines format")
65 |     print(f"ðŸ“ Output file: {output_path}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/convert_fx_csv_to_json.py:75:1
   |
73 |         print("  python convert_fx_csv_to_json.py data/input_data/fx_rates_historical_730_days.csv")
74 |         sys.exit(1)
75 |     
   | ^^^^
76 |     csv_path = sys.argv[1]
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/convert_fx_csv_to_json.py:77:1
   |
76 |     csv_path = sys.argv[1]
77 |     
   | ^^^^
78 |     if len(sys.argv) >= 3:
79 |         output_path = sys.argv[2]
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/convert_fx_csv_to_json.py:84:1
   |
82 |         csv_file = Path(csv_path)
83 |         output_path = csv_file.parent / f"{csv_file.stem}.json"
84 |     
   | ^^^^
85 |     if not Path(csv_path).exists():
86 |         print(f"âŒ Error: CSV file not found: {csv_path}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/convert_fx_csv_to_json.py:88:1
   |
86 |         print(f"âŒ Error: CSV file not found: {csv_path}")
87 |         sys.exit(1)
88 |     
   | ^^^^
89 |     convert_csv_to_json_lines(csv_path, str(output_path))
   |
help: Remove whitespace from blank line

F401 [*] `os` imported but unused
  --> scripts/final_cleanup.py:14:8
   |
12 | """
13 |
14 | import os
   |        ^^
15 | import shutil
16 | from pathlib import Path
   |
help: Remove unused import: `os`

UP035 `typing.List` is deprecated, use `list` instead
  --> scripts/final_cleanup.py:17:1
   |
15 | import shutil
16 | from pathlib import Path
17 | from typing import List
   | ^^^^^^^^^^^^^^^^^^^^^^^
18 |
19 | PROJECT_ROOT = Path(__file__).parent.parent
   |

F401 [*] `typing.List` imported but unused
  --> scripts/final_cleanup.py:17:20
   |
15 | import shutil
16 | from pathlib import Path
17 | from typing import List
   |                    ^^^^
18 |
19 | PROJECT_ROOT = Path(__file__).parent.parent
   |
help: Remove unused import: `typing.List`

W293 [*] Blank line contains whitespace
  --> scripts/final_cleanup.py:49:1
   |
47 |     """Main cleanup function."""
48 |     print("ðŸ” Starting final cleanup...")
49 |     
   | ^^^^
50 |     deleted_count = 0
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/final_cleanup.py:51:1
   |
50 |     deleted_count = 0
51 |     
   | ^^^^
52 |     # 1. Delete Python cache files
53 |     print(f"\nðŸ—‘ï¸  Deleting Python cache files...")
   |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
  --> scripts/final_cleanup.py:53:11
   |
52 |     # 1. Delete Python cache files
53 |     print(f"\nðŸ—‘ï¸  Deleting Python cache files...")
   |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
54 |     pycache_count = delete_pycache_dirs()
55 |     pyc_count = delete_pyc_files()
   |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
  --> scripts/final_cleanup.py:58:1
   |
56 |     deleted_count += pycache_count + pyc_count
57 |     print(f"  âœ… Deleted {pycache_count} __pycache__ directories and {pyc_count} .pyc files")
58 |     
   | ^^^^
59 |     # 2. Delete log files
60 |     print(f"\nðŸ“ Deleting log files...")
   |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
  --> scripts/final_cleanup.py:60:11
   |
59 |     # 2. Delete log files
60 |     print(f"\nðŸ“ Deleting log files...")
   |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
61 |     log_files = list(PROJECT_ROOT.rglob("*.log"))
62 |     for log_file in log_files:
   |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
  --> scripts/final_cleanup.py:72:1
   |
70 |         except Exception as e:
71 |             print(f"  âš ï¸  Failed to delete {log_file}: {e}")
72 |     
   | ^^^^
73 |     # 3. Delete .DS_Store files (macOS)
74 |     print(f"\nðŸŽ Deleting .DS_Store files...")
   |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
  --> scripts/final_cleanup.py:74:11
   |
73 |     # 3. Delete .DS_Store files (macOS)
74 |     print(f"\nðŸŽ Deleting .DS_Store files...")
   |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
75 |     ds_store_files = list(PROJECT_ROOT.rglob(".DS_Store"))
76 |     for ds_file in ds_store_files:
   |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
  --> scripts/final_cleanup.py:83:1
   |
81 |         except Exception as e:
82 |             print(f"  âš ï¸  Failed to delete {ds_file}: {e}")
83 |     
   | ^^^^
84 |     print(f"\nâœ… Cleanup complete! Deleted {deleted_count} files/directories.")
85 |     print("\nðŸ“ Summary:")
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> scripts/kafka_consumer.py:7:1
   |
 5 |   """
 6 |
 7 | / import json
 8 | | import logging
 9 | | import argparse
10 | | from kafka import KafkaConsumer
   | |_______________________________^
11 |
12 |   # Configure logging
   |
help: Organize imports

F821 Undefined name `time`
  --> scripts/kafka_consumer.py:44:22
   |
42 |         logger.info(f"Group ID: {self.consumer.config['group_id']}")
43 |
44 |         start_time = time.time()
   |                      ^^^^
45 |
46 |         try:
   |

F821 Undefined name `time`
  --> scripts/kafka_consumer.py:71:24
   |
70 |                     # Check timeout
71 |                     if time.time() - start_time > timeout_seconds:
   |                        ^^^^
72 |                         logger.info(f"Timeout reached: {timeout_seconds} seconds")
73 |                         break
   |

I001 [*] Import block is un-sorted or un-formatted
  --> scripts/kafka_producer.py:7:1
   |
 5 |   """
 6 |
 7 | / import os
 8 | | import sys
 9 | | import json
10 | | import time
11 | | import random
12 | | from datetime import datetime, timedelta
13 | | from typing import Dict, Any
   | |____________________________^
14 |
15 |   try:
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> scripts/kafka_producer.py:13:1
   |
11 | import random
12 | from datetime import datetime, timedelta
13 | from typing import Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
14 |
15 | try:
   |

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> scripts/kafka_producer.py:22:43
   |
22 | def create_sample_order(order_id: str) -> Dict[str, Any]:
   |                                           ^^^^
23 |     """
24 |     Create a sample order event.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> scripts/kafka_producer.py:25:1
   |
23 |     """
24 |     Create a sample order event.
25 |     
   | ^^^^
26 |     Args:
27 |         order_id: Order ID
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> scripts/kafka_producer.py:28:1
   |
26 |     Args:
27 |         order_id: Order ID
28 |         
   | ^^^^^^^^
29 |     Returns:
30 |         Order event dictionary
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/kafka_producer.py:34:1
   |
32 |     statuses = ["PLACED", "PAID", "SHIPPED", "CANCELLED"]
33 |     currencies = ["USD", "EUR", "GBP", "CAD", "AUD"]
34 |     
   | ^^^^
35 |     return {
36 |         "order_id": order_id,
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> scripts/kafka_producer.py:54:1
   |
52 |     """
53 |     Produce sample order events to Kafka.
54 |     
   | ^^^^
55 |     Args:
56 |         bootstrap_servers: Kafka bootstrap servers
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/kafka_producer.py:70:1
   |
68 |         'sasl.password': api_secret,
69 |     }
70 |     
   | ^^^^
71 |     producer = Producer(producer_config)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/kafka_producer.py:72:1
   |
71 |     producer = Producer(producer_config)
72 |     
   | ^^^^
73 |     print(f"Producing {num_orders} order events to topic '{topic}'...")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/kafka_producer.py:74:1
   |
73 |     print(f"Producing {num_orders} order events to topic '{topic}'...")
74 |     
   | ^^^^
75 |     try:
76 |         for i in range(num_orders):
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/kafka_producer.py:79:1
   |
77 |             order_id = f"O{random.randint(100000, 999999)}"
78 |             order_event = create_sample_order(order_id)
79 |             
   | ^^^^^^^^^^^^
80 |             # Serialize to JSON
81 |             message_value = json.dumps(order_event)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/kafka_producer.py:82:1
   |
80 |             # Serialize to JSON
81 |             message_value = json.dumps(order_event)
82 |             
   | ^^^^^^^^^^^^
83 |             # Produce message
84 |             producer.produce(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/kafka_producer.py:90:1
   |
88 | â€¦         callback=lambda err, msg: print(f"Message delivered: {msg.key().decode('utf-8')}") if not err else print(f"Message failed: {â€¦
89 | â€¦     )
90 | â€¦     
^^^^^^^^^^^
91 | â€¦     # Flush every 10 messages
92 | â€¦     if (i + 1) % 10 == 0:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/kafka_producer.py:95:1
   |
93 |                 producer.flush()
94 |                 print(f"Produced {i + 1} messages...")
95 |             
   | ^^^^^^^^^^^^
96 |             # Small delay between messages
97 |             time.sleep(0.1)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/kafka_producer.py:98:1
    |
 96 |             # Small delay between messages
 97 |             time.sleep(0.1)
 98 |         
    | ^^^^^^^^
 99 |         # Flush remaining messages
100 |         producer.flush()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/kafka_producer.py:102:1
    |
100 |         producer.flush()
101 |         print(f"âœ… Successfully produced {num_orders} order events!")
102 |         
    | ^^^^^^^^
103 |     except Exception as e:
104 |         print(f"âŒ Error producing messages: {e}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/kafka_producer.py:113:1
    |
111 |     """Main function to run the Kafka producer."""
112 |     import argparse
113 |     
    | ^^^^
114 |     parser = argparse.ArgumentParser(description="Produce sample order events to Kafka")
115 |     parser.add_argument("--bootstrap-servers", required=True, help="Kafka bootstrap servers")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/kafka_producer.py:121:1
    |
119 |     parser.add_argument("--num-orders", type=int, default=50, help="Number of orders to produce")
120 |     args = parser.parse_args()
121 |     
    | ^^^^
122 |     # Get configuration from environment variables if not provided
123 |     bootstrap_servers = args.bootstrap_servers or os.getenv("KAFKA_BOOTSTRAP")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/kafka_producer.py:127:1
    |
125 |     api_secret = args.api_secret or os.getenv("KAFKA_API_SECRET")
126 |     topic = args.topic or os.getenv("KAFKA_TOPIC", "orders_events")
127 |     
    | ^^^^
128 |     if not all([bootstrap_servers, api_key, api_secret]):
129 |         print("Error: Missing required configuration. Provide via arguments or environment variables:")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/kafka_producer.py:132:1
    |
130 |         print("  KAFKA_BOOTSTRAP, KAFKA_API_KEY, KAFKA_API_SECRET")
131 |         sys.exit(1)
132 |     
    | ^^^^
133 |     produce_orders(bootstrap_servers, api_key, api_secret, topic, args.num_orders)
    |
help: Remove whitespace from blank line

W292 [*] No newline at end of file
   --> scripts/kafka_producer.py:137:11
    |
136 | if __name__ == "__main__":
137 |     main()
    |           ^
    |
help: Add trailing newline

I001 [*] Import block is un-sorted or un-formatted
  --> scripts/maintenance/backfill_range.py:9:1
   |
 7 |   """
 8 |
 9 | / import sys
10 | | import argparse
11 | | import logging
12 | | from pathlib import Path
13 | | from datetime import datetime, timedelta
14 | | from typing import List
   | |_______________________^
15 |
16 |   # Add src to path
   |
help: Organize imports

UP035 `typing.List` is deprecated, use `list` instead
  --> scripts/maintenance/backfill_range.py:14:1
   |
12 | from pathlib import Path
13 | from datetime import datetime, timedelta
14 | from typing import List
   | ^^^^^^^^^^^^^^^^^^^^^^^
15 |
16 | # Add src to path
   |

I001 [*] Import block is un-sorted or un-formatted
  --> scripts/maintenance/backfill_range.py:19:1
   |
17 |   sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))
18 |
19 | / from project_a.utils.spark_session import build_spark
20 | | from project_a.utils.logging import setup_json_logging
21 | | from project_a.utils.path_resolver import resolve_path
22 | | import yaml
   | |___________^
23 |
24 |   logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
   |
help: Organize imports

UP015 [*] Unnecessary mode argument
  --> scripts/maintenance/backfill_range.py:30:28
   |
28 | def load_config(config_path: str) -> dict:
29 |     """Load configuration from YAML file."""
30 |     with open(config_path, 'r') as f:
   |                            ^^^
31 |         return yaml.safe_load(f)
   |
help: Remove mode argument

UP006 [*] Use `list` instead of `List` for type annotation
  --> scripts/maintenance/backfill_range.py:34:55
   |
34 | def parse_date_range(start_str: str, end_str: str) -> List[str]:
   |                                                       ^^^^
35 |     """
36 |     Parse date range into list of dates.
   |
help: Replace with `list`

W293 Blank line contains whitespace
  --> scripts/maintenance/backfill_range.py:37:1
   |
35 |     """
36 |     Parse date range into list of dates.
37 |     
   | ^^^^
38 |     Args:
39 |         start_str: Start date YYYY-MM-DD
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> scripts/maintenance/backfill_range.py:41:1
   |
39 |         start_str: Start date YYYY-MM-DD
40 |         end_str: End date YYYY-MM-DD
41 |         
   | ^^^^^^^^
42 |     Returns:
43 |         List of date strings
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/maintenance/backfill_range.py:47:1
   |
45 |     start_date = datetime.strptime(start_str, "%Y-%m-%d")
46 |     end_date = datetime.strptime(end_str, "%Y-%m-%d")
47 |     
   | ^^^^
48 |     dates = []
49 |     current = start_date
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/maintenance/backfill_range.py:53:1
   |
51 |         dates.append(current.strftime("%Y-%m-%d"))
52 |         current += timedelta(days=1)
53 |     
   | ^^^^
54 |     return dates
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> scripts/maintenance/backfill_range.py:67:1
   |
65 |     """
66 |     Backfill a table for a date range.
67 |     
   | ^^^^
68 |     Args:
69 |         spark: SparkSession
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> scripts/maintenance/backfill_range.py:75:1
   |
73 |         config: Configuration dict
74 |         layer: Optional layer (bronze, silver, gold)
75 |         
   | ^^^^^^^^
76 |     Returns:
77 |         Success status
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/maintenance/backfill_range.py:80:1
   |
78 |     """
79 |     logger.info(f"Starting backfill for {table} from {start_date} to {end_date}")
80 |     
   | ^^^^
81 |     # Parse date range
82 |     dates = parse_date_range(start_date, end_date)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/maintenance/backfill_range.py:84:1
   |
82 |     dates = parse_date_range(start_date, end_date)
83 |     logger.info(f"Will process {len(dates)} days")
84 |     
   | ^^^^
85 |     # Determine layer if not provided
86 |     if not layer:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/maintenance/backfill_range.py:96:1
   |
94 |         else:
95 |             layer = "bronze"  # Default
96 |     
   | ^^^^
97 |     # Process each date
98 |     success_count = 0
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/maintenance/backfill_range.py:101:1
    |
 99 |     for i, date in enumerate(dates, 1):
100 |         logger.info(f"[{i}/{len(dates)}] Processing {date} for {table}")
101 |         
    | ^^^^^^^^
102 |         try:
103 |             # Build backfill path with date partition
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/maintenance/backfill_range.py:106:1
    |
104 |             base_path = resolve_path(f"lake://{layer}", table, config=config)
105 |             partitioned_path = f"{base_path}/dt={date}"
106 |             
    | ^^^^^^^^^^^^
107 |             logger.info(f"Processing: {partitioned_path}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/maintenance/backfill_range.py:108:1
    |
107 |             logger.info(f"Processing: {partitioned_path}")
108 |             
    | ^^^^^^^^^^^^
109 |             # Check if data exists
110 |             # In real implementation, would run actual ETL job for this date
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/maintenance/backfill_range.py:114:1
    |
112 |             logger.info(f"âœ… Processed {date}")
113 |             success_count += 1
114 |             
    | ^^^^^^^^^^^^
115 |         except Exception as e:
116 |             logger.error(f"âŒ Failed to process {date}: {e}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/maintenance/backfill_range.py:118:1
    |
116 |             logger.error(f"âŒ Failed to process {date}: {e}")
117 |             return False
118 |     
    | ^^^^
119 |     logger.info(f"âœ… Backfill complete: {success_count}/{len(dates)} days successful")
120 |     return True
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> scripts/maintenance/backfill_range.py:132:1
    |
130 |   # Backfill orders table for October 2025
131 |   python backfill_range.py --table orders --start 2025-10-01 --end 2025-10-31
132 |   
    | ^^
133 |   # Backfill bronze layer only
134 |   python backfill_range.py --table snowflake_orders --layer bronze --start 2025-10-01 --end 2025-10-31
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/maintenance/backfill_range.py:137:1
    |
135 |         """
136 |     )
137 |     
    | ^^^^
138 |     parser.add_argument("--table", required=True, help="Table name to backfill")
139 |     parser.add_argument("--start", required=True, help="Start date YYYY-MM-DD")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/maintenance/backfill_range.py:144:1
    |
142 |     parser.add_argument("--config", default="config/local.yaml", help="Config file path")
143 |     parser.add_argument("--confirm", action="store_true", help="Confirm backfill (required)")
144 |     
    | ^^^^
145 |     args = parser.parse_args()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/maintenance/backfill_range.py:146:1
    |
145 |     args = parser.parse_args()
146 |     
    | ^^^^
147 |     # Safety check
148 |     if not args.confirm:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/maintenance/backfill_range.py:152:1
    |
150 |         logger.error("This prevents accidental reprocessing")
151 |         return 1
152 |     
    | ^^^^
153 |     # Load configuration
154 |     try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/maintenance/backfill_range.py:159:1
    |
157 |         logger.error(f"âŒ Failed to load config: {e}")
158 |         return 1
159 |     
    | ^^^^
160 |     # Setup logging
161 |     setup_json_logging(level="INFO", include_trace_id=True)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/maintenance/backfill_range.py:162:1
    |
160 |     # Setup logging
161 |     setup_json_logging(level="INFO", include_trace_id=True)
162 |     
    | ^^^^
163 |     # Build Spark session
164 |     spark = build_spark(config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/maintenance/backfill_range.py:165:1
    |
163 |     # Build Spark session
164 |     spark = build_spark(config)
165 |     
    | ^^^^
166 |     try:
167 |         # Run backfill
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/maintenance/backfill_range.py:176:1
    |
174 |             layer=args.layer
175 |         )
176 |         
    | ^^^^^^^^
177 |         if success:
178 |             logger.info("ðŸŽ‰ Backfill completed successfully")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/maintenance/backfill_range.py:183:1
    |
181 |             logger.error("âŒ Backfill failed")
182 |             return 1
183 |             
    | ^^^^^^^^^^^^
184 |     finally:
185 |         spark.stop()
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> scripts/maintenance/optimize_tables.py:11:1
   |
 9 |   """
10 |
11 | / import sys
12 | | import argparse
13 | | import logging
14 | | from pathlib import Path
15 | | from typing import List
   | |_______________________^
16 |
17 |   sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))
   |
help: Organize imports

UP035 `typing.List` is deprecated, use `list` instead
  --> scripts/maintenance/optimize_tables.py:15:1
   |
13 | import logging
14 | from pathlib import Path
15 | from typing import List
   | ^^^^^^^^^^^^^^^^^^^^^^^
16 |
17 | sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))
   |

I001 [*] Import block is un-sorted or un-formatted
  --> scripts/maintenance/optimize_tables.py:19:1
   |
17 |   sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))
18 |
19 | / from project_a.utils.spark_session import build_spark
20 | | from project_a.utils.path_resolver import resolve_path
21 | | from project_a.config_loader import load_config_resolved
   | |________________________________________________________^
22 |
23 |   logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
   |
help: Organize imports

UP006 [*] Use `list` instead of `List` for type annotation
  --> scripts/maintenance/optimize_tables.py:30:16
   |
28 |     spark,
29 |     table_path: str,
30 |     zorder_by: List[str] = None,
   |                ^^^^
31 |     retain_hours: int = 168
32 | ) -> None:
   |
help: Replace with `list`

W293 Blank line contains whitespace
  --> scripts/maintenance/optimize_tables.py:35:1
   |
33 |     """
34 |     Optimize Delta table with OPTIMIZE, ZORDER, and VACUUM.
35 |     
   | ^^^^
36 |     Args:
37 |         spark: SparkSession
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/maintenance/optimize_tables.py:43:1
   |
41 |     """
42 |     logger.info(f"ðŸ”„ Optimizing table: {table_path}")
43 |     
   | ^^^^
44 |     # Register as temporary table for SQL
45 |     table_name = table_path.split("/")[-1]
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/maintenance/optimize_tables.py:50:1
   |
48 |         SELECT * FROM delta.`{table_path}`
49 |     """)
50 |     
   | ^^^^
51 |     # 1. OPTIMIZE (small file compaction)
52 |     logger.info("ðŸ“¦ Running OPTIMIZE (compacting small files)...")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/maintenance/optimize_tables.py:53:1
   |
51 |     # 1. OPTIMIZE (small file compaction)
52 |     logger.info("ðŸ“¦ Running OPTIMIZE (compacting small files)...")
53 |     
   | ^^^^
54 |     if zorder_by:
55 |         zorder_cols = ", ".join(zorder_by)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/maintenance/optimize_tables.py:60:1
   |
58 |     else:
59 |         optimize_sql = f"OPTIMIZE delta.`{table_path}`"
60 |     
   | ^^^^
61 |     spark.sql(optimize_sql)
62 |     logger.info("âœ… OPTIMIZE complete")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/maintenance/optimize_tables.py:63:1
   |
61 |     spark.sql(optimize_sql)
62 |     logger.info("âœ… OPTIMIZE complete")
63 |     
   | ^^^^
64 |     # 2. VACUUM (remove old files)
65 |     logger.info(f"ðŸ§¹ Running VACUUM (retaining {retain_hours} hours)...")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/maintenance/optimize_tables.py:66:1
   |
64 |     # 2. VACUUM (remove old files)
65 |     logger.info(f"ðŸ§¹ Running VACUUM (retaining {retain_hours} hours)...")
66 |     
   | ^^^^
67 |     vacuum_sql = f"VACUUM delta.`{table_path}` RETAIN {retain_hours} HOURS"
68 |     spark.sql(vacuum_sql)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/maintenance/optimize_tables.py:70:1
   |
68 |     spark.sql(vacuum_sql)
69 |     logger.info("âœ… VACUUM complete")
70 |     
   | ^^^^
71 |     logger.info(f"âœ… Table optimization complete: {table_path}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/maintenance/optimize_tables.py:82:1
   |
80 |     parser.add_argument("--retain-hours", type=int, default=168, help="Hours to retain (default 168 = 7 days)")
81 |     parser.add_argument("--config", default="config/local.yaml", help="Config file")
82 |     
   | ^^^^
83 |     args = parser.parse_args()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/maintenance/optimize_tables.py:84:1
   |
83 |     args = parser.parse_args()
84 |     
   | ^^^^
85 |     # Load config
86 |     config = load_config_resolved(args.config)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/maintenance/optimize_tables.py:88:1
   |
86 |     config = load_config_resolved(args.config)
87 |     spark = build_spark(config)
88 |     
   | ^^^^
89 |     try:
90 |         # Resolve table path
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/maintenance/optimize_tables.py:96:1
   |
94 |             # Assume full path
95 |             table_path = args.table
96 |         
   | ^^^^^^^^
97 |         # Optimize
98 |         optimize_table(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/maintenance/optimize_tables.py:104:1
    |
102 |             retain_hours=args.retain_hours
103 |         )
104 |         
    | ^^^^^^^^
105 |         logger.info("ðŸŽ‰ Optimization complete")
106 |         return 0
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/maintenance/optimize_tables.py:107:1
    |
105 |         logger.info("ðŸŽ‰ Optimization complete")
106 |         return 0
107 |         
    | ^^^^^^^^
108 |     except Exception as e:
109 |         logger.error(f"âŒ Optimization failed: {e}", exc_info=True)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/maintenance/optimize_tables.py:111:1
    |
109 |         logger.error(f"âŒ Optimization failed: {e}", exc_info=True)
110 |         return 1
111 |         
    | ^^^^^^^^
112 |     finally:
113 |         spark.stop()
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> scripts/organize_data_directory.py:23:1
   |
21 |         stream_kafka_events_100000.csv
22 |   """
23 | / import shutil
24 | | from pathlib import Path
25 | | import logging
   | |______________^
26 |
27 |   logging.basicConfig(level=logging.INFO)
   |
help: Organize imports

W293 [*] Blank line contains whitespace
  --> scripts/organize_data_directory.py:39:1
   |
37 |     """Organize data/ to match aws/data/ structure."""
38 |     logger.info("ðŸ”§ Organizing data/ directory to match aws/data/ structure...")
39 |     
   | ^^^^
40 |     # Create samples directory structure
41 |     samples_dirs = {
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/organize_data_directory.py:48:1
   |
46 |         "kafka": SAMPLES_DIR / "kafka"
47 |     }
48 |     
   | ^^^^
49 |     for dir_name, dir_path in samples_dirs.items():
50 |         dir_path.mkdir(parents=True, exist_ok=True)
   |
help: Remove whitespace from blank line

B007 Loop control variable `dir_name` not used within loop body
  --> scripts/organize_data_directory.py:49:9
   |
47 |     }
48 |     
49 |     for dir_name, dir_path in samples_dirs.items():
   |         ^^^^^^^^
50 |         dir_path.mkdir(parents=True, exist_ok=True)
51 |         logger.info(f"âœ… Created directory: {dir_path}")
   |
help: Rename unused `dir_name` to `_dir_name`

W293 [*] Blank line contains whitespace
  --> scripts/organize_data_directory.py:52:1
   |
50 |         dir_path.mkdir(parents=True, exist_ok=True)
51 |         logger.info(f"âœ… Created directory: {dir_path}")
52 |     
   | ^^^^
53 |     # Map files from current data/ structure to new structure
54 |     file_mappings = []
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/organize_data_directory.py:55:1
   |
53 |     # Map files from current data/ structure to new structure
54 |     file_mappings = []
55 |     
   | ^^^^
56 |     # Check for CRM files
57 |     crm_sources = [
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/organize_data_directory.py:62:1
   |
60 |         DATA_DIR / "crm" / "opportunities.csv",
61 |     ]
62 |     
   | ^^^^
63 |     for src in crm_sources:
64 |         if src.exists():
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/organize_data_directory.py:67:1
   |
65 |             dst = SAMPLES_DIR / "crm" / src.name
66 |             file_mappings.append((src, dst))
67 |     
   | ^^^^
68 |     # Check for Snowflake files
69 |     snowflake_sources = [
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/organize_data_directory.py:74:1
   |
72 |         DATA_DIR / "snowflake" / "snowflake_products_10000.csv",
73 |     ]
74 |     
   | ^^^^
75 |     for src in snowflake_sources:
76 |         if src.exists():
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/organize_data_directory.py:79:1
   |
77 |             dst = SAMPLES_DIR / "snowflake" / src.name
78 |             file_mappings.append((src, dst))
79 |     
   | ^^^^
80 |     # Check for Redshift files
81 |     redshift_sources = [
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/organize_data_directory.py:84:1
   |
82 |         DATA_DIR / "redshift" / "redshift_customer_behavior_50000.csv",
83 |     ]
84 |     
   | ^^^^
85 |     for src in redshift_sources:
86 |         if src.exists():
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/organize_data_directory.py:89:1
   |
87 |             dst = SAMPLES_DIR / "redshift" / src.name
88 |             file_mappings.append((src, dst))
89 |     
   | ^^^^
90 |     # Check for FX files
91 |     fx_sources = [
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/organize_data_directory.py:95:1
   |
93 |         DATA_DIR / "fx" / "fx_rates_historical.csv",
94 |     ]
95 |     
   | ^^^^
96 |     for src in fx_sources:
97 |         if src.exists():
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/organize_data_directory.py:100:1
    |
 98 |             dst = SAMPLES_DIR / "fx" / src.name
 99 |             file_mappings.append((src, dst))
100 |     
    | ^^^^
101 |     # Check for Kafka files
102 |     kafka_sources = [
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/organize_data_directory.py:105:1
    |
103 |         DATA_DIR / "kafka" / "stream_kafka_events_100000.csv",
104 |     ]
105 |     
    | ^^^^
106 |     for src in kafka_sources:
107 |         if src.exists():
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/organize_data_directory.py:110:1
    |
108 |             dst = SAMPLES_DIR / "kafka" / src.name
109 |             file_mappings.append((src, dst))
110 |     
    | ^^^^
111 |     # Also check root data/ directory for files
112 |     for file in DATA_DIR.glob("*.csv"):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/organize_data_directory.py:129:1
    |
127 |             dst = SAMPLES_DIR / "kafka" / file.name
128 |             file_mappings.append((file, dst))
129 |     
    | ^^^^
130 |     for file in DATA_DIR.glob("*.json"):
131 |         if "fx" in file.name.lower() or "rate" in file.name.lower():
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/organize_data_directory.py:134:1
    |
132 |             dst = SAMPLES_DIR / "fx" / file.name
133 |             file_mappings.append((file, dst))
134 |     
    | ^^^^
135 |     # Move files
136 |     moved_count = 0
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/organize_data_directory.py:147:1
    |
145 |         elif dst.exists():
146 |             logger.info(f"â­ï¸  Skipped (already exists): {dst.name}")
147 |     
    | ^^^^
148 |     logger.info(f"âœ… Moved {moved_count} files")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/organize_data_directory.py:149:1
    |
148 |     logger.info(f"âœ… Moved {moved_count} files")
149 |     
    | ^^^^
150 |     # Clean up empty directories (but keep bronze, silver, gold for ETL output)
151 |     keep_dirs = {"bronze", "silver", "gold", "checkpoints", "_dq_results", "_checkpoints", "samples", "metrics"}
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/organize_data_directory.py:152:1
    |
150 |     # Clean up empty directories (but keep bronze, silver, gold for ETL output)
151 |     keep_dirs = {"bronze", "silver", "gold", "checkpoints", "_dq_results", "_checkpoints", "samples", "metrics"}
152 |     
    | ^^^^
153 |     for item in DATA_DIR.iterdir():
154 |         if item.is_dir() and item.name not in keep_dirs:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/organize_data_directory.py:164:1
    |
162 |             except Exception as e:
163 |                 logger.warning(f"Could not remove {item}: {e}")
164 |     
    | ^^^^
165 |     logger.info("âœ… Data directory organization complete!")
166 |     logger.info(f"ðŸ“ New structure: {SAMPLES_DIR.relative_to(PROJECT_ROOT)}/")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> scripts/organize_data_directory.py:167:1
    |
165 |     logger.info("âœ… Data directory organization complete!")
166 |     logger.info(f"ðŸ“ New structure: {SAMPLES_DIR.relative_to(PROJECT_ROOT)}/")
167 |     
    | ^^^^
168 |     # Show final structure
169 |     logger.info("\nðŸ“‹ Final structure:")
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> scripts/upload_jobs_to_s3.py:6:1
   |
 4 |   Run this before deploying jobs to EMR Serverless
 5 |   """
 6 | / import boto3
 7 | | import sys
 8 | | from pathlib import Path
   | |________________________^
 9 |
10 |   def upload_jobs_to_s3(artifacts_bucket: str, profile: str = "kunal21", region: str = "us-east-1"):
   |
help: Organize imports

W293 [*] Blank line contains whitespace
  --> scripts/upload_jobs_to_s3.py:13:1
   |
11 |     """Upload all job files to S3 artifacts bucket."""
12 |     s3 = boto3.Session(profile_name=profile, region_name=region).client('s3')
13 |     
   | ^^^^
14 |     repo_root = Path(__file__).parent.parent
15 |     jobs_to_upload = [
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/upload_jobs_to_s3.py:25:1
   |
23 |         ("tests/dev_secret_probe.py", "jobs/dev_secret_probe.py"),
24 |     ]
25 |     
   | ^^^^
26 |     # Also upload src directory (needed for imports)
27 |     src_dir = repo_root / "src"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/upload_jobs_to_s3.py:28:1
   |
26 |     # Also upload src directory (needed for imports)
27 |     src_dir = repo_root / "src"
28 |     
   | ^^^^
29 |     print(f"ðŸ“¦ Uploading jobs to s3://{artifacts_bucket}/")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/upload_jobs_to_s3.py:30:1
   |
29 |     print(f"ðŸ“¦ Uploading jobs to s3://{artifacts_bucket}/")
30 |     
   | ^^^^
31 |     for local_path, s3_key in jobs_to_upload:
32 |         local_file = repo_root / local_path
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/upload_jobs_to_s3.py:41:1
   |
39 |         else:
40 |             print(f"  âš ï¸  File not found: {local_path}")
41 |     
   | ^^^^
42 |     # Upload src directory recursively
43 |     print(f"\nðŸ“¦ Uploading src/ directory...")
   |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
  --> scripts/upload_jobs_to_s3.py:43:11
   |
42 |     # Upload src directory recursively
43 |     print(f"\nðŸ“¦ Uploading src/ directory...")
   |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
44 |     for py_file in src_dir.rglob("*.py"):
45 |         relative_path = py_file.relative_to(repo_root)
   |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
  --> scripts/upload_jobs_to_s3.py:52:1
   |
50 |         except Exception as e:
51 |             print(f"  âŒ Failed to upload {s3_key}: {e}")
52 |     
   | ^^^^
53 |     # Upload config files
54 |     config_dir = repo_root / "config"
   |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
  --> scripts/upload_jobs_to_s3.py:55:11
   |
53 |     # Upload config files
54 |     config_dir = repo_root / "config"
55 |     print(f"\nðŸ“¦ Uploading config/ directory...")
   |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
56 |     for config_file in config_dir.glob("*.yaml"):
57 |         s3_key = f"config/{config_file.name}"
   |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
  --> scripts/upload_jobs_to_s3.py:63:1
   |
61 |         except Exception as e:
62 |             print(f"  âŒ Failed to upload {s3_key}: {e}")
63 |     
   | ^^^^
64 |     print(f"\nâœ… Upload complete!")
   |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
  --> scripts/upload_jobs_to_s3.py:64:11
   |
62 |             print(f"  âŒ Failed to upload {s3_key}: {e}")
63 |     
64 |     print(f"\nâœ… Upload complete!")
   |           ^^^^^^^^^^^^^^^^^^^^^^^^
   |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
  --> scripts/upload_jobs_to_s3.py:72:1
   |
70 |         print("Example: python upload_jobs_to_s3.py my-etl-artifacts-demo-424570854632 kunal21")
71 |         sys.exit(1)
72 |     
   | ^^^^
73 |     artifacts_bucket = sys.argv[1]
74 |     profile = sys.argv[2] if len(sys.argv) > 2 else "kunal21"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> scripts/upload_jobs_to_s3.py:75:1
   |
73 |     artifacts_bucket = sys.argv[1]
74 |     profile = sys.argv[2] if len(sys.argv) > 2 else "kunal21"
75 |     
   | ^^^^
76 |     upload_jobs_to_s3(artifacts_bucket, profile)
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/dq/ge_checkpoint.py:6:1
   |
 4 |   """
 5 |
 6 | / import json
 7 | | import logging
 8 | | import sys
 9 | | from pathlib import Path
10 | | from typing import Dict, Any, List
11 | |
12 | | import boto3
13 | | from great_expectations import DataContext
14 | | from great_expectations.checkpoint import SimpleCheckpoint
15 | | from great_expectations.core.batch import RuntimeBatchRequest
   | |_____________________________________________________________^
16 |
17 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/dq/ge_checkpoint.py:10:1
   |
 8 | import sys
 9 | from pathlib import Path
10 | from typing import Dict, Any, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
11 |
12 | import boto3
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/dq/ge_checkpoint.py:10:1
   |
 8 | import sys
 9 | from pathlib import Path
10 | from typing import Dict, Any, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
11 |
12 | import boto3
   |

F401 [*] `great_expectations.checkpoint.SimpleCheckpoint` imported but unused
  --> src/dq/ge_checkpoint.py:14:43
   |
12 | import boto3
13 | from great_expectations import DataContext
14 | from great_expectations.checkpoint import SimpleCheckpoint
   |                                           ^^^^^^^^^^^^^^^^
15 | from great_expectations.core.batch import RuntimeBatchRequest
   |
help: Remove unused import: `great_expectations.checkpoint.SimpleCheckpoint`

W293 [*] Blank line contains whitespace
  --> src/dq/ge_checkpoint.py:22:1
   |
20 | class GECheckpointRunner:
21 |     """Great Expectations checkpoint runner for data quality validation."""
22 |     
   | ^^^^
23 |     def __init__(self, config: Dict[str, Any]):
24 |         """
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/dq/ge_checkpoint.py:23:32
   |
21 |     """Great Expectations checkpoint runner for data quality validation."""
22 |     
23 |     def __init__(self, config: Dict[str, Any]):
   |                                ^^^^
24 |         """
25 |         Initialize GE checkpoint runner.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/dq/ge_checkpoint.py:26:1
   |
24 |         """
25 |         Initialize GE checkpoint runner.
26 |         
   | ^^^^^^^^
27 |         Args:
28 |             config: Configuration dictionary with GE settings
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/dq/ge_checkpoint.py:33:1
   |
31 |         self.dq_config = config.get("dq", {})
32 |         self.s3_client = boto3.client("s3")
33 |         
   | ^^^^^^^^
34 |         # Initialize GE context
35 |         self.context = self._init_ge_context()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/dq/ge_checkpoint.py:36:1
   |
34 |         # Initialize GE context
35 |         self.context = self._init_ge_context()
36 |         
   | ^^^^^^^^
37 |     def _init_ge_context(self) -> DataContext:
38 |         """Initialize Great Expectations context."""
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/dq/ge_checkpoint.py:42:1
   |
40 |         if not ge_config_path.exists():
41 |             raise FileNotFoundError(f"GE config not found: {ge_config_path}")
42 |         
   | ^^^^^^^^
43 |         return DataContext(context_root_dir=str(ge_config_path.parent))
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/dq/ge_checkpoint.py:44:1
   |
43 |         return DataContext(context_root_dir=str(ge_config_path.parent))
44 |     
   | ^^^^
45 |     def run_checkpoint(self, table_name: str, layer: str, data_path: str) -> Dict[str, Any]:
46 |         """
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/dq/ge_checkpoint.py:45:78
   |
43 |         return DataContext(context_root_dir=str(ge_config_path.parent))
44 |     
45 |     def run_checkpoint(self, table_name: str, layer: str, data_path: str) -> Dict[str, Any]:
   |                                                                              ^^^^
46 |         """
47 |         Run GE checkpoint on specified table.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/dq/ge_checkpoint.py:48:1
   |
46 |         """
47 |         Run GE checkpoint on specified table.
48 |         
   | ^^^^^^^^
49 |         Args:
50 |             table_name: Name of the table to validate
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/dq/ge_checkpoint.py:53:1
   |
51 |             layer: Data layer (bronze/silver/gold)
52 |             data_path: S3 path to the data
53 |             
   | ^^^^^^^^^^^^
54 |         Returns:
55 |             Checkpoint result dictionary
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/dq/ge_checkpoint.py:58:1
   |
56 |         """
57 |         logger.info(f"Running GE checkpoint for {layer}.{table_name}")
58 |         
   | ^^^^^^^^
59 |         # Create runtime batch request
60 |         batch_request = RuntimeBatchRequest(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/dq/ge_checkpoint.py:67:1
   |
65 |             batch_identifiers={"default_identifier_name": "default_identifier"},
66 |         )
67 |         
   | ^^^^^^^^
68 |         # Run checkpoint
69 |         checkpoint_name = f"{layer}_{table_name}_checkpoint"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/dq/ge_checkpoint.py:74:1
   |
72 |             validations=[{"batch_request": batch_request}]
73 |         )
74 |         
   | ^^^^^^^^
75 |         # Analyze results
76 |         result = self._analyze_checkpoint_result(checkpoint_result, table_name, layer)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/dq/ge_checkpoint.py:77:1
   |
75 |         # Analyze results
76 |         result = self._analyze_checkpoint_result(checkpoint_result, table_name, layer)
77 |         
   | ^^^^^^^^
78 |         return result
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/dq/ge_checkpoint.py:79:1
   |
78 |         return result
79 |     
   | ^^^^
80 |     def _analyze_checkpoint_result(self, checkpoint_result: Any, table_name: str, layer: str) -> Dict[str, Any]:
81 |         """Analyze checkpoint result and determine if critical failures occurred."""
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/dq/ge_checkpoint.py:80:98
   |
78 |         return result
79 |     
80 |     def _analyze_checkpoint_result(self, checkpoint_result: Any, table_name: str, layer: str) -> Dict[str, Any]:
   |                                                                                                  ^^^^
81 |         """Analyze checkpoint result and determine if critical failures occurred."""
82 |         result = {
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/dq/ge_checkpoint.py:90:1
   |
88 |             "summary": {}
89 |         }
90 |         
   | ^^^^^^^^
91 |         # Extract validation results
92 |         validation_results = checkpoint_result.list_validation_results()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/dq/ge_checkpoint.py:93:1
   |
91 |         # Extract validation results
92 |         validation_results = checkpoint_result.list_validation_results()
93 |         
   | ^^^^^^^^
94 |         for validation_result in validation_results:
95 |             validation_id = validation_result.meta.get("run_id", "unknown")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/dq/ge_checkpoint.py:97:1
   |
95 |             validation_id = validation_result.meta.get("run_id", "unknown")
96 |             statistics = validation_result.statistics
97 |             
   | ^^^^^^^^^^^^
98 |             result["summary"][validation_id] = {
99 |                 "evaluated_expectations": statistics.get("evaluated_expectations", 0),
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/dq/ge_checkpoint.py:104:1
    |
102 |                 "success_percent": statistics.get("success_percent", 0)
103 |             }
104 |             
    | ^^^^^^^^^^^^
105 |             # Check for critical failures
106 |             for expectation_result in validation_result.results:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/dq/ge_checkpoint.py:110:1
    |
108 |                     expectation_type = expectation_result.expectation_config.get("expectation_type", "unknown")
109 |                     column = expectation_result.expectation_config.get("kwargs", {}).get("column", "unknown")
110 |                     
    | ^^^^^^^^^^^^^^^^^^^^
111 |                     # Determine severity based on expectation type
112 |                     severity = self._get_expectation_severity(expectation_type)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/dq/ge_checkpoint.py:113:1
    |
111 |                     # Determine severity based on expectation type
112 |                     severity = self._get_expectation_severity(expectation_type)
113 |                     
    | ^^^^^^^^^^^^^^^^^^^^
114 |                     failure_info = {
115 |                         "expectation_type": expectation_type,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/dq/ge_checkpoint.py:120:1
    |
118 |                         "details": expectation_result.result
119 |                     }
120 |                     
    | ^^^^^^^^^^^^^^^^^^^^
121 |                     if severity == "critical":
122 |                         result["critical_failures"].append(failure_info)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/dq/ge_checkpoint.py:125:1
    |
123 |                     else:
124 |                         result["warnings"].append(failure_info)
125 |         
    | ^^^^^^^^
126 |         # Determine overall success
127 |         result["success"] = len(result["critical_failures"]) == 0
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/dq/ge_checkpoint.py:128:1
    |
126 |         # Determine overall success
127 |         result["success"] = len(result["critical_failures"]) == 0
128 |         
    | ^^^^^^^^
129 |         return result
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/dq/ge_checkpoint.py:130:1
    |
129 |         return result
130 |     
    | ^^^^
131 |     def _get_expectation_severity(self, expectation_type: str) -> str:
132 |         """Determine severity level for expectation type."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/dq/ge_checkpoint.py:139:1
    |
137 |             "expect_table_row_count_to_be_between"
138 |         ]
139 |         
    | ^^^^^^^^
140 |         if expectation_type in critical_expectations:
141 |             return "critical"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/dq/ge_checkpoint.py:144:1
    |
142 |         else:
143 |             return "warning"
144 |     
    | ^^^^
145 |     def run_silver_quality_checks(self, silver_tables: List[str]) -> Dict[str, Any]:
146 |         """Run quality checks on all silver tables."""
    |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/dq/ge_checkpoint.py:145:56
    |
143 |             return "warning"
144 |     
145 |     def run_silver_quality_checks(self, silver_tables: List[str]) -> Dict[str, Any]:
    |                                                        ^^^^
146 |         """Run quality checks on all silver tables."""
147 |         results = {}
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/dq/ge_checkpoint.py:145:70
    |
143 |             return "warning"
144 |     
145 |     def run_silver_quality_checks(self, silver_tables: List[str]) -> Dict[str, Any]:
    |                                                                      ^^^^
146 |         """Run quality checks on all silver tables."""
147 |         results = {}
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/dq/ge_checkpoint.py:149:1
    |
147 |         results = {}
148 |         overall_success = True
149 |         
    | ^^^^^^^^
150 |         for table in silver_tables:
151 |             data_path = f"{self.config['s3']['silver_path']}/{table}"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/dq/ge_checkpoint.py:154:1
    |
152 |             result = self.run_checkpoint(table, "silver", data_path)
153 |             results[table] = result
154 |             
    | ^^^^^^^^^^^^
155 |             if not result["success"]:
156 |                 overall_success = False
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/dq/ge_checkpoint.py:158:1
    |
156 |                 overall_success = False
157 |                 logger.error(f"Critical failures in silver.{table}: {result['critical_failures']}")
158 |         
    | ^^^^^^^^
159 |         return {
160 |             "layer": "silver",
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/dq/ge_checkpoint.py:164:1
    |
162 |             "table_results": results
163 |         }
164 |     
    | ^^^^
165 |     def run_gold_quality_checks(self, gold_tables: List[str]) -> Dict[str, Any]:
166 |         """Run quality checks on all gold tables."""
    |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/dq/ge_checkpoint.py:165:52
    |
163 |         }
164 |     
165 |     def run_gold_quality_checks(self, gold_tables: List[str]) -> Dict[str, Any]:
    |                                                    ^^^^
166 |         """Run quality checks on all gold tables."""
167 |         results = {}
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/dq/ge_checkpoint.py:165:66
    |
163 |         }
164 |     
165 |     def run_gold_quality_checks(self, gold_tables: List[str]) -> Dict[str, Any]:
    |                                                                  ^^^^
166 |         """Run quality checks on all gold tables."""
167 |         results = {}
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/dq/ge_checkpoint.py:169:1
    |
167 |         results = {}
168 |         overall_success = True
169 |         
    | ^^^^^^^^
170 |         for table in gold_tables:
171 |             data_path = f"{self.config['s3']['gold_path']}/{table}"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/dq/ge_checkpoint.py:174:1
    |
172 |             result = self.run_checkpoint(table, "gold", data_path)
173 |             results[table] = result
174 |             
    | ^^^^^^^^^^^^
175 |             if not result["success"]:
176 |                 overall_success = False
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/dq/ge_checkpoint.py:178:1
    |
176 |                 overall_success = False
177 |                 logger.error(f"Critical failures in gold.{table}: {result['critical_failures']}")
178 |         
    | ^^^^^^^^
179 |         return {
180 |             "layer": "gold",
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/dq/ge_checkpoint.py:184:1
    |
182 |             "table_results": results
183 |         }
184 |     
    | ^^^^
185 |     def publish_results(self, results: Dict[str, Any]) -> str:
186 |         """Publish GE results to S3 and return Data Docs URL."""
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/dq/ge_checkpoint.py:185:40
    |
183 |         }
184 |     
185 |     def publish_results(self, results: Dict[str, Any]) -> str:
    |                                        ^^^^
186 |         """Publish GE results to S3 and return Data Docs URL."""
187 |         # Generate Data Docs
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/dq/ge_checkpoint.py:189:1
    |
187 |         # Generate Data Docs
188 |         self.context.build_data_docs()
189 |         
    | ^^^^^^^^
190 |         # Upload to S3
191 |         docs_bucket = self.dq_config.get("great_expectations_bucket", "")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/dq/ge_checkpoint.py:197:1
    |
195 |         else:
196 |             docs_url = "file://ge/uncommitted/data_docs/local_site/index.html"
197 |         
    | ^^^^^^^^
198 |         logger.info(f"Data Docs published to: {docs_url}")
199 |         return docs_url
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/dq/ge_checkpoint.py:200:1
    |
198 |         logger.info(f"Data Docs published to: {docs_url}")
199 |         return docs_url
200 |     
    | ^^^^
201 |     def _upload_data_docs_to_s3(self, bucket: str):
202 |         """Upload Data Docs to S3."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/dq/ge_checkpoint.py:204:1
    |
202 |         """Upload Data Docs to S3."""
203 |         docs_dir = Path("ge/uncommitted/data_docs/local_site")
204 |         
    | ^^^^^^^^
205 |         for file_path in docs_dir.rglob("*"):
206 |             if file_path.is_file():
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
   --> src/dq/ge_checkpoint.py:217:5
    |
215 |   def main():
216 |       """Main entry point for GE checkpoint runner."""
217 | /     import argparse
218 | |     from project_a.utils.config import load_conf
    | |________________________________________________^
219 |       
220 |       parser = argparse.ArgumentParser(description="Run Great Expectations data quality checks")
    |
help: Organize imports

W293 [*] Blank line contains whitespace
   --> src/dq/ge_checkpoint.py:219:1
    |
217 |     import argparse
218 |     from project_a.utils.config import load_conf
219 |     
    | ^^^^
220 |     parser = argparse.ArgumentParser(description="Run Great Expectations data quality checks")
221 |     parser.add_argument("--config", required=True, help="Configuration file path")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/dq/ge_checkpoint.py:224:1
    |
222 |     parser.add_argument("--layer", choices=["silver", "gold"], required=True, help="Data layer to check")
223 |     parser.add_argument("--tables", nargs="+", required=True, help="Tables to validate")
224 |     
    | ^^^^
225 |     args = parser.parse_args()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/dq/ge_checkpoint.py:226:1
    |
225 |     args = parser.parse_args()
226 |     
    | ^^^^
227 |     # Load configuration
228 |     config = load_conf(args.config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/dq/ge_checkpoint.py:229:1
    |
227 |     # Load configuration
228 |     config = load_conf(args.config)
229 |     
    | ^^^^
230 |     # Setup logging
231 |     logging.basicConfig(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/dq/ge_checkpoint.py:235:1
    |
233 |         format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
234 |     )
235 |     
    | ^^^^
236 |     # Run quality checks
237 |     runner = GECheckpointRunner(config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/dq/ge_checkpoint.py:238:1
    |
236 |     # Run quality checks
237 |     runner = GECheckpointRunner(config)
238 |     
    | ^^^^
239 |     if args.layer == "silver":
240 |         results = runner.run_silver_quality_checks(args.tables)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/dq/ge_checkpoint.py:243:1
    |
241 |     else:
242 |         results = runner.run_gold_quality_checks(args.tables)
243 |     
    | ^^^^
244 |     # Publish results
245 |     docs_url = runner.publish_results(results)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/dq/ge_checkpoint.py:246:1
    |
244 |     # Publish results
245 |     docs_url = runner.publish_results(results)
246 |     
    | ^^^^
247 |     # Print results
248 |     print(json.dumps(results, indent=2))
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/dq/ge_checkpoint.py:250:1
    |
248 |     print(json.dumps(results, indent=2))
249 |     print(f"\nData Docs URL: {docs_url}")
250 |     
    | ^^^^
251 |     # Exit with error code if critical failures
252 |     if not results["overall_success"]:
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/config_loader.py:12:1
   |
10 |   """
11 |
12 | / from __future__ import annotations
13 | |
14 | | import os
15 | | import re
16 | | from typing import Any, Optional, Dict
17 | |
18 | | import yaml
   | |___________^
19 |
20 |   # Matches ${ENV:VAR} or ${SECRET:scope:key} or ${paths.bronze_root} style variable references
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/config_loader.py:16:1
   |
14 | import os
15 | import re
16 | from typing import Any, Optional, Dict
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
17 |
18 | import yaml
   |

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/config_loader.py:28:37
   |
26 |     """Configuration loader class that provides config loading and validation."""
27 |
28 |     def __init__(self, config_path: Optional[str] = None, env: Optional[str] = None):
   |                                     ^^^^^^^^^^^^^
29 |         self.config_path = config_path
30 |         self.env = env
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/config_loader.py:28:64
   |
26 |     """Configuration loader class that provides config loading and validation."""
27 |
28 |     def __init__(self, config_path: Optional[str] = None, env: Optional[str] = None):
   |                                                                ^^^^^^^^^^^^^
29 |         self.config_path = config_path
30 |         self.env = env
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/config_loader.py:33:30
   |
31 |         self.config = None
32 |
33 |     def load_config(self) -> Dict[str, Any]:
   |                              ^^^^
34 |         """Load and resolve configuration."""
35 |         self.config = load_config_resolved(self.config_path, self.env)
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/config_loader.py:38:39
   |
36 |         return self.config
37 |
38 |     def validate_config(self, config: Dict[str, Any]) -> bool:
   |                                       ^^^^
39 |         """Validate configuration structure and required fields."""
40 |         try:
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/config_loader.py:69:29
   |
67 |             return False
68 |
69 |     def get_config(self) -> Dict[str, Any]:
   |                             ^^^^
70 |         """Get the loaded configuration."""
71 |         if self.config is None:
   |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/config_loader.py:93:49
   |
93 | def _resolve_value(value: Any, dbutils, config: Optional[Dict[str, Any]] = None) -> Any:
   |                                                 ^^^^^^^^^^^^^^^^^^^^^^^^
94 |     if not isinstance(value, str):
95 |         return value
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/config_loader.py:93:58
   |
93 | def _resolve_value(value: Any, dbutils, config: Optional[Dict[str, Any]] = None) -> Any:
   |                                                          ^^^^
94 |     if not isinstance(value, str):
95 |         return value
   |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/config_loader.py:137:49
    |
137 | def _resolve_secrets(obj: Any, dbutils, config: Optional[Dict[str, Any]] = None) -> Any:
    |                                                 ^^^^^^^^^^^^^^^^^^^^^^^^
138 |     if isinstance(obj, dict):
139 |         # Pass config to nested resolution
    |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/config_loader.py:137:58
    |
137 | def _resolve_secrets(obj: Any, dbutils, config: Optional[Dict[str, Any]] = None) -> Any:
    |                                                          ^^^^
138 |     if isinstance(obj, dict):
139 |         # Pass config to nested resolution
    |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/config_loader.py:150:39
    |
150 | def load_config_resolved(config_path: Optional[str] = None, env: Optional[str] = None) -> Dict[str, Any]:
    |                                       ^^^^^^^^^^^^^
151 |     """
152 |     Load config from a path, or pick by environment if path not provided.
    |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/config_loader.py:150:66
    |
150 | def load_config_resolved(config_path: Optional[str] = None, env: Optional[str] = None) -> Dict[str, Any]:
    |                                                                  ^^^^^^^^^^^^^
151 |     """
152 |     Load config from a path, or pick by environment if path not provided.
    |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/config_loader.py:150:91
    |
150 | def load_config_resolved(config_path: Optional[str] = None, env: Optional[str] = None) -> Dict[str, Any]:
    |                                                                                           ^^^^
151 |     """
152 |     Load config from a path, or pick by environment if path not provided.
    |
help: Replace with `dict`

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/core/__init__.py:10:1
   |
 8 |   """
 9 |
10 | / from project_a.core.config import ProjectConfig
11 | | from project_a.core.context import JobContext
12 | | from project_a.core.base_job import BaseJob
   | |___________________________________________^
13 |
14 |   __all__ = ["ProjectConfig", "JobContext", "BaseJob"]
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/core/base_job.py:14:1
   |
12 | import logging
13 | from abc import ABC, abstractmethod
14 | from typing import Any, Dict, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
15 |
16 | from project_a.core.config import ProjectConfig
   |

W293 Blank line contains whitespace
  --> src/project_a/core/base_job.py:25:1
   |
23 |     """
24 |     Abstract base class for all ETL jobs.
25 |     
   | ^^^^
26 |     Provides:
27 |     - Configuration management
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/core/base_job.py:32:1
   |
30 |     - Logging setup
31 |     """
32 |     
   | ^^^^
33 |     def __init__(self, config: ProjectConfig):
34 |         """
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/core/base_job.py:36:1
   |
34 |         """
35 |         Initialize job.
36 |         
   | ^^^^^^^^
37 |         Args:
38 |             config: ProjectConfig instance
   |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/core/base_job.py:41:19
   |
39 |         """
40 |         self.config = config
41 |         self.ctx: Optional[JobContext] = None
   |                   ^^^^^^^^^^^^^^^^^^^^
42 |     
43 |     @abstractmethod
   |
help: Convert to `X | None`

W293 [*] Blank line contains whitespace
  --> src/project_a/core/base_job.py:42:1
   |
40 |         self.config = config
41 |         self.ctx: Optional[JobContext] = None
42 |     
   | ^^^^
43 |     @abstractmethod
44 |     def run(self, ctx: JobContext) -> Dict[str, Any]:
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/core/base_job.py:44:39
   |
43 |     @abstractmethod
44 |     def run(self, ctx: JobContext) -> Dict[str, Any]:
   |                                       ^^^^
45 |         """
46 |         Execute the job logic.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/core/base_job.py:47:1
   |
45 |         """
46 |         Execute the job logic.
47 |         
   | ^^^^^^^^
48 |         Args:
49 |             ctx: JobContext with SparkSession
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/core/base_job.py:50:1
   |
48 |         Args:
49 |             ctx: JobContext with SparkSession
50 |         
   | ^^^^^^^^
51 |         Returns:
52 |             Dictionary with job results (e.g., row counts, paths)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/core/base_job.py:55:1
   |
53 |         """
54 |         pass
55 |     
   | ^^^^
56 |     def execute(self) -> Dict[str, Any]:
57 |         """
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/core/base_job.py:56:26
   |
54 |         pass
55 |     
56 |     def execute(self) -> Dict[str, Any]:
   |                          ^^^^
57 |         """
58 |         Execute the job with context management.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/core/base_job.py:59:1
   |
57 |         """
58 |         Execute the job with context management.
59 |         
   | ^^^^^^^^
60 |         Returns:
61 |             Dictionary with job results
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/core/base_job.py:64:1
   |
62 |         """
63 |         logger.info(f"Starting job: {self.__class__.__name__}")
64 |         
   | ^^^^^^^^
65 |         try:
66 |             with JobContext(self.config, app_name=self.__class__.__name__) as ctx:
   |
help: Remove whitespace from blank line

F841 [*] Local variable `e` is assigned to but never used
  --> src/project_a/core/base_job.py:71:29
   |
69 |                 logger.info(f"Job completed successfully: {self.__class__.__name__}")
70 |                 return result
71 |         except Exception as e:
   |                             ^
72 |             logger.error(f"Job failed: {self.__class__.__name__}", exc_info=True)
73 |             raise
   |
help: Remove assignment to unused variable `e`

W293 [*] Blank line contains whitespace
  --> src/project_a/core/base_job.py:74:1
   |
72 |             logger.error(f"Job failed: {self.__class__.__name__}", exc_info=True)
73 |             raise
74 |     
   | ^^^^
75 |     @property
76 |     def spark(self) -> SparkSession:
   |
help: Remove whitespace from blank line

F821 Undefined name `SparkSession`
  --> src/project_a/core/base_job.py:76:24
   |
75 |     @property
76 |     def spark(self) -> SparkSession:
   |                        ^^^^^^^^^^^^
77 |         """Get SparkSession from context."""
78 |         if self.ctx is None or self.ctx.spark is None:
   |

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/core/config.py:13:1
   |
11 | import re
12 | from pathlib import Path
13 | from typing import Any, Dict, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
14 |
15 | import boto3
   |

W293 Blank line contains whitespace
  --> src/project_a/core/config.py:23:1
   |
21 |     """
22 |     Centralized configuration management for Project A.
23 |     
   | ^^^^
24 |     Handles:
25 |     - Loading from local files or S3
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/core/config.py:30:1
   |
28 |     - Type-safe access to config sections
29 |     """
30 |     
   | ^^^^
31 |     def __init__(self, config_path: str, env: Optional[str] = None):
32 |         """
   |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/core/config.py:31:47
   |
29 |     """
30 |     
31 |     def __init__(self, config_path: str, env: Optional[str] = None):
   |                                               ^^^^^^^^^^^^^
32 |         """
33 |         Initialize configuration.
   |
help: Convert to `X | None`

W293 Blank line contains whitespace
  --> src/project_a/core/config.py:34:1
   |
32 |         """
33 |         Initialize configuration.
34 |         
   | ^^^^^^^^
35 |         Args:
36 |             config_path: Path to config file (local or s3://...)
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/core/config.py:40:27
   |
38 |         """
39 |         self.config_path = config_path
40 |         self._raw_config: Dict[str, Any] = {}
   |                           ^^^^
41 |         self._config: Dict[str, Any] = {}
42 |         self._env = env
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/core/config.py:41:23
   |
39 |         self.config_path = config_path
40 |         self._raw_config: Dict[str, Any] = {}
41 |         self._config: Dict[str, Any] = {}
   |                       ^^^^
42 |         self._env = env
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/core/config.py:43:1
   |
41 |         self._config: Dict[str, Any] = {}
42 |         self._env = env
43 |         
   | ^^^^^^^^
44 |         self._load()
45 |         self._resolve()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/core/config.py:46:1
   |
44 |         self._load()
45 |         self._resolve()
46 |     
   | ^^^^
47 |     def _load(self) -> None:
48 |         """Load config from file (local or S3)."""
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/core/config.py:54:1
   |
52 |             bucket = path_parts[0]
53 |             key = path_parts[1] if len(path_parts) > 1 else ""
54 |             
   | ^^^^^^^^^^^^
55 |             try:
56 |                 obj = s3.get_object(Bucket=bucket, Key=key)
   |
help: Remove whitespace from blank line

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
  --> src/project_a/core/config.py:60:17
   |
58 |                 self._raw_config = yaml.safe_load(content)
59 |             except ClientError as e:
60 |                 raise ValueError(f"Failed to load config from S3: {e}")
   |                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
61 |         else:
62 |             config_file = Path(self.config_path)
   |

W293 [*] Blank line contains whitespace
  --> src/project_a/core/config.py:65:1
   |
63 |             if not config_file.exists():
64 |                 raise FileNotFoundError(f"Config file not found: {self.config_path}")
65 |             
   | ^^^^^^^^^^^^
66 |             with open(config_file) as f:
67 |                 self._raw_config = yaml.safe_load(f)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/core/config.py:68:1
   |
66 |             with open(config_file) as f:
67 |                 self._raw_config = yaml.safe_load(f)
68 |     
   | ^^^^
69 |     def _resolve_value(self, value: Any) -> Any:
70 |         """Resolve secret and variable references in a value."""
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/core/config.py:73:1
   |
71 |         if not isinstance(value, str):
72 |             return value
73 |         
   | ^^^^^^^^
74 |         # Resolve environment variables: ${ENV:VAR}
75 |         def replace_env(match: re.Match) -> str:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/core/config.py:78:1
   |
76 |             var_name = match.group(1)
77 |             return os.environ.get(var_name, "")
78 |         
   | ^^^^^^^^
79 |         value = re.sub(r"\$\{ENV:([^}]+)\}", replace_env, value)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/core/config.py:80:1
   |
79 |         value = re.sub(r"\$\{ENV:([^}]+)\}", replace_env, value)
80 |         
   | ^^^^^^^^
81 |         # Resolve config variables: ${paths.bronze_root}
82 |         def replace_var(match: re.Match) -> str:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/core/config.py:97:1
   |
95 |                 pass
96 |             return match.group(0)
97 |         
   | ^^^^^^^^
98 |         return re.sub(r"\$\{([^}]+)\}", replace_var, value)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/core/config.py:99:1
    |
 98 |         return re.sub(r"\$\{([^}]+)\}", replace_var, value)
 99 |     
    | ^^^^
100 |     def _resolve(self) -> None:
101 |         """Recursively resolve all variable references."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/core/config.py:111:1
    |
109 |                 return [resolve_recursive(v) for v in obj]
110 |             return self._resolve_value(obj)
111 |         
    | ^^^^^^^^
112 |         self._config = resolve_recursive(self._raw_config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/core/config.py:113:1
    |
112 |         self._config = resolve_recursive(self._raw_config)
113 |     
    | ^^^^
114 |     @property
115 |     def environment(self) -> str:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/core/config.py:118:1
    |
116 |         """Get environment name (dev/staging/prod)."""
117 |         return self._config.get("environment") or self._config.get("env") or "dev"
118 |     
    | ^^^^
119 |     @property
120 |     def project_name(self) -> str:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/core/config.py:123:1
    |
121 |         """Get project name."""
122 |         return self._config.get("project_name", "project-a")
123 |     
    | ^^^^
124 |     @property
125 |     def paths(self) -> Dict[str, str]:
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/core/config.py:125:24
    |
124 |     @property
125 |     def paths(self) -> Dict[str, str]:
    |                        ^^^^
126 |         """Get path configurations."""
127 |         return self._config.get("paths", {})
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/core/config.py:128:1
    |
126 |         """Get path configurations."""
127 |         return self._config.get("paths", {})
128 |     
    | ^^^^
129 |     @property
130 |     def sources(self) -> Dict[str, Any]:
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/core/config.py:130:26
    |
129 |     @property
130 |     def sources(self) -> Dict[str, Any]:
    |                          ^^^^
131 |         """Get source configurations."""
132 |         return self._config.get("sources", {})
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/core/config.py:133:1
    |
131 |         """Get source configurations."""
132 |         return self._config.get("sources", {})
133 |     
    | ^^^^
134 |     @property
135 |     def tables(self) -> Dict[str, Dict[str, str]]:
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/core/config.py:135:25
    |
134 |     @property
135 |     def tables(self) -> Dict[str, Dict[str, str]]:
    |                         ^^^^
136 |         """Get table name definitions."""
137 |         return self._config.get("tables", {})
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/core/config.py:135:35
    |
134 |     @property
135 |     def tables(self) -> Dict[str, Dict[str, str]]:
    |                                   ^^^^
136 |         """Get table name definitions."""
137 |         return self._config.get("tables", {})
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/core/config.py:138:1
    |
136 |         """Get table name definitions."""
137 |         return self._config.get("tables", {})
138 |     
    | ^^^^
139 |     @property
140 |     def aws(self) -> Dict[str, Any]:
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/core/config.py:140:22
    |
139 |     @property
140 |     def aws(self) -> Dict[str, Any]:
    |                      ^^^^
141 |         """Get AWS configuration."""
142 |         return self._config.get("aws", {})
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/core/config.py:143:1
    |
141 |         """Get AWS configuration."""
142 |         return self._config.get("aws", {})
143 |     
    | ^^^^
144 |     @property
145 |     def buckets(self) -> Dict[str, str]:
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/core/config.py:145:26
    |
144 |     @property
145 |     def buckets(self) -> Dict[str, str]:
    |                          ^^^^
146 |         """Get S3 bucket names."""
147 |         return self._config.get("buckets", {})
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/core/config.py:148:1
    |
146 |         """Get S3 bucket names."""
147 |         return self._config.get("buckets", {})
148 |     
    | ^^^^
149 |     @property
150 |     def glue(self) -> Dict[str, str]:
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/core/config.py:150:23
    |
149 |     @property
150 |     def glue(self) -> Dict[str, str]:
    |                       ^^^^
151 |         """Get Glue database names."""
152 |         return self._config.get("glue", {})
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/core/config.py:153:1
    |
151 |         """Get Glue database names."""
152 |         return self._config.get("glue", {})
153 |     
    | ^^^^
154 |     @property
155 |     def emr(self) -> Dict[str, Any]:
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/core/config.py:155:22
    |
154 |     @property
155 |     def emr(self) -> Dict[str, Any]:
    |                      ^^^^
156 |         """Get EMR configuration."""
157 |         return self._config.get("emr", {})
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/core/config.py:158:1
    |
156 |         """Get EMR configuration."""
157 |         return self._config.get("emr", {})
158 |     
    | ^^^^
159 |     @property
160 |     def kafka(self) -> Dict[str, Any]:
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/core/config.py:160:24
    |
159 |     @property
160 |     def kafka(self) -> Dict[str, Any]:
    |                        ^^^^
161 |         """Get Kafka configuration."""
162 |         return self._config.get("sources", {}).get("kafka", {})
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/core/config.py:163:1
    |
161 |         """Get Kafka configuration."""
162 |         return self._config.get("sources", {}).get("kafka", {})
163 |     
    | ^^^^
164 |     def get(self, key: str, default: Any = None) -> Any:
165 |         """Get a config value by key (supports dot notation)."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/core/config.py:176:1
    |
174 |                 return default
175 |         return result
176 |     
    | ^^^^
177 |     def is_local(self) -> bool:
178 |         """Check if running in local environment."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/core/config.py:181:1
    |
179 |         env = self.environment.lower()
180 |         return env in ("local", "dev_local")
181 |     
    | ^^^^
182 |     def is_aws(self) -> bool:
183 |         """Check if running on AWS (EMR)."""
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/core/context.py:11:1
   |
 9 |   """
10 |
11 | / import logging
12 | | import os
13 | | from typing import Dict, Optional
14 | |
15 | | from pyspark.sql import SparkSession
16 | |
17 | | from project_a.core.config import ProjectConfig
18 | | from project_a.utils.path_resolver import resolve_data_path, is_local_environment
   | |_________________________________________________________________________________^
19 |
20 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

F401 [*] `os` imported but unused
  --> src/project_a/core/context.py:12:8
   |
11 | import logging
12 | import os
   |        ^^
13 | from typing import Dict, Optional
   |
help: Remove unused import: `os`

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/core/context.py:13:1
   |
11 | import logging
12 | import os
13 | from typing import Dict, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
14 |
15 | from pyspark.sql import SparkSession
   |

F401 [*] `typing.Dict` imported but unused
  --> src/project_a/core/context.py:13:20
   |
11 | import logging
12 | import os
13 | from typing import Dict, Optional
   |                    ^^^^
14 |
15 | from pyspark.sql import SparkSession
   |
help: Remove unused import: `typing.Dict`

F401 [*] `project_a.utils.path_resolver.is_local_environment` imported but unused
  --> src/project_a/core/context.py:18:62
   |
17 | from project_a.core.config import ProjectConfig
18 | from project_a.utils.path_resolver import resolve_data_path, is_local_environment
   |                                                              ^^^^^^^^^^^^^^^^^^^^
19 |
20 | logger = logging.getLogger(__name__)
   |
help: Remove unused import: `project_a.utils.path_resolver.is_local_environment`

W293 Blank line contains whitespace
  --> src/project_a/core/context.py:26:1
   |
24 |     """
25 |     Context manager for ETL jobs.
26 |     
   | ^^^^
27 |     Handles:
28 |     - SparkSession creation and configuration
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/core/context.py:33:1
   |
31 |     - AWS credentials
32 |     """
33 |     
   | ^^^^
34 |     def __init__(self, config: ProjectConfig, app_name: str = "project_a"):
35 |         """
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/core/context.py:37:1
   |
35 |         """
36 |         Initialize job context.
37 |         
   | ^^^^^^^^
38 |         Args:
39 |             config: ProjectConfig instance
   |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/core/context.py:44:21
   |
42 |         self.config = config
43 |         self.app_name = app_name
44 |         self.spark: Optional[SparkSession] = None
   |                     ^^^^^^^^^^^^^^^^^^^^^^
45 |         self._is_local = config.is_local()
   |
help: Convert to `X | None`

W293 [*] Blank line contains whitespace
  --> src/project_a/core/context.py:46:1
   |
44 |         self.spark: Optional[SparkSession] = None
45 |         self._is_local = config.is_local()
46 |     
   | ^^^^
47 |     def __enter__(self) -> "JobContext":
48 |         """Enter context - create SparkSession."""
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/core/context.py:51:1
   |
49 |         self.spark = self._build_spark()
50 |         return self
51 |     
   | ^^^^
52 |     def __exit__(self, exc_type, exc_val, exc_tb) -> None:
53 |         """Exit context - stop SparkSession."""
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/core/context.py:59:1
   |
57 |             except Exception as e:
58 |                 logger.warning(f"Error stopping SparkSession: {e}")
59 |     
   | ^^^^
60 |     def _build_spark(self) -> SparkSession:
61 |         """Build and configure SparkSession."""
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/core/context.py:63:1
   |
61 |         """Build and configure SparkSession."""
62 |         builder = SparkSession.builder.appName(self.app_name)
63 |         
   | ^^^^^^^^
64 |         if self._is_local:
65 |             builder = builder.master("local[*]")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/core/context.py:75:1
   |
73 |             )
74 |             logger.info("Delta Lake extensions enabled for EMR")
75 |         
   | ^^^^^^^^
76 |         # Spark optimizations
77 |         builder = (
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/core/context.py:86:1
   |
84 |             .config("spark.eventLog.enabled", "false")  # Prevent log directory conflicts
85 |         )
86 |         
   | ^^^^^^^^
87 |         # Stop existing session for local mode
88 |         if self._is_local:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/core/context.py:97:1
   |
95 |             except Exception:
96 |                 pass
97 |             
   | ^^^^^^^^^^^^
98 |             try:
99 |                 existing_spark = SparkSession.getActiveSession()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/core/context.py:105:1
    |
103 |             except Exception:
104 |                 pass
105 |         
    | ^^^^^^^^
106 |         spark = builder.getOrCreate()
107 |         logger.info("SparkSession created successfully")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/core/context.py:108:1
    |
106 |         spark = builder.getOrCreate()
107 |         logger.info("SparkSession created successfully")
108 |         
    | ^^^^^^^^
109 |         # Configure AWS S3 if needed
110 |         self._configure_s3(spark)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/core/context.py:111:1
    |
109 |         # Configure AWS S3 if needed
110 |         self._configure_s3(spark)
111 |         
    | ^^^^^^^^
112 |         return spark
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/core/context.py:113:1
    |
112 |         return spark
113 |     
    | ^^^^
114 |     def _configure_s3(self, spark: SparkSession) -> None:
115 |         """Configure AWS S3 access."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/core/context.py:122:1
    |
120 |             for v in paths.values()
121 |         )
122 |         
    | ^^^^^^^^
123 |         if not has_s3:
124 |             sources = self.config.sources
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/core/context.py:133:1
    |
131 |                 if has_s3:
132 |                     break
133 |         
    | ^^^^^^^^
134 |         if has_s3:
135 |             try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/core/context.py:150:1
    |
148 |             except Exception as e:
149 |                 logger.warning(f"AWS S3 configuration failed: {e}")
150 |     
    | ^^^^
151 |     def resolve_path(self, layer: str, source: Optional[str] = None, table: Optional[str] = None) -> str:
152 |         """
    |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/core/context.py:151:48
    |
149 |                 logger.warning(f"AWS S3 configuration failed: {e}")
150 |     
151 |     def resolve_path(self, layer: str, source: Optional[str] = None, table: Optional[str] = None) -> str:
    |                                                ^^^^^^^^^^^^^
152 |         """
153 |         Resolve data path for a layer.
    |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/core/context.py:151:77
    |
149 |                 logger.warning(f"AWS S3 configuration failed: {e}")
150 |     
151 |     def resolve_path(self, layer: str, source: Optional[str] = None, table: Optional[str] = None) -> str:
    |                                                                             ^^^^^^^^^^^^^
152 |         """
153 |         Resolve data path for a layer.
    |
help: Convert to `X | None`

W293 Blank line contains whitespace
   --> src/project_a/core/context.py:154:1
    |
152 |         """
153 |         Resolve data path for a layer.
154 |         
    | ^^^^^^^^
155 |         Args:
156 |             layer: Data layer (bronze, silver, gold)
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/core/context.py:159:1
    |
157 |             source: Optional source name (crm, snowflake, etc.)
158 |             table: Optional table name
159 |         
    | ^^^^^^^^
160 |         Returns:
161 |             Resolved path (file:// or s3://)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/core/context.py:164:1
    |
162 |         """
163 |         return resolve_data_path(self.config._config, layer, source, table)
164 |     
    | ^^^^
165 |     def get_kafka_bootstrap_servers(self) -> str:
166 |         """Get Kafka bootstrap servers based on environment."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/core/context.py:168:1
    |
166 |         """Get Kafka bootstrap servers based on environment."""
167 |         kafka_cfg = self.config.kafka
168 |         
    | ^^^^^^^^
169 |         if self._is_local:
170 |             return kafka_cfg.get("local_bootstrap_servers", "localhost:9092")
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/delta_utils.py:6:1
   |
 4 |   """
 5 |
 6 | / import logging
 7 | | from typing import Any, Optional, Dict, List
 8 | |
 9 | | from delta.tables import DeltaTable
10 | | from pyspark.sql import DataFrame, SparkSession
11 | | from pyspark.sql import functions as F
   | |______________________________________^
12 |
13 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
 --> src/project_a/delta_utils.py:7:1
  |
6 | import logging
7 | from typing import Any, Optional, Dict, List
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
8 |
9 | from delta.tables import DeltaTable
  |

UP035 `typing.List` is deprecated, use `list` instead
 --> src/project_a/delta_utils.py:7:1
  |
6 | import logging
7 | from typing import Any, Optional, Dict, List
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
8 |
9 | from delta.tables import DeltaTable
  |

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/delta_utils.py:16:50
   |
16 | def spark_session(app: str = "projecta", config: Optional[Dict[str, Any]] = None) -> SparkSession:
   |                                                  ^^^^^^^^^^^^^^^^^^^^^^^^
17 |     """Create Spark session with Delta Lake extensions."""
18 |     builder = (
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/delta_utils.py:16:59
   |
16 | def spark_session(app: str = "projecta", config: Optional[Dict[str, Any]] = None) -> SparkSession:
   |                                                           ^^^^
17 |     """Create Spark session with Delta Lake extensions."""
18 |     builder = (
   |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/delta_utils.py:44:61
   |
44 | def write_staging(df: DataFrame, path: str, partition_cols: Optional[List[str]] = None) -> None:
   |                                                             ^^^^^^^^^^^^^^^^^^^
45 |     """Write DataFrame to staging path with audit columns."""
46 |     staging_df = df.withColumn("ingest_ts", F.current_timestamp()).withColumn(
   |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/delta_utils.py:44:70
   |
44 | def write_staging(df: DataFrame, path: str, partition_cols: Optional[List[str]] = None) -> None:
   |                                                                      ^^^^
45 |     """Write DataFrame to staging path with audit columns."""
46 |     staging_df = df.withColumn("ingest_ts", F.current_timestamp()).withColumn(
   |
help: Replace with `list`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/delta_utils.py:64:28
   |
62 |     publish_path: str,
63 |     key: str,
64 |     additional_conditions: Optional[str] = None,
   |                            ^^^^^^^^^^^^^
65 | ) -> None:
66 |     """Merge staging data into publish table with SCD2 pattern."""
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/delta_utils.py:100:59
    |
 99 | def optimize_table(
100 |     spark: SparkSession, table_path: str, zorder_columns: Optional[List[str]] = None
    |                                                           ^^^^^^^^^^^^^^^^^^^
101 | ) -> None:
102 |     """Optimize Delta table with Z-ordering."""
    |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/delta_utils.py:100:68
    |
 99 | def optimize_table(
100 |     spark: SparkSession, table_path: str, zorder_columns: Optional[List[str]] = None
    |                                                                    ^^^^
101 | ) -> None:
102 |     """Optimize Delta table with Z-ordering."""
    |
help: Replace with `list`

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/dq/comprehensive_validator.py:16:1
   |
14 | 10. Performance optimization
15 | """
16 | from typing import Dict, List, Any, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
17 | from pyspark.sql import SparkSession, DataFrame
18 | from pyspark.sql import functions as F
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/dq/comprehensive_validator.py:16:1
   |
14 | 10. Performance optimization
15 | """
16 | from typing import Dict, List, Any, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
17 | from pyspark.sql import SparkSession, DataFrame
18 | from pyspark.sql import functions as F
   |

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/dq/comprehensive_validator.py:16:1
   |
14 |   10. Performance optimization
15 |   """
16 | / from typing import Dict, List, Any, Optional
17 | | from pyspark.sql import SparkSession, DataFrame
18 | | from pyspark.sql import functions as F
19 | | from datetime import datetime
20 | | import logging
21 | |
22 | | from project_a.dq.schema_drift_checker import SchemaDriftChecker
23 | | from project_a.dq.referential_integrity import ReferentialIntegrityChecker
24 | | from project_a.dq.kafka_streaming_validator import KafkaStreamingValidator
25 | | from project_a.dq.performance_optimizer import PerformanceOptimizer
   | |___________________________________________________________________^
26 |
27 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

F401 [*] `typing.List` imported but unused
  --> src/project_a/dq/comprehensive_validator.py:16:26
   |
14 | 10. Performance optimization
15 | """
16 | from typing import Dict, List, Any, Optional
   |                          ^^^^
17 | from pyspark.sql import SparkSession, DataFrame
18 | from pyspark.sql import functions as F
   |
help: Remove unused import

F401 [*] `typing.Optional` imported but unused
  --> src/project_a/dq/comprehensive_validator.py:16:37
   |
14 | 10. Performance optimization
15 | """
16 | from typing import Dict, List, Any, Optional
   |                                     ^^^^^^^^
17 | from pyspark.sql import SparkSession, DataFrame
18 | from pyspark.sql import functions as F
   |
help: Remove unused import

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/comprehensive_validator.py:32:1
   |
30 | class ComprehensiveValidator:
31 |     """Comprehensive data quality validator for all layers."""
32 |     
   | ^^^^
33 |     def __init__(self, spark: SparkSession):
34 |         self.spark = spark
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/dq/comprehensive_validator.py:39:23
   |
37 |         self.kafka_validator = KafkaStreamingValidator(spark)
38 |         self.performance_optimizer = PerformanceOptimizer(spark)
39 |         self.results: Dict[str, Any] = {}
   |                       ^^^^
40 |     
41 |     def validate_bronze_layer(
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/comprehensive_validator.py:40:1
   |
38 |         self.performance_optimizer = PerformanceOptimizer(spark)
39 |         self.results: Dict[str, Any] = {}
40 |     
   | ^^^^
41 |     def validate_bronze_layer(
42 |         self,
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/dq/comprehensive_validator.py:43:22
   |
41 |     def validate_bronze_layer(
42 |         self,
43 |         bronze_data: Dict[str, DataFrame],
   |                      ^^^^
44 |         expected_schemas: Dict[str, Any]
45 |     ) -> Dict[str, Any]:
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/dq/comprehensive_validator.py:44:27
   |
42 |         self,
43 |         bronze_data: Dict[str, DataFrame],
44 |         expected_schemas: Dict[str, Any]
   |                           ^^^^
45 |     ) -> Dict[str, Any]:
46 |         """Validate Bronze layer data."""
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/dq/comprehensive_validator.py:45:10
   |
43 |         bronze_data: Dict[str, DataFrame],
44 |         expected_schemas: Dict[str, Any]
45 |     ) -> Dict[str, Any]:
   |          ^^^^
46 |         """Validate Bronze layer data."""
47 |         logger.info("ðŸ” Validating Bronze layer...")
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/comprehensive_validator.py:48:1
   |
46 |         """Validate Bronze layer data."""
47 |         logger.info("ðŸ” Validating Bronze layer...")
48 |         
   | ^^^^^^^^
49 |         results = {
50 |             "layer": "bronze",
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/comprehensive_validator.py:55:1
   |
53 |             "overall_status": "PASS"
54 |         }
55 |         
   | ^^^^^^^^
56 |         for table_name, df in bronze_data.items():
57 |             table_results = {
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/comprehensive_validator.py:63:1
   |
61 |                 "uniqueness": {}
62 |             }
63 |             
   | ^^^^^^^^^^^^
64 |             # Schema validation
65 |             if table_name in expected_schemas:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/comprehensive_validator.py:71:1
   |
69 |                 table_results["schema_valid"] = not schema_result["drift_detected"]
70 |                 table_results["schema_issues"] = schema_result
71 |             
   | ^^^^^^^^^^^^
72 |             # Null analysis
73 |             table_results["null_analysis"] = self._analyze_nulls(df, table_name)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/comprehensive_validator.py:74:1
   |
72 |             # Null analysis
73 |             table_results["null_analysis"] = self._analyze_nulls(df, table_name)
74 |             
   | ^^^^^^^^^^^^
75 |             # Uniqueness check (if ID column exists)
76 |             id_cols = [col for col in df.columns if col.endswith("_id")]
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/comprehensive_validator.py:83:1
   |
81 |                 )
82 |                 table_results["uniqueness"] = uniqueness_result
83 |             
   | ^^^^^^^^^^^^
84 |             results["tables"][table_name] = table_results
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/comprehensive_validator.py:85:1
   |
84 |             results["tables"][table_name] = table_results
85 |             
   | ^^^^^^^^^^^^
86 |             if not table_results["schema_valid"] or not uniqueness_result.get("valid", True):
87 |                 results["overall_status"] = "FAIL"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/comprehensive_validator.py:88:1
   |
86 |             if not table_results["schema_valid"] or not uniqueness_result.get("valid", True):
87 |                 results["overall_status"] = "FAIL"
88 |         
   | ^^^^^^^^
89 |         self.results["bronze"] = results
90 |         return results
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/comprehensive_validator.py:91:1
   |
89 |         self.results["bronze"] = results
90 |         return results
91 |     
   | ^^^^
92 |     def validate_silver_layer(
93 |         self,
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/dq/comprehensive_validator.py:94:22
   |
92 |     def validate_silver_layer(
93 |         self,
94 |         silver_data: Dict[str, DataFrame],
   |                      ^^^^
95 |         bronze_data: Dict[str, DataFrame]
96 |     ) -> Dict[str, Any]:
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/dq/comprehensive_validator.py:95:22
   |
93 |         self,
94 |         silver_data: Dict[str, DataFrame],
95 |         bronze_data: Dict[str, DataFrame]
   |                      ^^^^
96 |     ) -> Dict[str, Any]:
97 |         """Validate Silver layer data and relationships."""
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/dq/comprehensive_validator.py:96:10
   |
94 |         silver_data: Dict[str, DataFrame],
95 |         bronze_data: Dict[str, DataFrame]
96 |     ) -> Dict[str, Any]:
   |          ^^^^
97 |         """Validate Silver layer data and relationships."""
98 |         logger.info("ðŸ” Validating Silver layer...")
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:99:1
    |
 97 |         """Validate Silver layer data and relationships."""
 98 |         logger.info("ðŸ” Validating Silver layer...")
 99 |         
    | ^^^^^^^^
100 |         results = {
101 |             "layer": "silver",
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:107:1
    |
105 |             "overall_status": "PASS"
106 |         }
107 |         
    | ^^^^^^^^
108 |         # Validate each silver table
109 |         for table_name, df in silver_data.items():
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:115:1
    |
113 |                 "timestamp_validation": {}
114 |             }
115 |             
    | ^^^^^^^^^^^^
116 |             # Timestamp validation
117 |             timestamp_cols = [col for col in df.columns if "timestamp" in col.lower() or "date" in col.lower()]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:120:1
    |
118 |             if timestamp_cols:
119 |                 table_results["timestamp_validation"] = self._validate_timestamps(df, timestamp_cols[0])
120 |             
    | ^^^^^^^^^^^^
121 |             results["tables"][table_name] = table_results
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:122:1
    |
121 |             results["tables"][table_name] = table_results
122 |         
    | ^^^^^^^^
123 |         # Check referential integrity
124 |         if "orders_silver" in silver_data and "customers_silver" in silver_data:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:132:1
    |
130 |             if not ref_result["valid"]:
131 |                 results["overall_status"] = "FAIL"
132 |         
    | ^^^^^^^^
133 |         if "orders_silver" in silver_data and "products_silver" in silver_data:
134 |             ref_result = self.ref_integrity_checker.check_orders_products(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:141:1
    |
139 |             if not ref_result["valid"]:
140 |                 results["overall_status"] = "FAIL"
141 |         
    | ^^^^^^^^
142 |         self.results["silver"] = results
143 |         return results
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:144:1
    |
142 |         self.results["silver"] = results
143 |         return results
144 |     
    | ^^^^
145 |     def validate_gold_layer(
146 |         self,
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/dq/comprehensive_validator.py:147:20
    |
145 |     def validate_gold_layer(
146 |         self,
147 |         gold_data: Dict[str, DataFrame],
    |                    ^^^^
148 |         silver_data: Dict[str, DataFrame]
149 |     ) -> Dict[str, Any]:
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/dq/comprehensive_validator.py:148:22
    |
146 |         self,
147 |         gold_data: Dict[str, DataFrame],
148 |         silver_data: Dict[str, DataFrame]
    |                      ^^^^
149 |     ) -> Dict[str, Any]:
150 |         """Validate Gold layer data."""
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/dq/comprehensive_validator.py:149:10
    |
147 |         gold_data: Dict[str, DataFrame],
148 |         silver_data: Dict[str, DataFrame]
149 |     ) -> Dict[str, Any]:
    |          ^^^^
150 |         """Validate Gold layer data."""
151 |         logger.info("ðŸ” Validating Gold layer...")
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:152:1
    |
150 |         """Validate Gold layer data."""
151 |         logger.info("ðŸ” Validating Gold layer...")
152 |         
    | ^^^^^^^^
153 |         results = {
154 |             "layer": "gold",
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:159:1
    |
157 |             "overall_status": "PASS"
158 |         }
159 |         
    | ^^^^^^^^
160 |         for table_name, df in gold_data.items():
161 |             table_results = {
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:166:1
    |
164 |                 "semantic_validation": {}
165 |             }
166 |             
    | ^^^^^^^^^^^^
167 |             # Semantic validation based on table type
168 |             if "fact_orders" in table_name:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:172:1
    |
170 |             elif "dim_customer" in table_name:
171 |                 table_results["semantic_validation"] = self._validate_dim_customer(df)
172 |             
    | ^^^^^^^^^^^^
173 |             results["tables"][table_name] = table_results
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:174:1
    |
173 |             results["tables"][table_name] = table_results
174 |         
    | ^^^^^^^^
175 |         self.results["gold"] = results
176 |         return results
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:177:1
    |
175 |         self.results["gold"] = results
176 |         return results
177 |     
    | ^^^^
178 |     def _analyze_nulls(self, df: DataFrame, table_name: str) -> Dict[str, Any]:
179 |         """Analyze null values in DataFrame."""
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/dq/comprehensive_validator.py:178:65
    |
176 |         return results
177 |     
178 |     def _analyze_nulls(self, df: DataFrame, table_name: str) -> Dict[str, Any]:
    |                                                                 ^^^^
179 |         """Analyze null values in DataFrame."""
180 |         null_counts = {}
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:182:1
    |
180 |         null_counts = {}
181 |         total_rows = df.count()
182 |         
    | ^^^^^^^^
183 |         for col in df.columns:
184 |             null_count = df.filter(F.col(col).isNull()).count()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:191:1
    |
189 |                 "critical": null_pct > 50 and col.endswith("_id")
190 |             }
191 |         
    | ^^^^^^^^
192 |         return null_counts
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:193:1
    |
192 |         return null_counts
193 |     
    | ^^^^
194 |     def _validate_timestamps(self, df: DataFrame, timestamp_col: str) -> Dict[str, Any]:
195 |         """Validate timestamp column."""
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/dq/comprehensive_validator.py:194:74
    |
192 |         return null_counts
193 |     
194 |     def _validate_timestamps(self, df: DataFrame, timestamp_col: str) -> Dict[str, Any]:
    |                                                                          ^^^^
195 |         """Validate timestamp column."""
196 |         result = {
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:201:1
    |
199 |             "issues": []
200 |         }
201 |         
    | ^^^^^^^^
202 |         # Check for null timestamps
203 |         null_count = df.filter(F.col(timestamp_col).isNull()).count()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:207:1
    |
205 |             result["issues"].append(f"{null_count} null timestamps")
206 |             result["valid"] = False
207 |         
    | ^^^^^^^^
208 |         # Check for future timestamps
209 |         future_count = df.filter(F.col(timestamp_col) > F.current_timestamp()).count()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:213:1
    |
211 |             result["issues"].append(f"{future_count} future timestamps")
212 |             result["valid"] = False
213 |         
    | ^^^^^^^^
214 |         # Check for very old timestamps (older than 10 years)
215 |         from datetime import timedelta
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:220:1
    |
218 |         if old_count > 0:
219 |             result["issues"].append(f"{old_count} timestamps older than 10 years")
220 |         
    | ^^^^^^^^
221 |         return result
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:222:1
    |
221 |         return result
222 |     
    | ^^^^
223 |     def _validate_fact_orders(self, df: DataFrame) -> Dict[str, Any]:
224 |         """Validate fact_orders semantic rules."""
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/dq/comprehensive_validator.py:223:55
    |
221 |         return result
222 |     
223 |     def _validate_fact_orders(self, df: DataFrame) -> Dict[str, Any]:
    |                                                       ^^^^
224 |         """Validate fact_orders semantic rules."""
225 |         result = {
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:229:1
    |
227 |             "issues": []
228 |         }
229 |         
    | ^^^^^^^^
230 |         # Check total_amount >= 0
231 |         if "sales_amount" in df.columns:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:236:1
    |
234 |                 result["issues"].append(f"{negative_count} orders with negative sales_amount")
235 |                 result["valid"] = False
236 |         
    | ^^^^^^^^
237 |         # Check quantity >= 1
238 |         if "quantity" in df.columns:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:243:1
    |
241 |                 result["issues"].append(f"{invalid_qty} orders with quantity < 1")
242 |                 result["valid"] = False
243 |         
    | ^^^^^^^^
244 |         return result
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:245:1
    |
244 |         return result
245 |     
    | ^^^^
246 |     def _validate_dim_customer(self, df: DataFrame) -> Dict[str, Any]:
247 |         """Validate dim_customer semantic rules."""
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/dq/comprehensive_validator.py:246:56
    |
244 |         return result
245 |     
246 |     def _validate_dim_customer(self, df: DataFrame) -> Dict[str, Any]:
    |                                                        ^^^^
247 |         """Validate dim_customer semantic rules."""
248 |         result = {
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:252:1
    |
250 |             "issues": []
251 |         }
252 |         
    | ^^^^^^^^
253 |         # Check for valid emails (basic check)
254 |         if "email" in df.columns:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:260:1
    |
258 |             if invalid_email > 0:
259 |                 result["issues"].append(f"{invalid_email} invalid email addresses")
260 |         
    | ^^^^^^^^
261 |         return result
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:262:1
    |
261 |         return result
262 |     
    | ^^^^
263 |     def generate_comprehensive_report(self) -> str:
264 |         """Generate comprehensive DQ report."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:272:1
    |
270 |             ""
271 |         ]
272 |         
    | ^^^^^^^^
273 |         for layer, results in self.results.items():
274 |             report.append(f"\n{'=' * 70}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:279:1
    |
277 |             report.append(f"Status: {results.get('overall_status', 'UNKNOWN')}")
278 |             report.append("")
279 |             
    | ^^^^^^^^^^^^
280 |             if "tables" in results:
281 |                 for table_name, table_results in results["tables"].items():
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:284:1
    |
282 |                     report.append(f"  Table: {table_name}")
283 |                     report.append(f"    Row Count: {table_results.get('row_count', 0):,}")
284 |                     
    | ^^^^^^^^^^^^^^^^^^^^
285 |                     if "schema_valid" in table_results:
286 |                         status = "âœ…" if table_results["schema_valid"] else "âŒ"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:288:1
    |
286 |                         status = "âœ…" if table_results["schema_valid"] else "âŒ"
287 |                         report.append(f"    Schema: {status}")
288 |                     
    | ^^^^^^^^^^^^^^^^^^^^
289 |                     if "null_analysis" in table_results:
290 |                         critical_nulls = [
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:296:1
    |
294 |                         if critical_nulls:
295 |                             report.append(f"    âš ï¸  Critical nulls in: {', '.join(critical_nulls)}")
296 |             
    | ^^^^^^^^^^^^
297 |             if "relationships" in results:
298 |                 report.append("  Relationships:")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:302:1
    |
300 |                     status = "âœ…" if rel_result.get("valid", False) else "âŒ"
301 |                     report.append(f"    {status} {rel_name}: {rel_result.get('orphaned_count', 0)} orphaned keys")
302 |         
    | ^^^^^^^^
303 |         report.append("\n" + "=" * 70)
304 |         report.append("END OF REPORT")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:306:1
    |
304 |         report.append("END OF REPORT")
305 |         report.append("=" * 70)
306 |         
    | ^^^^^^^^
307 |         return "\n".join(report)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:308:1
    |
307 |         return "\n".join(report)
308 |     
    | ^^^^
309 |     def get_summary(self) -> Dict[str, Any]:
310 |         """Get summary of all validation results."""
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/dq/comprehensive_validator.py:309:30
    |
307 |         return "\n".join(report)
308 |     
309 |     def get_summary(self) -> Dict[str, Any]:
    |                              ^^^^
310 |         """Get summary of all validation results."""
311 |         summary = {
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/comprehensive_validator.py:317:1
    |
315 |             "timestamp": datetime.utcnow().isoformat()
316 |         }
317 |         
    | ^^^^^^^^
318 |         return summary
    |
help: Remove whitespace from blank line

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/dq/file_integrity_checker.py:12:1
   |
10 | - EMR input sizes correct
11 | """
12 | from typing import Dict, List, Any, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
13 | from pathlib import Path
14 | import boto3
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/dq/file_integrity_checker.py:12:1
   |
10 | - EMR input sizes correct
11 | """
12 | from typing import Dict, List, Any, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
13 | from pathlib import Path
14 | import boto3
   |

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/dq/file_integrity_checker.py:12:1
   |
10 |   - EMR input sizes correct
11 |   """
12 | / from typing import Dict, List, Any, Optional
13 | | from pathlib import Path
14 | | import boto3
15 | | from botocore.exceptions import ClientError
16 | | import logging
   | |______________^
17 |
18 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/file_integrity_checker.py:23:1
   |
21 | class FileIntegrityChecker:
22 |     """Check file integrity between local and AWS S3."""
23 |     
   | ^^^^
24 |     def __init__(self, aws_profile: Optional[str] = None, aws_region: str = "us-east-1"):
25 |         self.aws_profile = aws_profile
   |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/dq/file_integrity_checker.py:24:37
   |
22 |     """Check file integrity between local and AWS S3."""
23 |     
24 |     def __init__(self, aws_profile: Optional[str] = None, aws_region: str = "us-east-1"):
   |                                     ^^^^^^^^^^^^^
25 |         self.aws_profile = aws_profile
26 |         self.aws_region = aws_region
   |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/dq/file_integrity_checker.py:28:22
   |
26 |         self.aws_region = aws_region
27 |         self.s3_client = boto3.Session(profile_name=aws_profile, region_name=aws_region).client("s3")
28 |         self.issues: List[Dict[str, Any]] = []
   |                      ^^^^
29 |     
30 |     def compare_local_s3(
   |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/dq/file_integrity_checker.py:28:27
   |
26 |         self.aws_region = aws_region
27 |         self.s3_client = boto3.Session(profile_name=aws_profile, region_name=aws_region).client("s3")
28 |         self.issues: List[Dict[str, Any]] = []
   |                           ^^^^
29 |     
30 |     def compare_local_s3(
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/file_integrity_checker.py:29:1
   |
27 |         self.s3_client = boto3.Session(profile_name=aws_profile, region_name=aws_region).client("s3")
28 |         self.issues: List[Dict[str, Any]] = []
29 |     
   | ^^^^
30 |     def compare_local_s3(
31 |         self,
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/dq/file_integrity_checker.py:35:10
   |
33 |         s3_path: str,
34 |         file_name: str
35 |     ) -> Dict[str, Any]:
   |          ^^^^
36 |         """
37 |         Compare local file with S3 object.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/dq/file_integrity_checker.py:38:1
   |
36 |         """
37 |         Compare local file with S3 object.
38 |         
   | ^^^^^^^^
39 |         Args:
40 |             local_path: Local file path
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/dq/file_integrity_checker.py:43:1
   |
41 |             s3_path: S3 path (s3://bucket/key)
42 |             file_name: Descriptive name for the file
43 |         
   | ^^^^^^^^
44 |         Returns:
45 |             Comparison results
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/file_integrity_checker.py:48:1
   |
46 |         """
47 |         logger.info(f"Comparing {file_name}: local vs S3")
48 |         
   | ^^^^^^^^
49 |         result = {
50 |             "file": file_name,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/file_integrity_checker.py:58:1
   |
56 |             "valid": False
57 |         }
58 |         
   | ^^^^^^^^
59 |         # Check local file
60 |         local_file = Path(local_path)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/file_integrity_checker.py:71:1
   |
69 |             })
70 |             return result
71 |         
   | ^^^^^^^^
72 |         # Check S3 object
73 |         try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/file_integrity_checker.py:93:1
   |
91 |                 })
92 |             return result
93 |         
   | ^^^^^^^^
94 |         # Compare sizes
95 |         result["sizes_match"] = result["local_size"] == result["s3_size"]
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/file_integrity_checker.py:97:1
   |
95 |         result["sizes_match"] = result["local_size"] == result["s3_size"]
96 |         result["valid"] = result["sizes_match"]
97 |         
   | ^^^^^^^^
98 |         if not result["sizes_match"]:
99 |             self.issues.append({
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/file_integrity_checker.py:112:1
    |
110 |         else:
111 |             logger.info(f"âœ… {file_name}: sizes match ({result['local_size']} bytes)")
112 |         
    | ^^^^^^^^
113 |         return result
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/file_integrity_checker.py:114:1
    |
113 |         return result
114 |     
    | ^^^^
115 |     def check_s3_object_integrity(self, s3_path: str) -> Dict[str, Any]:
116 |         """Check S3 object integrity (ETag, checksum)."""
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/dq/file_integrity_checker.py:115:58
    |
113 |         return result
114 |     
115 |     def check_s3_object_integrity(self, s3_path: str) -> Dict[str, Any]:
    |                                                          ^^^^
116 |         """Check S3 object integrity (ETag, checksum)."""
117 |         try:
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/file_integrity_checker.py:120:1
    |
118 |             bucket, key = s3_path.replace("s3://", "").split("/", 1)
119 |             response = self.s3_client.head_object(Bucket=bucket, Key=key)
120 |             
    | ^^^^^^^^^^^^
121 |             return {
122 |                 "valid": True,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/file_integrity_checker.py:133:1
    |
131 |                 "error": str(e)
132 |             }
133 |     
    | ^^^^
134 |     def check_partition_count(
135 |         self,
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/dq/file_integrity_checker.py:138:10
    |
136 |         local_dir: Path,
137 |         s3_prefix: str
138 |     ) -> Dict[str, Any]:
    |          ^^^^
139 |         """Compare partition counts between local and S3."""
140 |         # Count local partitions
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/file_integrity_checker.py:142:1
    |
140 |         # Count local partitions
141 |         local_partitions = len([p for p in local_dir.iterdir() if p.is_dir()])
142 |         
    | ^^^^^^^^
143 |         # Count S3 partitions
144 |         try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/file_integrity_checker.py:148:1
    |
146 |             paginator = self.s3_client.get_paginator("list_objects_v2")
147 |             s3_partitions = set()
148 |             
    | ^^^^^^^^^^^^
149 |             for page in paginator.paginate(Bucket=bucket, Prefix=prefix, Delimiter="/"):
150 |                 for prefix_obj in page.get("CommonPrefixes", []):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/file_integrity_checker.py:152:1
    |
150 |                 for prefix_obj in page.get("CommonPrefixes", []):
151 |                     s3_partitions.add(prefix_obj["Prefix"])
152 |             
    | ^^^^^^^^^^^^
153 |             s3_partition_count = len(s3_partitions)
154 |         except Exception as e:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/file_integrity_checker.py:160:1
    |
158 |                 "local_partitions": local_partitions
159 |             }
160 |         
    | ^^^^^^^^
161 |         result = {
162 |             "valid": local_partitions == s3_partition_count,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/file_integrity_checker.py:166:1
    |
164 |             "s3_partitions": s3_partition_count
165 |         }
166 |         
    | ^^^^^^^^
167 |         if not result["valid"]:
168 |             self.issues.append({
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/file_integrity_checker.py:173:1
    |
171 |                 "s3": s3_partition_count
172 |             })
173 |         
    | ^^^^^^^^
174 |         return result
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/file_integrity_checker.py:175:1
    |
174 |         return result
175 |     
    | ^^^^
176 |     def get_all_issues(self) -> List[Dict[str, Any]]:
177 |         """Get all detected integrity issues."""
    |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/dq/file_integrity_checker.py:176:33
    |
174 |         return result
175 |     
176 |     def get_all_issues(self) -> List[Dict[str, Any]]:
    |                                 ^^^^
177 |         """Get all detected integrity issues."""
178 |         return self.issues
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/dq/file_integrity_checker.py:176:38
    |
174 |         return result
175 |     
176 |     def get_all_issues(self) -> List[Dict[str, Any]]:
    |                                      ^^^^
177 |         """Get all detected integrity issues."""
178 |         return self.issues
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/file_integrity_checker.py:179:1
    |
177 |         """Get all detected integrity issues."""
178 |         return self.issues
179 |     
    | ^^^^
180 |     def generate_report(self) -> str:
181 |         """Generate integrity report."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/file_integrity_checker.py:184:1
    |
182 |         if not self.issues:
183 |             return "âœ… No file integrity issues detected."
184 |         
    | ^^^^^^^^
185 |         report = ["ðŸ” File Integrity Report", "=" * 50, ""]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/file_integrity_checker.py:186:1
    |
185 |         report = ["ðŸ” File Integrity Report", "=" * 50, ""]
186 |         
    | ^^^^^^^^
187 |         for issue in self.issues:
188 |             report.append(f"File: {issue.get('file', 'Unknown')}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/file_integrity_checker.py:195:1
    |
193 |                 report.append(f"  Difference: {issue['difference']:,} bytes")
194 |             report.append("")
195 |         
    | ^^^^^^^^
196 |         return "\n".join(report)
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/dq/gate.py:7:1
   |
 5 |   """
 6 |
 7 | / from __future__ import annotations
 8 | |
 9 | | from dataclasses import dataclass
10 | | from datetime import datetime
11 | |
12 | | from pyspark.sql import DataFrame, SparkSession
13 | | from pyspark.sql import functions as F
14 | | from typing import Optional, Dict, List
   | |_______________________________________^
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/dq/gate.py:14:1
   |
12 | from pyspark.sql import DataFrame, SparkSession
13 | from pyspark.sql import functions as F
14 | from typing import Optional, Dict, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/dq/gate.py:14:1
   |
12 | from pyspark.sql import DataFrame, SparkSession
13 | from pyspark.sql import functions as F
14 | from typing import Optional, Dict, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   |

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/dq/gate.py:23:22
   |
21 |     table_name: str
22 |     total_rows: int
23 |     null_violations: Dict[str, int]
   |                      ^^^^
24 |     passed: bool
   |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/dq/gate.py:27:63
   |
27 | def run_not_null_checks(df: DataFrame, table_name: str, cols: List[str]) -> DQCheckResult:
   |                                                               ^^^^
28 |     """
29 |     Run not-null checks on specified columns.
   |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/dq/gate.py:40:22
   |
38 |     """
39 |     total_rows = df.count()
40 |     null_violations: Dict[str, int] = {}
   |                      ^^^^
41 |
42 |     for c in cols:
   |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/dq/gate.py:57:67
   |
57 | def run_uniqueness_check(df: DataFrame, table_name: str, pk_cols: List[str]) -> Dict[str, int]:
   |                                                                   ^^^^
58 |     """
59 |     Check uniqueness of primary key columns.
   |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/dq/gate.py:57:81
   |
57 | def run_uniqueness_check(df: DataFrame, table_name: str, pk_cols: List[str]) -> Dict[str, int]:
   |                                                                                 ^^^^
58 |     """
59 |     Check uniqueness of primary key columns.
   |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/dq/gate.py:84:44
   |
83 | def run_range_check(
84 |     df: DataFrame, col_name: str, min_val: Optional[float] = None, max_val: Optional[float] = None
   |                                            ^^^^^^^^^^^^^^^
85 | ) -> int:
86 |     """
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/dq/gate.py:84:77
   |
83 | def run_range_check(
84 |     df: DataFrame, col_name: str, min_val: Optional[float] = None, max_val: Optional[float] = None
   |                                                                             ^^^^^^^^^^^^^^^
85 | ) -> int:
86 |     """
   |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/dq/gate.py:141:18
    |
139 |     df: DataFrame,
140 |     table_name: str,
141 |     primary_key: List[str],
    |                  ^^^^
142 |     required_cols: List[str],
143 |     output_path: Optional[str] = None,
    |
help: Replace with `list`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/dq/gate.py:142:20
    |
140 |     table_name: str,
141 |     primary_key: List[str],
142 |     required_cols: List[str],
    |                    ^^^^
143 |     output_path: Optional[str] = None,
144 |     check_uniqueness: bool = True,
    |
help: Replace with `list`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/dq/gate.py:143:18
    |
141 |     primary_key: List[str],
142 |     required_cols: List[str],
143 |     output_path: Optional[str] = None,
    |                  ^^^^^^^^^^^^^
144 |     check_uniqueness: bool = True,
145 |     range_checks: Optional[Dict[str, Dict[str, float]]] = None,
    |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/dq/gate.py:145:19
    |
143 |     output_path: Optional[str] = None,
144 |     check_uniqueness: bool = True,
145 |     range_checks: Optional[Dict[str, Dict[str, float]]] = None,
    |                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
146 | ) -> DQCheckResult:
147 |     """
    |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/dq/gate.py:145:28
    |
143 |     output_path: Optional[str] = None,
144 |     check_uniqueness: bool = True,
145 |     range_checks: Optional[Dict[str, Dict[str, float]]] = None,
    |                            ^^^^
146 | ) -> DQCheckResult:
147 |     """
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/dq/gate.py:145:38
    |
143 |     output_path: Optional[str] = None,
144 |     check_uniqueness: bool = True,
145 |     range_checks: Optional[Dict[str, Dict[str, float]]] = None,
    |                                      ^^^^
146 | ) -> DQCheckResult:
147 |     """
    |
help: Replace with `dict`

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/dq/kafka_streaming_validator.py:13:1
   |
11 | - Out-of-order events
12 | """
13 | from typing import Dict, List, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
14 | from pyspark.sql import SparkSession, DataFrame
15 | from pyspark.sql import functions as F
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/dq/kafka_streaming_validator.py:13:1
   |
11 | - Out-of-order events
12 | """
13 | from typing import Dict, List, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
14 | from pyspark.sql import SparkSession, DataFrame
15 | from pyspark.sql import functions as F
   |

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/dq/kafka_streaming_validator.py:13:1
   |
11 |   - Out-of-order events
12 |   """
13 | / from typing import Dict, List, Any
14 | | from pyspark.sql import SparkSession, DataFrame
15 | | from pyspark.sql import functions as F
16 | | from datetime import datetime, timedelta
17 | | import logging
   | |______________^
18 |
19 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

F401 [*] `datetime.datetime` imported but unused
  --> src/project_a/dq/kafka_streaming_validator.py:16:22
   |
14 | from pyspark.sql import SparkSession, DataFrame
15 | from pyspark.sql import functions as F
16 | from datetime import datetime, timedelta
   |                      ^^^^^^^^
17 | import logging
   |
help: Remove unused import: `datetime.datetime`

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/kafka_streaming_validator.py:24:1
   |
22 | class KafkaStreamingValidator:
23 |     """Validate Kafka streaming data fitness."""
24 |     
   | ^^^^
25 |     def __init__(self, spark: SparkSession):
26 |         self.spark = spark
   |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/dq/kafka_streaming_validator.py:27:22
   |
25 |     def __init__(self, spark: SparkSession):
26 |         self.spark = spark
27 |         self.issues: List[Dict[str, Any]] = []
   |                      ^^^^
28 |     
29 |     def validate_streaming_fitness(
   |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/dq/kafka_streaming_validator.py:27:27
   |
25 |     def __init__(self, spark: SparkSession):
26 |         self.spark = spark
27 |         self.issues: List[Dict[str, Any]] = []
   |                           ^^^^
28 |     
29 |     def validate_streaming_fitness(
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/kafka_streaming_validator.py:28:1
   |
26 |         self.spark = spark
27 |         self.issues: List[Dict[str, Any]] = []
28 |     
   | ^^^^
29 |     def validate_streaming_fitness(
30 |         self,
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/dq/kafka_streaming_validator.py:35:10
   |
33 |         event_type_col: str = "event_type",
34 |         session_col: str = "session_id"
35 |     ) -> Dict[str, Any]:
   |          ^^^^
36 |         """Comprehensive streaming fitness validation."""
37 |         logger.info("ðŸ” Validating Kafka streaming fitness...")
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/kafka_streaming_validator.py:38:1
   |
36 |         """Comprehensive streaming fitness validation."""
37 |         logger.info("ðŸ” Validating Kafka streaming fitness...")
38 |         
   | ^^^^^^^^
39 |         result = {
40 |             "valid": True,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/kafka_streaming_validator.py:49:1
   |
47 |             "issues": []
48 |         }
49 |         
   | ^^^^^^^^
50 |         # Check timestamp monotonicity
51 |         timestamp_result = self._check_timestamp_monotonicity(df, timestamp_col)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/kafka_streaming_validator.py:56:1
   |
54 |             result["valid"] = False
55 |             result["issues"].append("Timestamps are not monotonically increasing")
56 |         
   | ^^^^^^^^
57 |         # Check event type diversity
58 |         event_diversity = self._check_event_type_diversity(df, event_type_col)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/kafka_streaming_validator.py:62:1
   |
60 |         if event_diversity.get("unique_types", 0) < 3:
61 |             result["issues"].append("Low event type diversity")
62 |         
   | ^^^^^^^^
63 |         # Check session consistency
64 |         session_result = self._check_session_consistency(df, session_col, timestamp_col)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/kafka_streaming_validator.py:66:1
   |
64 |         session_result = self._check_session_consistency(df, session_col, timestamp_col)
65 |         result["session_consistency"] = session_result
66 |         
   | ^^^^^^^^
67 |         # Check cardinality
68 |         cardinality = self._check_cardinality(df)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/kafka_streaming_validator.py:70:1
   |
68 |         cardinality = self._check_cardinality(df)
69 |         result["cardinality"] = cardinality
70 |         
   | ^^^^^^^^
71 |         # Check for late events (events older than 1 hour from latest)
72 |         late_events = self._check_late_events(df, timestamp_col)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/kafka_streaming_validator.py:74:1
   |
72 |         late_events = self._check_late_events(df, timestamp_col)
73 |         result["late_events"] = late_events
74 |         
   | ^^^^^^^^
75 |         # Check for out-of-order events
76 |         out_of_order = self._check_out_of_order(df, timestamp_col, session_col)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/kafka_streaming_validator.py:78:1
   |
76 |         out_of_order = self._check_out_of_order(df, timestamp_col, session_col)
77 |         result["out_of_order"] = out_of_order
78 |         
   | ^^^^^^^^
79 |         if result["issues"]:
80 |             self.issues.append(result)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/kafka_streaming_validator.py:84:1
   |
82 |         else:
83 |             logger.info("âœ… Streaming data is fit for Kafka/Structured Streaming")
84 |         
   | ^^^^^^^^
85 |         return result
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/kafka_streaming_validator.py:86:1
   |
85 |         return result
86 |     
   | ^^^^
87 |     def _check_timestamp_monotonicity(self, df: DataFrame, timestamp_col: str) -> Dict[str, Any]:
88 |         """Check if timestamps are monotonically increasing."""
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/dq/kafka_streaming_validator.py:87:83
   |
85 |         return result
86 |     
87 |     def _check_timestamp_monotonicity(self, df: DataFrame, timestamp_col: str) -> Dict[str, Any]:
   |                                                                                   ^^^^
88 |         """Check if timestamps are monotonically increasing."""
89 |         from pyspark.sql.window import Window
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/kafka_streaming_validator.py:90:1
   |
88 |         """Check if timestamps are monotonically increasing."""
89 |         from pyspark.sql.window import Window
90 |         
   | ^^^^^^^^
91 |         window = Window.orderBy(timestamp_col)
92 |         df_with_prev = df.withColumn("prev_timestamp", F.lag(timestamp_col).over(window))
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/kafka_streaming_validator.py:93:1
   |
91 |         window = Window.orderBy(timestamp_col)
92 |         df_with_prev = df.withColumn("prev_timestamp", F.lag(timestamp_col).over(window))
93 |         
   | ^^^^^^^^
94 |         # Count rows where current < previous
95 |         violations = df_with_prev.filter(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/kafka_streaming_validator.py:99:1
    |
 97 |             F.col("prev_timestamp").isNotNull()
 98 |         ).count()
 99 |         
    | ^^^^^^^^
100 |         total_rows = df.count()
101 |         violation_pct = (violations / total_rows * 100) if total_rows > 0 else 0
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/kafka_streaming_validator.py:102:1
    |
100 |         total_rows = df.count()
101 |         violation_pct = (violations / total_rows * 100) if total_rows > 0 else 0
102 |         
    | ^^^^^^^^
103 |         return {
104 |             "monotonic": violations == 0,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/kafka_streaming_validator.py:108:1
    |
106 |             "violation_percentage": round(violation_pct, 2)
107 |         }
108 |     
    | ^^^^
109 |     def _check_event_type_diversity(self, df: DataFrame, event_type_col: str) -> Dict[str, Any]:
110 |         """Check event type diversity."""
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/dq/kafka_streaming_validator.py:109:82
    |
107 |         }
108 |     
109 |     def _check_event_type_diversity(self, df: DataFrame, event_type_col: str) -> Dict[str, Any]:
    |                                                                                  ^^^^
110 |         """Check event type diversity."""
111 |         if event_type_col not in df.columns:
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/kafka_streaming_validator.py:113:1
    |
111 |         if event_type_col not in df.columns:
112 |             return {"unique_types": 0, "distribution": {}}
113 |         
    | ^^^^^^^^
114 |         event_counts = df.groupBy(event_type_col).count().collect()
115 |         distribution = {row[event_type_col]: row["count"] for row in event_counts}
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/kafka_streaming_validator.py:116:1
    |
114 |         event_counts = df.groupBy(event_type_col).count().collect()
115 |         distribution = {row[event_type_col]: row["count"] for row in event_counts}
116 |         
    | ^^^^^^^^
117 |         return {
118 |             "unique_types": len(distribution),
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/kafka_streaming_validator.py:121:1
    |
119 |             "distribution": distribution
120 |         }
121 |     
    | ^^^^
122 |     def _check_session_consistency(
123 |         self,
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/dq/kafka_streaming_validator.py:127:10
    |
125 |         session_col: str,
126 |         timestamp_col: str
127 |     ) -> Dict[str, Any]:
    |          ^^^^
128 |         """Check session ID consistency."""
129 |         if session_col not in df.columns:
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/kafka_streaming_validator.py:131:1
    |
129 |         if session_col not in df.columns:
130 |             return {"valid": False, "issue": "session_id column not found"}
131 |         
    | ^^^^^^^^
132 |         # Check for sessions with gaps (potential issues)
133 |         from pyspark.sql.window import Window
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/kafka_streaming_validator.py:134:1
    |
132 |         # Check for sessions with gaps (potential issues)
133 |         from pyspark.sql.window import Window
134 |         
    | ^^^^^^^^
135 |         window = Window.partitionBy(session_col).orderBy(timestamp_col)
136 |         df_with_gaps = df.withColumn("time_diff", 
    |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
   --> src/project_a/dq/kafka_streaming_validator.py:136:50
    |
135 |         window = Window.partitionBy(session_col).orderBy(timestamp_col)
136 |         df_with_gaps = df.withColumn("time_diff", 
    |                                                  ^
137 |             F.col(timestamp_col).cast("long") - F.lag(timestamp_col).over(window).cast("long"))
    |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/kafka_streaming_validator.py:138:1
    |
136 |         df_with_gaps = df.withColumn("time_diff", 
137 |             F.col(timestamp_col).cast("long") - F.lag(timestamp_col).over(window).cast("long"))
138 |         
    | ^^^^^^^^
139 |         # Sessions with gaps > 1 hour
140 |         large_gaps = df_with_gaps.filter(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/kafka_streaming_validator.py:143:1
    |
141 |             (F.col("time_diff") > 3600) & F.col("time_diff").isNotNull()
142 |         ).count()
143 |         
    | ^^^^^^^^
144 |         return {
145 |             "valid": large_gaps == 0,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/kafka_streaming_validator.py:149:1
    |
147 |             "total_sessions": df.select(session_col).distinct().count()
148 |         }
149 |     
    | ^^^^
150 |     def _check_cardinality(self, df: DataFrame) -> Dict[str, Any]:
151 |         """Check data cardinality metrics."""
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/dq/kafka_streaming_validator.py:150:52
    |
148 |         }
149 |     
150 |     def _check_cardinality(self, df: DataFrame) -> Dict[str, Any]:
    |                                                    ^^^^
151 |         """Check data cardinality metrics."""
152 |         total_rows = df.count()
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/kafka_streaming_validator.py:153:1
    |
151 |         """Check data cardinality metrics."""
152 |         total_rows = df.count()
153 |         
    | ^^^^^^^^
154 |         # Check unique customers
155 |         customer_cardinality = 0
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/kafka_streaming_validator.py:158:1
    |
156 |         if "customer_id" in df.columns:
157 |             customer_cardinality = df.select("customer_id").distinct().count()
158 |         
    | ^^^^^^^^
159 |         # Check unique sessions
160 |         session_cardinality = 0
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/kafka_streaming_validator.py:163:1
    |
161 |         if "session_id" in df.columns:
162 |             session_cardinality = df.select("session_id").distinct().count()
163 |         
    | ^^^^^^^^
164 |         return {
165 |             "total_rows": total_rows,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/kafka_streaming_validator.py:171:1
    |
169 |             "rows_per_session": round(total_rows / session_cardinality, 2) if session_cardinality > 0 else 0
170 |         }
171 |     
    | ^^^^
172 |     def _check_late_events(self, df: DataFrame, timestamp_col: str) -> Dict[str, Any]:
173 |         """Check for late events (events older than 1 hour from latest)."""
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/dq/kafka_streaming_validator.py:172:72
    |
170 |         }
171 |     
172 |     def _check_late_events(self, df: DataFrame, timestamp_col: str) -> Dict[str, Any]:
    |                                                                        ^^^^
173 |         """Check for late events (events older than 1 hour from latest)."""
174 |         if df.count() == 0:
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/kafka_streaming_validator.py:176:1
    |
174 |         if df.count() == 0:
175 |             return {"late_events_count": 0, "latest_timestamp": None}
176 |         
    | ^^^^^^^^
177 |         latest_ts = df.agg(F.max(timestamp_col).alias("max_ts")).collect()[0]["max_ts"]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/kafka_streaming_validator.py:178:1
    |
177 |         latest_ts = df.agg(F.max(timestamp_col).alias("max_ts")).collect()[0]["max_ts"]
178 |         
    | ^^^^^^^^
179 |         if latest_ts is None:
180 |             return {"late_events_count": 0, "latest_timestamp": None}
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/kafka_streaming_validator.py:181:1
    |
179 |         if latest_ts is None:
180 |             return {"late_events_count": 0, "latest_timestamp": None}
181 |         
    | ^^^^^^^^
182 |         # Events older than 1 hour from latest
183 |         one_hour_ago = latest_ts - timedelta(hours=1)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/kafka_streaming_validator.py:185:1
    |
183 |         one_hour_ago = latest_ts - timedelta(hours=1)
184 |         late_events = df.filter(F.col(timestamp_col) < F.lit(one_hour_ago)).count()
185 |         
    | ^^^^^^^^
186 |         return {
187 |             "late_events_count": late_events,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/kafka_streaming_validator.py:191:1
    |
189 |             "threshold_hours": 1
190 |         }
191 |     
    | ^^^^
192 |     def _check_out_of_order(
193 |         self,
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/dq/kafka_streaming_validator.py:197:10
    |
195 |         timestamp_col: str,
196 |         session_col: str
197 |     ) -> Dict[str, Any]:
    |          ^^^^
198 |         """Check for out-of-order events within sessions."""
199 |         if session_col not in df.columns:
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/kafka_streaming_validator.py:201:1
    |
199 |         if session_col not in df.columns:
200 |             return {"out_of_order_count": 0, "sessions_affected": 0}
201 |         
    | ^^^^^^^^
202 |         from pyspark.sql.window import Window
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/kafka_streaming_validator.py:203:1
    |
202 |         from pyspark.sql.window import Window
203 |         
    | ^^^^^^^^
204 |         window = Window.partitionBy(session_col).orderBy(timestamp_col)
205 |         df_with_order = df.withColumn("prev_ts", F.lag(timestamp_col).over(window))
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/kafka_streaming_validator.py:206:1
    |
204 |         window = Window.partitionBy(session_col).orderBy(timestamp_col)
205 |         df_with_order = df.withColumn("prev_ts", F.lag(timestamp_col).over(window))
206 |         
    | ^^^^^^^^
207 |         # Events where current < previous within same session
208 |         out_of_order = df_with_order.filter(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/kafka_streaming_validator.py:212:1
    |
210 |             F.col("prev_ts").isNotNull()
211 |         )
212 |         
    | ^^^^^^^^
213 |         out_of_order_count = out_of_order.count()
214 |         sessions_affected = out_of_order.select(session_col).distinct().count() if out_of_order_count > 0 else 0
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/kafka_streaming_validator.py:215:1
    |
213 |         out_of_order_count = out_of_order.count()
214 |         sessions_affected = out_of_order.select(session_col).distinct().count() if out_of_order_count > 0 else 0
215 |         
    | ^^^^^^^^
216 |         return {
217 |             "out_of_order_count": out_of_order_count,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/kafka_streaming_validator.py:220:1
    |
218 |             "sessions_affected": sessions_affected
219 |         }
220 |     
    | ^^^^
221 |     def get_all_issues(self) -> List[Dict[str, Any]]:
222 |         """Get all detected streaming fitness issues."""
    |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/dq/kafka_streaming_validator.py:221:33
    |
219 |         }
220 |     
221 |     def get_all_issues(self) -> List[Dict[str, Any]]:
    |                                 ^^^^
222 |         """Get all detected streaming fitness issues."""
223 |         return self.issues
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/dq/kafka_streaming_validator.py:221:38
    |
219 |         }
220 |     
221 |     def get_all_issues(self) -> List[Dict[str, Any]]:
    |                                      ^^^^
222 |         """Get all detected streaming fitness issues."""
223 |         return self.issues
    |
help: Replace with `dict`

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/dq/performance_optimizer.py:11:1
   |
 9 | - Date columns in correct format
10 | """
11 | from typing import Dict, List, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
12 | from pyspark.sql import SparkSession, DataFrame
13 | from pyspark.sql import functions as F
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/dq/performance_optimizer.py:11:1
   |
 9 | - Date columns in correct format
10 | """
11 | from typing import Dict, List, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
12 | from pyspark.sql import SparkSession, DataFrame
13 | from pyspark.sql import functions as F
   |

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/dq/performance_optimizer.py:11:1
   |
 9 |   - Date columns in correct format
10 |   """
11 | / from typing import Dict, List, Any
12 | | from pyspark.sql import SparkSession, DataFrame
13 | | from pyspark.sql import functions as F
14 | | import logging
   | |______________^
15 |
16 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

F401 [*] `pyspark.sql.functions` imported but unused
  --> src/project_a/dq/performance_optimizer.py:13:38
   |
11 | from typing import Dict, List, Any
12 | from pyspark.sql import SparkSession, DataFrame
13 | from pyspark.sql import functions as F
   |                                      ^
14 | import logging
   |
help: Remove unused import: `pyspark.sql.functions`

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/performance_optimizer.py:21:1
   |
19 | class PerformanceOptimizer:
20 |     """Check and optimize data for Spark performance."""
21 |     
   | ^^^^
22 |     def __init__(self, spark: SparkSession):
23 |         self.spark = spark
   |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/dq/performance_optimizer.py:24:22
   |
22 |     def __init__(self, spark: SparkSession):
23 |         self.spark = spark
24 |         self.issues: List[Dict[str, Any]] = []
   |                      ^^^^
25 |         self.recommendations: List[str] = []
   |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/dq/performance_optimizer.py:24:27
   |
22 |     def __init__(self, spark: SparkSession):
23 |         self.spark = spark
24 |         self.issues: List[Dict[str, Any]] = []
   |                           ^^^^
25 |         self.recommendations: List[str] = []
   |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/dq/performance_optimizer.py:25:31
   |
23 |         self.spark = spark
24 |         self.issues: List[Dict[str, Any]] = []
25 |         self.recommendations: List[str] = []
   |                               ^^^^
26 |     
27 |     def analyze_table_performance(
   |
help: Replace with `list`

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/performance_optimizer.py:26:1
   |
24 |         self.issues: List[Dict[str, Any]] = []
25 |         self.recommendations: List[str] = []
26 |     
   | ^^^^
27 |     def analyze_table_performance(
28 |         self,
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/dq/performance_optimizer.py:32:10
   |
30 |         table_name: str,
31 |         is_dimension: bool = False
32 |     ) -> Dict[str, Any]:
   |          ^^^^
33 |         """Analyze table for performance optimization opportunities."""
34 |         logger.info(f"ðŸ” Analyzing performance for {table_name}...")
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/performance_optimizer.py:35:1
   |
33 |         """Analyze table for performance optimization opportunities."""
34 |         logger.info(f"ðŸ” Analyzing performance for {table_name}...")
35 |         
   | ^^^^^^^^
36 |         result = {
37 |             "table": table_name,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/performance_optimizer.py:46:1
   |
44 |             "recommendations": []
45 |         }
46 |         
   | ^^^^^^^^
47 |         row_count = result["row_count"]
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/performance_optimizer.py:48:1
   |
47 |         row_count = result["row_count"]
48 |         
   | ^^^^^^^^
49 |         # Check if suitable for broadcast join (dimension tables < 10MB)
50 |         if is_dimension:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/performance_optimizer.py:61:1
   |
59 |                         f"Table is {estimated_size_mb:.1f}MB - consider reducing size for broadcast join"
60 |                     )
61 |         
   | ^^^^^^^^
62 |         # Check for data skew
63 |         skew_result = self._check_data_skew(df, table_name)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/performance_optimizer.py:66:1
   |
64 |         result["skew_detected"] = skew_result["has_skew"]
65 |         result["skew_details"] = skew_result
66 |         
   | ^^^^^^^^
67 |         # Check column types
68 |         type_result = self._check_column_types(df)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/performance_optimizer.py:70:1
   |
68 |         type_result = self._check_column_types(df)
69 |         result["type_optimizations"] = type_result
70 |         
   | ^^^^^^^^
71 |         # Check partitioning
72 |         if not is_dimension:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/performance_optimizer.py:76:1
   |
74 |             result["partitioning_optimal"] = partition_result["optimal"]
75 |             result["partitioning_details"] = partition_result
76 |         
   | ^^^^^^^^
77 |         if result["recommendations"]:
78 |             self.recommendations.extend([
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/performance_optimizer.py:81:1
   |
79 |                 f"{table_name}: {rec}" for rec in result["recommendations"]
80 |             ])
81 |         
   | ^^^^^^^^
82 |         return result
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/performance_optimizer.py:83:1
   |
82 |         return result
83 |     
   | ^^^^
84 |     def _check_data_skew(self, df: DataFrame, table_name: str) -> Dict[str, Any]:
85 |         """Check for data skew in key columns."""
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/dq/performance_optimizer.py:84:67
   |
82 |         return result
83 |     
84 |     def _check_data_skew(self, df: DataFrame, table_name: str) -> Dict[str, Any]:
   |                                                                   ^^^^
85 |         """Check for data skew in key columns."""
86 |         # Check for ID columns that might cause skew
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/performance_optimizer.py:88:1
   |
86 |         # Check for ID columns that might cause skew
87 |         id_columns = [col for col in df.columns if col.endswith("_id")]
88 |         
   | ^^^^^^^^
89 |         skew_results = {}
90 |         has_skew = False
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/performance_optimizer.py:91:1
   |
89 |         skew_results = {}
90 |         has_skew = False
91 |         
   | ^^^^^^^^
92 |         for id_col in id_columns[:3]:  # Check first 3 ID columns
93 |             value_counts = df.groupBy(id_col).count()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/performance_optimizer.py:95:1
   |
93 |             value_counts = df.groupBy(id_col).count()
94 |             counts = [row["count"] for row in value_counts.collect()]
95 |             
   | ^^^^^^^^^^^^
96 |             if counts:
97 |                 max_count = max(counts)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/performance_optimizer.py:100:1
    |
 98 |                 avg_count = sum(counts) / len(counts)
 99 |                 skew_ratio = max_count / avg_count if avg_count > 0 else 0
100 |                 
    | ^^^^^^^^^^^^^^^^
101 |                 skew_results[id_col] = {
102 |                     "max_count": max_count,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/performance_optimizer.py:107:1
    |
105 |                     "has_skew": skew_ratio > 10  # Threshold: 10x average
106 |                 }
107 |                 
    | ^^^^^^^^^^^^^^^^
108 |                 if skew_results[id_col]["has_skew"]:
109 |                     has_skew = True
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/performance_optimizer.py:110:1
    |
108 |                 if skew_results[id_col]["has_skew"]:
109 |                     has_skew = True
110 |         
    | ^^^^^^^^
111 |         return {
112 |             "has_skew": has_skew,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/performance_optimizer.py:115:1
    |
113 |             "column_details": skew_results
114 |         }
115 |     
    | ^^^^
116 |     def _check_column_types(self, df: DataFrame) -> List[Dict[str, Any]]:
117 |         """Check for column type optimizations."""
    |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/dq/performance_optimizer.py:116:53
    |
114 |         }
115 |     
116 |     def _check_column_types(self, df: DataFrame) -> List[Dict[str, Any]]:
    |                                                     ^^^^
117 |         """Check for column type optimizations."""
118 |         optimizations = []
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/dq/performance_optimizer.py:116:58
    |
114 |         }
115 |     
116 |     def _check_column_types(self, df: DataFrame) -> List[Dict[str, Any]]:
    |                                                          ^^^^
117 |         """Check for column type optimizations."""
118 |         optimizations = []
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/performance_optimizer.py:119:1
    |
117 |         """Check for column type optimizations."""
118 |         optimizations = []
119 |         
    | ^^^^^^^^
120 |         for field in df.schema.fields:
121 |             field_type = str(field.dataType)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/performance_optimizer.py:122:1
    |
120 |         for field in df.schema.fields:
121 |             field_type = str(field.dataType)
122 |             
    | ^^^^^^^^^^^^
123 |             # Check for string columns that could be more efficient
124 |             if "StringType" in field_type:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/performance_optimizer.py:132:1
    |
130 |                         "recommendation": "Consider using numeric or date type if applicable"
131 |                     })
132 |             
    | ^^^^^^^^^^^^
133 |             # Check for large decimal precision
134 |             if "DecimalType" in field_type:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/performance_optimizer.py:140:1
    |
138 |                     "recommendation": "Verify decimal precision is necessary"
139 |                 })
140 |         
    | ^^^^^^^^
141 |         return optimizations
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/performance_optimizer.py:142:1
    |
141 |         return optimizations
142 |     
    | ^^^^
143 |     def _check_partitioning(
144 |         self,
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/dq/performance_optimizer.py:147:10
    |
145 |         df: DataFrame,
146 |         table_name: str
147 |     ) -> Dict[str, Any]:
    |          ^^^^
148 |         """Check if table is properly partitioned."""
149 |         result = {
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/performance_optimizer.py:155:1
    |
153 |             "recommendation": None
154 |         }
155 |         
    | ^^^^^^^^
156 |         # Check for date columns suitable for partitioning
157 |         date_columns = [
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/performance_optimizer.py:161:1
    |
159 |             if "date" in col.lower() or "timestamp" in col.lower()
160 |         ]
161 |         
    | ^^^^^^^^
162 |         if date_columns:
163 |             result["has_date_column"] = True
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/performance_optimizer.py:169:1
    |
167 |         else:
168 |             result["recommendation"] = "Consider adding date column for partitioning"
169 |         
    | ^^^^^^^^
170 |         return result
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/performance_optimizer.py:171:1
    |
170 |         return result
171 |     
    | ^^^^
172 |     def get_recommendations(self) -> List[str]:
173 |         """Get all performance recommendations."""
    |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/dq/performance_optimizer.py:172:38
    |
170 |         return result
171 |     
172 |     def get_recommendations(self) -> List[str]:
    |                                      ^^^^
173 |         """Get all performance recommendations."""
174 |         return self.recommendations
    |
help: Replace with `list`

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/performance_optimizer.py:175:1
    |
173 |         """Get all performance recommendations."""
174 |         return self.recommendations
175 |     
    | ^^^^
176 |     def generate_report(self) -> str:
177 |         """Generate performance optimization report."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/performance_optimizer.py:179:1
    |
177 |         """Generate performance optimization report."""
178 |         report = ["ðŸš€ Performance Optimization Report", "=" * 50, ""]
179 |         
    | ^^^^^^^^
180 |         if self.recommendations:
181 |             report.append("Recommendations:")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/performance_optimizer.py:186:1
    |
184 |         else:
185 |             report.append("âœ… No performance optimizations needed.")
186 |         
    | ^^^^^^^^
187 |         return "\n".join(report)
    |
help: Remove whitespace from blank line

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/dq/referential_integrity.py:11:1
   |
 9 | - Kafka events reference real customer IDs
10 | """
11 | from typing import Dict, List, Any, Optional, Tuple
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
12 | from pyspark.sql import SparkSession, DataFrame
13 | from pyspark.sql import functions as F
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/dq/referential_integrity.py:11:1
   |
 9 | - Kafka events reference real customer IDs
10 | """
11 | from typing import Dict, List, Any, Optional, Tuple
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
12 | from pyspark.sql import SparkSession, DataFrame
13 | from pyspark.sql import functions as F
   |

UP035 `typing.Tuple` is deprecated, use `tuple` instead
  --> src/project_a/dq/referential_integrity.py:11:1
   |
 9 | - Kafka events reference real customer IDs
10 | """
11 | from typing import Dict, List, Any, Optional, Tuple
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
12 | from pyspark.sql import SparkSession, DataFrame
13 | from pyspark.sql import functions as F
   |

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/dq/referential_integrity.py:11:1
   |
 9 |   - Kafka events reference real customer IDs
10 |   """
11 | / from typing import Dict, List, Any, Optional, Tuple
12 | | from pyspark.sql import SparkSession, DataFrame
13 | | from pyspark.sql import functions as F
14 | | import logging
   | |______________^
15 |
16 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

F401 [*] `typing.Optional` imported but unused
  --> src/project_a/dq/referential_integrity.py:11:37
   |
 9 | - Kafka events reference real customer IDs
10 | """
11 | from typing import Dict, List, Any, Optional, Tuple
   |                                     ^^^^^^^^
12 | from pyspark.sql import SparkSession, DataFrame
13 | from pyspark.sql import functions as F
   |
help: Remove unused import

F401 [*] `typing.Tuple` imported but unused
  --> src/project_a/dq/referential_integrity.py:11:47
   |
 9 | - Kafka events reference real customer IDs
10 | """
11 | from typing import Dict, List, Any, Optional, Tuple
   |                                               ^^^^^
12 | from pyspark.sql import SparkSession, DataFrame
13 | from pyspark.sql import functions as F
   |
help: Remove unused import

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/referential_integrity.py:21:1
   |
19 | class ReferentialIntegrityChecker:
20 |     """Check referential integrity between related tables."""
21 |     
   | ^^^^
22 |     def __init__(self, spark: SparkSession):
23 |         self.spark = spark
   |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/dq/referential_integrity.py:24:22
   |
22 |     def __init__(self, spark: SparkSession):
23 |         self.spark = spark
24 |         self.issues: List[Dict[str, Any]] = []
   |                      ^^^^
25 |     
26 |     def check_foreign_key(
   |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/dq/referential_integrity.py:24:27
   |
22 |     def __init__(self, spark: SparkSession):
23 |         self.spark = spark
24 |         self.issues: List[Dict[str, Any]] = []
   |                           ^^^^
25 |     
26 |     def check_foreign_key(
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/referential_integrity.py:25:1
   |
23 |         self.spark = spark
24 |         self.issues: List[Dict[str, Any]] = []
25 |     
   | ^^^^
26 |     def check_foreign_key(
27 |         self,
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/dq/referential_integrity.py:34:10
   |
32 |         relationship_name: str,
33 |         allow_null: bool = False
34 |     ) -> Dict[str, Any]:
   |          ^^^^
35 |         """
36 |         Check that all foreign key values in fact table exist in dimension table.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/dq/referential_integrity.py:37:1
   |
35 |         """
36 |         Check that all foreign key values in fact table exist in dimension table.
37 |         
   | ^^^^^^^^
38 |         Args:
39 |             fact_df: Fact table DataFrame
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/dq/referential_integrity.py:45:1
   |
43 |             relationship_name: Descriptive name for the relationship
44 |             allow_null: Whether NULL foreign keys are allowed
45 |         
   | ^^^^^^^^
46 |         Returns:
47 |             Dictionary with validation results
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/referential_integrity.py:50:1
   |
48 |         """
49 |         logger.info(f"Checking referential integrity: {relationship_name}")
50 |         
   | ^^^^^^^^
51 |         # Get distinct foreign keys from fact table
52 |         fact_keys = fact_df.select(fact_key).distinct()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/referential_integrity.py:53:1
   |
51 |         # Get distinct foreign keys from fact table
52 |         fact_keys = fact_df.select(fact_key).distinct()
53 |         
   | ^^^^^^^^
54 |         if allow_null:
55 |             fact_keys = fact_keys.filter(F.col(fact_key).isNotNull())
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/referential_integrity.py:56:1
   |
54 |         if allow_null:
55 |             fact_keys = fact_keys.filter(F.col(fact_key).isNotNull())
56 |         
   | ^^^^^^^^
57 |         # Get distinct primary keys from dimension table
58 |         dim_keys = dim_df.select(dim_key).distinct()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/referential_integrity.py:59:1
   |
57 |         # Get distinct primary keys from dimension table
58 |         dim_keys = dim_df.select(dim_key).distinct()
59 |         
   | ^^^^^^^^
60 |         # Find orphaned foreign keys
61 |         orphaned = fact_keys.join(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/referential_integrity.py:66:1
   |
64 |             "left_anti"
65 |         )
66 |         
   | ^^^^^^^^
67 |         orphaned_count = orphaned.count()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/referential_integrity.py:68:1
   |
67 |         orphaned_count = orphaned.count()
68 |         
   | ^^^^^^^^
69 |         result = {
70 |             "relationship": relationship_name,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/referential_integrity.py:77:1
   |
75 |             "orphaned_keys": []
76 |         }
77 |         
   | ^^^^^^^^
78 |         if orphaned_count > 0:
79 |             # Sample orphaned keys (limit to 100 for performance)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/referential_integrity.py:82:1
   |
80 |             orphaned_sample = orphaned.limit(100).collect()
81 |             result["orphaned_keys"] = [row[fact_key] for row in orphaned_sample]
82 |             
   | ^^^^^^^^^^^^
83 |             self.issues.append(result)
84 |             logger.warning(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/referential_integrity.py:90:1
   |
88 |         else:
89 |             logger.info(f"âœ… Referential integrity OK for {relationship_name}")
90 |         
   | ^^^^^^^^
91 |         return result
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/referential_integrity.py:92:1
   |
91 |         return result
92 |     
   | ^^^^
93 |     def check_orders_customers(
94 |         self,
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/dq/referential_integrity.py:97:10
   |
95 |         orders_df: DataFrame,
96 |         customers_df: DataFrame
97 |     ) -> Dict[str, Any]:
   |          ^^^^
98 |         """Check that all order.customer_id exist in customers."""
99 |         return self.check_foreign_key(
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/referential_integrity.py:106:1
    |
104 |             "orders â†’ customers"
105 |         )
106 |     
    | ^^^^
107 |     def check_orders_products(
108 |         self,
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/dq/referential_integrity.py:111:10
    |
109 |         orders_df: DataFrame,
110 |         products_df: DataFrame
111 |     ) -> Dict[str, Any]:
    |          ^^^^
112 |         """Check that all order.product_id exist in products."""
113 |         return self.check_foreign_key(
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/referential_integrity.py:120:1
    |
118 |             "orders â†’ products"
119 |         )
120 |     
    | ^^^^
121 |     def check_opportunities_accounts(
122 |         self,
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/dq/referential_integrity.py:125:10
    |
123 |         opportunities_df: DataFrame,
124 |         accounts_df: DataFrame
125 |     ) -> Dict[str, Any]:
    |          ^^^^
126 |         """Check that all opportunity.account_id exist in accounts."""
127 |         return self.check_foreign_key(
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/referential_integrity.py:134:1
    |
132 |             "opportunities â†’ accounts"
133 |         )
134 |     
    | ^^^^
135 |     def check_behavior_customers(
136 |         self,
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/dq/referential_integrity.py:139:10
    |
137 |         behavior_df: DataFrame,
138 |         customers_df: DataFrame
139 |     ) -> Dict[str, Any]:
    |          ^^^^
140 |         """Check that all behavior.customer_id exist in customers."""
141 |         return self.check_foreign_key(
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/referential_integrity.py:148:1
    |
146 |             "behavior â†’ customers"
147 |         )
148 |     
    | ^^^^
149 |     def check_kafka_customers(
150 |         self,
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/dq/referential_integrity.py:153:10
    |
151 |         kafka_df: DataFrame,
152 |         customers_df: DataFrame
153 |     ) -> Dict[str, Any]:
    |          ^^^^
154 |         """Check that all kafka event customer_ids exist in customers."""
155 |         return self.check_foreign_key(
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/referential_integrity.py:163:1
    |
161 |             allow_null=True  # Some events might not have customer_id
162 |         )
163 |     
    | ^^^^
164 |     def check_duplicate_ids(
165 |         self,
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/dq/referential_integrity.py:169:10
    |
167 |         id_column: str,
168 |         table_name: str
169 |     ) -> Dict[str, Any]:
    |          ^^^^
170 |         """Check for duplicate primary keys."""
171 |         logger.info(f"Checking for duplicate {id_column} in {table_name}")
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/referential_integrity.py:172:1
    |
170 |         """Check for duplicate primary keys."""
171 |         logger.info(f"Checking for duplicate {id_column} in {table_name}")
172 |         
    | ^^^^^^^^
173 |         duplicates = df.groupBy(id_column).count().filter(F.col("count") > 1)
174 |         duplicate_count = duplicates.count()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/referential_integrity.py:175:1
    |
173 |         duplicates = df.groupBy(id_column).count().filter(F.col("count") > 1)
174 |         duplicate_count = duplicates.count()
175 |         
    | ^^^^^^^^
176 |         result = {
177 |             "table": table_name,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/referential_integrity.py:183:1
    |
181 |             "duplicate_ids": []
182 |         }
183 |         
    | ^^^^^^^^
184 |         if duplicate_count > 0:
185 |             duplicate_sample = duplicates.limit(100).collect()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/referential_integrity.py:190:1
    |
188 |                 for row in duplicate_sample
189 |             ]
190 |             
    | ^^^^^^^^^^^^
191 |             self.issues.append(result)
192 |             logger.warning(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/referential_integrity.py:197:1
    |
195 |         else:
196 |             logger.info(f"âœ… No duplicate {id_column} in {table_name}")
197 |         
    | ^^^^^^^^
198 |         return result
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/referential_integrity.py:199:1
    |
198 |         return result
199 |     
    | ^^^^
200 |     def get_all_issues(self) -> List[Dict[str, Any]]:
201 |         """Get all detected referential integrity issues."""
    |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/dq/referential_integrity.py:200:33
    |
198 |         return result
199 |     
200 |     def get_all_issues(self) -> List[Dict[str, Any]]:
    |                                 ^^^^
201 |         """Get all detected referential integrity issues."""
202 |         return self.issues
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/dq/referential_integrity.py:200:38
    |
198 |         return result
199 |     
200 |     def get_all_issues(self) -> List[Dict[str, Any]]:
    |                                      ^^^^
201 |         """Get all detected referential integrity issues."""
202 |         return self.issues
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/referential_integrity.py:203:1
    |
201 |         """Get all detected referential integrity issues."""
202 |         return self.issues
203 |     
    | ^^^^
204 |     def generate_report(self) -> str:
205 |         """Generate a human-readable report of all issues."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/referential_integrity.py:208:1
    |
206 |         if not self.issues:
207 |             return "âœ… No referential integrity issues detected."
208 |         
    | ^^^^^^^^
209 |         report = ["ðŸ” Referential Integrity Report", "=" * 50, ""]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/referential_integrity.py:210:1
    |
209 |         report = ["ðŸ” Referential Integrity Report", "=" * 50, ""]
210 |         
    | ^^^^^^^^
211 |         for issue in self.issues:
212 |             if "relationship" in issue:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/referential_integrity.py:223:1
    |
221 |                     report.append(f"  Sample: {[d['id'] for d in issue['duplicate_ids'][:10]]}")
222 |             report.append("")
223 |         
    | ^^^^^^^^
224 |         return "\n".join(report)
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/dq/run_ge.py:5:1
   |
 3 |   """
 4 |
 5 | / import logging
 6 | | from typing import Any, Optional, Dict, List
 7 | |
 8 | | from great_expectations.checkpoint import SimpleCheckpoint
 9 | | from great_expectations.data_context import get_context
   | |_______________________________________________________^
10 |
11 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
 --> src/project_a/dq/run_ge.py:6:1
  |
5 | import logging
6 | from typing import Any, Optional, Dict, List
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
7 |
8 | from great_expectations.checkpoint import SimpleCheckpoint
  |

UP035 `typing.List` is deprecated, use `list` instead
 --> src/project_a/dq/run_ge.py:6:1
  |
5 | import logging
6 | from typing import Any, Optional, Dict, List
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
7 |
8 | from great_expectations.checkpoint import SimpleCheckpoint
  |

F401 [*] `typing.List` imported but unused
 --> src/project_a/dq/run_ge.py:6:41
  |
5 | import logging
6 | from typing import Any, Optional, Dict, List
  |                                         ^^^^
7 |
8 | from great_expectations.checkpoint import SimpleCheckpoint
  |
help: Remove unused import: `typing.List`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/dq/run_ge.py:14:56
   |
14 | def run_checkpoint(checkpoint_name: str, context_path: Optional[str] = None) -> Dict[str, Any]:
   |                                                        ^^^^^^^^^^^^^
15 |     """
16 |     Run Great Expectations checkpoint.
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/dq/run_ge.py:14:81
   |
14 | def run_checkpoint(checkpoint_name: str, context_path: Optional[str] = None) -> Dict[str, Any]:
   |                                                                                 ^^^^
15 |     """
16 |     Run Great Expectations checkpoint.
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/dq/run_ge.py:47:68
   |
47 | def run_contract_validation(contract_path: str, data_path: str) -> Dict[str, Any]:
   |                                                                    ^^^^
48 |     """
49 |     Run contract-based validation using Great Expectations.
   |
help: Replace with `dict`

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/dq/schema_drift_checker.py:12:1
   |
10 | - ID pattern consistency
11 | """
12 | from typing import Dict, List, Any, Optional, Tuple
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
13 | from pyspark.sql import SparkSession, DataFrame
14 | from pyspark.sql.types import StructType, StructField, DataType
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/dq/schema_drift_checker.py:12:1
   |
10 | - ID pattern consistency
11 | """
12 | from typing import Dict, List, Any, Optional, Tuple
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
13 | from pyspark.sql import SparkSession, DataFrame
14 | from pyspark.sql.types import StructType, StructField, DataType
   |

UP035 `typing.Tuple` is deprecated, use `tuple` instead
  --> src/project_a/dq/schema_drift_checker.py:12:1
   |
10 | - ID pattern consistency
11 | """
12 | from typing import Dict, List, Any, Optional, Tuple
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
13 | from pyspark.sql import SparkSession, DataFrame
14 | from pyspark.sql.types import StructType, StructField, DataType
   |

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/dq/schema_drift_checker.py:12:1
   |
10 |   - ID pattern consistency
11 |   """
12 | / from typing import Dict, List, Any, Optional, Tuple
13 | | from pyspark.sql import SparkSession, DataFrame
14 | | from pyspark.sql.types import StructType, StructField, DataType
15 | | import logging
   | |______________^
16 |
17 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

F401 [*] `typing.Optional` imported but unused
  --> src/project_a/dq/schema_drift_checker.py:12:37
   |
10 | - ID pattern consistency
11 | """
12 | from typing import Dict, List, Any, Optional, Tuple
   |                                     ^^^^^^^^
13 | from pyspark.sql import SparkSession, DataFrame
14 | from pyspark.sql.types import StructType, StructField, DataType
   |
help: Remove unused import

F401 [*] `typing.Tuple` imported but unused
  --> src/project_a/dq/schema_drift_checker.py:12:47
   |
10 | - ID pattern consistency
11 | """
12 | from typing import Dict, List, Any, Optional, Tuple
   |                                               ^^^^^
13 | from pyspark.sql import SparkSession, DataFrame
14 | from pyspark.sql.types import StructType, StructField, DataType
   |
help: Remove unused import

F401 [*] `pyspark.sql.types.StructField` imported but unused
  --> src/project_a/dq/schema_drift_checker.py:14:43
   |
12 | from typing import Dict, List, Any, Optional, Tuple
13 | from pyspark.sql import SparkSession, DataFrame
14 | from pyspark.sql.types import StructType, StructField, DataType
   |                                           ^^^^^^^^^^^
15 | import logging
   |
help: Remove unused import

F401 [*] `pyspark.sql.types.DataType` imported but unused
  --> src/project_a/dq/schema_drift_checker.py:14:56
   |
12 | from typing import Dict, List, Any, Optional, Tuple
13 | from pyspark.sql import SparkSession, DataFrame
14 | from pyspark.sql.types import StructType, StructField, DataType
   |                                                        ^^^^^^^^
15 | import logging
   |
help: Remove unused import

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/schema_drift_checker.py:22:1
   |
20 | class SchemaDriftChecker:
21 |     """Check for schema drift in DataFrames."""
22 |     
   | ^^^^
23 |     def __init__(self, spark: SparkSession):
24 |         self.spark = spark
   |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/dq/schema_drift_checker.py:25:22
   |
23 |     def __init__(self, spark: SparkSession):
24 |         self.spark = spark
25 |         self.issues: List[Dict[str, Any]] = []
   |                      ^^^^
26 |     
27 |     def compare_schemas(
   |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/dq/schema_drift_checker.py:25:27
   |
23 |     def __init__(self, spark: SparkSession):
24 |         self.spark = spark
25 |         self.issues: List[Dict[str, Any]] = []
   |                           ^^^^
26 |     
27 |     def compare_schemas(
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/schema_drift_checker.py:26:1
   |
24 |         self.spark = spark
25 |         self.issues: List[Dict[str, Any]] = []
26 |     
   | ^^^^
27 |     def compare_schemas(
28 |         self,
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/dq/schema_drift_checker.py:33:10
   |
31 |         table_name: str,
32 |         strict: bool = True
33 |     ) -> Dict[str, Any]:
   |          ^^^^
34 |         """
35 |         Compare actual schema against expected schema.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/dq/schema_drift_checker.py:36:1
   |
34 |         """
35 |         Compare actual schema against expected schema.
36 |         
   | ^^^^^^^^
37 |         Args:
38 |             actual: Actual schema from DataFrame
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/dq/schema_drift_checker.py:42:1
   |
40 |             table_name: Name of table being checked
41 |             strict: If True, fail on any drift. If False, warn only.
42 |         
   | ^^^^^^^^
43 |         Returns:
44 |             Dictionary with comparison results
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/schema_drift_checker.py:48:1
   |
46 |         actual_fields = {f.name: f for f in actual.fields}
47 |         expected_fields = {f.name: f for f in expected.fields}
48 |         
   | ^^^^^^^^
49 |         result = {
50 |             "table": table_name,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/schema_drift_checker.py:58:1
   |
56 |             "id_pattern_issues": []
57 |         }
58 |         
   | ^^^^^^^^
59 |         # Check for missing columns
60 |         for col_name, expected_field in expected_fields.items():
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/schema_drift_checker.py:68:1
   |
66 |                 })
67 |                 result["drift_detected"] = True
68 |         
   | ^^^^^^^^
69 |         # Check for new columns
70 |         for col_name, actual_field in actual_fields.items():
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/schema_drift_checker.py:79:1
   |
77 |                 if strict:
78 |                     result["drift_detected"] = True
79 |         
   | ^^^^^^^^
80 |         # Check for type changes
81 |         for col_name in set(actual_fields.keys()) & set(expected_fields.keys()):
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/schema_drift_checker.py:84:1
   |
82 |             actual_field = actual_fields[col_name]
83 |             expected_field = expected_fields[col_name]
84 |             
   | ^^^^^^^^^^^^
85 |             if str(actual_field.dataType) != str(expected_field.dataType):
86 |                 result["type_changes"].append({
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/dq/schema_drift_checker.py:92:1
   |
90 |                 })
91 |                 result["drift_detected"] = True
92 |             
   | ^^^^^^^^^^^^
93 |             if actual_field.nullable != expected_field.nullable:
94 |                 result["nullability_changes"].append({
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/schema_drift_checker.py:101:1
    |
 99 |                 if strict:
100 |                     result["drift_detected"] = True
101 |         
    | ^^^^^^^^
102 |         # Check ID pattern consistency
103 |         id_columns = [col for col in actual_fields.keys() if col.endswith("_id")]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/schema_drift_checker.py:111:1
    |
109 |                     result["id_pattern_issues"].extend(pattern_issues)
110 |                     result["drift_detected"] = True
111 |         
    | ^^^^^^^^
112 |         if result["drift_detected"]:
113 |             self.issues.append(result)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/schema_drift_checker.py:115:1
    |
113 |             self.issues.append(result)
114 |             logger.warning(f"Schema drift detected in {table_name}: {result}")
115 |         
    | ^^^^^^^^
116 |         return result
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/schema_drift_checker.py:117:1
    |
116 |         return result
117 |     
    | ^^^^
118 |     def _check_id_pattern(self, schema: StructType, id_column: str) -> List[Dict[str, Any]]:
119 |         """Check if ID column values match expected pattern."""
    |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/dq/schema_drift_checker.py:118:72
    |
116 |         return result
117 |     
118 |     def _check_id_pattern(self, schema: StructType, id_column: str) -> List[Dict[str, Any]]:
    |                                                                        ^^^^
119 |         """Check if ID column values match expected pattern."""
120 |         # This would require reading data, so we'll return empty for now
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/dq/schema_drift_checker.py:118:77
    |
116 |         return result
117 |     
118 |     def _check_id_pattern(self, schema: StructType, id_column: str) -> List[Dict[str, Any]]:
    |                                                                             ^^^^
119 |         """Check if ID column values match expected pattern."""
120 |         # This would require reading data, so we'll return empty for now
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/schema_drift_checker.py:123:1
    |
121 |         # In practice, you'd sample the data and check patterns
122 |         return []
123 |     
    | ^^^^
124 |     def validate_dataframe(
125 |         self,
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/dq/schema_drift_checker.py:130:10
    |
128 |         table_name: str,
129 |         strict: bool = True
130 |     ) -> Dict[str, Any]:
    |          ^^^^
131 |         """Validate a DataFrame against expected schema."""
132 |         actual_schema = df.schema
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/schema_drift_checker.py:134:1
    |
132 |         actual_schema = df.schema
133 |         return self.compare_schemas(actual_schema, expected_schema, table_name, strict)
134 |     
    | ^^^^
135 |     def get_all_issues(self) -> List[Dict[str, Any]]:
136 |         """Get all detected schema drift issues."""
    |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/dq/schema_drift_checker.py:135:33
    |
133 |         return self.compare_schemas(actual_schema, expected_schema, table_name, strict)
134 |     
135 |     def get_all_issues(self) -> List[Dict[str, Any]]:
    |                                 ^^^^
136 |         """Get all detected schema drift issues."""
137 |         return self.issues
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/dq/schema_drift_checker.py:135:38
    |
133 |         return self.compare_schemas(actual_schema, expected_schema, table_name, strict)
134 |     
135 |     def get_all_issues(self) -> List[Dict[str, Any]]:
    |                                      ^^^^
136 |         """Get all detected schema drift issues."""
137 |         return self.issues
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/schema_drift_checker.py:138:1
    |
136 |         """Get all detected schema drift issues."""
137 |         return self.issues
138 |     
    | ^^^^
139 |     def generate_report(self) -> str:
140 |         """Generate a human-readable report of all issues."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/schema_drift_checker.py:143:1
    |
141 |         if not self.issues:
142 |             return "âœ… No schema drift detected."
143 |         
    | ^^^^^^^^
144 |         report = ["ðŸ” Schema Drift Report", "=" * 50, ""]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/schema_drift_checker.py:145:1
    |
144 |         report = ["ðŸ” Schema Drift Report", "=" * 50, ""]
145 |         
    | ^^^^^^^^
146 |         for issue in self.issues:
147 |             report.append(f"Table: {issue['table']}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/schema_drift_checker.py:149:1
    |
147 |             report.append(f"Table: {issue['table']}")
148 |             report.append("-" * 30)
149 |             
    | ^^^^^^^^^^^^
150 |             if issue["missing_columns"]:
151 |                 report.append("âŒ Missing Columns:")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/schema_drift_checker.py:154:1
    |
152 |                 for col in issue["missing_columns"]:
153 |                     report.append(f"  - {col['column']} ({col['expected_type']})")
154 |             
    | ^^^^^^^^^^^^
155 |             if issue["new_columns"]:
156 |                 report.append("âš ï¸  New Columns:")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/schema_drift_checker.py:159:1
    |
157 |                 for col in issue["new_columns"]:
158 |                     report.append(f"  - {col['column']} ({col['actual_type']})")
159 |             
    | ^^^^^^^^^^^^
160 |             if issue["type_changes"]:
161 |                 report.append("âŒ Type Changes:")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/schema_drift_checker.py:164:1
    |
162 |                 for col in issue["type_changes"]:
163 |                     report.append(f"  - {col['column']}: {col['expected']} â†’ {col['actual']}")
164 |             
    | ^^^^^^^^^^^^
165 |             if issue["nullability_changes"]:
166 |                 report.append("âš ï¸  Nullability Changes:")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/schema_drift_checker.py:169:1
    |
167 |                 for col in issue["nullability_changes"]:
168 |                     report.append(f"  - {col['column']}: nullable={col['expected_nullable']} â†’ {col['actual_nullable']}")
169 |             
    | ^^^^^^^^^^^^
170 |             report.append("")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/dq/schema_drift_checker.py:171:1
    |
170 |             report.append("")
171 |         
    | ^^^^^^^^
172 |         return "\n".join(report)
    |
help: Remove whitespace from blank line

F401 [*] `typing.Any` imported but unused
  --> src/project_a/extract/fx_json_reader.py:11:20
   |
 9 | import logging
10 | from pathlib import Path
11 | from typing import Any, Optional, Union
   |                    ^^^
12 |
13 | from pyspark.sql import DataFrame, SparkSession
   |
help: Remove unused import

F401 [*] `typing.Optional` imported but unused
  --> src/project_a/extract/fx_json_reader.py:11:25
   |
 9 | import logging
10 | from pathlib import Path
11 | from typing import Any, Optional, Union
   |                         ^^^^^^^^
12 |
13 | from pyspark.sql import DataFrame, SparkSession
   |
help: Remove unused import

F401 [*] `typing.Union` imported but unused
  --> src/project_a/extract/fx_json_reader.py:11:35
   |
 9 | import logging
10 | from pathlib import Path
11 | from typing import Any, Optional, Union
   |                                   ^^^^^
12 |
13 | from pyspark.sql import DataFrame, SparkSession
   |
help: Remove unused import

W293 [*] Blank line contains whitespace
   --> src/project_a/extract/fx_json_reader.py:135:1
    |
133 |     """
134 |     from project_a.schemas.bronze_schemas import FX_RATES_SCHEMA
135 |     
    | ^^^^
136 |     # Handle both config dict and string path
137 |     if isinstance(bronze_root_or_config, dict):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/extract/fx_json_reader.py:158:1
    |
156 |         # Add /fx suffix
157 |         fx_base = f"{bronze_root.rstrip('/')}/fx"
158 |     
    | ^^^^
159 |     # Try multiple possible JSON locations
160 |     json_paths = [
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/extract/fx_json_reader.py:186:1
    |
184 |                 logger.debug("JSON not found at %s: %s", json_path, e2)
185 |                 continue
186 |         
    | ^^^^^^^^
187 |         # If JSON failed, try CSV
188 |         if df_raw is None:
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/io/__init__.py:10:1
   |
 8 |   """
 9 |
10 | / from project_a.io.reader import (
11 | |     read_bronze_table,
12 | |     read_csv_with_schema,
13 | |     read_json_with_schema,
14 | |     read_delta_table,
15 | | )
16 | | from project_a.io.writer import (
17 | |     write_silver_table,
18 | |     write_gold_table,
19 | |     validate_schema_before_write,
20 | | )
   | |_^
21 |
22 |   __all__ = [
   |
help: Organize imports

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/io/reader.py:12:1
   |
10 |   """
11 |
12 | / import logging
13 | | from typing import Optional, Dict, Any
14 | |
15 | | from pyspark.sql import DataFrame, SparkSession
16 | | from pyspark.sql.types import StructType
   | |________________________________________^
17 |
18 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/io/reader.py:13:1
   |
12 | import logging
13 | from typing import Optional, Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
14 |
15 | from pyspark.sql import DataFrame, SparkSession
   |

F401 [*] `typing.Dict` imported but unused
  --> src/project_a/io/reader.py:13:30
   |
12 | import logging
13 | from typing import Optional, Dict, Any
   |                              ^^^^
14 |
15 | from pyspark.sql import DataFrame, SparkSession
   |
help: Remove unused import

F401 [*] `typing.Any` imported but unused
  --> src/project_a/io/reader.py:13:36
   |
12 | import logging
13 | from typing import Optional, Dict, Any
   |                                    ^^^
14 |
15 | from pyspark.sql import DataFrame, SparkSession
   |
help: Remove unused import

W293 Blank line contains whitespace
  --> src/project_a/io/reader.py:29:1
   |
27 |     """
28 |     Read a bronze table with automatic format detection.
29 |     
   | ^^^^
30 |     Tries: Delta -> Parquet -> CSV -> JSON
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/io/reader.py:31:1
   |
30 |     Tries: Delta -> Parquet -> CSV -> JSON
31 |     
   | ^^^^
32 |     Args:
33 |         spark: SparkSession
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/io/reader.py:37:1
   |
35 |         schema: Expected schema
36 |         format: Force format (delta, parquet, csv, json) or "auto" for detection
37 |     
   | ^^^^
38 |     Returns:
39 |         DataFrame with schema applied
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/io/reader.py:54:1
   |
52 |         except Exception as e:
53 |             logger.debug(f"Delta read failed: {e}")
54 |         
   | ^^^^^^^^
55 |         # Try Parquet
56 |         try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/io/reader.py:66:1
   |
64 |         except Exception as e:
65 |             logger.debug(f"Parquet read failed: {e}")
66 |         
   | ^^^^^^^^
67 |         # Try CSV
68 |         try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/io/reader.py:74:1
   |
72 |         except Exception as e:
73 |             logger.debug(f"CSV read failed: {e}")
74 |         
   | ^^^^^^^^
75 |         # Try JSON
76 |         try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/io/reader.py:82:1
   |
80 |         except Exception as e:
81 |             logger.debug(f"JSON read failed: {e}")
82 |         
   | ^^^^^^^^
83 |         # All formats failed - return empty DataFrame with schema
84 |         logger.warning(f"âš ï¸ All read attempts failed for {path}, returning empty DataFrame")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/io/reader.py:86:1
   |
84 |         logger.warning(f"âš ï¸ All read attempts failed for {path}, returning empty DataFrame")
85 |         return spark.createDataFrame([], schema)
86 |     
   | ^^^^
87 |     # Force format
88 |     if format == "delta":
   |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/io/reader.py:137:13
    |
135 |     spark: SparkSession,
136 |     path: str,
137 |     schema: Optional[StructType] = None,
    |             ^^^^^^^^^^^^^^^^^^^^
138 | ) -> DataFrame:
139 |     """Read Delta table."""
    |
help: Convert to `X | None`

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/io/writer.py:9:1
   |
 7 |   """
 8 |
 9 | / import logging
10 | | from typing import Optional, List
11 | |
12 | | from pyspark.sql import DataFrame, SparkSession
13 | | from pyspark.sql.types import StructType
14 | |
15 | | from project_a.utils.contracts import validate_dataframe_schema
   | |_______________________________________________________________^
16 |
17 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/io/writer.py:10:1
   |
 9 | import logging
10 | from typing import Optional, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
11 |
12 | from pyspark.sql import DataFrame, SparkSession
   |

W293 Blank line contains whitespace
  --> src/project_a/io/writer.py:28:1
   |
26 |     """
27 |     Validate DataFrame schema before writing.
28 |     
   | ^^^^
29 |     Args:
30 |         df: DataFrame to validate
   |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/io/writer.py:43:21
   |
41 |     path: str,
42 |     table_name: str,
43 |     partition_cols: Optional[List[str]] = None,
   |                     ^^^^^^^^^^^^^^^^^^^
44 |     expected_schema: Optional[StructType] = None,
45 |     mode: str = "overwrite",
   |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/io/writer.py:43:30
   |
41 |     path: str,
42 |     table_name: str,
43 |     partition_cols: Optional[List[str]] = None,
   |                              ^^^^
44 |     expected_schema: Optional[StructType] = None,
45 |     mode: str = "overwrite",
   |
help: Replace with `list`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/io/writer.py:44:22
   |
42 |     table_name: str,
43 |     partition_cols: Optional[List[str]] = None,
44 |     expected_schema: Optional[StructType] = None,
   |                      ^^^^^^^^^^^^^^^^^^^^
45 |     mode: str = "overwrite",
46 | ) -> None:
   |
help: Convert to `X | None`

W293 Blank line contains whitespace
  --> src/project_a/io/writer.py:49:1
   |
47 |     """
48 |     Write DataFrame to Silver layer (Delta or Parquet).
49 |     
   | ^^^^
50 |     Args:
51 |         spark: SparkSession
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/io/writer.py:62:1
   |
60 |     if expected_schema:
61 |         validate_schema_before_write(df, expected_schema, table_name, fail_on_mismatch=True)
62 |     
   | ^^^^
63 |     # Check if Delta Lake is available
64 |     try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/io/writer.py:69:1
   |
67 |     except Exception:
68 |         has_delta = False
69 |     
   | ^^^^
70 |     writer = df.write.mode(mode)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/io/writer.py:71:1
   |
70 |     writer = df.write.mode(mode)
71 |     
   | ^^^^
72 |     if partition_cols:
73 |         # Check that partition columns exist
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/io/writer.py:79:1
   |
77 |         else:
78 |             writer = writer.partitionBy(*partition_cols)
79 |     
   | ^^^^
80 |     if has_delta:
81 |         writer.format("delta").save(path)
   |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/io/writer.py:93:21
   |
91 |     path: str,
92 |     table_name: str,
93 |     partition_cols: Optional[List[str]] = None,
   |                     ^^^^^^^^^^^^^^^^^^^
94 |     expected_schema: Optional[StructType] = None,
95 |     mode: str = "overwrite",
   |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/io/writer.py:93:30
   |
91 |     path: str,
92 |     table_name: str,
93 |     partition_cols: Optional[List[str]] = None,
   |                              ^^^^
94 |     expected_schema: Optional[StructType] = None,
95 |     mode: str = "overwrite",
   |
help: Replace with `list`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/io/writer.py:94:22
   |
92 |     table_name: str,
93 |     partition_cols: Optional[List[str]] = None,
94 |     expected_schema: Optional[StructType] = None,
   |                      ^^^^^^^^^^^^^^^^^^^^
95 |     mode: str = "overwrite",
96 | ) -> None:
   |
help: Convert to `X | None`

W293 Blank line contains whitespace
   --> src/project_a/io/writer.py:99:1
    |
 97 |     """
 98 |     Write DataFrame to Gold layer (Delta or Parquet).
 99 |     
    | ^^^^
100 |     Same as write_silver_table but with Gold-specific logging.
101 |     """
    |
help: Remove whitespace from blank line

F841 Local variable `run_id` is assigned to but never used
  --> src/project_a/jobs/publish_gold_to_snowflake.py:76:5
   |
74 |         config["environment"] = "emr"
75 |
76 |     run_id = str(uuid.uuid4())
   |     ^^^^^^
77 |     start_time = time.time()
78 |     spark = build_spark(config)
   |
help: Remove assignment to unused variable `run_id`

F841 Local variable `start_time` is assigned to but never used
  --> src/project_a/jobs/publish_gold_to_snowflake.py:77:5
   |
76 |     run_id = str(uuid.uuid4())
77 |     start_time = time.time()
   |     ^^^^^^^^^^
78 |     spark = build_spark(config)
   |
help: Remove assignment to unused variable `start_time`

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/__main__.py:6:1
   |
 4 |   """
 5 |
 6 | / import os
 7 | | import sys
 8 | | import yaml
 9 | | import click
10 | | import time
11 | | from pathlib import Path
12 | |
13 | | from .logging_config import configure_logging, new_run_id, log_pipeline_event, log_metric
14 | | from .metrics import create_metrics
15 | | from .utils.spark_session import build_spark
16 | | from .pipeline import run_pipeline
17 | | from .lineage import emit
   | |_________________________^
   |
help: Organize imports

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/__main__.py:23:1
   |
21 |     """Load configuration based on environment"""
22 |     config_dir = Path("config")
23 |     
   | ^^^^
24 |     # Load default config
25 |     with open(config_dir / "default.yaml") as f:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/__main__.py:27:1
   |
25 |     with open(config_dir / "default.yaml") as f:
26 |         config = yaml.safe_load(f)
27 |     
   | ^^^^
28 |     # Overlay environment-specific config
29 |     env_config_path = config_dir / f"{env}.yaml"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/__main__.py:34:1
   |
32 |             env_config = yaml.safe_load(f)
33 |             config.update(env_config)
34 |     
   | ^^^^
35 |     # Override with environment variables
36 |     for key, value in os.environ.items():
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/__main__.py:40:1
   |
38 |             config_key = key[4:].lower()
39 |             config[config_key] = value
40 |     
   | ^^^^
41 |     return config
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/__main__.py:50:1
   |
48 | def main(proc_date: str, env: str, config_file: str):
49 |     """PySpark Data Engineering Pipeline"""
50 |     
   | ^^^^
51 |     # Generate run ID and configure logging
52 |     run_id = new_run_id()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/__main__.py:54:1
   |
52 |     run_id = new_run_id()
53 |     logger, _ = configure_logging(run_id)
54 |     
   | ^^^^
55 |     # Load configuration
56 |     if config_file:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/__main__.py:61:1
   |
59 |     else:
60 |         config = load_config(env)
61 |     
   | ^^^^
62 |     # Log pipeline start with enhanced context
63 |     log_pipeline_event(logger, "pipeline_started", "main", run_id, 
   |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
  --> src/project_a/legacy/__main__.py:63:67
   |
62 |     # Log pipeline start with enhanced context
63 |     log_pipeline_event(logger, "pipeline_started", "main", run_id, 
   |                                                                   ^
64 |                       env=env, proc_date=proc_date)
   |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/__main__.py:65:1
   |
63 |     log_pipeline_event(logger, "pipeline_started", "main", run_id, 
64 |                       env=env, proc_date=proc_date)
65 |     
   | ^^^^
66 |     # Create metrics sink with run_id
67 |     metrics = create_metrics(config, run_id)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/__main__.py:68:1
   |
66 |     # Create metrics sink with run_id
67 |     metrics = create_metrics(config, run_id)
68 |     
   | ^^^^
69 |     # Track pipeline start time
70 |     start_time = time.time()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/__main__.py:71:1
   |
69 |     # Track pipeline start time
70 |     start_time = time.time()
71 |     
   | ^^^^
72 |     # Emit lineage start event
73 |     emit({
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/__main__.py:80:1
   |
78 |         "outputs": []
79 |     }, config)
80 |     
   | ^^^^
81 |     try:
82 |         # Build Spark session
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/__main__.py:86:1
   |
84 |         spark = build_spark(config)
85 |         log_pipeline_event(logger, "spark_session_ready", "setup", run_id)
86 |         
   | ^^^^^^^^
87 |         # Run pipeline
88 |         log_pipeline_event(logger, "pipeline_execution_started", "execution", run_id)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/__main__.py:90:1
   |
88 |         log_pipeline_event(logger, "pipeline_execution_started", "execution", run_id)
89 |         run_pipeline(spark, config, run_id)
90 |         
   | ^^^^^^^^
91 |         # Calculate and log duration
92 |         duration = time.time() - start_time
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/__main__.py:95:1
   |
93 |         log_metric(logger, "pipeline.duration.seconds", duration, run_id)
94 |         metrics.timing("pipeline.duration", duration * 1000, {"stage": "total"})
95 |         
   | ^^^^^^^^
96 |         # Emit lineage complete event
97 |         emit({
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/__main__.py:104:1
    |
102 |             "outputs": []
103 |         }, config)
104 |         
    | ^^^^^^^^
105 |         log_pipeline_event(logger, "pipeline_completed_successfully", "completion", run_id,
106 |                           duration_seconds=duration, status="success")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/__main__.py:107:1
    |
105 |         log_pipeline_event(logger, "pipeline_completed_successfully", "completion", run_id,
106 |                           duration_seconds=duration, status="success")
107 |         
    | ^^^^^^^^
108 |     except Exception as e:
109 |         import traceback
    |
help: Remove whitespace from blank line

F401 [*] `traceback` imported but unused
   --> src/project_a/legacy/__main__.py:109:16
    |
108 |     except Exception as e:
109 |         import traceback
    |                ^^^^^^^^^
110 |         duration = time.time() - start_time
    |
help: Remove unused import: `traceback`

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/__main__.py:111:1
    |
109 |         import traceback
110 |         duration = time.time() - start_time
111 |         
    | ^^^^^^^^
112 |         # Log error with enhanced context
113 |         log_pipeline_event(logger, "pipeline_failed", "error", run_id,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/__main__.py:115:1
    |
113 |         log_pipeline_event(logger, "pipeline_failed", "error", run_id,
114 |                           error=str(e), duration_seconds=duration, status="failed")
115 |         
    | ^^^^^^^^
116 |         # Log error metrics
117 |         log_metric(logger, "pipeline.errors.count", 1, run_id)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/__main__.py:119:1
    |
117 |         log_metric(logger, "pipeline.errors.count", 1, run_id)
118 |         metrics.incr("pipeline.errors", 1, {"error_type": type(e).__name__})
119 |         
    | ^^^^^^^^
120 |         # Emit lineage fail event
121 |         emit({
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/__main__.py:129:1
    |
127 |             "errorMessage": str(e)
128 |         }, config)
129 |         
    | ^^^^^^^^
130 |         sys.exit(1)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/__main__.py:131:1
    |
130 |         sys.exit(1)
131 |     
    | ^^^^
132 |     finally:
133 |         if 'spark' in locals():
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/advanced_dq_monitoring.py:12:1
   |
10 |   """
11 |
12 | / import logging
13 | | import os
14 | | import json
15 | | import time
16 | | from datetime import datetime, timedelta
17 | | from typing import Any, Dict, List, Optional, Union
18 | | from enum import Enum
19 | | from dataclasses import dataclass, field
20 | |
21 | | from pyspark.sql import DataFrame, SparkSession
22 | | from pyspark.sql.functions import col, isnan
   | |____________________________________________^
23 |
24 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/legacy/advanced_dq_monitoring.py:17:1
   |
15 | import time
16 | from datetime import datetime, timedelta
17 | from typing import Any, Dict, List, Optional, Union
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
18 | from enum import Enum
19 | from dataclasses import dataclass, field
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/legacy/advanced_dq_monitoring.py:17:1
   |
15 | import time
16 | from datetime import datetime, timedelta
17 | from typing import Any, Dict, List, Optional, Union
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
18 | from enum import Enum
19 | from dataclasses import dataclass, field
   |

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/advanced_dq_monitoring.py:50:17
   |
48 |     severity: QualitySeverity
49 |     check_type: str
50 |     parameters: Dict[str, Any]
   |                 ^^^^
51 |     expected_result: Any
52 |     tolerance: Optional[float] = None
   |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/advanced_dq_monitoring.py:52:16
   |
50 |     parameters: Dict[str, Any]
51 |     expected_result: Any
52 |     tolerance: Optional[float] = None
   |                ^^^^^^^^^^^^^^^
53 |     enabled: bool = True
54 |     created_at: datetime = field(default_factory=datetime.now)
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/advanced_dq_monitoring.py:55:15
   |
53 |     enabled: bool = True
54 |     created_at: datetime = field(default_factory=datetime.now)
55 |     last_run: Optional[datetime] = None
   |               ^^^^^^^^^^^^^^^^^^
56 |     success_count: int = 0
57 |     failure_count: int = 0
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/advanced_dq_monitoring.py:67:16
   |
65 |     actual_result: Any
66 |     expected_result: Any
67 |     tolerance: Optional[float]
   |                ^^^^^^^^^^^^^^^
68 |     execution_time_ms: float
69 |     error_message: Optional[str] = None
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/advanced_dq_monitoring.py:69:20
   |
67 |     tolerance: Optional[float]
68 |     execution_time_ms: float
69 |     error_message: Optional[str] = None
   |                    ^^^^^^^^^^^^^
70 |     timestamp: datetime = field(default_factory=datetime.now)
71 |     metadata: Dict[str, Any] = field(default_factory=dict)
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/advanced_dq_monitoring.py:71:15
   |
69 |     error_message: Optional[str] = None
70 |     timestamp: datetime = field(default_factory=datetime.now)
71 |     metadata: Dict[str, Any] = field(default_factory=dict)
   |               ^^^^
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/advanced_dq_monitoring.py:79:53
   |
77 |     """
78 |
79 |     def __init__(self, spark: SparkSession, config: Dict[str, Any]):
   |                                                     ^^^^
80 |         self.spark = spark
81 |         self.config = config
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/advanced_dq_monitoring.py:82:30
   |
80 |         self.spark = spark
81 |         self.config = config
82 |         self.quality_checks: Dict[str, QualityCheck] = {}
   |                              ^^^^
83 |         self.quality_results: List[QualityResult] = []
84 |         self.sla_configs: Dict[str, Any] = {}
   |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/legacy/advanced_dq_monitoring.py:83:31
   |
81 |         self.config = config
82 |         self.quality_checks: Dict[str, QualityCheck] = {}
83 |         self.quality_results: List[QualityResult] = []
   |                               ^^^^
84 |         self.sla_configs: Dict[str, Any] = {}
85 |         self.alert_configs: Dict[str, Any] = {}
   |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/advanced_dq_monitoring.py:84:27
   |
82 |         self.quality_checks: Dict[str, QualityCheck] = {}
83 |         self.quality_results: List[QualityResult] = []
84 |         self.sla_configs: Dict[str, Any] = {}
   |                           ^^^^
85 |         self.alert_configs: Dict[str, Any] = {}
86 |         self._load_quality_configs()
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/advanced_dq_monitoring.py:85:29
   |
83 |         self.quality_results: List[QualityResult] = []
84 |         self.sla_configs: Dict[str, Any] = {}
85 |         self.alert_configs: Dict[str, Any] = {}
   |                             ^^^^
86 |         self._load_quality_configs()
   |
help: Replace with `dict`

UP007 [*] Use `X | Y` for type annotations
   --> src/project_a/legacy/advanced_dq_monitoring.py:175:38
    |
174 |     def create_range_check(self, table_name: str, column_name: str,
175 |                           min_value: Union[int, float], max_value: Union[int, float],
    |                                      ^^^^^^^^^^^^^^^^^
176 |                           severity: QualitySeverity = QualitySeverity.MEDIUM) -> QualityCheck:
177 |         """
    |
help: Convert to `X | Y`

UP007 [*] Use `X | Y` for type annotations
   --> src/project_a/legacy/advanced_dq_monitoring.py:175:68
    |
174 |     def create_range_check(self, table_name: str, column_name: str,
175 |                           min_value: Union[int, float], max_value: Union[int, float],
    |                                                                    ^^^^^^^^^^^^^^^^^
176 |                           severity: QualitySeverity = QualitySeverity.MEDIUM) -> QualityCheck:
177 |         """
    |
help: Convert to `X | Y`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/advanced_dq_monitoring.py:314:78
    |
312 |             return error_result
313 |
314 |     def _run_completeness_check(self, check: QualityCheck, df: DataFrame) -> Dict[str, Any]:
    |                                                                              ^^^^
315 |         """Run completeness check."""
316 |         try:
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/advanced_dq_monitoring.py:355:76
    |
353 |             }
354 |
355 |     def _run_uniqueness_check(self, check: QualityCheck, df: DataFrame) -> Dict[str, Any]:
    |                                                                            ^^^^
356 |         """Run uniqueness check."""
357 |         try:
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/advanced_dq_monitoring.py:396:71
    |
394 |             }
395 |
396 |     def _run_range_check(self, check: QualityCheck, df: DataFrame) -> Dict[str, Any]:
    |                                                                       ^^^^
397 |         """Run range check."""
398 |         try:
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/advanced_dq_monitoring.py:443:73
    |
441 |             }
442 |
443 |     def _run_pattern_check(self, check: QualityCheck, df: DataFrame) -> Dict[str, Any]:
    |                                                                         ^^^^
444 |         """Run pattern check."""
445 |         try:
    |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/legacy/advanced_dq_monitoring.py:490:73
    |
488 |             }
489 |
490 |     def run_all_quality_checks(self, df: DataFrame, table_name: str) -> List[QualityResult]:
    |                                                                         ^^^^
491 |         """
492 |         Run all quality checks for a table.
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/advanced_dq_monitoring.py:510:71
    |
508 |         return results
509 |
510 |     def check_sla_compliance(self, table_name: str, sla_name: str) -> Dict[str, Any]:
    |                                                                       ^^^^
511 |         """
512 |         Check SLA compliance for a table.
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/advanced_dq_monitoring.py:584:61
    |
582 |     def generate_quality_report(self, table_name: str = None,
583 |                               start_date: datetime = None,
584 |                               end_date: datetime = None) -> Dict[str, Any]:
    |                                                             ^^^^
585 |         """
586 |         Generate comprehensive quality report.
    |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/legacy/advanced_dq_monitoring.py:667:50
    |
665 |             return {"error": str(e)}
666 |
667 |     def _generate_recommendations(self, results: List[QualityResult]) -> List[str]:
    |                                                  ^^^^
668 |         """Generate recommendations based on quality results."""
669 |         recommendations = []
    |
help: Replace with `list`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/legacy/advanced_dq_monitoring.py:667:74
    |
665 |             return {"error": str(e)}
666 |
667 |     def _generate_recommendations(self, results: List[QualityResult]) -> List[str]:
    |                                                                          ^^^^
668 |         """Generate recommendations based on quality results."""
669 |         recommendations = []
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/advanced_dq_monitoring.py:777:62
    |
777 | def setup_advanced_data_quality(spark: SparkSession, config: Dict[str, Any]) -> AdvancedDataQualityManager:
    |                                                              ^^^^
778 |     """
779 |     Setup advanced data quality management for the project.
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/advanced_dq_monitoring.py:842:60
    |
841 | def run_quality_pipeline(dq_manager: AdvancedDataQualityManager,
842 |                         table_name: str, df: DataFrame) -> Dict[str, Any]:
    |                                                            ^^^^
843 |     """
844 |     Run complete quality pipeline for a table.
    |
help: Replace with `dict`

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/api/customer_api.py:7:1
   |
 5 |   """
 6 |
 7 | / import os
 8 | | import sys
 9 | | import logging
10 | | from pathlib import Path
11 | | from typing import Optional, List
12 | | from datetime import datetime
   | |_____________________________^
13 |
14 |   # Add project root
   |
help: Organize imports

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/legacy/api/customer_api.py:11:1
   |
 9 | import logging
10 | from pathlib import Path
11 | from typing import Optional, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
12 | from datetime import datetime
   |

F401 `fastapi.responses.JSONResponse` imported but unused; consider using `importlib.util.find_spec` to test for availability
  --> src/project_a/legacy/api/customer_api.py:19:35
   |
17 | try:
18 |     from fastapi import FastAPI, HTTPException, Query
19 |     from fastapi.responses import JSONResponse
   |                                   ^^^^^^^^^^^^
20 |     from pydantic import BaseModel
21 |     FASTAPI_AVAILABLE = True
   |
help: Remove unused import: `fastapi.responses.JSONResponse`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/api/customer_api.py:48:22
   |
46 |     total_orders: int
47 |     total_revenue: float
48 |     last_order_date: Optional[str]
   |                      ^^^^^^^^^^^^^
49 |     customer_segment: Optional[str]
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/api/customer_api.py:49:23
   |
47 |     total_revenue: float
48 |     last_order_date: Optional[str]
49 |     customer_segment: Optional[str]
   |                       ^^^^^^^^^^^^^
   |
help: Convert to `X | None`

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/api/customer_api.py:57:1
   |
55 |         """Health check endpoint."""
56 |         return {"status": "healthy", "timestamp": datetime.utcnow().isoformat()}
57 |     
   | ^^^^
58 |     @app.get("/customer/{customer_id}", response_model=CustomerResponse)
59 |     def get_customer(customer_id: str):
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/api/customer_api.py:62:1
   |
60 |         """
61 |         Get customer 360 data by customer ID.
62 |         
   | ^^^^^^^^
63 |         Args:
64 |             customer_id: Customer ID
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/api/customer_api.py:65:1
   |
63 |         Args:
64 |             customer_id: Customer ID
65 |             
   | ^^^^^^^^^^^^
66 |         Returns:
67 |             Customer 360 record
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/api/customer_api.py:72:1
   |
70 |             # Initialize Spark (lazy initialization)
71 |             spark = _get_spark_session()
72 |             
   | ^^^^^^^^^^^^
73 |             # Load customer_360 table
74 |             config = _load_config()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/api/customer_api.py:77:1
   |
75 |             gold_path = config.get("data_lake", {}).get("gold_path", "data/lakehouse_delta/gold")
76 |             customer_df = spark.read.format("delta").load(f"{gold_path}/customer_360")
77 |             
   | ^^^^^^^^^^^^
78 |             # Filter by customer_id
79 |             customer = customer_df.filter(col("customer_id") == customer_id).first()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/api/customer_api.py:80:1
   |
78 |             # Filter by customer_id
79 |             customer = customer_df.filter(col("customer_id") == customer_id).first()
80 |             
   | ^^^^^^^^^^^^
81 |             if customer is None:
82 |                 raise HTTPException(status_code=404, detail=f"Customer {customer_id} not found")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/api/customer_api.py:83:1
   |
81 |             if customer is None:
82 |                 raise HTTPException(status_code=404, detail=f"Customer {customer_id} not found")
83 |             
   | ^^^^^^^^^^^^
84 |             return CustomerResponse(
85 |                 customer_id=customer["customer_id"],
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/api/customer_api.py:92:1
   |
90 |                 customer_segment=customer.get("customer_segment")
91 |             )
92 |             
   | ^^^^^^^^^^^^
93 |         except Exception as e:
94 |             logger.error(f"Error fetching customer {customer_id}: {e}")
   |
help: Remove whitespace from blank line

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
  --> src/project_a/legacy/api/customer_api.py:95:13
   |
93 |         except Exception as e:
94 |             logger.error(f"Error fetching customer {customer_id}: {e}")
95 |             raise HTTPException(status_code=500, detail=str(e))
   |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
96 |     
97 |     @app.get("/customers", response_model=List[CustomerResponse])
   |

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/api/customer_api.py:96:1
   |
94 |             logger.error(f"Error fetching customer {customer_id}: {e}")
95 |             raise HTTPException(status_code=500, detail=str(e))
96 |     
   | ^^^^
97 |     @app.get("/customers", response_model=List[CustomerResponse])
98 |     def list_customers(
   |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/legacy/api/customer_api.py:97:43
   |
95 |             raise HTTPException(status_code=500, detail=str(e))
96 |     
97 |     @app.get("/customers", response_model=List[CustomerResponse])
   |                                           ^^^^
98 |     def list_customers(
99 |         limit: int = Query(100, ge=1, le=1000),
   |
help: Replace with `list`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/api/customer_api.py:101:18
    |
 99 |         limit: int = Query(100, ge=1, le=1000),
100 |         offset: int = Query(0, ge=0),
101 |         segment: Optional[str] = None
    |                  ^^^^^^^^^^^^^
102 |     ):
103 |         """
    |
help: Convert to `X | None`

W293 Blank line contains whitespace
   --> src/project_a/legacy/api/customer_api.py:105:1
    |
103 |         """
104 |         List customers with pagination and filtering.
105 |         
    | ^^^^^^^^
106 |         Args:
107 |             limit: Maximum number of records
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/api/customer_api.py:110:1
    |
108 |             offset: Offset for pagination
109 |             segment: Filter by customer segment
110 |             
    | ^^^^^^^^^^^^
111 |         Returns:
112 |             List of customer records
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/api/customer_api.py:119:1
    |
117 |             gold_path = config.get("data_lake", {}).get("gold_path", "data/lakehouse_delta/gold")
118 |             customer_df = spark.read.format("delta").load(f"{gold_path}/customer_360")
119 |             
    | ^^^^^^^^^^^^
120 |             # Apply filters
121 |             if segment:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/api/customer_api.py:123:1
    |
121 |             if segment:
122 |                 customer_df = customer_df.filter(col("customer_segment") == segment)
123 |             
    | ^^^^^^^^^^^^
124 |             # Pagination
125 |             customers = customer_df.limit(limit).offset(offset).collect()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/api/customer_api.py:126:1
    |
124 |             # Pagination
125 |             customers = customer_df.limit(limit).offset(offset).collect()
126 |             
    | ^^^^^^^^^^^^
127 |             return [
128 |                 CustomerResponse(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/api/customer_api.py:138:1
    |
136 |                 for c in customers
137 |             ]
138 |             
    | ^^^^^^^^^^^^
139 |         except Exception as e:
140 |             logger.error(f"Error listing customers: {e}")
    |
help: Remove whitespace from blank line

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> src/project_a/legacy/api/customer_api.py:141:13
    |
139 |         except Exception as e:
140 |             logger.error(f"Error listing customers: {e}")
141 |             raise HTTPException(status_code=500, detail=str(e))
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
142 |     
143 |     @app.get("/stats")
    |

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/api/customer_api.py:142:1
    |
140 |             logger.error(f"Error listing customers: {e}")
141 |             raise HTTPException(status_code=500, detail=str(e))
142 |     
    | ^^^^
143 |     @app.get("/stats")
144 |     def get_stats():
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/api/customer_api.py:151:1
    |
149 |             gold_path = config.get("data_lake", {}).get("gold_path", "data/lakehouse_delta/gold")
150 |             customer_df = spark.read.format("delta").load(f"{gold_path}/customer_360")
151 |             
    | ^^^^^^^^^^^^
152 |             total_customers = customer_df.count()
153 |             total_revenue = customer_df.agg({"total_revenue": "sum"}).collect()[0][0] or 0
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/api/customer_api.py:155:1
    |
153 |             total_revenue = customer_df.agg({"total_revenue": "sum"}).collect()[0][0] or 0
154 |             total_orders = customer_df.agg({"total_orders": "sum"}).collect()[0][0] or 0
155 |             
    | ^^^^^^^^^^^^
156 |             return {
157 |                 "total_customers": total_customers,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/api/customer_api.py:162:1
    |
160 |                 "timestamp": datetime.utcnow().isoformat()
161 |             }
162 |             
    | ^^^^^^^^^^^^
163 |         except Exception as e:
164 |             logger.error(f"Error fetching stats: {e}")
    |
help: Remove whitespace from blank line

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> src/project_a/legacy/api/customer_api.py:165:13
    |
163 |         except Exception as e:
164 |             logger.error(f"Error fetching stats: {e}")
165 |             raise HTTPException(status_code=500, detail=str(e))
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/api/customer_api.py:175:1
    |
173 |     """Get or create Spark session."""
174 |     global _spark_session
175 |     
    | ^^^^
176 |     if _spark_session is None:
177 |         from project_a.utils.spark_session import build_spark
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
   --> src/project_a/legacy/api/customer_api.py:177:9
    |
176 |       if _spark_session is None:
177 | /         from project_a.utils.spark_session import build_spark
178 | |         from project_a.utils.config import load_conf
    | |____________________________________________________^
179 |           
180 |           config = load_conf()
    |
help: Organize imports

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/api/customer_api.py:179:1
    |
177 |         from project_a.utils.spark_session import build_spark
178 |         from project_a.utils.config import load_conf
179 |         
    | ^^^^^^^^
180 |         config = load_conf()
181 |         _spark_session = build_spark(app_name="customer_api", config=config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/api/customer_api.py:182:1
    |
180 |         config = load_conf()
181 |         _spark_session = build_spark(app_name="customer_api", config=config)
182 |     
    | ^^^^
183 |     return _spark_session
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/api/customer_api.py:194:1
    |
192 | if __name__ == "__main__":
193 |     import uvicorn
194 |     
    | ^^^^
195 |     if not FASTAPI_AVAILABLE:
196 |         logger.error("FastAPI not installed. Install with: pip install fastapi uvicorn")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/api/customer_api.py:198:1
    |
196 |         logger.error("FastAPI not installed. Install with: pip install fastapi uvicorn")
197 |         sys.exit(1)
198 |     
    | ^^^^
199 |     # Run API server
200 |     port = int(os.getenv("API_PORT", "8000"))
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/cicd_manager.py:13:1
   |
11 |   """
12 |
13 | / import logging
14 | | import os
15 | | import json
16 | | import yaml
17 | | import time
18 | | import subprocess
19 | | from datetime import datetime
20 | | from typing import Any, Dict, List, Optional
21 | | from dataclasses import dataclass, field
22 | | from enum import Enum
   | |_____________________^
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/legacy/cicd_manager.py:20:1
   |
18 | import subprocess
19 | from datetime import datetime
20 | from typing import Any, Dict, List, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
21 | from dataclasses import dataclass, field
22 | from enum import Enum
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/legacy/cicd_manager.py:20:1
   |
18 | import subprocess
19 | from datetime import datetime
20 | from typing import Any, Dict, List, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
21 | from dataclasses import dataclass, field
22 | from enum import Enum
   |

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/cicd_manager.py:60:16
   |
58 |     version: str
59 |     config_file: str
60 |     variables: Dict[str, Any]
   |                ^^^^
61 |     secrets: Dict[str, str]
62 |     rollback_enabled: bool = True
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/cicd_manager.py:61:14
   |
59 |     config_file: str
60 |     variables: Dict[str, Any]
61 |     secrets: Dict[str, str]
   |              ^^^^
62 |     rollback_enabled: bool = True
63 |     health_check_enabled: bool = True
   |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/cicd_manager.py:77:15
   |
75 |     version: str
76 |     start_time: datetime
77 |     end_time: Optional[datetime] = None
   |               ^^^^^^^^^^^^^^^^^^
78 |     duration_minutes: Optional[float] = None
79 |     logs: List[str] = field(default_factory=list)
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/cicd_manager.py:78:23
   |
76 |     start_time: datetime
77 |     end_time: Optional[datetime] = None
78 |     duration_minutes: Optional[float] = None
   |                       ^^^^^^^^^^^^^^^
79 |     logs: List[str] = field(default_factory=list)
80 |     errors: List[str] = field(default_factory=list)
   |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/legacy/cicd_manager.py:79:11
   |
77 |     end_time: Optional[datetime] = None
78 |     duration_minutes: Optional[float] = None
79 |     logs: List[str] = field(default_factory=list)
   |           ^^^^
80 |     errors: List[str] = field(default_factory=list)
81 |     warnings: List[str] = field(default_factory=list)
   |
help: Replace with `list`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/legacy/cicd_manager.py:80:13
   |
78 |     duration_minutes: Optional[float] = None
79 |     logs: List[str] = field(default_factory=list)
80 |     errors: List[str] = field(default_factory=list)
   |             ^^^^
81 |     warnings: List[str] = field(default_factory=list)
82 |     rollback_triggered: bool = False
   |
help: Replace with `list`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/legacy/cicd_manager.py:81:15
   |
79 |     logs: List[str] = field(default_factory=list)
80 |     errors: List[str] = field(default_factory=list)
81 |     warnings: List[str] = field(default_factory=list)
   |               ^^^^
82 |     rollback_triggered: bool = False
83 |     rollback_reason: Optional[str] = None
   |
help: Replace with `list`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/cicd_manager.py:83:22
   |
81 |     warnings: List[str] = field(default_factory=list)
82 |     rollback_triggered: bool = False
83 |     rollback_reason: Optional[str] = None
   |                      ^^^^^^^^^^^^^
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/cicd_manager.py:91:32
   |
89 |     """
90 |
91 |     def __init__(self, config: Dict[str, Any]):
   |                                ^^^^
92 |         self.config = config
93 |         self.deployments: Dict[str, DeploymentResult] = {}
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/cicd_manager.py:93:27
   |
91 |     def __init__(self, config: Dict[str, Any]):
92 |         self.config = config
93 |         self.deployments: Dict[str, DeploymentResult] = {}
   |                           ^^^^
94 |         self.environment_configs: Dict[Environment, Dict[str, Any]] = {}
95 |         self.deployment_history: List[DeploymentResult] = []
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/cicd_manager.py:94:35
   |
92 |         self.config = config
93 |         self.deployments: Dict[str, DeploymentResult] = {}
94 |         self.environment_configs: Dict[Environment, Dict[str, Any]] = {}
   |                                   ^^^^
95 |         self.deployment_history: List[DeploymentResult] = []
96 |         self._load_environment_configs()
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/cicd_manager.py:94:53
   |
92 |         self.config = config
93 |         self.deployments: Dict[str, DeploymentResult] = {}
94 |         self.environment_configs: Dict[Environment, Dict[str, Any]] = {}
   |                                                     ^^^^
95 |         self.deployment_history: List[DeploymentResult] = []
96 |         self._load_environment_configs()
   |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/legacy/cicd_manager.py:95:34
   |
93 |         self.deployments: Dict[str, DeploymentResult] = {}
94 |         self.environment_configs: Dict[Environment, Dict[str, Any]] = {}
95 |         self.deployment_history: List[DeploymentResult] = []
   |                                  ^^^^
96 |         self._load_environment_configs()
   |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/cicd_manager.py:119:62
    |
117 |     def create_deployment(self, environment: Environment, version: str,
118 |                          deployment_type: DeploymentType = DeploymentType.BLUE_GREEN,
119 |                          config_file: str = None, variables: Dict[str, Any] = None,
    |                                                              ^^^^
120 |                          secrets: Dict[str, str] = None) -> DeploymentConfig:
121 |         """
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/cicd_manager.py:120:35
    |
118 |                          deployment_type: DeploymentType = DeploymentType.BLUE_GREEN,
119 |                          config_file: str = None, variables: Dict[str, Any] = None,
120 |                          secrets: Dict[str, str] = None) -> DeploymentConfig:
    |                                   ^^^^
121 |         """
122 |         Create a new deployment configuration.
    |
help: Replace with `dict`

F841 Local variable `env_config` is assigned to but never used
   --> src/project_a/legacy/cicd_manager.py:450:13
    |
448 |         """Execute the actual deployment."""
449 |         try:
450 |             env_config = self.environment_configs[deployment_config.environment]
    |             ^^^^^^^^^^
451 |             deployment_result.logs.append(f"Executing {deployment_config.deployment_type.value} deployment...")
    |
help: Remove assignment to unused variable `env_config`

UP015 [*] Unnecessary mode argument
   --> src/project_a/legacy/cicd_manager.py:454:54
    |
453 |             # Load configuration
454 |             with open(deployment_config.config_file, 'r') as f:
    |                                                      ^^^
455 |                 if deployment_config.config_file.endswith('.yaml') or deployment_config.config_file.endswith('.yml'):
456 |                     config_data = yaml.safe_load(f)
    |
help: Remove mode argument

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/cicd_manager.py:489:59
    |
487 |             return False
488 |
489 |     def _execute_blue_green_deployment(self, config_data: Dict[str, Any],
    |                                                           ^^^^
490 |                                      deployment_result: DeploymentResult) -> bool:
491 |         """Execute blue-green deployment."""
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/cicd_manager.py:525:56
    |
523 |             return False
524 |
525 |     def _execute_rolling_deployment(self, config_data: Dict[str, Any],
    |                                                        ^^^^
526 |                                   deployment_result: DeploymentResult) -> bool:
527 |         """Execute rolling deployment."""
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/cicd_manager.py:555:55
    |
553 |             return False
554 |
555 |     def _execute_canary_deployment(self, config_data: Dict[str, Any],
    |                                                       ^^^^
556 |                                  deployment_result: DeploymentResult) -> bool:
557 |         """Execute canary deployment."""
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/cicd_manager.py:590:58
    |
588 |             return False
589 |
590 |     def _execute_immediate_deployment(self, config_data: Dict[str, Any],
    |                                                          ^^^^
591 |                                     deployment_result: DeploymentResult) -> bool:
592 |         """Execute immediate deployment."""
    |
help: Replace with `dict`

UP015 [*] Unnecessary mode argument
   --> src/project_a/legacy/cicd_manager.py:620:54
    |
619 |             # Load configuration
620 |             with open(deployment_config.config_file, 'r') as f:
    |                                                      ^^^
621 |                 if deployment_config.config_file.endswith('.yaml') or deployment_config.config_file.endswith('.yml'):
622 |                     config_data = yaml.safe_load(f)
    |
help: Remove mode argument

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/cicd_manager.py:641:53
    |
639 |             return False
640 |
641 |     def _run_health_checks_green(self, config_data: Dict[str, Any],
    |                                                     ^^^^
642 |                                 deployment_result: DeploymentResult) -> bool:
643 |         """Run health checks for blue-green deployment."""
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/cicd_manager.py:665:55
    |
663 |             return False
664 |
665 |     def _run_health_checks_rolling(self, config_data: Dict[str, Any],
    |                                                       ^^^^
666 |                                   deployment_result: DeploymentResult) -> bool:
667 |         """Run health checks for rolling deployment."""
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/cicd_manager.py:683:54
    |
681 |             return False
682 |
683 |     def _run_health_checks_canary(self, config_data: Dict[str, Any],
    |                                                      ^^^^
684 |                                  deployment_result: DeploymentResult) -> bool:
685 |         """Run health checks for canary deployment."""
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/cicd_manager.py:703:57
    |
701 |             return False
702 |
703 |     def _run_health_checks_immediate(self, config_data: Dict[str, Any],
    |                                                         ^^^^
704 |                                    deployment_result: DeploymentResult) -> bool:
705 |         """Run health checks for immediate deployment."""
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/cicd_manager.py:734:46
    |
732 |             return False
733 |
734 |     def _run_canary_tests(self, config_data: Dict[str, Any],
    |                                              ^^^^
735 |                          deployment_result: DeploymentResult) -> bool:
736 |         """Run canary tests."""
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/cicd_manager.py:784:57
    |
782 |             return False
783 |
784 |     def _apply_environment_variables(self, config_data: Dict[str, Any],
    |                                                         ^^^^
785 |                                    variables: Dict[str, Any]) -> Dict[str, Any]:
786 |         """Apply environment variables to configuration."""
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/cicd_manager.py:785:47
    |
784 |     def _apply_environment_variables(self, config_data: Dict[str, Any],
785 |                                    variables: Dict[str, Any]) -> Dict[str, Any]:
    |                                               ^^^^
786 |         """Apply environment variables to configuration."""
787 |         try:
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/cicd_manager.py:785:66
    |
784 |     def _apply_environment_variables(self, config_data: Dict[str, Any],
785 |                                    variables: Dict[str, Any]) -> Dict[str, Any]:
    |                                                                  ^^^^
786 |         """Apply environment variables to configuration."""
787 |         try:
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/cicd_manager.py:803:43
    |
801 |             return config_data
802 |
803 |     def _apply_secrets(self, config_data: Dict[str, Any],
    |                                           ^^^^
804 |                       secrets: Dict[str, str]) -> Dict[str, Any]:
805 |         """Apply secrets to configuration."""
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/cicd_manager.py:804:32
    |
803 |     def _apply_secrets(self, config_data: Dict[str, Any],
804 |                       secrets: Dict[str, str]) -> Dict[str, Any]:
    |                                ^^^^
805 |         """Apply secrets to configuration."""
806 |         try:
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/cicd_manager.py:804:51
    |
803 |     def _apply_secrets(self, config_data: Dict[str, Any],
804 |                       secrets: Dict[str, str]) -> Dict[str, Any]:
    |                                                   ^^^^
805 |         """Apply secrets to configuration."""
806 |         try:
    |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/cicd_manager.py:878:60
    |
876 |             return False
877 |
878 |     def get_deployment_status(self, deployment_id: str) -> Optional[DeploymentResult]:
    |                                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^
879 |         """Get deployment status by ID."""
880 |         return self.deployments.get(deployment_id)
    |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/legacy/cicd_manager.py:883:66
    |
882 |     def get_deployment_history(self, environment: Environment = None,
883 |                              status: DeploymentStatus = None) -> List[DeploymentResult]:
    |                                                                  ^^^^
884 |         """Get deployment history with optional filtering."""
885 |         filtered_history = self.deployment_history
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/cicd_manager.py:971:32
    |
971 | def setup_cicd_manager(config: Dict[str, Any]) -> CICDManager:
    |                                ^^^^
972 |     """
973 |     Setup CI/CD manager with configuration.
    |
help: Replace with `dict`

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/cli.py:5:1
   |
 3 |   CLI for PySpark ETL Pipeline
 4 |   """
 5 | / import argparse
 6 | | import logging
 7 | | import sys
 8 | | import os
 9 | | from pathlib import Path
   | |________________________^
10 |
11 |   # Add src to path
   |
help: Organize imports

F401 [*] `pathlib.Path` imported but unused
  --> src/project_a/legacy/cli.py:9:21
   |
 7 | import sys
 8 | import os
 9 | from pathlib import Path
   |                     ^^^^
10 |
11 | # Add src to path
   |
help: Remove unused import: `pathlib.Path`

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/cli.py:14:1
   |
12 |   sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))
13 |
14 | / from project_a.production_pipeline import ProductionETLPipeline
15 | | from project_a.standard_etl_pipeline import StandardETLPipeline
16 | | import yaml
   | |___________^
17 |
18 |   logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
   |
help: Organize imports

F401 [*] `project_a.production_pipeline.ProductionETLPipeline` imported but unused
  --> src/project_a/legacy/cli.py:14:43
   |
12 | sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))
13 |
14 | from project_a.production_pipeline import ProductionETLPipeline
   |                                           ^^^^^^^^^^^^^^^^^^^^^
15 | from project_a.standard_etl_pipeline import StandardETLPipeline
16 | import yaml
   |
help: Remove unused import: `project_a.production_pipeline.ProductionETLPipeline`

UP015 [*] Unnecessary mode argument
  --> src/project_a/legacy/cli.py:24:32
   |
22 |     """Load configuration from YAML file"""
23 |     try:
24 |         with open(config_path, 'r') as f:
   |                                ^^^
25 |             config_data = yaml.safe_load(f)
26 |         logger.info(f"âœ… Configuration loaded from {config_path}")
   |
help: Remove mode argument

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/cli.py:35:1
   |
33 |     """Run data ingestion stage"""
34 |     logger.info("ðŸ”„ Starting data ingestion...")
35 |     
   | ^^^^
36 |     # Check if input data exists
37 |     input_dir = "data/input_data"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/cli.py:41:1
   |
39 |         logger.error(f"âŒ Input data directory not found: {input_dir}")
40 |         return False
41 |     
   | ^^^^
42 |     files = [f for f in os.listdir(input_dir) if f.endswith(('.csv', '.json'))]
43 |     logger.info(f"ðŸ“Š Found {len(files)} input files for ingestion")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/cli.py:44:1
   |
42 |     files = [f for f in os.listdir(input_dir) if f.endswith(('.csv', '.json'))]
43 |     logger.info(f"ðŸ“Š Found {len(files)} input files for ingestion")
44 |     
   | ^^^^
45 |     # Log input data summary
46 |     for file in files:
   |
help: Remove whitespace from blank line

UP015 [*] Unnecessary mode argument
  --> src/project_a/legacy/cli.py:58:38
   |
56 |             try:
57 |                 import json
58 |                 with open(file_path, 'r') as f:
   |                                      ^^^
59 |                     data = json.load(f)
60 |                 if isinstance(data, list):
   |
help: Remove mode argument

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/cli.py:66:1
   |
64 |             except Exception as e:
65 |                 logger.warning(f"âš ï¸ Could not read {file}: {e}")
66 |     
   | ^^^^
67 |     logger.info("âœ… Data ingestion completed")
68 |     return True
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/cli.py:73:1
   |
71 |     """Run data transformation stage"""
72 |     logger.info("ðŸ”„ Starting data transformation...")
73 |     
   | ^^^^
74 |     try:
75 |         # Use the standard pipeline for transformation
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/cli.py:78:1
   |
76 |         pipeline = StandardETLPipeline(config)
77 |         success = pipeline.run_pipeline()
78 |         
   | ^^^^^^^^
79 |         if success:
80 |             logger.info("âœ… Data transformation completed")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/cli.py:92:1
   |
90 |     """Run data validation stage"""
91 |     logger.info("ðŸ”„ Starting data validation...")
92 |     
   | ^^^^
93 |     try:
94 |         # Check if Delta Lake tables exist and have data
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/cli.py:96:1
   |
94 |         # Check if Delta Lake tables exist and have data
95 |         delta_path = config.get('delta_path', 'data/lakehouse_delta_standard')
96 |         
   | ^^^^^^^^
97 |         validation_results = []
98 |         layers = ['bronze', 'silver', 'gold']
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/cli.py:99:1
    |
 97 |         validation_results = []
 98 |         layers = ['bronze', 'silver', 'gold']
 99 |         
    | ^^^^^^^^
100 |         for layer in layers:
101 |             layer_path = os.path.join(delta_path, layer)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/cli.py:113:1
    |
111 |                         if os.path.exists(delta_log_path):
112 |                             log_files = [f for f in os.listdir(delta_log_path) if f.endswith('.json')]
113 |                         
    | ^^^^^^^^^^^^^^^^^^^^^^^^
114 |                         validation_results.append({
115 |                             'layer': layer,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/cli.py:120:1
    |
118 |                             'transaction_logs': len(log_files)
119 |                         })
120 |                         
    | ^^^^^^^^^^^^^^^^^^^^^^^^
121 |                         logger.info(f"ðŸ“Š {layer}.{table_dir}: {len(parquet_files)} parquet files, {len(log_files)} versions")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/cli.py:122:1
    |
121 |                         logger.info(f"ðŸ“Š {layer}.{table_dir}: {len(parquet_files)} parquet files, {len(log_files)} versions")
122 |         
    | ^^^^^^^^
123 |         # Summary
124 |         total_tables = len(validation_results)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/cli.py:127:1
    |
125 |         total_files = sum(r['parquet_files'] for r in validation_results)
126 |         total_versions = sum(r['transaction_logs'] for r in validation_results)
127 |         
    | ^^^^^^^^
128 |         logger.info(f"âœ… Validation completed: {total_tables} tables, {total_files} files, {total_versions} versions")
129 |         return True
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/cli.py:130:1
    |
128 |         logger.info(f"âœ… Validation completed: {total_tables} tables, {total_files} files, {total_versions} versions")
129 |         return True
130 |         
    | ^^^^^^^^
131 |     except Exception as e:
132 |         logger.error(f"âŒ Validation error: {e}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/cli.py:138:1
    |
136 |     """Run data loading/publishing stage (MERGE into Gold)"""
137 |     logger.info("ðŸ”„ Starting data loading/publishing...")
138 |     
    | ^^^^
139 |     try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/cli.py:140:1
    |
139 |     try:
140 |         
    | ^^^^^^^^
141 |         # Get gold layer path
142 |         delta_path = config.get('delta_path', 'data/lakehouse_delta_standard')
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/cli.py:144:1
    |
142 |         delta_path = config.get('delta_path', 'data/lakehouse_delta_standard')
143 |         gold_path = os.path.join(delta_path, 'gold')
144 |         
    | ^^^^^^^^
145 |         if not os.path.exists(gold_path):
146 |             logger.warning(f"âš ï¸ Gold layer not found at {gold_path}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/cli.py:148:1
    |
146 |             logger.warning(f"âš ï¸ Gold layer not found at {gold_path}")
147 |             return False
148 |         
    | ^^^^^^^^
149 |         # Process each gold table
150 |         for table_dir in os.listdir(gold_path):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/cli.py:154:1
    |
152 |             if os.path.isdir(table_path):
153 |                 logger.info(f"ðŸ“Š Processing gold table: {table_dir}")
154 |                 
    | ^^^^^^^^^^^^^^^^
155 |                 try:
156 |                     # Check for parquet files instead of reading with Spark
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/cli.py:162:1
    |
160 |                     if os.path.exists(delta_log_path):
161 |                         log_files = [f for f in os.listdir(delta_log_path) if f.endswith('.json')]
162 |                     
    | ^^^^^^^^^^^^^^^^^^^^
163 |                     # Log the write path and file counts
164 |                     logger.info(f"âœ… Gold written to: {table_path}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/cli.py:167:1
    |
165 |                     logger.info(f"ðŸ“Š Gold parquet files: {len(parquet_files)}")
166 |                     logger.info(f"ðŸ“Š Gold versions: {len(log_files)}")
167 |                     
    | ^^^^^^^^^^^^^^^^^^^^
168 |                     # Show recent transaction logs
169 |                     if log_files:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/cli.py:172:1
    |
170 |                         latest_log = sorted(log_files)[-1]
171 |                         logger.info(f"ðŸ“ Latest version: {latest_log}")
172 |                         
    | ^^^^^^^^^^^^^^^^^^^^^^^^
173 |                 except Exception as e:
174 |                     logger.error(f"âŒ Error processing {table_dir}: {e}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/cli.py:178:1
    |
176 |         logger.info("âœ… Data loading/publishing completed")
177 |         return True
178 |         
    | ^^^^^^^^
179 |     except Exception as e:
180 |         logger.error(f"âŒ Loading error: {e}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/cli.py:190:1
    |
188 |     parser.add_argument("--cmd", choices=["ingest", "transform", "validate", "pipeline", "full"], required=True,
189 |                        help="Command to run")
190 |     
    | ^^^^
191 |     args = parser.parse_args()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/cli.py:192:1
    |
191 |     args = parser.parse_args()
192 |     
    | ^^^^
193 |     # Load configuration
194 |     config = load_config(args.config, args.env)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/cli.py:195:1
    |
193 |     # Load configuration
194 |     config = load_config(args.config, args.env)
195 |     
    | ^^^^
196 |     # Execute command
197 |     if args.cmd == "ingest":
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/cli.py:208:1
    |
206 |         # Run all stages in sequence
207 |         logger.info("ðŸš€ Running complete ETL pipeline...")
208 |         
    | ^^^^^^^^
209 |         stages = [
210 |             ("Ingest", run_ingest),
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/cli.py:215:1
    |
213 |             ("Load", run_load)
214 |         ]
215 |         
    | ^^^^^^^^
216 |         success = True
217 |         for stage_name, stage_func in stages:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/cli.py:225:1
    |
223 |                 break
224 |             logger.info(f"âœ… Stage {stage_name} completed")
225 |         
    | ^^^^^^^^
226 |         if success:
227 |             logger.info("ðŸŽ‰ Complete ETL pipeline finished successfully!")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/cli.py:230:1
    |
228 |         else:
229 |             logger.error("âŒ ETL pipeline failed")
230 |     
    | ^^^^
231 |     sys.exit(0 if success else 1)
    |
help: Remove whitespace from blank line

W292 [*] No newline at end of file
   --> src/project_a/legacy/cli.py:234:11
    |
233 | if __name__ == "__main__":
234 |     main()
    |           ^
    |
help: Add trailing newline

I001 [*] Import block is un-sorted or un-formatted
 --> src/project_a/legacy/common/__init__.py:5:1
  |
3 |   """
4 |
5 | / from .scd2 import apply_scd2, SCD2Config
6 | | from .dq import require_not_null, require_unique_keys, control_total
  | |____________________________________________________________________^
7 |
8 |   __all__ = [
  |
help: Organize imports

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/common/dq.py:8:1
   |
 6 |   """
 7 |
 8 | / import json
 9 | | import logging
10 | | from typing import Dict, List, Any, Optional
11 | | from datetime import datetime
12 | |
13 | | from pyspark.sql import DataFrame, functions as F
14 | | from pyspark.sql.types import StructType
   | |________________________________________^
15 |
16 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/legacy/common/dq.py:10:1
   |
 8 | import json
 9 | import logging
10 | from typing import Dict, List, Any, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
11 | from datetime import datetime
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/legacy/common/dq.py:10:1
   |
 8 | import json
 9 | import logging
10 | from typing import Dict, List, Any, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
11 | from datetime import datetime
   |

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/legacy/common/dq.py:21:14
   |
19 | def require_not_null(
20 |     df: DataFrame,
21 |     columns: List[str],
   |              ^^^^
22 |     fail_on_violation: bool = True
23 | ) -> Dict[str, Any]:
   |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/common/dq.py:23:6
   |
21 |     columns: List[str],
22 |     fail_on_violation: bool = True
23 | ) -> Dict[str, Any]:
   |      ^^^^
24 |     """
25 |     Check that specified columns contain no null values.
   |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/legacy/common/dq.py:66:18
   |
64 | def require_unique_keys(
65 |     df: DataFrame,
66 |     key_columns: List[str],
   |                  ^^^^
67 |     fail_on_violation: bool = True
68 | ) -> Dict[str, Any]:
   |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/common/dq.py:68:6
   |
66 |     key_columns: List[str],
67 |     fail_on_violation: bool = True
68 | ) -> Dict[str, Any]:
   |      ^^^^
69 |     """
70 |     Check that specified key columns form unique combinations.
   |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/common/dq.py:122:21
    |
120 |     df: DataFrame,
121 |     value_column: str,
122 |     expected_total: Optional[float] = None,
    |                     ^^^^^^^^^^^^^^^
123 |     tolerance_percentage: float = 0.01,  # 1% tolerance
124 |     fail_on_violation: bool = True
    |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/common/dq.py:125:6
    |
123 |     tolerance_percentage: float = 0.01,  # 1% tolerance
124 |     fail_on_violation: bool = True
125 | ) -> Dict[str, Any]:
    |      ^^^^
126 |     """
127 |     Check control total for a numeric column.
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/common/dq.py:181:6
    |
179 |     strict: bool = True,
180 |     fail_on_violation: bool = True
181 | ) -> Dict[str, Any]:
    |      ^^^^
182 |     """
183 |     Validate DataFrame schema against expected schema.
    |
help: Replace with `dict`

C401 Unnecessary generator (rewrite as a set comprehension)
   --> src/project_a/legacy/common/dq.py:195:21
    |
193 |     """
194 |     actual_schema = df.schema
195 |     actual_fields = set(field.name for field in actual_schema.fields)
    |                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
196 |     expected_fields = set(field.name for field in expected_schema.fields)
    |
help: Rewrite as a set comprehension

C401 Unnecessary generator (rewrite as a set comprehension)
   --> src/project_a/legacy/common/dq.py:196:23
    |
194 |     actual_schema = df.schema
195 |     actual_fields = set(field.name for field in actual_schema.fields)
196 |     expected_fields = set(field.name for field in expected_schema.fields)
    |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
197 |
198 |     missing_fields = expected_fields - actual_fields
    |
help: Rewrite as a set comprehension

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/legacy/common/dq.py:221:13
    |
219 | def run_dq_checks(
220 |     df: DataFrame,
221 |     checks: List[Dict[str, Any]],
    |             ^^^^
222 |     fail_fast: bool = True
223 | ) -> Dict[str, Any]:
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/common/dq.py:221:18
    |
219 | def run_dq_checks(
220 |     df: DataFrame,
221 |     checks: List[Dict[str, Any]],
    |                  ^^^^
222 |     fail_fast: bool = True
223 | ) -> Dict[str, Any]:
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/common/dq.py:223:6
    |
221 |     checks: List[Dict[str, Any]],
222 |     fail_fast: bool = True
223 | ) -> Dict[str, Any]:
    |      ^^^^
224 |     """
225 |     Run multiple data quality checks on a DataFrame.
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/common/dq.py:305:14
    |
304 | def generate_dq_report(
305 |     results: Dict[str, Any],
    |              ^^^^
306 |     output_path: Optional[str] = None
307 | ) -> str:
    |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/common/dq.py:306:18
    |
304 | def generate_dq_report(
305 |     results: Dict[str, Any],
306 |     output_path: Optional[str] = None
    |                  ^^^^^^^^^^^^^
307 | ) -> str:
308 |     """
    |
help: Convert to `X | None`

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/common/scd2.py:8:1
   |
 6 |   """
 7 |
 8 | / import logging
 9 | | from dataclasses import dataclass
10 | | from datetime import datetime
11 | | from typing import List, Optional, Dict, Any
12 | |
13 | | from pyspark.sql import DataFrame, SparkSession, functions as F
14 | | from pyspark.sql.types import TimestampType
15 | | from delta.tables import DeltaTable
   | |___________________________________^
16 |
17 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/legacy/common/scd2.py:11:1
   |
 9 | from dataclasses import dataclass
10 | from datetime import datetime
11 | from typing import List, Optional, Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
12 |
13 | from pyspark.sql import DataFrame, SparkSession, functions as F
   |

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/legacy/common/scd2.py:11:1
   |
 9 | from dataclasses import dataclass
10 | from datetime import datetime
11 | from typing import List, Optional, Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
12 |
13 | from pyspark.sql import DataFrame, SparkSession, functions as F
   |

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/legacy/common/scd2.py:24:21
   |
22 |     """Configuration for SCD Type 2 operations."""
23 |     business_key: str
24 |     change_columns: List[str]
   |                     ^^^^
25 |     effective_from_column: str = "effective_from"
26 |     effective_to_column: str = "effective_to"
   |
help: Replace with `list`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/common/scd2.py:28:27
   |
26 |     effective_to_column: str = "effective_to"
27 |     is_current_column: str = "is_current"
28 |     surrogate_key_column: Optional[str] = "surrogate_key"
   |                           ^^^^^^^^^^^^^
29 |     hash_column: str = "hash_diff"
30 |     updated_at_column: str = "updated_at"
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/common/scd2.py:31:16
   |
29 |     hash_column: str = "hash_diff"
30 |     updated_at_column: str = "updated_at"
31 |     ts_column: Optional[str] = None
   |                ^^^^^^^^^^^^^
32 |     add_surrogate_key: bool = True
33 |     handle_late_arriving: bool = True
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/common/scd2.py:42:21
   |
40 |     target_path: str,
41 |     config: SCD2Config,
42 |     effective_from: Optional[datetime] = None
   |                     ^^^^^^^^^^^^^^^^^^
43 | ) -> Dict[str, Any]:
44 |     """
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/common/scd2.py:43:6
   |
41 |     config: SCD2Config,
42 |     effective_from: Optional[datetime] = None
43 | ) -> Dict[str, Any]:
   |      ^^^^
44 |     """
45 |     Apply SCD Type 2 logic to handle slowly changing dimensions.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/legacy/common/scd2.py:46:1
   |
44 |     """
45 |     Apply SCD Type 2 logic to handle slowly changing dimensions.
46 |     
   | ^^^^
47 |     Args:
48 |         spark: SparkSession instance
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/common/scd2.py:53:1
   |
51 |         config: SCD2 configuration
52 |         effective_from: Optional effective from timestamp
53 |         
   | ^^^^^^^^
54 |     Returns:
55 |         Dictionary with processing results
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/common/scd2.py:178:6
    |
176 |     table_path: str,
177 |     config: SCD2Config
178 | ) -> Dict[str, Any]:
    |      ^^^^
179 |     """
180 |     Validate SCD2 table integrity and constraints.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/legacy/common/scd2.py:181:1
    |
179 |     """
180 |     Validate SCD2 table integrity and constraints.
181 |     
    | ^^^^
182 |     Args:
183 |         spark: SparkSession instance
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/common/scd2.py:186:1
    |
184 |         table_path: Path to the SCD2 table
185 |         config: SCD2 configuration used
186 |         
    | ^^^^^^^^
187 |     Returns:
188 |         Dictionary with validation results
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/common/scd2.py:192:1
    |
190 |     try:
191 |         df = spark.read.format("delta").load(table_path)
192 |         
    | ^^^^^^^^
193 |         validation_results = {
194 |             "success": True,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/common/scd2.py:207:1
    |
205 |             config.hash_column
206 |         ]
207 |         
    | ^^^^^^^^
208 |         missing_columns = [col for col in required_columns if col not in df.columns]
209 |         if missing_columns:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/common/scd2.py:220:1
    |
218 |             F.col("count") > 1
219 |         )
220 |         
    | ^^^^^^^^
221 |         if duplicate_current.count() > 0:
222 |             validation_results["errors"].append(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/common/scd2.py:236:1
    |
234 |                 (F.col(f"a.{config.effective_to_column}") > F.col(f"b.{config.effective_from_column}"))
235 |             )
236 |             
    | ^^^^^^^^^^^^
237 |             if overlapping.count() > 0:
238 |                 validation_results["warnings"].append(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/common/scd2.py:246:1
    |
244 |             F.col(config.business_key).isNull()
245 |         ).count()
246 |         
    | ^^^^^^^^
247 |         if null_business_keys > 0:
248 |             validation_results["errors"].append(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/common/scd2.py:259:1
    |
257 |             (F.col(config.effective_from_column) >= F.col(config.effective_to_column))
258 |         ).count()
259 |         
    | ^^^^^^^^
260 |         if invalid_dates > 0:
261 |             validation_results["errors"].append(
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/common/scd2.py:285:1
    |
283 |     """
284 |     Get complete history for a specific business key.
285 |     
    | ^^^^
286 |     Args:
287 |         spark: SparkSession instance
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/common/scd2.py:291:1
    |
289 |         config: SCD2 configuration
290 |         business_key_value: Value of the business key to get history for
291 |         
    | ^^^^^^^^
292 |     Returns:
293 |         DataFrame with complete history for the business key
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/common/scd2.py:296:1
    |
294 |     """
295 |     df = spark.read.format("delta").load(table_path)
296 |     
    | ^^^^
297 |     return df.filter(
298 |         F.col(config.business_key) == business_key_value
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/common/scd2.py:309:1
    |
307 |     """
308 |     Get all current records from SCD2 table.
309 |     
    | ^^^^
310 |     Args:
311 |         spark: SparkSession instance
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/common/scd2.py:314:1
    |
312 |         table_path: Path to the SCD2 table
313 |         config: SCD2 configuration
314 |         
    | ^^^^^^^^
315 |     Returns:
316 |         DataFrame with only current records
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/common/scd2.py:319:1
    |
317 |     """
318 |     df = spark.read.format("delta").load(table_path)
319 |     
    | ^^^^
320 |     return df.filter(F.col(config.is_current_column).isTrue())
    |
help: Remove whitespace from blank line

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/legacy/config_loader.py:16:1
   |
14 | import os
15 | import re
16 | from typing import Any, Dict, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
17 |
18 | import yaml
   |

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/config_loader.py:28:37
   |
26 |     """Configuration loader class that provides config loading and validation."""
27 |
28 |     def __init__(self, config_path: Optional[str] = None, env: Optional[str] = None):
   |                                     ^^^^^^^^^^^^^
29 |         self.config_path = config_path
30 |         self.env = env
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/config_loader.py:28:64
   |
26 |     """Configuration loader class that provides config loading and validation."""
27 |
28 |     def __init__(self, config_path: Optional[str] = None, env: Optional[str] = None):
   |                                                                ^^^^^^^^^^^^^
29 |         self.config_path = config_path
30 |         self.env = env
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/config_loader.py:33:30
   |
31 |         self.config = None
32 |
33 |     def load_config(self) -> Dict[str, Any]:
   |                              ^^^^
34 |         """Load and resolve configuration."""
35 |         self.config = load_config_resolved(self.config_path, self.env)
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/config_loader.py:38:39
   |
36 |         return self.config
37 |
38 |     def validate_config(self, config: Dict[str, Any]) -> bool:
   |                                       ^^^^
39 |         """Validate configuration structure and required fields."""
40 |         try:
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/config_loader.py:69:29
   |
67 |             return False
68 |
69 |     def get_config(self) -> Dict[str, Any]:
   |                             ^^^^
70 |         """Get the loaded configuration."""
71 |         if self.config is None:
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/config_loader.py:93:49
   |
93 | def _resolve_value(value: Any, dbutils, config: Dict[str, Any] = None) -> Any:
   |                                                 ^^^^
94 |     if not isinstance(value, str):
95 |         return value
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/config_loader.py:137:49
    |
137 | def _resolve_secrets(obj: Any, dbutils, config: Dict[str, Any] = None) -> Any:
    |                                                 ^^^^
138 |     if isinstance(obj, dict):
139 |         # Pass config to nested resolution
    |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/config_loader.py:151:18
    |
150 | def load_config_resolved(
151 |     config_path: Optional[str] = None, env: Optional[str] = None
    |                  ^^^^^^^^^^^^^
152 | ) -> Dict[str, Any]:
153 |     """
    |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/config_loader.py:151:45
    |
150 | def load_config_resolved(
151 |     config_path: Optional[str] = None, env: Optional[str] = None
    |                                             ^^^^^^^^^^^^^
152 | ) -> Dict[str, Any]:
153 |     """
    |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/config_loader.py:152:6
    |
150 | def load_config_resolved(
151 |     config_path: Optional[str] = None, env: Optional[str] = None
152 | ) -> Dict[str, Any]:
    |      ^^^^
153 |     """
154 |     Load config from a path, or pick by environment if path not provided.
    |
help: Replace with `dict`

UP015 [*] Unnecessary mode argument
   --> src/project_a/legacy/config_loader.py:172:28
    |
170 |         config_path = candidates.get(env, "config/local.yaml")
171 |
172 |     with open(config_path, "r") as f:
    |                            ^^^
173 |         cfg = yaml.safe_load(f)
    |
help: Remove mode argument

I001 [*] Import block is un-sorted or un-formatted
 --> src/project_a/legacy/config_model.py:6:1
  |
4 |   """
5 |
6 | / from pydantic import BaseModel, Field
7 | | from typing import Optional, Dict, Any
  | |______________________________________^
  |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
 --> src/project_a/legacy/config_model.py:7:1
  |
6 | from pydantic import BaseModel, Field
7 | from typing import Optional, Dict, Any
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  |

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/config_model.py:13:18
   |
11 |     """Input/Output configuration."""
12 |     format: str = Field(default="csv", description="Default file format")
13 |     compression: Optional[str] = Field(default=None, description="Compression type")
   |                  ^^^^^^^^^^^^^
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/config_model.py:20:15
   |
18 |     customers: str = Field(..., description="Customers table path")
19 |     orders: str = Field(..., description="Orders table path")
20 |     products: Optional[str] = Field(default=None, description="Products table path")
   |               ^^^^^^^^^^^^^
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/config_model.py:27:11
   |
25 |     bronze: PathSet = Field(..., description="Bronze layer paths")
26 |     silver: PathSet = Field(..., description="Silver layer paths")
27 |     gold: Optional[PathSet] = Field(default=None, description="Gold layer paths")
   |           ^^^^^^^^^^^^^^^^^
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/config_model.py:34:20
   |
32 |     shuffle_partitions: int = Field(default=400, description="Number of shuffle partitions")
33 |     enable_aqe: bool = Field(default=True, description="Enable Adaptive Query Execution")
34 |     driver_memory: Optional[str] = Field(default=None, description="Driver memory")
   |                    ^^^^^^^^^^^^^
35 |     executor_memory: Optional[str] = Field(default=None, description="Executor memory")
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/config_model.py:35:22
   |
33 |     enable_aqe: bool = Field(default=True, description="Enable Adaptive Query Execution")
34 |     driver_memory: Optional[str] = Field(default=None, description="Driver memory")
35 |     executor_memory: Optional[str] = Field(default=None, description="Executor memory")
   |                      ^^^^^^^^^^^^^
   |
help: Convert to `X | None`

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/config_model.py:53:1
   |
51 |     spark: SparkConfig = Field(default_factory=SparkConfig, description="Spark configuration")
52 |     dq: DataQuality = Field(default_factory=DataQuality, description="Data quality configuration")
53 |     
   | ^^^^
54 |     # Allow additional fields for flexibility
55 |     class Config:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/config_model.py:57:1
   |
55 |     class Config:
56 |         extra = "allow"
57 |     
   | ^^^^
58 |     def validate_paths(self) -> None:
59 |         """Validate that all paths are properly configured."""
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/config_model.py:68:1
   |
66 |         if not self.paths.silver.orders:
67 |             raise ValueError("Silver orders path is required")
68 |     
   | ^^^^
69 |     def get_spark_config(self) -> Dict[str, Any]:
70 |         """Get Spark configuration as dictionary."""
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/config_model.py:69:35
   |
67 |             raise ValueError("Silver orders path is required")
68 |     
69 |     def get_spark_config(self) -> Dict[str, Any]:
   |                                   ^^^^
70 |         """Get Spark configuration as dictionary."""
71 |         return {
   |
help: Replace with `dict`

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/data_contracts.py:7:1
   |
 5 |   """
 6 |
 7 | / import logging
 8 | | import json
 9 | | from datetime import datetime
10 | | from typing import Dict, List, Any, Union
11 | | from pathlib import Path
12 | | from dataclasses import dataclass, asdict
13 | | from enum import Enum
14 | |
15 | | from pyspark.sql import SparkSession, DataFrame
16 | | from pyspark.sql.types import (
17 | |     StructType, StructField, StringType, IntegerType, DoubleType,
18 | |     TimestampType, BooleanType, DateType, DecimalType, ArrayType, MapType
19 | | )
   | |_^
20 |
21 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/legacy/data_contracts.py:10:1
   |
 8 | import json
 9 | from datetime import datetime
10 | from typing import Dict, List, Any, Union
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
11 | from pathlib import Path
12 | from dataclasses import dataclass, asdict
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/legacy/data_contracts.py:10:1
   |
 8 | import json
 9 | from datetime import datetime
10 | from typing import Dict, List, Any, Union
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
11 | from pathlib import Path
12 | from dataclasses import dataclass, asdict
   |

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/data_contracts.py:47:17
   |
45 |     constraint_type: str  # not_null, unique, range, regex, custom
46 |     column: str
47 |     parameters: Dict[str, Any]
   |                 ^^^^
48 |     severity: ContractSeverity = ContractSeverity.ERROR
49 |     description: str = ""
   |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/legacy/data_contracts.py:59:18
   |
57 |     version: str
58 |     schema: StructType
59 |     constraints: List[DataConstraint]
   |                  ^^^^
60 |     partition_columns: List[str] = None
61 |     clustering_columns: List[str] = None
   |
help: Replace with `list`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/legacy/data_contracts.py:60:24
   |
58 |     schema: StructType
59 |     constraints: List[DataConstraint]
60 |     partition_columns: List[str] = None
   |                        ^^^^
61 |     clustering_columns: List[str] = None
62 |     retention_days: int = 365
   |
help: Replace with `list`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/legacy/data_contracts.py:61:25
   |
59 |     constraints: List[DataConstraint]
60 |     partition_columns: List[str] = None
61 |     clustering_columns: List[str] = None
   |                         ^^^^
62 |     retention_days: int = 365
63 |     created_at: str = None
   |
help: Replace with `list`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/legacy/data_contracts.py:67:11
   |
65 |     owner: str = "data_team"
66 |     description: str = ""
67 |     tags: List[str] = None
   |           ^^^^
68 |     schema_evolution_policy: str = "strict"  # strict, flexible, backward_compatible
   |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/data_contracts.py:82:25
   |
81 |         # Initialize contracts
82 |         self.contracts: Dict[str, TableContract] = {}
   |                         ^^^^
83 |         self._load_existing_contracts()
84 |         self._create_default_contracts()
   |
help: Replace with `dict`

UP015 [*] Unnecessary mode argument
  --> src/project_a/legacy/data_contracts.py:91:46
   |
89 |             for contract_file in self.contracts_dir.glob("*.json"):
90 |                 try:
91 |                     with open(contract_file, 'r') as f:
   |                                              ^^^
92 |                         contract_data = json.load(f)
   |
help: Remove mode argument

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/data_contracts.py:131:46
    |
129 |             logger.warning(f"Failed to load existing contracts: {e}")
130 |
131 |     def _schema_from_json(self, schema_json: Dict[str, Any]) -> StructType:
    |                                              ^^^^
132 |         """Reconstruct PySpark schema from JSON."""
133 |         fields = []
    |
help: Replace with `dict`

UP007 [*] Use `X | Y` for type annotations
   --> src/project_a/legacy/data_contracts.py:147:42
    |
145 |         return StructType(fields)
146 |
147 |     def _type_from_json(self, type_data: Union[str, Dict[str, Any]]) -> Any:
    |                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^
148 |         """Convert JSON type representation to PySpark type."""
149 |         if isinstance(type_data, str):
    |
help: Convert to `X | Y`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/data_contracts.py:147:53
    |
145 |         return StructType(fields)
146 |
147 |     def _type_from_json(self, type_data: Union[str, Dict[str, Any]]) -> Any:
    |                                                     ^^^^
148 |         """Convert JSON type representation to PySpark type."""
149 |         if isinstance(type_data, str):
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/data_contracts.py:478:68
    |
476 |         )
477 |
478 |     def validate_contract(self, df: DataFrame, table_name: str) -> Dict[str, Any]:
    |                                                                    ^^^^
479 |         """Validate DataFrame against its contract."""
480 |         if table_name not in self.contracts:
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/data_contracts.py:532:75
    |
530 |         return validation_result
531 |
532 |     def _validate_schema(self, df: DataFrame, contract: TableContract) -> Dict[str, Any]:
    |                                                                           ^^^^
533 |         """Validate DataFrame schema against contract schema."""
534 |         result = {
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/data_contracts.py:586:80
    |
584 |         return result
585 |
586 |     def _validate_constraints(self, df: DataFrame, contract: TableContract) -> Dict[str, Any]:
    |                                                                                ^^^^
587 |         """Validate DataFrame against contract constraints."""
588 |         result = {
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/data_contracts.py:616:89
    |
614 |         return result
615 |
616 |     def _validate_single_constraint(self, df: DataFrame, constraint: DataConstraint) -> Dict[str, Any]:
    |                                                                                         ^^^^
617 |         """Validate a single constraint."""
618 |         result = {
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/data_contracts.py:679:60
    |
677 |         return result
678 |
679 |     def _generate_recommendations(self, validation_result: Dict[str, Any], contract: TableContract) -> List[str]:
    |                                                            ^^^^
680 |         """Generate recommendations based on validation results."""
681 |         recommendations = []
    |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/legacy/data_contracts.py:679:104
    |
677 |         return result
678 |
679 |     def _generate_recommendations(self, validation_result: Dict[str, Any], contract: TableContract) -> List[str]:
    |                                                                                                        ^^^^
680 |         """Generate recommendations based on validation results."""
681 |         recommendations = []
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/data_contracts.py:709:70
    |
707 |         return recommendations
708 |
709 |     def detect_schema_drift(self, df: DataFrame, table_name: str) -> Dict[str, Any]:
    |                                                                      ^^^^
710 |         """Detect schema drift and suggest evolution."""
711 |         if table_name not in self.contracts:
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/data_contracts.py:787:61
    |
785 |         return drift_result
786 |
787 |     def _is_backward_incompatible_change(self, type_change: Dict[str, Any]) -> bool:
    |                                                             ^^^^
788 |         """Check if a type change is backward incompatible."""
789 |         old_type = type_change["old_type"]
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/data_contracts.py:808:65
    |
806 |         return True
807 |
808 |     def _generate_evolution_recommendations(self, drift_result: Dict[str, Any], contract: TableContract) -> List[str]:
    |                                                                 ^^^^
809 |         """Generate recommendations for schema evolution."""
810 |         recommendations = []
    |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/legacy/data_contracts.py:808:109
    |
806 |         return True
807 |
808 |     def _generate_evolution_recommendations(self, drift_result: Dict[str, Any], contract: TableContract) -> List[str]:
    |                                                                                                             ^^^^
809 |         """Generate recommendations for schema evolution."""
810 |         recommendations = []
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/data_contracts.py:830:33
    |
829 |     def evolve_contract(self, table_name: str, evolution_type: SchemaChangeType,
830 |                        details: Dict[str, Any]) -> Dict[str, Any]:
    |                                 ^^^^
831 |         """Evolve a contract based on schema changes."""
832 |         if table_name not in self.contracts:
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/data_contracts.py:830:52
    |
829 |     def evolve_contract(self, table_name: str, evolution_type: SchemaChangeType,
830 |                        details: Dict[str, Any]) -> Dict[str, Any]:
    |                                                    ^^^^
831 |         """Evolve a contract based on schema changes."""
832 |         if table_name not in self.contracts:
    |
help: Replace with `dict`

E722 Do not use bare `except`
   --> src/project_a/legacy/data_contracts.py:911:9
    |
909 |                 return f"{parts[0]}.{parts[1]}.{patch}"
910 |             return f"{version}.1"
911 |         except:
    |         ^^^^^^
912 |             return f"{version}.1"
    |

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/data_contracts.py:967:39
    |
965 |             raise
966 |
967 |     def get_contract_summary(self) -> Dict[str, Any]:
    |                                       ^^^^
968 |         """Get summary of all contracts."""
969 |         summary = {
    |
help: Replace with `dict`

I001 [*] Import block is un-sorted or un-formatted
   --> src/project_a/legacy/data_contracts.py:987:5
    |
985 |   def main():
986 |       """Main entry point for data contract management."""
987 | /     from .utils import get_spark_session
988 | |     from .config_loader import load_config_resolved
    | |___________________________________________________^
989 |
990 |       # Load configuration
    |
help: Organize imports

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/data_quality_suite.py:12:1
   |
10 |   """
11 |
12 | / import json
13 | | from datetime import datetime, timedelta
14 | | from typing import Dict, List, Any, Optional
15 | |
16 | | from pyspark.sql import DataFrame, SparkSession, functions as F
17 | | from pyspark.sql.types import StructType
18 | | from pyspark.sql.window import Window
19 | |
20 | | from project_a.logging_setup import get_logger
   | |______________________________________________^
21 |
22 |   logger = get_logger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/legacy/data_quality_suite.py:14:1
   |
12 | import json
13 | from datetime import datetime, timedelta
14 | from typing import Dict, List, Any, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
15 |
16 | from pyspark.sql import DataFrame, SparkSession, functions as F
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/legacy/data_quality_suite.py:14:1
   |
12 | import json
13 | from datetime import datetime, timedelta
14 | from typing import Dict, List, Any, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
15 |
16 | from pyspark.sql import DataFrame, SparkSession, functions as F
   |

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/data_quality_suite.py:35:78
   |
33 |         self.validation_results = {}
34 |
35 |     def validate_schema(self, df: DataFrame, expected_schema: StructType) -> Dict[str, Any]:
   |                                                                              ^^^^
36 |         """
37 |         Validate DataFrame schema against expected schema.
   |
help: Replace with `dict`

C401 Unnecessary generator (rewrite as a set comprehension)
  --> src/project_a/legacy/data_quality_suite.py:48:29
   |
46 |         try:
47 |             actual_schema = df.schema
48 |             actual_fields = set(field.name for field in actual_schema.fields)
   |                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
49 |             expected_fields = set(field.name for field in expected_schema.fields)
   |
help: Rewrite as a set comprehension

C401 Unnecessary generator (rewrite as a set comprehension)
  --> src/project_a/legacy/data_quality_suite.py:49:31
   |
47 |             actual_schema = df.schema
48 |             actual_fields = set(field.name for field in actual_schema.fields)
49 |             expected_fields = set(field.name for field in expected_schema.fields)
   |                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
50 |
51 |             missing_fields = expected_fields - actual_fields
   |
help: Rewrite as a set comprehension

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/data_quality_suite.py:69:58
   |
67 |             return {"valid": False, "error": str(e)}
68 |
69 |     def check_completeness(self, df: DataFrame, columns: Optional[List[str]] = None) -> Dict[str, Any]:
   |                                                          ^^^^^^^^^^^^^^^^^^^
70 |         """
71 |         Check data completeness for specified columns.
   |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/legacy/data_quality_suite.py:69:67
   |
67 |             return {"valid": False, "error": str(e)}
68 |
69 |     def check_completeness(self, df: DataFrame, columns: Optional[List[str]] = None) -> Dict[str, Any]:
   |                                                                   ^^^^
70 |         """
71 |         Check data completeness for specified columns.
   |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/data_quality_suite.py:69:89
   |
67 |             return {"valid": False, "error": str(e)}
68 |
69 |     def check_completeness(self, df: DataFrame, columns: Optional[List[str]] = None) -> Dict[str, Any]:
   |                                                                                         ^^^^
70 |         """
71 |         Check data completeness for specified columns.
   |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/legacy/data_quality_suite.py:113:56
    |
111 |             return {"error": str(e)}
112 |
113 |     def check_uniqueness(self, df: DataFrame, columns: List[str]) -> Dict[str, Any]:
    |                                                        ^^^^
114 |         """
115 |         Check uniqueness of specified columns.
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/data_quality_suite.py:113:70
    |
111 |             return {"error": str(e)}
112 |
113 |     def check_uniqueness(self, df: DataFrame, columns: List[str]) -> Dict[str, Any]:
    |                                                                      ^^^^
114 |         """
115 |         Check uniqueness of specified columns.
    |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/legacy/data_quality_suite.py:143:55
    |
141 |             return {"error": str(e)}
142 |
143 |     def check_consistency(self, df: DataFrame, rules: List[Dict[str, Any]]) -> Dict[str, Any]:
    |                                                       ^^^^
144 |         """
145 |         Check data consistency against defined rules.
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/data_quality_suite.py:143:60
    |
141 |             return {"error": str(e)}
142 |
143 |     def check_consistency(self, df: DataFrame, rules: List[Dict[str, Any]]) -> Dict[str, Any]:
    |                                                            ^^^^
144 |         """
145 |         Check data consistency against defined rules.
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/data_quality_suite.py:143:80
    |
141 |             return {"error": str(e)}
142 |
143 |     def check_consistency(self, df: DataFrame, rules: List[Dict[str, Any]]) -> Dict[str, Any]:
    |                                                                                ^^^^
144 |         """
145 |         Check data consistency against defined rules.
    |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/legacy/data_quality_suite.py:187:61
    |
185 |             return {"error": str(e)}
186 |
187 |     def check_accuracy(self, df: DataFrame, accuracy_rules: List[Dict[str, Any]]) -> Dict[str, Any]:
    |                                                             ^^^^
188 |         """
189 |         Check data accuracy against defined rules.
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/data_quality_suite.py:187:66
    |
185 |             return {"error": str(e)}
186 |
187 |     def check_accuracy(self, df: DataFrame, accuracy_rules: List[Dict[str, Any]]) -> Dict[str, Any]:
    |                                                                  ^^^^
188 |         """
189 |         Check data accuracy against defined rules.
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/data_quality_suite.py:187:86
    |
185 |             return {"error": str(e)}
186 |
187 |     def check_accuracy(self, df: DataFrame, accuracy_rules: List[Dict[str, Any]]) -> Dict[str, Any]:
    |                                                                                      ^^^^
188 |         """
189 |         Check data accuracy against defined rules.
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/data_quality_suite.py:262:55
    |
261 |     def check_timeliness(self, df: DataFrame, timestamp_column: str,
262 |                         max_delay_hours: int = 24) -> Dict[str, Any]:
    |                                                       ^^^^
263 |         """
264 |         Check data timeliness.
    |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/data_quality_suite.py:305:60
    |
303 |             return {"error": str(e)}
304 |
305 |     def calculate_statistics(self, df: DataFrame, columns: Optional[List[str]] = None) -> Dict[str, Any]:
    |                                                            ^^^^^^^^^^^^^^^^^^^
306 |         """
307 |         Calculate basic statistics for numeric columns.
    |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/legacy/data_quality_suite.py:305:69
    |
303 |             return {"error": str(e)}
304 |
305 |     def calculate_statistics(self, df: DataFrame, columns: Optional[List[str]] = None) -> Dict[str, Any]:
    |                                                                     ^^^^
306 |         """
307 |         Calculate basic statistics for numeric columns.
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/data_quality_suite.py:305:91
    |
303 |             return {"error": str(e)}
304 |
305 |     def calculate_statistics(self, df: DataFrame, columns: Optional[List[str]] = None) -> Dict[str, Any]:
    |                                                                                           ^^^^
306 |         """
307 |         Calculate basic statistics for numeric columns.
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/data_quality_suite.py:358:73
    |
357 |     def detect_anomalies(self, df: DataFrame, column: str,
358 |                         method: str = "iqr", threshold: float = 1.5) -> Dict[str, Any]:
    |                                                                         ^^^^
359 |         """
360 |         Detect anomalies in a numeric column.
    |
help: Replace with `dict`

F841 Local variable `window_spec` is assigned to but never used
   --> src/project_a/legacy/data_quality_suite.py:377:17
    |
375 |             if method == "iqr":
376 |                 # Calculate quartiles using window functions
377 |                 window_spec = Window.orderBy(F.col(column))
    |                 ^^^^^^^^^^^
378 |
379 |                 quartiles_df = df.select(
    |
help: Remove assignment to unused variable `window_spec`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/data_quality_suite.py:450:52
    |
448 |             return {"error": str(e)}
449 |
450 |     def generate_quality_report(self, output_path: Optional[str] = None) -> str:
    |                                                    ^^^^^^^^^^^^^
451 |         """
452 |         Generate comprehensive data quality report.
    |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/data_quality_suite.py:488:53
    |
487 |     def run_comprehensive_validation(self, df: DataFrame,
488 |                                    expected_schema: Optional[StructType] = None,
    |                                                     ^^^^^^^^^^^^^^^^^^^^
489 |                                    completeness_columns: Optional[List[str]] = None,
490 |                                    uniqueness_columns: Optional[List[str]] = None,
    |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/data_quality_suite.py:489:58
    |
487 |     def run_comprehensive_validation(self, df: DataFrame,
488 |                                    expected_schema: Optional[StructType] = None,
489 |                                    completeness_columns: Optional[List[str]] = None,
    |                                                          ^^^^^^^^^^^^^^^^^^^
490 |                                    uniqueness_columns: Optional[List[str]] = None,
491 |                                    consistency_rules: Optional[List[Dict[str, Any]]] = None,
    |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/legacy/data_quality_suite.py:489:67
    |
487 |     def run_comprehensive_validation(self, df: DataFrame,
488 |                                    expected_schema: Optional[StructType] = None,
489 |                                    completeness_columns: Optional[List[str]] = None,
    |                                                                   ^^^^
490 |                                    uniqueness_columns: Optional[List[str]] = None,
491 |                                    consistency_rules: Optional[List[Dict[str, Any]]] = None,
    |
help: Replace with `list`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/data_quality_suite.py:490:56
    |
488 | â€¦                     expected_schema: Optional[StructType] = None,
489 | â€¦                     completeness_columns: Optional[List[str]] = None,
490 | â€¦                     uniqueness_columns: Optional[List[str]] = None,
    |                                           ^^^^^^^^^^^^^^^^^^^
491 | â€¦                     consistency_rules: Optional[List[Dict[str, Any]]] = None,
492 | â€¦                     accuracy_rules: Optional[List[Dict[str, Any]]] = None,
    |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/legacy/data_quality_suite.py:490:65
    |
488 | â€¦                     expected_schema: Optional[StructType] = None,
489 | â€¦                     completeness_columns: Optional[List[str]] = None,
490 | â€¦                     uniqueness_columns: Optional[List[str]] = None,
    |                                                    ^^^^
491 | â€¦                     consistency_rules: Optional[List[Dict[str, Any]]] = None,
492 | â€¦                     accuracy_rules: Optional[List[Dict[str, Any]]] = None,
    |
help: Replace with `list`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/data_quality_suite.py:491:55
    |
489 | â€¦                     completeness_columns: Optional[List[str]] = None,
490 | â€¦                     uniqueness_columns: Optional[List[str]] = None,
491 | â€¦                     consistency_rules: Optional[List[Dict[str, Any]]] = None,
    |                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
492 | â€¦                     accuracy_rules: Optional[List[Dict[str, Any]]] = None,
493 | â€¦                     timestamp_column: Optional[str] = None) -> Dict[str, Any]:
    |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/legacy/data_quality_suite.py:491:64
    |
489 | â€¦                     completeness_columns: Optional[List[str]] = None,
490 | â€¦                     uniqueness_columns: Optional[List[str]] = None,
491 | â€¦                     consistency_rules: Optional[List[Dict[str, Any]]] = None,
    |                                                   ^^^^
492 | â€¦                     accuracy_rules: Optional[List[Dict[str, Any]]] = None,
493 | â€¦                     timestamp_column: Optional[str] = None) -> Dict[str, Any]:
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/data_quality_suite.py:491:69
    |
489 | â€¦                     completeness_columns: Optional[List[str]] = None,
490 | â€¦                     uniqueness_columns: Optional[List[str]] = None,
491 | â€¦                     consistency_rules: Optional[List[Dict[str, Any]]] = None,
    |                                                        ^^^^
492 | â€¦                     accuracy_rules: Optional[List[Dict[str, Any]]] = None,
493 | â€¦                     timestamp_column: Optional[str] = None) -> Dict[str, Any]:
    |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/data_quality_suite.py:492:52
    |
490 |                                    uniqueness_columns: Optional[List[str]] = None,
491 |                                    consistency_rules: Optional[List[Dict[str, Any]]] = None,
492 |                                    accuracy_rules: Optional[List[Dict[str, Any]]] = None,
    |                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
493 |                                    timestamp_column: Optional[str] = None) -> Dict[str, Any]:
494 |         """
    |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/legacy/data_quality_suite.py:492:61
    |
490 |                                    uniqueness_columns: Optional[List[str]] = None,
491 |                                    consistency_rules: Optional[List[Dict[str, Any]]] = None,
492 |                                    accuracy_rules: Optional[List[Dict[str, Any]]] = None,
    |                                                             ^^^^
493 |                                    timestamp_column: Optional[str] = None) -> Dict[str, Any]:
494 |         """
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/data_quality_suite.py:492:66
    |
490 |                                    uniqueness_columns: Optional[List[str]] = None,
491 |                                    consistency_rules: Optional[List[Dict[str, Any]]] = None,
492 |                                    accuracy_rules: Optional[List[Dict[str, Any]]] = None,
    |                                                                  ^^^^
493 |                                    timestamp_column: Optional[str] = None) -> Dict[str, Any]:
494 |         """
    |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/data_quality_suite.py:493:54
    |
491 |                                    consistency_rules: Optional[List[Dict[str, Any]]] = None,
492 |                                    accuracy_rules: Optional[List[Dict[str, Any]]] = None,
493 |                                    timestamp_column: Optional[str] = None) -> Dict[str, Any]:
    |                                                      ^^^^^^^^^^^^^
494 |         """
495 |         Run comprehensive data quality validation.
    |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/data_quality_suite.py:493:79
    |
491 |                                    consistency_rules: Optional[List[Dict[str, Any]]] = None,
492 |                                    accuracy_rules: Optional[List[Dict[str, Any]]] = None,
493 |                                    timestamp_column: Optional[str] = None) -> Dict[str, Any]:
    |                                                                               ^^^^
494 |         """
495 |         Run comprehensive data quality validation.
    |
help: Replace with `dict`

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/delta_lake_standard.py:4:1
   |
 2 |   Proper Delta Lake Implementation Following Standards
 3 |   """
 4 | / import os
 5 | | import pandas as pd
 6 | | import json
 7 | | import uuid
 8 | | from datetime import datetime
 9 | | from typing import Dict, Any, List
10 | | import logging
   | |______________^
11 |
12 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/legacy/delta_lake_standard.py:9:1
   |
 7 | import uuid
 8 | from datetime import datetime
 9 | from typing import Dict, Any, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 | import logging
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/legacy/delta_lake_standard.py:9:1
   |
 7 | import uuid
 8 | from datetime import datetime
 9 | from typing import Dict, Any, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 | import logging
   |

F401 [*] `typing.List` imported but unused
  --> src/project_a/legacy/delta_lake_standard.py:9:31
   |
 7 | import uuid
 8 | from datetime import datetime
 9 | from typing import Dict, Any, List
   |                               ^^^^
10 | import logging
   |
help: Remove unused import: `typing.List`

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/delta_lake_standard.py:16:1
   |
14 | class StandardDeltaLake:
15 |     """Proper Delta Lake implementation following official standards"""
16 |     
   | ^^^^
17 |     def __init__(self, table_path: str):
18 |         self.table_path = table_path
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/delta_lake_standard.py:21:1
   |
19 |         self.delta_log_dir = os.path.join(table_path, "_delta_log")
20 |         os.makedirs(self.delta_log_dir, exist_ok=True)
21 |     
   | ^^^^
22 |     def create_initial_table(self, data: pd.DataFrame, table_name: str, schema: Dict[str, str]):
23 |         """Create initial Delta Lake table with proper metadata"""
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/delta_lake_standard.py:22:81
   |
20 |         os.makedirs(self.delta_log_dir, exist_ok=True)
21 |     
22 |     def create_initial_table(self, data: pd.DataFrame, table_name: str, schema: Dict[str, str]):
   |                                                                                 ^^^^
23 |         """Create initial Delta Lake table with proper metadata"""
24 |         logger.info(f"Creating initial Delta Lake table: {table_name}")
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/delta_lake_standard.py:25:1
   |
23 |         """Create initial Delta Lake table with proper metadata"""
24 |         logger.info(f"Creating initial Delta Lake table: {table_name}")
25 |         
   | ^^^^^^^^
26 |         # Write data as parquet
27 |         parquet_file = os.path.join(self.table_path, f"part-00000-{table_name}.parquet")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/delta_lake_standard.py:29:1
   |
27 |         parquet_file = os.path.join(self.table_path, f"part-00000-{table_name}.parquet")
28 |         data.to_parquet(parquet_file, index=False)
29 |         
   | ^^^^^^^^
30 |         # Create proper Delta Lake transaction log
31 |         transaction_log = {
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/delta_lake_standard.py:56:1
   |
54 |             }
55 |         }
56 |         
   | ^^^^^^^^
57 |         # Write transaction log
58 |         self._write_transaction_log(transaction_log, 0)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/delta_lake_standard.py:60:1
   |
58 |         self._write_transaction_log(transaction_log, 0)
59 |         logger.info(f"âœ… Created initial table with {len(data)} records")
60 |     
   | ^^^^
61 |     def append_data(self, new_data: pd.DataFrame, table_name: str, version: int):
62 |         """Append new data to Delta Lake table"""
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/delta_lake_standard.py:64:1
   |
62 |         """Append new data to Delta Lake table"""
63 |         logger.info(f"Appending data to {table_name} (version {version})")
64 |         
   | ^^^^^^^^
65 |         # Write new parquet file
66 |         parquet_file = os.path.join(self.table_path, f"part-{version:05d}-{table_name}-v{version+1}.parquet")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/delta_lake_standard.py:68:1
   |
66 |         parquet_file = os.path.join(self.table_path, f"part-{version:05d}-{table_name}-v{version+1}.parquet")
67 |         new_data.to_parquet(parquet_file, index=False)
68 |         
   | ^^^^^^^^
69 |         # Create append transaction log
70 |         transaction_log = {
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/delta_lake_standard.py:80:1
   |
78 |             }
79 |         }
80 |         
   | ^^^^^^^^
81 |         # Write transaction log
82 |         self._write_transaction_log(transaction_log, version)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/delta_lake_standard.py:84:1
   |
82 |         self._write_transaction_log(transaction_log, version)
83 |         logger.info(f"âœ… Appended {len(new_data)} records (version {version})")
84 |     
   | ^^^^
85 |     def update_data(self, updated_data: pd.DataFrame, table_name: str, version: int):
86 |         """Update data in Delta Lake table"""
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/delta_lake_standard.py:88:1
   |
86 |         """Update data in Delta Lake table"""
87 |         logger.info(f"Updating data in {table_name} (version {version})")
88 |         
   | ^^^^^^^^
89 |         # Write new parquet file
90 |         parquet_file = os.path.join(self.table_path, f"part-{version:05d}-{table_name}-v{version+1}.parquet")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/delta_lake_standard.py:92:1
   |
90 |         parquet_file = os.path.join(self.table_path, f"part-{version:05d}-{table_name}-v{version+1}.parquet")
91 |         updated_data.to_parquet(parquet_file, index=False)
92 |         
   | ^^^^^^^^
93 |         # Create update transaction log
94 |         transaction_log = {
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:104:1
    |
102 |             }
103 |         }
104 |         
    | ^^^^^^^^
105 |         # Write transaction log
106 |         self._write_transaction_log(transaction_log, version)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:108:1
    |
106 |         self._write_transaction_log(transaction_log, version)
107 |         logger.info(f"âœ… Updated to {len(updated_data)} records (version {version})")
108 |     
    | ^^^^
109 |     def _create_schema_string(self, schema: Dict[str, str]) -> str:
110 |         """Create proper Delta Lake schema string"""
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/delta_lake_standard.py:109:45
    |
107 |         logger.info(f"âœ… Updated to {len(updated_data)} records (version {version})")
108 |     
109 |     def _create_schema_string(self, schema: Dict[str, str]) -> str:
    |                                             ^^^^
110 |         """Create proper Delta Lake schema string"""
111 |         fields = []
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:126:1
    |
124 |             else:
125 |                 delta_type = 'string'
126 |             
    | ^^^^^^^^^^^^
127 |             fields.append({
128 |                 "name": col_name,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:133:1
    |
131 |                 "metadata": {}
132 |             })
133 |         
    | ^^^^^^^^
134 |         schema_obj = {
135 |             "type": "struct",
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:138:1
    |
136 |             "fields": fields
137 |         }
138 |         
    | ^^^^^^^^
139 |         return json.dumps(schema_obj)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:140:1
    |
139 |         return json.dumps(schema_obj)
140 |     
    | ^^^^
141 |     def _create_stats_json(self, data: pd.DataFrame) -> str:
142 |         """Create proper statistics JSON for Delta Lake"""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:149:1
    |
147 |             "nullCount": {}
148 |         }
149 |         
    | ^^^^^^^^
150 |         # Calculate statistics for numeric columns
151 |         for col in data.columns:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:156:1
    |
154 |                 stats["maxValues"][col] = float(data[col].max())
155 |             stats["nullCount"][col] = int(data[col].isnull().sum())
156 |         
    | ^^^^^^^^
157 |         return json.dumps(stats)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:158:1
    |
157 |         return json.dumps(stats)
158 |     
    | ^^^^
159 |     def _write_transaction_log(self, transaction_log: Dict[str, Any], version: int):
160 |         """Write transaction log with proper format"""
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/delta_lake_standard.py:159:55
    |
157 |         return json.dumps(stats)
158 |     
159 |     def _write_transaction_log(self, transaction_log: Dict[str, Any], version: int):
    |                                                       ^^^^
160 |         """Write transaction log with proper format"""
161 |         log_file = os.path.join(self.delta_log_dir, f"{version:020d}.json")
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:164:1
    |
162 |         with open(log_file, 'w') as f:
163 |             json.dump(transaction_log, f, indent=2)
164 |         
    | ^^^^^^^^
165 |         # Create checksum file
166 |         checksum_file = os.path.join(self.delta_log_dir, f"{version:020d}.crc")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:169:1
    |
167 |         with open(checksum_file, 'w') as f:
168 |             f.write("crc_placeholder")
169 |     
    | ^^^^
170 |     def get_table_info(self) -> Dict[str, Any]:
171 |         """Get table information"""
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/delta_lake_standard.py:170:33
    |
168 |             f.write("crc_placeholder")
169 |     
170 |     def get_table_info(self) -> Dict[str, Any]:
    |                                 ^^^^
171 |         """Get table information"""
172 |         parquet_files = [f for f in os.listdir(self.table_path) if f.endswith('.parquet')]
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:174:1
    |
172 |         parquet_files = [f for f in os.listdir(self.table_path) if f.endswith('.parquet')]
173 |         log_files = [f for f in os.listdir(self.delta_log_dir) if f.endswith('.json')]
174 |         
    | ^^^^^^^^
175 |         return {
176 |             "table_path": self.table_path,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:181:1
    |
179 |             "latest_version": len(log_files) - 1 if log_files else -1
180 |         }
181 |     
    | ^^^^
182 |     def show_version_history(self):
183 |         """Show version history"""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:185:1
    |
183 |         """Show version history"""
184 |         log_files = sorted([f for f in os.listdir(self.delta_log_dir) if f.endswith('.json')])
185 |         
    | ^^^^^^^^
186 |         print(f"ðŸ“Š Version history for {self.table_path}:")
187 |         for i, log_file in enumerate(log_files):
    |
help: Remove whitespace from blank line

UP015 [*] Unnecessary mode argument
   --> src/project_a/legacy/delta_lake_standard.py:189:33
    |
187 |         for i, log_file in enumerate(log_files):
188 |             log_path = os.path.join(self.delta_log_dir, log_file)
189 |             with open(log_path, 'r') as f:
    |                                 ^^^
190 |                 log_data = json.load(f)
    |
help: Remove mode argument

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:191:1
    |
189 |             with open(log_path, 'r') as f:
190 |                 log_data = json.load(f)
191 |             
    | ^^^^^^^^^^^^
192 |             if 'metaData' in log_data:
193 |                 print(f"   Version {i}: Initial load")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:205:1
    |
203 |     """Create Delta Lake tables following proper standards"""
204 |     logger.info("ðŸš€ Creating standard Delta Lake tables...")
205 |     
    | ^^^^
206 |     # Define table schemas
207 |     schemas = {
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:228:1
    |
226 |         }
227 |     }
228 |     
    | ^^^^
229 |     # Create tables for each layer
230 |     layers = ["bronze", "silver", "gold"]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:231:1
    |
229 |     # Create tables for each layer
230 |     layers = ["bronze", "silver", "gold"]
231 |     
    | ^^^^
232 |     for layer in layers:
233 |         layer_path = f"data/lakehouse_delta_standard/{layer}"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:235:1
    |
233 |         layer_path = f"data/lakehouse_delta_standard/{layer}"
234 |         os.makedirs(layer_path, exist_ok=True)
235 |         
    | ^^^^^^^^
236 |         for table_name, schema in schemas.items():
237 |             if layer == "gold":
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:252:1
    |
250 |                         "total_revenue": "float64"
251 |                     }
252 |             
    | ^^^^^^^^^^^^
253 |             table_path = os.path.join(layer_path, table_name)
254 |             os.makedirs(table_path, exist_ok=True)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:255:1
    |
253 |             table_path = os.path.join(layer_path, table_name)
254 |             os.makedirs(table_path, exist_ok=True)
255 |             
    | ^^^^^^^^^^^^
256 |             # Create Delta Lake instance
257 |             delta_lake = StandardDeltaLake(table_path)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:258:1
    |
256 |             # Create Delta Lake instance
257 |             delta_lake = StandardDeltaLake(table_path)
258 |             
    | ^^^^^^^^^^^^
259 |             # Generate sample data
260 |             if "customers" in table_name:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:268:1
    |
266 |             elif "revenue" in table_name:
267 |                 data = generate_revenue_data()
268 |             
    | ^^^^^^^^^^^^
269 |             # Create initial table
270 |             delta_lake.create_initial_table(data, table_name, schema)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:271:1
    |
269 |             # Create initial table
270 |             delta_lake.create_initial_table(data, table_name, schema)
271 |             
    | ^^^^^^^^^^^^
272 |             # Create multiple versions
273 |             for version in range(1, 6):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:282:1
    |
280 |                 elif "revenue" in table_name:
281 |                     new_data = generate_updated_revenue_data(data, version)
282 |                 
    | ^^^^^^^^^^^^^^^^
283 |                 delta_lake.append_data(new_data, table_name, version)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:284:1
    |
283 |                 delta_lake.append_data(new_data, table_name, version)
284 |             
    | ^^^^^^^^^^^^
285 |             # Show version history
286 |             delta_lake.show_version_history()
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
   --> src/project_a/legacy/delta_lake_standard.py:292:5
    |
290 |   def generate_customers_data():
291 |       """Generate sample customers data"""
292 | /     import numpy as np
293 | |     from datetime import datetime, timedelta
    | |____________________________________________^
294 |       
295 |       data = {
    |
help: Organize imports

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:294:1
    |
292 |     import numpy as np
293 |     from datetime import datetime, timedelta
294 |     
    | ^^^^
295 |     data = {
296 |         'customer_id': range(1, 1001),
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:303:1
    |
301 |         'country': np.random.choice(['USA', 'Canada', 'UK', 'Germany'], 1000)
302 |     }
303 |     
    | ^^^^
304 |     return pd.DataFrame(data)
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
   --> src/project_a/legacy/delta_lake_standard.py:309:5
    |
307 |   def generate_orders_data():
308 |       """Generate sample orders data"""
309 | /     import numpy as np
310 | |     from datetime import datetime, timedelta
    | |____________________________________________^
311 |       
312 |       data = {
    |
help: Organize imports

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:311:1
    |
309 |     import numpy as np
310 |     from datetime import datetime, timedelta
311 |     
    | ^^^^
312 |     data = {
313 |         'order_id': [f'ORD_{i:06d}' for i in range(1, 5001)],
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:322:1
    |
320 |         'status': np.random.choice(['Completed', 'Processing', 'Cancelled'], 5000)
321 |     }
322 |     
    | ^^^^
323 |     data['total_amount'] = data['quantity'] * data['unit_price']
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:324:1
    |
323 |     data['total_amount'] = data['quantity'] * data['unit_price']
324 |     
    | ^^^^
325 |     return pd.DataFrame(data)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:335:1
    |
333 |         'avg_lifetime_days': [183.90, 200.52, 215.67]
334 |     }
335 |     
    | ^^^^
336 |     return pd.DataFrame(data)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:345:1
    |
343 |         'total_revenue': [333242.69, 884222.00, 445123.45, 678901.23]
344 |     }
345 |     
    | ^^^^
346 |     return pd.DataFrame(data)
    |
help: Remove whitespace from blank line

F841 Local variable `new_data` is assigned to but never used
   --> src/project_a/legacy/delta_lake_standard.py:351:5
    |
349 | def generate_updated_customers_data(original_data, version):
350 |     """Generate updated customers data"""
351 |     new_data = original_data.copy()
    |     ^^^^^^^^
352 |     
353 |     # Add new customers
    |
help: Remove assignment to unused variable `new_data`

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:352:1
    |
350 |     """Generate updated customers data"""
351 |     new_data = original_data.copy()
352 |     
    | ^^^^
353 |     # Add new customers
354 |     new_customers = original_data.head(50).copy()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:358:1
    |
356 |     new_customers['name'] = [f'Customer_v{version}_{i}' for i in range(len(original_data) + version*50 + 1, len(original_data) + (verâ€¦
357 |     new_customers['email'] = [f'customer_v{version}_{i}@example.com' for i in range(len(original_data) + version*50 + 1, len(originalâ€¦
358 |     
    | ^^^^
359 |     return pd.concat([original_data, new_customers], ignore_index=True)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:365:1
    |
363 |     """Generate updated orders data"""
364 |     new_data = original_data.copy()
365 |     
    | ^^^^
366 |     # Increase amounts
367 |     new_data['total_amount'] = new_data['total_amount'] * (1.1 ** version)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:369:1
    |
367 |     new_data['total_amount'] = new_data['total_amount'] * (1.1 ** version)
368 |     new_data['unit_price'] = new_data['unit_price'] * (1.1 ** version)
369 |     
    | ^^^^
370 |     return new_data
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:376:1
    |
374 |     """Generate updated analytics data"""
375 |     new_data = original_data.copy()
376 |     
    | ^^^^
377 |     # Increase metrics
378 |     new_data['customer_count'] = new_data['customer_count'] + (version * 10)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:380:1
    |
378 |     new_data['customer_count'] = new_data['customer_count'] + (version * 10)
379 |     new_data['avg_lifetime_days'] = new_data['avg_lifetime_days'] + (version * 5)
380 |     
    | ^^^^
381 |     return new_data
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:387:1
    |
385 |     """Generate updated revenue data"""
386 |     new_data = original_data.copy()
387 |     
    | ^^^^
388 |     # Increase revenue
389 |     new_data['total_revenue'] = new_data['total_revenue'] * (1.15 ** version)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_lake_standard.py:391:1
    |
389 |     new_data['total_revenue'] = new_data['total_revenue'] * (1.15 ** version)
390 |     new_data['month'] = new_data['month'] + version
391 |     
    | ^^^^
392 |     return new_data
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/delta_utils.py:5:1
   |
 3 |   """
 4 |
 5 | / import logging
 6 | | from typing import Optional
 7 | | from pyspark.sql import SparkSession, DataFrame
 8 | | from pyspark.sql.functions import col
   | |_____________________________________^
 9 |
10 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

F401 [*] `typing.Optional` imported but unused
 --> src/project_a/legacy/delta_utils.py:6:20
  |
5 | import logging
6 | from typing import Optional
  |                    ^^^^^^^^
7 | from pyspark.sql import SparkSession, DataFrame
8 | from pyspark.sql.functions import col
  |
help: Remove unused import: `typing.Optional`

W293 Blank line contains whitespace
  --> src/project_a/legacy/delta_utils.py:16:1
   |
14 |     """
15 |     Read data from a Delta table.
16 |     
   | ^^^^
17 |     Args:
18 |         spark: SparkSession object
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/delta_utils.py:20:1
   |
18 |         spark: SparkSession object
19 |         path: Path to the Delta table
20 |         
   | ^^^^^^^^
21 |     Returns:
22 |         DataFrame with data from the Delta table
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/delta_utils.py:36:1
   |
34 |     """
35 |     Write DataFrame to a Delta table.
36 |     
   | ^^^^
37 |     Args:
38 |         df: DataFrame to write
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/delta_utils.py:54:1
   |
52 |     Utility class for Delta Lake operations.
53 |     """
54 |     
   | ^^^^
55 |     def __init__(self, spark: SparkSession):
56 |         self.spark = spark
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/delta_utils.py:57:1
   |
55 |     def __init__(self, spark: SparkSession):
56 |         self.spark = spark
57 |     
   | ^^^^
58 |     def optimize_table(self, table_path: str) -> dict:
59 |         """
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/delta_utils.py:61:1
   |
59 |         """
60 |         Optimize a Delta table.
61 |         
   | ^^^^^^^^
62 |         Args:
63 |             table_path: Path to the Delta table
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/delta_utils.py:64:1
   |
62 |         Args:
63 |             table_path: Path to the Delta table
64 |             
   | ^^^^^^^^^^^^
65 |         Returns:
66 |             Dictionary with optimization results
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/delta_utils.py:76:1
   |
74 |             logger.error(f"Failed to optimize table {table_path}: {e}")
75 |             return {"success": False, "error": str(e)}
76 |     
   | ^^^^
77 |     def set_retention_policy(self, table_path: str, retention_days: int = 7) -> dict:
78 |         """
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/delta_utils.py:80:1
   |
78 |         """
79 |         Set retention policy for a Delta table.
80 |         
   | ^^^^^^^^
81 |         Args:
82 |             table_path: Path to the Delta table
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/delta_utils.py:84:1
   |
82 |             table_path: Path to the Delta table
83 |             retention_days: Number of days to retain history
84 |             
   | ^^^^^^^^^^^^
85 |         Returns:
86 |             Dictionary with retention policy results
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/delta_utils.py:96:1
   |
94 |             logger.error(f"Failed to set retention policy for {table_path}: {e}")
95 |             return {"success": False, "error": str(e)}
96 |     
   | ^^^^
97 |     def cleanup_old_partitions(self, table_path: str, partition_column: str, days_to_keep: int = 30) -> dict:
98 |         """
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/delta_utils.py:100:1
    |
 98 |         """
 99 |         Clean up old partitions from a Delta table.
100 |         
    | ^^^^^^^^
101 |         Args:
102 |             table_path: Path to the Delta table
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/delta_utils.py:105:1
    |
103 |             partition_column: Name of the partition column
104 |             days_to_keep: Number of days of data to keep
105 |             
    | ^^^^^^^^^^^^
106 |         Returns:
107 |             Dictionary with cleanup results
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_utils.py:112:1
    |
110 |             from delta.tables import DeltaTable
111 |             from pyspark.sql.functions import current_date, date_sub
112 |             
    | ^^^^^^^^^^^^
113 |             delta_table = DeltaTable.forPath(self.spark, table_path)
114 |             cutoff_date = current_date() - date_sub(current_date(), days_to_keep)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_utils.py:115:1
    |
113 |             delta_table = DeltaTable.forPath(self.spark, table_path)
114 |             cutoff_date = current_date() - date_sub(current_date(), days_to_keep)
115 |             
    | ^^^^^^^^^^^^
116 |             # Delete old partitions
117 |             delta_table.delete(col(partition_column) < cutoff_date)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_utils.py:118:1
    |
116 |             # Delete old partitions
117 |             delta_table.delete(col(partition_column) < cutoff_date)
118 |             
    | ^^^^^^^^^^^^
119 |             return {"success": True, "message": f"Cleaned up partitions older than {days_to_keep} days"}
120 |         except Exception as e:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_utils.py:123:1
    |
121 |             logger.error(f"Failed to cleanup old partitions for {table_path}: {e}")
122 |             return {"success": False, "error": str(e)}
123 |     
    | ^^^^
124 |     def get_table_stats(self, table_path: str) -> dict:
125 |         """
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/delta_utils.py:127:1
    |
125 |         """
126 |         Get statistics for a Delta table.
127 |         
    | ^^^^^^^^
128 |         Args:
129 |             table_path: Path to the Delta table
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/delta_utils.py:130:1
    |
128 |         Args:
129 |             table_path: Path to the Delta table
130 |             
    | ^^^^^^^^^^^^
131 |         Returns:
132 |             Dictionary with table statistics
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_utils.py:138:1
    |
136 |             count = df.count()
137 |             columns = len(df.columns)
138 |             
    | ^^^^^^^^^^^^
139 |             return {
140 |                 "success": True,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_utils.py:148:1
    |
146 |             logger.error(f"Failed to get stats for {table_path}: {e}")
147 |             return {"success": False, "error": str(e)}
148 |     
    | ^^^^
149 |     def run_maintenance_routine(self, table_path: str) -> dict:
150 |         """
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/delta_utils.py:152:1
    |
150 |         """
151 |         Run a complete maintenance routine on a Delta table.
152 |         
    | ^^^^^^^^
153 |         Args:
154 |             table_path: Path to the Delta table
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/delta_utils.py:155:1
    |
153 |         Args:
154 |             table_path: Path to the Delta table
155 |             
    | ^^^^^^^^^^^^
156 |         Returns:
157 |             Dictionary with maintenance results
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_utils.py:161:1
    |
159 |         try:
160 |             results = []
161 |             
    | ^^^^^^^^^^^^
162 |             # Optimize table
163 |             opt_result = self.optimize_table(table_path)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_utils.py:165:1
    |
163 |             opt_result = self.optimize_table(table_path)
164 |             results.append(opt_result)
165 |             
    | ^^^^^^^^^^^^
166 |             # Set retention policy
167 |             retention_result = self.set_retention_policy(table_path)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_utils.py:169:1
    |
167 |             retention_result = self.set_retention_policy(table_path)
168 |             results.append(retention_result)
169 |             
    | ^^^^^^^^^^^^
170 |             # Get stats
171 |             stats_result = self.get_table_stats(table_path)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/delta_utils.py:173:1
    |
171 |             stats_result = self.get_table_stats(table_path)
172 |             results.append(stats_result)
173 |             
    | ^^^^^^^^^^^^
174 |             return {
175 |                 "success": all(r.get("success", False) for r in results),
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/disaster_recovery.py:6:1
   |
 4 |   """
 5 |
 6 | / import logging
 7 | | import json
 8 | | import os
 9 | | import time
10 | | from datetime import datetime, timedelta
11 | | from typing import Dict, List, Any, Optional
12 | | from pathlib import Path
13 | | from dataclasses import dataclass, asdict
14 | |
15 | | from pyspark.sql import SparkSession
   | |____________________________________^
16 |
17 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/legacy/disaster_recovery.py:11:1
   |
 9 | import time
10 | from datetime import datetime, timedelta
11 | from typing import Dict, List, Any, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
12 | from pathlib import Path
13 | from dataclasses import dataclass, asdict
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/legacy/disaster_recovery.py:11:1
   |
 9 | import time
10 | from datetime import datetime, timedelta
11 | from typing import Dict, List, Any, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
12 | from pathlib import Path
13 | from dataclasses import dataclass, asdict
   |

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/disaster_recovery.py:21:5
   |
19 |   # Azure imports
20 |   try:
21 | /     from azure.storage.blob import BlobServiceClient
22 | |     from azure.identity import DefaultAzureCredential
23 | |     from azure.keyvault.secrets import SecretClient
   | |___________________________________________________^
24 |       AZURE_AVAILABLE = True
25 |   except ImportError:
   |
help: Organize imports

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/disaster_recovery.py:42:23
   |
40 |     encryption: bool = True
41 |     cross_region: bool = False
42 |     secondary_region: Optional[str] = None
   |                       ^^^^^^^^^^^^^
   |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/legacy/disaster_recovery.py:52:13
   |
50 |     target_storage: str
51 |     replication_type: str  # sync, async, geo
52 |     tables: List[str]
   |             ^^^^
53 |     frequency_minutes: int
54 |     enabled: bool = True
   |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/disaster_recovery.py:63:53
   |
61 |     """
62 |
63 |     def __init__(self, spark: SparkSession, config: Dict[str, Any]):
   |                                                     ^^^^
64 |         self.spark = spark
65 |         self.config = config
   |
help: Replace with `dict`

UP015 [*] Unnecessary mode argument
   --> src/project_a/legacy/disaster_recovery.py:122:42
    |
120 |         try:
121 |             if os.path.exists(strategy_path):
122 |                 with open(strategy_path, 'r') as f:
    |                                          ^^^
123 |                     strategy_data = json.load(f)
    |
help: Remove mode argument

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/legacy/disaster_recovery.py:167:107
    |
165 |             raise
166 |
167 |     def load_replication_configs(self, config_dir: str = "data/disaster_recovery/replication_configs") -> List[ReplicationConfig]:
    |                                                                                                           ^^^^
168 |         """Load replication configurations from directory."""
169 |         configs = []
    |
help: Replace with `list`

UP015 [*] Unnecessary mode argument
   --> src/project_a/legacy/disaster_recovery.py:176:48
    |
174 |                 for config_file in config_path.glob("*.json"):
175 |                     try:
176 |                         with open(config_file, 'r') as f:
    |                                                ^^^
177 |                             config_data = json.load(f)
    |
help: Remove mode argument

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/legacy/disaster_recovery.py:197:72
    |
195 |             return []
196 |
197 |     def _create_default_replication_configs(self, config_dir: Path) -> List[ReplicationConfig]:
    |                                                                        ^^^^
198 |         """Create default replication configurations."""
199 |         configs = []
    |
help: Replace with `list`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/legacy/disaster_recovery.py:236:64
    |
234 |         return configs
235 |
236 |     def execute_backup(self, strategy: BackupStrategy, tables: List[str]) -> Dict[str, Any]:
    |                                                                ^^^^
237 |         """Execute backup operation based on strategy."""
238 |         backup_result = {
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/disaster_recovery.py:236:78
    |
234 |         return configs
235 |
236 |     def execute_backup(self, strategy: BackupStrategy, tables: List[str]) -> Dict[str, Any]:
    |                                                                              ^^^^
237 |         """Execute backup operation based on strategy."""
238 |         backup_result = {
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/disaster_recovery.py:298:93
    |
296 |         return backup_result
297 |
298 |     def _backup_table(self, table_name: str, backup_dir: Path, strategy: BackupStrategy) -> Dict[str, Any]:
    |                                                                                             ^^^^
299 |         """Backup individual table."""
300 |         table_backup = {
    |
help: Replace with `dict`

E722 Do not use bare `except`
   --> src/project_a/legacy/disaster_recovery.py:333:17
    |
331 |                     df = self.spark.read.parquet(table_path)
332 |                     table_backup["record_count"] = df.count()
333 |                 except:
    |                 ^^^^^^
334 |                     table_backup["record_count"] = 0
    |

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/disaster_recovery.py:354:51
    |
352 |         return table_backup
353 |
354 |     def _get_table_path(self, table_name: str) -> Optional[str]:
    |                                                   ^^^^^^^^^^^^^
355 |         """Get the file path for a table."""
356 |         table_mappings = {
    |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/disaster_recovery.py:443:65
    |
441 |             logger.warning(f"Failed to cleanup old backups: {e}")
442 |
443 |     def execute_replication(self, config: ReplicationConfig) -> Dict[str, Any]:
    |                                                                 ^^^^
444 |         """Execute replication operation based on configuration."""
445 |         replication_result = {
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/disaster_recovery.py:489:79
    |
487 |         return replication_result
488 |
489 |     def _replicate_table(self, table_name: str, config: ReplicationConfig) -> Dict[str, Any]:
    |                                                                               ^^^^
490 |         """Replicate individual table."""
491 |         table_repl = {
    |
help: Replace with `dict`

E722 Do not use bare `except`
   --> src/project_a/legacy/disaster_recovery.py:524:17
    |
522 |                     df = self.spark.read.parquet(source_path)
523 |                     table_repl["record_count"] = df.count()
524 |                 except:
    |                 ^^^^^^
525 |                     table_repl["record_count"] = 0
    |

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/disaster_recovery.py:555:32
    |
553 |             logger.warning(f"Failed to save replication metrics: {e}")
554 |
555 |     def get_dr_status(self) -> Dict[str, Any]:
    |                                ^^^^
556 |         """Get overall disaster recovery status."""
557 |         status = {
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/disaster_recovery.py:590:34
    |
588 |         return status
589 |
590 |     def run_dr_workflow(self) -> Dict[str, Any]:
    |                                  ^^^^
591 |         """Run complete disaster recovery workflow."""
592 |         workflow_result = {
    |
help: Replace with `dict`

I001 [*] Import block is un-sorted or un-formatted
   --> src/project_a/legacy/disaster_recovery.py:647:5
    |
645 |   def main():
646 |       """Main entry point for disaster recovery operations."""
647 | /     from .utils import get_spark_session
648 | |     from .config_loader import load_config_resolved
    | |___________________________________________________^
649 |
650 |       # Load configuration
    |
help: Organize imports

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/dq/run_ge_customer_behavior.py:6:1
   |
 4 |   Runs GE checks on customer behavior table with fail-fast on critical violations.
 5 |   """
 6 | / import os
 7 | | import logging
 8 | | from typing import Optional, Dict, Any
 9 | | from pyspark.sql import SparkSession
   | |____________________________________^
10 |
11 |   try:
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
 --> src/project_a/legacy/dq/run_ge_customer_behavior.py:8:1
  |
6 | import os
7 | import logging
8 | from typing import Optional, Dict, Any
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
9 | from pyspark.sql import SparkSession
  |

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/dq/run_ge_customer_behavior.py:29:18
   |
29 | def main(config: Optional[Dict[str, Any]] = None):
   |                  ^^^^^^^^^^^^^^^^^^^^^^^^
30 |     """Run GE DQ checks on customer behavior bronze table."""
31 |     if not GE_AVAILABLE:
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/dq/run_ge_customer_behavior.py:29:27
   |
29 | def main(config: Optional[Dict[str, Any]] = None):
   |                           ^^^^
30 |     """Run GE DQ checks on customer behavior bronze table."""
31 |     if not GE_AVAILABLE:
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/dq/run_ge_customer_behavior.py:34:1
   |
32 |         logger.warning("Great Expectations not installed, skipping DQ checks")
33 |         return
34 |     
   | ^^^^
35 |     spark = get_spark()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/dq/run_ge_customer_behavior.py:36:1
   |
35 |     spark = get_spark()
36 |     
   | ^^^^
37 |     # 1. Read bronze table (Redshift â†’ S3 â†’ bronze landed data)
38 |     # Use config-based bucket resolution
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/dq/run_ge_customer_behavior.py:47:1
   |
45 |         .format("parquet") \
46 |         .load(f"s3a://{bucket}/bronze/customer_behavior/")
47 |     
   | ^^^^
48 |     logger.info(f"Loaded {df.count()} records from bronze/customer_behavior")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/dq/run_ge_customer_behavior.py:49:1
   |
48 |     logger.info(f"Loaded {df.count()} records from bronze/customer_behavior")
49 |     
   | ^^^^
50 |     # 2. Convert to pandas for GE (or use SparkDFDataset)
51 |     try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/dq/run_ge_customer_behavior.py:56:1
   |
54 |         logger.error(f"Failed to convert to pandas: {e}")
55 |         raise
56 |     
   | ^^^^
57 |     # Initialize GE context
58 |     context = gx.get_context()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/dq/run_ge_customer_behavior.py:59:1
   |
57 |     # Initialize GE context
58 |     context = gx.get_context()
59 |     
   | ^^^^
60 |     # 3. Ad-hoc suite (can be loaded from config/dq.yaml)
61 |     suite = context.add_or_update_expectation_suite("customer_behavior_suite")
   |
help: Remove whitespace from blank line

F841 Local variable `suite` is assigned to but never used
  --> src/project_a/legacy/dq/run_ge_customer_behavior.py:61:5
   |
60 |     # 3. Ad-hoc suite (can be loaded from config/dq.yaml)
61 |     suite = context.add_or_update_expectation_suite("customer_behavior_suite")
   |     ^^^^^
62 |     
63 |     # Create validator
   |
help: Remove assignment to unused variable `suite`

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/dq/run_ge_customer_behavior.py:62:1
   |
60 |     # 3. Ad-hoc suite (can be loaded from config/dq.yaml)
61 |     suite = context.add_or_update_expectation_suite("customer_behavior_suite")
62 |     
   | ^^^^
63 |     # Create validator
64 |     batch = context.sources.pandas_default.read_dataframe(pdf)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/dq/run_ge_customer_behavior.py:66:1
   |
64 |     batch = context.sources.pandas_default.read_dataframe(pdf)
65 |     validator = batch.validate(expectation_suite_name="customer_behavior_suite")
66 |     
   | ^^^^
67 |     # Core DQ checks
68 |     logger.info("Running DQ expectations...")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/dq/run_ge_customer_behavior.py:69:1
   |
67 |     # Core DQ checks
68 |     logger.info("Running DQ expectations...")
69 |     
   | ^^^^
70 |     validator.expect_column_values_to_not_be_null("event_id")
71 |     validator.expect_column_values_to_not_be_null("customer_id")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/dq/run_ge_customer_behavior.py:78:1
   |
76 |     )
77 |     validator.expect_column_values_to_match_regex("session_id", r"^SESS-\d{6}$")
78 |     
   | ^^^^
79 |     # Run validation
80 |     result = validator.validate()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/dq/run_ge_customer_behavior.py:81:1
   |
79 |     # Run validation
80 |     result = validator.validate()
81 |     
   | ^^^^
82 |     # 4. Fail fast on critical violations
83 |     if not result["success"]:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/dq/run_ge_customer_behavior.py:86:1
   |
84 |         error_msg = f"DQ failed for customer_behavior: {result.get('statistics', {})}"
85 |         logger.error(error_msg)
86 |         
   | ^^^^^^^^
87 |         # In prod: send to SNS/Slack/CloudWatch
88 |         # send_alert("DQ_FAILURE", error_msg)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/dq/run_ge_customer_behavior.py:89:1
   |
87 |         # In prod: send to SNS/Slack/CloudWatch
88 |         # send_alert("DQ_FAILURE", error_msg)
89 |         
   | ^^^^^^^^
90 |         raise RuntimeError(error_msg)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/dq/run_ge_customer_behavior.py:91:1
   |
90 |         raise RuntimeError(error_msg)
91 |     
   | ^^^^
92 |     logger.info("âœ… DQ passed for customer_behavior")
93 |     return result
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/dq/smoke.py:6:1
   |
 4 |   """
 5 |
 6 | / import logging
 7 | | from pyspark.sql import DataFrame
 8 | | from typing import List, Optional
   | |_________________________________^
 9 |
10 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/legacy/dq/smoke.py:8:1
   |
 6 | import logging
 7 | from pyspark.sql import DataFrame
 8 | from typing import List, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 9 |
10 | logger = logging.getLogger(__name__)
   |

F401 [*] `typing.Optional` imported but unused
  --> src/project_a/legacy/dq/smoke.py:8:26
   |
 6 | import logging
 7 | from pyspark.sql import DataFrame
 8 | from typing import List, Optional
   |                          ^^^^^^^^
 9 |
10 | logger = logging.getLogger(__name__)
   |
help: Remove unused import: `typing.Optional`

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/dq/smoke.py:18:1
   |
16 |     null_count = df.filter(f"{col} IS NULL").count()
17 |     null_percentage = null_count / total_count if total_count > 0 else 0
18 |     
   | ^^^^
19 |     if null_percentage > threshold:
20 |         raise ValueError(f"Column '{col}' has {null_percentage:.2%} nulls (threshold: {threshold:.2%})")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/dq/smoke.py:21:1
   |
19 |     if null_percentage > threshold:
20 |         raise ValueError(f"Column '{col}' has {null_percentage:.2%} nulls (threshold: {threshold:.2%})")
21 |     
   | ^^^^
22 |     logger.info(f"âœ“ Column '{col}': {null_percentage:.2%} nulls (within threshold)")
   |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/legacy/dq/smoke.py:25:47
   |
25 | def assert_no_duplicates(df: DataFrame, cols: List[str], threshold: float = 0.05) -> None:
   |                                               ^^^^
26 |     """Assert that duplicate records are within acceptable limits."""
27 |     total_count = df.count()
   |
help: Replace with `list`

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/dq/smoke.py:30:1
   |
28 |     distinct_count = df.select(*cols).distinct().count()
29 |     duplicate_percentage = (total_count - distinct_count) / total_count if total_count > 0 else 0
30 |     
   | ^^^^
31 |     if duplicate_percentage > threshold:
32 |         raise ValueError(f"Duplicate records: {duplicate_percentage:.2%} (threshold: {threshold:.2%})")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/dq/smoke.py:33:1
   |
31 |     if duplicate_percentage > threshold:
32 |         raise ValueError(f"Duplicate records: {duplicate_percentage:.2%} (threshold: {threshold:.2%})")
33 |     
   | ^^^^
34 |     logger.info(f"âœ“ Duplicates: {duplicate_percentage:.2%} (within threshold)")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/dq/smoke.py:40:1
   |
38 |     """Assert that a numeric column has only positive values."""
39 |     negative_count = df.selectExpr(f"sum(case when {col} < 0 then 1 else 0 end) as n").collect()[0]["n"]
40 |     
   | ^^^^
41 |     if negative_count > 0:
42 |         raise ValueError(f"Column '{col}' has {negative_count} negative values")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/dq/smoke.py:43:1
   |
41 |     if negative_count > 0:
42 |         raise ValueError(f"Column '{col}' has {negative_count} negative values")
43 |     
   | ^^^^
44 |     logger.info(f"âœ“ Column '{col}': no negative values")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/dq/smoke.py:50:1
   |
48 |     """Assert that a date column has valid dates."""
49 |     invalid_count = df.selectExpr(f"sum(case when {col} is null or {col} < '1900-01-01' then 1 else 0 end) as n").collect()[0]["n"]
50 |     
   | ^^^^
51 |     if invalid_count > 0:
52 |         raise ValueError(f"Column '{col}' has {invalid_count} invalid dates")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/dq/smoke.py:53:1
   |
51 |     if invalid_count > 0:
52 |         raise ValueError(f"Column '{col}' has {invalid_count} invalid dates")
53 |     
   | ^^^^
54 |     logger.info(f"âœ“ Column '{col}': valid dates")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/dq/smoke.py:60:1
   |
58 |     """Data quality checks for customers table."""
59 |     logger.info("Running data quality checks for customers...")
60 |     
   | ^^^^
61 |     # Check required columns
62 |     assert_non_null(df, "customer_id", threshold=0.0)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/dq/smoke.py:65:1
   |
63 |     assert_non_null(df, "name", threshold=0.05)
64 |     assert_non_null(df, "email", threshold=0.05)
65 |     
   | ^^^^
66 |     # Check for duplicates
67 |     assert_no_duplicates(df, ["customer_id"], threshold=0.0)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/dq/smoke.py:68:1
   |
66 |     # Check for duplicates
67 |     assert_no_duplicates(df, ["customer_id"], threshold=0.0)
68 |     
   | ^^^^
69 |     # Check email format (basic validation)
70 |     invalid_emails = df.filter("email NOT LIKE '%@%'").count()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/dq/smoke.py:73:1
   |
71 |     if invalid_emails > 0:
72 |         logger.warning(f"Found {invalid_emails} emails without '@' symbol")
73 |     
   | ^^^^
74 |     logger.info("âœ“ Customers data quality checks passed")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/dq/smoke.py:80:1
   |
78 |     """Data quality checks for orders table."""
79 |     logger.info("Running data quality checks for orders...")
80 |     
   | ^^^^
81 |     # Check required columns
82 |     assert_non_null(df, "order_id", threshold=0.0)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/dq/smoke.py:85:1
   |
83 |     assert_non_null(df, "customer_id", threshold=0.0)
84 |     assert_non_null(df, "order_date", threshold=0.05)
85 |     
   | ^^^^
86 |     # Check for duplicates
87 |     assert_no_duplicates(df, ["order_id"], threshold=0.0)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/dq/smoke.py:88:1
   |
86 |     # Check for duplicates
87 |     assert_no_duplicates(df, ["order_id"], threshold=0.0)
88 |     
   | ^^^^
89 |     # Check numeric columns
90 |     if "total_amount" in df.columns:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/dq/smoke.py:94:1
   |
92 |     if "quantity" in df.columns:
93 |         assert_positive_values(df, "quantity")
94 |     
   | ^^^^
95 |     # Check date validity
96 |     assert_valid_dates(df, "order_date")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/dq/smoke.py:97:1
   |
95 |     # Check date validity
96 |     assert_valid_dates(df, "order_date")
97 |     
   | ^^^^
98 |     logger.info("âœ“ Orders data quality checks passed")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/dq/smoke.py:104:1
    |
102 |     """Data quality checks for products table."""
103 |     logger.info("Running data quality checks for products...")
104 |     
    | ^^^^
105 |     # Check required columns
106 |     assert_non_null(df, "product_id", threshold=0.0)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/dq/smoke.py:108:1
    |
106 |     assert_non_null(df, "product_id", threshold=0.0)
107 |     assert_non_null(df, "product_name", threshold=0.05)
108 |     
    | ^^^^
109 |     # Check for duplicates
110 |     assert_no_duplicates(df, ["product_id"], threshold=0.0)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/dq/smoke.py:111:1
    |
109 |     # Check for duplicates
110 |     assert_no_duplicates(df, ["product_id"], threshold=0.0)
111 |     
    | ^^^^
112 |     # Check price if exists
113 |     if "price" in df.columns:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/dq/smoke.py:115:1
    |
113 |     if "price" in df.columns:
114 |         assert_positive_values(df, "price")
115 |     
    | ^^^^
116 |     logger.info("âœ“ Products data quality checks passed")
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
 --> src/project_a/legacy/dr/dr_runner.py:1:1
  |
1 | / import json
2 | | import time
3 | | from pathlib import Path
4 | | from ..logging_setup import get_logger
  | |______________________________________^
5 |
6 |   log = get_logger("dr")
  |
help: Organize imports

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/enterprise_data_platform.py:13:1
   |
11 |   """
12 |
13 | / import logging
14 | | import os
15 | | import json
16 | | import time
17 | | from datetime import datetime
18 | | from typing import Any, Dict, List
19 | | from dataclasses import dataclass, field
20 | |
21 | | from pyspark.sql import DataFrame, SparkSession
22 | |
23 | | # Import our enterprise modules
24 | | from .unity_catalog import UnityCatalogManager
25 | | from .azure_security import AzureSecurityManager
26 | | from .disaster_recovery import DisasterRecoveryExecutor
27 | | from .advanced_dq_monitoring import AdvancedDataQualityManager, QualitySeverity
28 | | from .monitoring import PipelineMonitor
29 | | from .delta_utils import DeltaUtils
   | |___________________________________^
30 |
31 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/legacy/enterprise_data_platform.py:18:1
   |
16 | import time
17 | from datetime import datetime
18 | from typing import Any, Dict, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
19 | from dataclasses import dataclass, field
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/legacy/enterprise_data_platform.py:18:1
   |
16 | import time
17 | from datetime import datetime
18 | from typing import Any, Dict, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
19 | from dataclasses import dataclass, field
   |

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/legacy/enterprise_data_platform.py:44:13
   |
42 |     performance_status: str
43 |     last_check: datetime = field(default_factory=datetime.now)
44 |     issues: List[str] = field(default_factory=list)
   |             ^^^^
45 |     recommendations: List[str] = field(default_factory=list)
   |
help: Replace with `list`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/legacy/enterprise_data_platform.py:45:22
   |
43 |     last_check: datetime = field(default_factory=datetime.now)
44 |     issues: List[str] = field(default_factory=list)
45 |     recommendations: List[str] = field(default_factory=list)
   |                      ^^^^
   |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/enterprise_data_platform.py:66:53
   |
64 |     """
65 |
66 |     def __init__(self, spark: SparkSession, config: Dict[str, Any]):
   |                                                     ^^^^
67 |         self.spark = spark
68 |         self.config = config
   |
help: Replace with `dict`

F841 Local variable `dr_config` is assigned to but never used
   --> src/project_a/legacy/enterprise_data_platform.py:115:13
    |
114 |             # Initialize Disaster Recovery
115 |             dr_config = self.config.get("disaster_recovery", {})
    |             ^^^^^^^^^
116 |             self.dr_manager = DisasterRecoveryExecutor(self.spark, self.config)
117 |             logger.info("Disaster Recovery Manager initialized")
    |
help: Remove assignment to unused variable `dr_config`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/enterprise_data_platform.py:280:73
    |
278 |             self.health_status.issues.append(f"Health check failed: {str(e)}")
279 |
280 |     def create_data_pipeline(self, pipeline_name: str, pipeline_config: Dict[str, Any]) -> Dict[str, Any]:
    |                                                                         ^^^^
281 |         """
282 |         Create a new data pipeline with all enterprise features.
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/enterprise_data_platform.py:280:92
    |
278 |             self.health_status.issues.append(f"Health check failed: {str(e)}")
279 |
280 |     def create_data_pipeline(self, pipeline_name: str, pipeline_config: Dict[str, Any]) -> Dict[str, Any]:
    |                                                                                            ^^^^
281 |         """
282 |         Create a new data pipeline with all enterprise features.
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/enterprise_data_platform.py:391:74
    |
389 |             return {"error": str(e)}
390 |
391 |     def run_data_quality_checks(self, table_name: str, df: DataFrame) -> Dict[str, Any]:
    |                                                                          ^^^^
392 |         """
393 |         Run comprehensive data quality checks on a table.
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/enterprise_data_platform.py:443:80
    |
441 |             return {"error": str(e)}
442 |
443 |     def optimize_table_performance(self, table_path: str, optimization_config: Dict[str, Any]) -> Dict[str, Any]:
    |                                                                                ^^^^
444 |         """
445 |         Optimize table performance using Delta Lake features.
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/enterprise_data_platform.py:443:99
    |
441 |             return {"error": str(e)}
442 |
443 |     def optimize_table_performance(self, table_path: str, optimization_config: Dict[str, Any]) -> Dict[str, Any]:
    |                                                                                                   ^^^^
444 |         """
445 |         Optimize table performance using Delta Lake features.
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/enterprise_data_platform.py:497:39
    |
495 |             return {"error": str(e)}
496 |
497 |     def get_platform_summary(self) -> Dict[str, Any]:
    |                                       ^^^^
498 |         """
499 |         Get comprehensive platform summary.
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/enterprise_data_platform.py:664:65
    |
664 | def setup_enterprise_data_platform(spark: SparkSession, config: Dict[str, Any]) -> EnterpriseDataPlatform:
    |                                                                 ^^^^
665 |     """
666 |     Setup the complete enterprise data platform.
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/enterprise_data_platform.py:688:68
    |
688 | def run_platform_health_check(platform: EnterpriseDataPlatform) -> Dict[str, Any]:
    |                                                                    ^^^^
689 |     """
690 |     Run a comprehensive platform health check.
    |
help: Replace with `dict`

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/gold_writer.py:4:1
   |
 2 |   Gold Layer Writer with MERGE Operations
 3 |   """
 4 | / import logging
 5 | | import os
 6 | | from typing import Optional
 7 | | from pyspark.sql import SparkSession, DataFrame
 8 | | from delta.tables import DeltaTable
   | |___________________________________^
 9 |
10 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

F401 [*] `typing.Optional` imported but unused
 --> src/project_a/legacy/gold_writer.py:6:20
  |
4 | import logging
5 | import os
6 | from typing import Optional
  |                    ^^^^^^^^
7 | from pyspark.sql import SparkSession, DataFrame
8 | from delta.tables import DeltaTable
  |
help: Remove unused import: `typing.Optional`

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/gold_writer.py:14:1
   |
12 | class GoldWriter:
13 |     """Gold layer writer with idempotent MERGE operations"""
14 |     
   | ^^^^
15 |     def __init__(self, spark: SparkSession):
16 |         self.spark = spark
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/gold_writer.py:17:1
   |
15 |     def __init__(self, spark: SparkSession):
16 |         self.spark = spark
17 |     
   | ^^^^
18 |     def publish_gold(self, source_df: DataFrame, gold_path: str, key: str = "customer_id") -> bool:
19 |         """
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/gold_writer.py:21:1
   |
19 |         """
20 |         Publish data to Gold layer using MERGE operation for idempotency
21 |         
   | ^^^^^^^^
22 |         Args:
23 |             source_df: DataFrame to publish
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/gold_writer.py:26:1
   |
24 |             gold_path: Path to Gold layer table
25 |             key: Primary key for MERGE operation
26 |             
   | ^^^^^^^^^^^^
27 |         Returns:
28 |             bool: True if successful, False otherwise
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/gold_writer.py:33:1
   |
31 |             # Ensure directory exists
32 |             os.makedirs(os.path.dirname(gold_path), exist_ok=True)
33 |             
   | ^^^^^^^^^^^^
34 |             # Check if table exists
35 |             table_exists = os.path.exists(gold_path)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/gold_writer.py:36:1
   |
34 |             # Check if table exists
35 |             table_exists = os.path.exists(gold_path)
36 |             
   | ^^^^^^^^^^^^
37 |             if table_exists:
38 |                 # Table exists - use MERGE operation
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/gold_writer.py:40:1
   |
38 |                 # Table exists - use MERGE operation
39 |                 logger.info(f"ðŸ“Š Merging data into existing Gold table: {gold_path}")
40 |                 
   | ^^^^^^^^^^^^^^^^
41 |                 target = DeltaTable.forPath(self.spark, gold_path)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/gold_writer.py:42:1
   |
41 |                 target = DeltaTable.forPath(self.spark, gold_path)
42 |                 
   | ^^^^^^^^^^^^^^^^
43 |                 # Perform MERGE operation
44 |                 (target.alias("t")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/gold_writer.py:49:1
   |
47 |                  .whenNotMatchedInsertAll()
48 |                  .execute())
49 |                 
   | ^^^^^^^^^^^^^^^^
50 |                 logger.info(f"âœ… MERGE completed for {gold_path}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/gold_writer.py:51:1
   |
50 |                 logger.info(f"âœ… MERGE completed for {gold_path}")
51 |                 
   | ^^^^^^^^^^^^^^^^
52 |             else:
53 |                 # Table doesn't exist - create new table
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/gold_writer.py:55:1
   |
53 |                 # Table doesn't exist - create new table
54 |                 logger.info(f"ðŸ“Š Creating new Gold table: {gold_path}")
55 |                 
   | ^^^^^^^^^^^^^^^^
56 |                 source_df.write \
57 |                     .format("delta") \
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/gold_writer.py:60:1
   |
58 |                     .mode("overwrite") \
59 |                     .save(gold_path)
60 |                 
   | ^^^^^^^^^^^^^^^^
61 |                 logger.info(f"âœ… New table created at {gold_path}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/gold_writer.py:62:1
   |
61 |                 logger.info(f"âœ… New table created at {gold_path}")
62 |             
   | ^^^^^^^^^^^^
63 |             # Log the write path and count
64 |             final_df = self.spark.read.format("delta").load(gold_path)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/gold_writer.py:66:1
   |
64 |             final_df = self.spark.read.format("delta").load(gold_path)
65 |             row_count = final_df.count()
66 |             
   | ^^^^^^^^^^^^
67 |             logger.info(f"âœ… Gold written to: {gold_path}")
68 |             logger.info(f"ðŸ“Š Gold row count: {row_count}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/gold_writer.py:69:1
   |
67 |             logger.info(f"âœ… Gold written to: {gold_path}")
68 |             logger.info(f"ðŸ“Š Gold row count: {row_count}")
69 |             
   | ^^^^^^^^^^^^
70 |             # Show version history
71 |             try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/gold_writer.py:77:1
   |
75 |             except Exception as e:
76 |                 logger.warning(f"âš ï¸ Could not get version history: {e}")
77 |             
   | ^^^^^^^^^^^^
78 |             return True
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/gold_writer.py:79:1
   |
78 |             return True
79 |             
   | ^^^^^^^^^^^^
80 |         except Exception as e:
81 |             logger.error(f"âŒ Error publishing to Gold layer {gold_path}: {e}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/gold_writer.py:83:1
   |
81 |             logger.error(f"âŒ Error publishing to Gold layer {gold_path}: {e}")
82 |             return False
83 |     
   | ^^^^
84 |     def publish_customer_analytics(self, customer_df: DataFrame, gold_base_path: str) -> bool:
85 |         """Publish customer analytics to Gold layer"""
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/gold_writer.py:87:1
   |
85 |         """Publish customer analytics to Gold layer"""
86 |         gold_path = os.path.join(gold_base_path, "customer_analytics")
87 |         
   | ^^^^^^^^
88 |         # Generate customer analytics
89 |         analytics_df = self._generate_customer_analytics(customer_df)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/gold_writer.py:90:1
   |
88 |         # Generate customer analytics
89 |         analytics_df = self._generate_customer_analytics(customer_df)
90 |         
   | ^^^^^^^^
91 |         return self.publish_gold(analytics_df, gold_path, "segment")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/gold_writer.py:92:1
   |
91 |         return self.publish_gold(analytics_df, gold_path, "segment")
92 |     
   | ^^^^
93 |     def publish_order_analytics(self, order_df: DataFrame, gold_base_path: str) -> bool:
94 |         """Publish order analytics to Gold layer"""
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/gold_writer.py:96:1
   |
94 |         """Publish order analytics to Gold layer"""
95 |         gold_path = os.path.join(gold_base_path, "order_analytics")
96 |         
   | ^^^^^^^^
97 |         # Generate order analytics
98 |         analytics_df = self._generate_order_analytics(order_df)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/gold_writer.py:99:1
    |
 97 |         # Generate order analytics
 98 |         analytics_df = self._generate_order_analytics(order_df)
 99 |         
    | ^^^^^^^^
100 |         return self.publish_gold(analytics_df, gold_path, "amount_category")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/gold_writer.py:101:1
    |
100 |         return self.publish_gold(analytics_df, gold_path, "amount_category")
101 |     
    | ^^^^
102 |     def publish_monthly_revenue(self, order_df: DataFrame, gold_base_path: str) -> bool:
103 |         """Publish monthly revenue to Gold layer"""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/gold_writer.py:105:1
    |
103 |         """Publish monthly revenue to Gold layer"""
104 |         gold_path = os.path.join(gold_base_path, "monthly_revenue")
105 |         
    | ^^^^^^^^
106 |         # Generate monthly revenue
107 |         revenue_df = self._generate_monthly_revenue(order_df)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/gold_writer.py:108:1
    |
106 |         # Generate monthly revenue
107 |         revenue_df = self._generate_monthly_revenue(order_df)
108 |         
    | ^^^^^^^^
109 |         return self.publish_gold(revenue_df, gold_path, "month")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/gold_writer.py:110:1
    |
109 |         return self.publish_gold(revenue_df, gold_path, "month")
110 |     
    | ^^^^
111 |     def _generate_customer_analytics(self, customer_df: DataFrame) -> DataFrame:
112 |         """Generate customer analytics from customer data"""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/gold_writer.py:114:1
    |
112 |         """Generate customer analytics from customer data"""
113 |         from pyspark.sql import functions as F
114 |         
    | ^^^^^^^^
115 |         return customer_df.groupBy("segment") \
116 |             .agg(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/gold_writer.py:120:1
    |
118 |                 F.avg(F.datediff(F.current_date(), F.col("created_date"))).alias("avg_lifetime_days")
119 |             )
120 |     
    | ^^^^
121 |     def _generate_order_analytics(self, order_df: DataFrame) -> DataFrame:
122 |         """Generate order analytics from order data"""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/gold_writer.py:124:1
    |
122 |         """Generate order analytics from order data"""
123 |         from pyspark.sql import functions as F
124 |         
    | ^^^^^^^^
125 |         # Create amount categories
126 |         order_with_category = order_df.withColumn(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/gold_writer.py:132:1
    |
130 |             .otherwise("Large")
131 |         )
132 |         
    | ^^^^^^^^
133 |         return order_with_category.groupBy("amount_category") \
134 |             .agg(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/gold_writer.py:139:1
    |
137 |                 F.avg("amount").alias("avg_amount")
138 |             )
139 |     
    | ^^^^
140 |     def _generate_monthly_revenue(self, order_df: DataFrame) -> DataFrame:
141 |         """Generate monthly revenue from order data"""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/gold_writer.py:143:1
    |
141 |         """Generate monthly revenue from order data"""
142 |         from pyspark.sql import functions as F
143 |         
    | ^^^^^^^^
144 |         return order_df.withColumn("month", F.month("order_date")) \
145 |             .groupBy("month") \
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/incremental_loading.py:12:1
   |
10 |   """
11 |
12 | / import logging
13 | | from datetime import datetime
14 | | from typing import Dict, Any, List, Optional
15 | |
16 | | from pyspark.sql import DataFrame, SparkSession, functions as F
17 | | from pyspark.sql.types import TimestampType
18 | | from delta.tables import DeltaTable
   | |___________________________________^
19 |
20 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/legacy/incremental_loading.py:14:1
   |
12 | import logging
13 | from datetime import datetime
14 | from typing import Dict, Any, List, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
15 |
16 | from pyspark.sql import DataFrame, SparkSession, functions as F
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/legacy/incremental_loading.py:14:1
   |
12 | import logging
13 | from datetime import datetime
14 | from typing import Dict, Any, List, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
15 |
16 | from pyspark.sql import DataFrame, SparkSession, functions as F
   |

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/incremental_loading.py:33:56
   |
31 |         self.watermark_column = watermark_column
32 |
33 |     def get_latest_watermark(self, table_path: str) -> Optional[datetime]:
   |                                                        ^^^^^^^^^^^^^^^^^^
34 |         """
35 |         Get the latest watermark from a Delta table.
   |
help: Convert to `X | None`

W293 Blank line contains whitespace
  --> src/project_a/legacy/incremental_loading.py:36:1
   |
34 |         """
35 |         Get the latest watermark from a Delta table.
36 |         
   | ^^^^^^^^
37 |         Args:
38 |             table_path: Path to the Delta table
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/incremental_loading.py:39:1
   |
37 |         Args:
38 |             table_path: Path to the Delta table
39 |             
   | ^^^^^^^^^^^^
40 |         Returns:
41 |             Latest watermark timestamp or None if table is empty
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/incremental_loading.py:45:1
   |
43 |         try:
44 |             df = self.spark.read.format("delta").load(table_path)
45 |             
   | ^^^^^^^^^^^^
46 |             if df.rdd.isEmpty():
47 |                 return None
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/incremental_loading.py:48:1
   |
46 |             if df.rdd.isEmpty():
47 |                 return None
48 |                 
   | ^^^^^^^^^^^^^^^^
49 |             result = df.agg(F.max(self.watermark_column)).limit(1).first()
50 |             return result[0] if result and result[0] else None
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/incremental_loading.py:51:1
   |
49 |             result = df.agg(F.max(self.watermark_column)).limit(1).first()
50 |             return result[0] if result and result[0] else None
51 |             
   | ^^^^^^^^^^^^
52 |         except Exception as e:
53 |             logger.warning(f"Could not get watermark from {table_path}: {e}")
   |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
  --> src/project_a/legacy/incremental_loading.py:57:14
   |
56 |     def filter_incremental_data(
57 |         self, 
   |              ^
58 |         source_df: DataFrame, 
59 |         last_watermark: Optional[datetime] = None
   |
help: Remove trailing whitespace

W291 [*] Trailing whitespace
  --> src/project_a/legacy/incremental_loading.py:58:30
   |
56 |     def filter_incremental_data(
57 |         self, 
58 |         source_df: DataFrame, 
   |                              ^
59 |         last_watermark: Optional[datetime] = None
60 |     ) -> DataFrame:
   |
help: Remove trailing whitespace

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/incremental_loading.py:59:25
   |
57 |         self, 
58 |         source_df: DataFrame, 
59 |         last_watermark: Optional[datetime] = None
   |                         ^^^^^^^^^^^^^^^^^^
60 |     ) -> DataFrame:
61 |         """
   |
help: Convert to `X | None`

W293 Blank line contains whitespace
  --> src/project_a/legacy/incremental_loading.py:63:1
   |
61 |         """
62 |         Filter source data to only include records newer than the watermark.
63 |         
   | ^^^^^^^^
64 |         Args:
65 |             source_df: Source DataFrame
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/incremental_loading.py:67:1
   |
65 |             source_df: Source DataFrame
66 |             last_watermark: Last processed watermark timestamp
67 |             
   | ^^^^^^^^^^^^
68 |         Returns:
69 |             Filtered DataFrame with only new records
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/incremental_loading.py:73:1
   |
71 |         if last_watermark is None:
72 |             return source_df
73 |             
   | ^^^^^^^^^^^^
74 |         return source_df.filter(F.col(self.watermark_column) > last_watermark)
   |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/legacy/incremental_loading.py:81:25
   |
79 |         target_path: str,
80 |         business_key: str,
81 |         change_columns: List[str],
   |                         ^^^^
82 |         last_watermark: Optional[datetime] = None
83 |     ) -> Dict[str, Any]:
   |
help: Replace with `list`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/incremental_loading.py:82:25
   |
80 |         business_key: str,
81 |         change_columns: List[str],
82 |         last_watermark: Optional[datetime] = None
   |                         ^^^^^^^^^^^^^^^^^^
83 |     ) -> Dict[str, Any]:
84 |         """
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/incremental_loading.py:83:10
   |
81 |         change_columns: List[str],
82 |         last_watermark: Optional[datetime] = None
83 |     ) -> Dict[str, Any]:
   |          ^^^^
84 |         """
85 |         Apply SCD Type 2 logic to handle slowly changing dimensions.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/legacy/incremental_loading.py:86:1
   |
84 |         """
85 |         Apply SCD Type 2 logic to handle slowly changing dimensions.
86 |         
   | ^^^^^^^^
87 |         Args:
88 |             source_df: Source DataFrame with dimension data
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/incremental_loading.py:93:1
   |
91 |             change_columns: List of columns to monitor for changes
92 |             last_watermark: Last processed watermark timestamp
93 |             
   | ^^^^^^^^^^^^
94 |         Returns:
95 |             Dictionary with processing results
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/incremental_loading.py:100:1
    |
 98 |             # Filter for incremental data
 99 |             incremental_df = self.filter_incremental_data(source_df, last_watermark)
100 |             
    | ^^^^^^^^^^^^
101 |             if incremental_df.rdd.isEmpty():
102 |                 logger.info("No new dimension changes to process")
    |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
   --> src/project_a/legacy/incremental_loading.py:104:37
    |
102 |                 logger.info("No new dimension changes to process")
103 |                 return {
104 |                     "success": True, 
    |                                     ^
105 |                     "records_processed": 0, 
106 |                     "message": "No new data"
    |
help: Remove trailing whitespace

W291 [*] Trailing whitespace
   --> src/project_a/legacy/incremental_loading.py:105:44
    |
103 |                 return {
104 |                     "success": True, 
105 |                     "records_processed": 0, 
    |                                            ^
106 |                     "message": "No new data"
107 |                 }
    |
help: Remove trailing whitespace

W291 [*] Trailing whitespace
   --> src/project_a/legacy/incremental_loading.py:112:69
    |
110 |             hash_expr = F.sha2(
111 |                 F.concat_ws("||", *[
112 |                     F.coalesce(F.col(col).cast("string"), F.lit("")) 
    |                                                                     ^
113 |                     for col in change_columns
114 |                 ]), 
    |
help: Remove trailing whitespace

W291 [*] Trailing whitespace
   --> src/project_a/legacy/incremental_loading.py:114:20
    |
112 |                     F.coalesce(F.col(col).cast("string"), F.lit("")) 
113 |                     for col in change_columns
114 |                 ]), 
    |                    ^
115 |                 256
116 |             )
    |
help: Remove trailing whitespace

W291 [*] Trailing whitespace
   --> src/project_a/legacy/incremental_loading.py:120:34
    |
118 |             # Prepare staged data
119 |             staged_df = incremental_df.withColumn(
120 |                 "effective_from", 
    |                                  ^
121 |                 F.col(self.watermark_column).cast(TimestampType())
122 |             ).withColumn(
    |
help: Remove trailing whitespace

W291 [*] Trailing whitespace
   --> src/project_a/legacy/incremental_loading.py:123:32
    |
121 |                 F.col(self.watermark_column).cast(TimestampType())
122 |             ).withColumn(
123 |                 "effective_to", 
    |                                ^
124 |                 F.lit(None).cast(TimestampType())
125 |             ).withColumn(
    |
help: Remove trailing whitespace

W291 [*] Trailing whitespace
   --> src/project_a/legacy/incremental_loading.py:126:30
    |
124 |                 F.lit(None).cast(TimestampType())
125 |             ).withColumn(
126 |                 "is_current", 
    |                              ^
127 |                 F.lit(True)
128 |             ).withColumn(
    |
help: Remove trailing whitespace

W291 [*] Trailing whitespace
   --> src/project_a/legacy/incremental_loading.py:129:29
    |
127 |                 F.lit(True)
128 |             ).withColumn(
129 |                 "hash_diff", 
    |                             ^
130 |                 hash_expr
131 |             ).withColumn(
    |
help: Remove trailing whitespace

W291 [*] Trailing whitespace
   --> src/project_a/legacy/incremental_loading.py:132:33
    |
130 |                 hash_expr
131 |             ).withColumn(
132 |                 "surrogate_key", 
    |                                 ^
133 |                 F.expr("uuid()")
134 |             )
    |
help: Remove trailing whitespace

W291 [*] Trailing whitespace
   --> src/project_a/legacy/incremental_loading.py:156:30
    |
154 |                 F.col("is_current").isTrue()
155 |             ).select(
156 |                 business_key, 
    |                              ^
157 |                 "hash_diff"
158 |             ).withColumnRenamed("hash_diff", "t_hash_diff")
    |
help: Remove trailing whitespace

W291 [*] Trailing whitespace
   --> src/project_a/legacy/incremental_loading.py:166:50
    |
164 |                 "left"
165 |             ).filter(
166 |                 F.col("t.t_hash_diff").isNull() | 
    |                                                  ^
167 |                 (F.col("t.t_hash_diff") != F.col("s.hash_diff"))
168 |             ).select("s.*")
    |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/incremental_loading.py:180:1
    |
178 |             # Apply MERGE operation
179 |             merge_condition = f"t.{business_key} = s.{business_key} AND t.is_current = true"
180 |             
    | ^^^^^^^^^^^^
181 |             target_table.alias("t").merge(
182 |                 changes_df.alias("s"), 
    |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
   --> src/project_a/legacy/incremental_loading.py:182:39
    |
181 |             target_table.alias("t").merge(
182 |                 changes_df.alias("s"), 
    |                                       ^
183 |                 merge_condition
184 |             ).whenMatchedUpdate(
    |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/incremental_loading.py:197:1
    |
195 |             processed_count = changes_df.count()
196 |             logger.info(f"SCD2 updated {processed_count} rows")
197 |             
    | ^^^^^^^^^^^^
198 |             return {
199 |                 "success": True,
    |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
   --> src/project_a/legacy/incremental_loading.py:209:14
    |
208 |     def detect_changes(
209 |         self, 
    |              ^
210 |         source_df: DataFrame, 
211 |         target_df: DataFrame, 
    |
help: Remove trailing whitespace

W291 [*] Trailing whitespace
   --> src/project_a/legacy/incremental_loading.py:210:30
    |
208 |     def detect_changes(
209 |         self, 
210 |         source_df: DataFrame, 
    |                              ^
211 |         target_df: DataFrame, 
212 |         key_columns: List[str]
    |
help: Remove trailing whitespace

W291 [*] Trailing whitespace
   --> src/project_a/legacy/incremental_loading.py:211:30
    |
209 |         self, 
210 |         source_df: DataFrame, 
211 |         target_df: DataFrame, 
    |                              ^
212 |         key_columns: List[str]
213 |     ) -> Dict[str, DataFrame]:
    |
help: Remove trailing whitespace

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/legacy/incremental_loading.py:212:22
    |
210 |         source_df: DataFrame, 
211 |         target_df: DataFrame, 
212 |         key_columns: List[str]
    |                      ^^^^
213 |     ) -> Dict[str, DataFrame]:
214 |         """
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/incremental_loading.py:213:10
    |
211 |         target_df: DataFrame, 
212 |         key_columns: List[str]
213 |     ) -> Dict[str, DataFrame]:
    |          ^^^^
214 |         """
215 |         Detect changes between source and target DataFrames.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/legacy/incremental_loading.py:216:1
    |
214 |         """
215 |         Detect changes between source and target DataFrames.
216 |         
    | ^^^^^^^^
217 |         Args:
218 |             source_df: Source DataFrame
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/incremental_loading.py:221:1
    |
219 |             target_df: Target DataFrame
220 |             key_columns: Columns to use as keys for comparison
221 |             
    | ^^^^^^^^^^^^
222 |         Returns:
223 |             Dictionary with 'inserts', 'updates', and 'deletes' DataFrames
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/incremental_loading.py:242:1
    |
240 |         # Join on key columns and filter for non-key column differences
241 |         non_key_columns = [col for col in source_df.columns if col not in key_columns]
242 |         
    | ^^^^^^^^
243 |         if non_key_columns:
244 |             update_conditions = [
    |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
   --> src/project_a/legacy/incremental_loading.py:245:55
    |
243 |         if non_key_columns:
244 |             update_conditions = [
245 |                 F.col(f"s.{col}") != F.col(f"t.{col}") 
    |                                                       ^
246 |                 for col in non_key_columns
247 |             ]
    |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/incremental_loading.py:248:1
    |
246 |                 for col in non_key_columns
247 |             ]
248 |             
    | ^^^^^^^^^^^^
249 |             updates = source_df.alias("s").join(
250 |                 target_df.alias("t"),
    |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/legacy/incremental_loading.py:268:22
    |
266 |         source_df: DataFrame,
267 |         target_path: str,
268 |         key_columns: List[str],
    |                      ^^^^
269 |         last_watermark: Optional[datetime] = None
270 |     ) -> Dict[str, Any]:
    |
help: Replace with `list`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/incremental_loading.py:269:25
    |
267 |         target_path: str,
268 |         key_columns: List[str],
269 |         last_watermark: Optional[datetime] = None
    |                         ^^^^^^^^^^^^^^^^^^
270 |     ) -> Dict[str, Any]:
271 |         """
    |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/incremental_loading.py:270:10
    |
268 |         key_columns: List[str],
269 |         last_watermark: Optional[datetime] = None
270 |     ) -> Dict[str, Any]:
    |          ^^^^
271 |         """
272 |         Apply Change Data Capture (CDC) changes to target table.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/legacy/incremental_loading.py:273:1
    |
271 |         """
272 |         Apply Change Data Capture (CDC) changes to target table.
273 |         
    | ^^^^^^^^
274 |         Args:
275 |             source_df: Source DataFrame with CDC data
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/incremental_loading.py:279:1
    |
277 |             key_columns: Columns to use as keys
278 |             last_watermark: Last processed watermark timestamp
279 |             
    | ^^^^^^^^^^^^
280 |         Returns:
281 |             Dictionary with processing results
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/incremental_loading.py:286:1
    |
284 |             # Filter for incremental data
285 |             incremental_df = self.filter_incremental_data(source_df, last_watermark)
286 |             
    | ^^^^^^^^^^^^
287 |             if incremental_df.rdd.isEmpty():
288 |                 return {
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/incremental_loading.py:309:1
    |
307 |             # Detect changes
308 |             changes = self.detect_changes(incremental_df, target_df, key_columns)
309 |             
    | ^^^^^^^^^^^^
310 |             # Apply changes using MERGE
311 |             merge_condition = " AND ".join([
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/incremental_loading.py:314:1
    |
312 |                 f"t.{col} = s.{col}" for col in key_columns
313 |             ])
314 |             
    | ^^^^^^^^^^^^
315 |             # Handle inserts and updates
316 |             target_table.alias("t").merge(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/incremental_loading.py:333:1
    |
331 |                     for row in limited_deletes.collect()
332 |                 ])
333 |                 
    | ^^^^^^^^^^^^^^^^
334 |                 if delete_condition:
335 |                     target_table.delete(delete_condition)
    |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
   --> src/project_a/legacy/incremental_loading.py:338:45
    |
337 |             total_processed = (
338 |                 changes["inserts"].count() + 
    |                                             ^
339 |                 changes["updates"].count() + 
340 |                 changes["deletes"].count()
    |
help: Remove trailing whitespace

W291 [*] Trailing whitespace
   --> src/project_a/legacy/incremental_loading.py:339:45
    |
337 |             total_processed = (
338 |                 changes["inserts"].count() + 
339 |                 changes["updates"].count() + 
    |                                             ^
340 |                 changes["deletes"].count()
341 |             )
    |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/incremental_loading.py:342:1
    |
340 |                 changes["deletes"].count()
341 |             )
342 |             
    | ^^^^^^^^^^^^
343 |             return {
344 |                 "success": True,
    |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/incremental_loading.py:357:28
    |
355 |         table_path: str,
356 |         watermark_column: str,
357 |         partition_columns: Optional[List[str]] = None
    |                            ^^^^^^^^^^^^^^^^^^^
358 |     ) -> None:
359 |         """
    |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/legacy/incremental_loading.py:357:37
    |
355 |         table_path: str,
356 |         watermark_column: str,
357 |         partition_columns: Optional[List[str]] = None
    |                                     ^^^^
358 |     ) -> None:
359 |         """
    |
help: Replace with `list`

W293 Blank line contains whitespace
   --> src/project_a/legacy/incremental_loading.py:361:1
    |
359 |         """
360 |         Create a watermark tracking table for incremental processing.
361 |         
    | ^^^^^^^^
362 |         Args:
363 |             table_path: Path for the watermark table
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/incremental_loading.py:368:1
    |
366 |         """
367 |         schema = self.spark.createDataFrame([], self.spark.read.format("delta").load(table_path).schema)
368 |         
    | ^^^^^^^^
369 |         watermark_df = schema.withColumn(
370 |             watermark_column,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/incremental_loading.py:378:1
    |
377 |         write_opts = watermark_df.write.format("delta").mode("overwrite")
378 |         
    | ^^^^^^^^
379 |         if partition_columns:
380 |             write_opts = write_opts.partitionBy(*partition_columns)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/incremental_loading.py:381:1
    |
379 |         if partition_columns:
380 |             write_opts = write_opts.partitionBy(*partition_columns)
381 |             
    | ^^^^^^^^^^^^
382 |         write_opts.save(table_path)
    |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/legacy/incremental_loading.py:387:28
    |
385 |         self,
386 |         table_path: str,
387 |         partition_columns: List[str],
    |                            ^^^^
388 |         z_order_columns: Optional[List[str]] = None
389 |     ) -> None:
    |
help: Replace with `list`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/incremental_loading.py:388:26
    |
386 |         table_path: str,
387 |         partition_columns: List[str],
388 |         z_order_columns: Optional[List[str]] = None
    |                          ^^^^^^^^^^^^^^^^^^^
389 |     ) -> None:
390 |         """
    |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/legacy/incremental_loading.py:388:35
    |
386 |         table_path: str,
387 |         partition_columns: List[str],
388 |         z_order_columns: Optional[List[str]] = None
    |                                   ^^^^
389 |     ) -> None:
390 |         """
    |
help: Replace with `list`

W293 Blank line contains whitespace
   --> src/project_a/legacy/incremental_loading.py:392:1
    |
390 |         """
391 |         Optimize Delta table for incremental processing.
392 |         
    | ^^^^^^^^
393 |         Args:
394 |             table_path: Path to the Delta table
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/incremental_loading.py:400:1
    |
398 |         try:
399 |             target_table = DeltaTable.forPath(self.spark, table_path)
400 |             
    | ^^^^^^^^^^^^
401 |             # Optimize the table
402 |             target_table.optimize().execute()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/incremental_loading.py:403:1
    |
401 |             # Optimize the table
402 |             target_table.optimize().execute()
403 |             
    | ^^^^^^^^^^^^
404 |             # Z-order if specified
405 |             if z_order_columns:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/incremental_loading.py:407:1
    |
405 |             if z_order_columns:
406 |                 target_table.optimize().executeZOrderBy(z_order_columns)
407 |                 
    | ^^^^^^^^^^^^^^^^
408 |             # Vacuum to remove old files
409 |             target_table.vacuum(168)  # Keep 7 days of history
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/incremental_loading.py:410:1
    |
408 |             # Vacuum to remove old files
409 |             target_table.vacuum(168)  # Keep 7 days of history
410 |             
    | ^^^^^^^^^^^^
411 |             logger.info(f"Optimized table {table_path} for incremental processing")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/incremental_loading.py:412:1
    |
411 |             logger.info(f"Optimized table {table_path} for incremental processing")
412 |             
    | ^^^^^^^^^^^^
413 |         except Exception as e:
414 |             logger.error(f"Failed to optimize table {table_path}: {e}")
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/ingestion_pipeline.py:6:1
   |
 4 |   """
 5 |
 6 | / import logging
 7 | | import json
 8 | | import os
 9 | | from datetime import datetime, timedelta
10 | | from typing import Dict, Optional, Any
11 | |
12 | | from pyspark.sql import SparkSession, DataFrame
13 | | from pyspark.sql.functions import (
14 | |     col, when, current_timestamp, year, month, dayofmonth,
15 | |     lower, trim, datediff, count, avg, countDistinct
16 | | )
17 | |
18 | | from project_a.config_loader import load_config_resolved
19 | | from project_a.dq_checks import DQChecks
20 | | from project_a.delta_utils import write_delta, read_delta
21 | | from project_a.incremental_loading import IncrementalLoader
22 | | from project_a.metrics_collector import MetricsCollector
23 | | from project_a.lineage_tracker import LineageTracker
   | |____________________________________________________^
24 |
25 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/legacy/ingestion_pipeline.py:10:1
   |
 8 | import os
 9 | from datetime import datetime, timedelta
10 | from typing import Dict, Optional, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
11 |
12 | from pyspark.sql import SparkSession, DataFrame
   |

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/ingestion_pipeline.py:38:53
   |
36 |     """
37 |
38 |     def __init__(self, spark: SparkSession, config: Dict[str, Any]):
   |                                                     ^^^^
39 |         self.spark = spark
40 |         self.config = config
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/ingestion_pipeline.py:55:36
   |
53 |         os.makedirs(self.checkpoint_base, exist_ok=True)
54 |
55 |     def run_full_pipeline(self) -> Dict[str, Any]:
   |                                    ^^^^
56 |         """Execute complete ingestion pipeline."""
57 |         start_time = datetime.now()
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/ingestion_pipeline.py:102:41
    |
100 |             raise
101 |
102 |     def _validate_bronze_layer(self) -> Dict[str, Any]:
    |                                         ^^^^
103 |         """Validate data quality in bronze layer."""
104 |         logger.info("ðŸ” Validating bronze layer data quality")
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/ingestion_pipeline.py:126:66
    |
124 |         return metrics
125 |
126 |     def _validate_table(self, df: DataFrame, table_name: str) -> Dict[str, Any]:
    |                                                                  ^^^^
127 |         """Validate individual table data quality."""
128 |         metrics = {"record_count": 0, "quality_issues": 0}
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/ingestion_pipeline.py:150:53
    |
148 |         return metrics
149 |
150 |     def _validate_customers(self, df: DataFrame) -> Dict[str, Any]:
    |                                                     ^^^^
151 |         """Validate customer data quality."""
152 |         issues = 0
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/ingestion_pipeline.py:179:50
    |
177 |         return {"quality_issues": issues}
178 |
179 |     def _validate_orders(self, df: DataFrame) -> Dict[str, Any]:
    |                                                  ^^^^
180 |         """Validate order data quality."""
181 |         issues = 0
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/ingestion_pipeline.py:196:51
    |
194 |         return {"quality_issues": issues}
195 |
196 |     def _validate_returns(self, df: DataFrame) -> Dict[str, Any]:
    |                                                   ^^^^
197 |         """Validate returns data quality."""
198 |         issues = 0
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/ingestion_pipeline.py:217:52
    |
215 |         return {"quality_issues": issues}
216 |
217 |     def _validate_products(self, df: DataFrame) -> Dict[str, Any]:
    |                                                    ^^^^
218 |         """Validate product data quality."""
219 |         issues = 0
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/ingestion_pipeline.py:234:40
    |
232 |         return {"quality_issues": issues}
233 |
234 |     def _process_silver_layer(self) -> Dict[str, Any]:
    |                                        ^^^^
235 |         """Process data from bronze to silver layer."""
236 |         logger.info("ðŸ”„ Processing silver layer transformations")
    |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/ingestion_pipeline.py:310:46
    |
308 |         return metrics
309 |
310 |     def _transform_customers_silver(self) -> Optional[DataFrame]:
    |                                              ^^^^^^^^^^^^^^^^^^^
311 |         """Transform customers to silver layer."""
312 |         try:
    |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/ingestion_pipeline.py:346:45
    |
344 |             return None
345 |
346 |     def _transform_products_silver(self) -> Optional[DataFrame]:
    |                                             ^^^^^^^^^^^^^^^^^^^
347 |         """Transform products to silver layer."""
348 |         try:
    |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/ingestion_pipeline.py:370:43
    |
368 |             return None
369 |
370 |     def _transform_orders_silver(self) -> Optional[DataFrame]:
    |                                           ^^^^^^^^^^^^^^^^^^^
371 |         """Transform orders to silver layer."""
372 |         try:
    |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/ingestion_pipeline.py:397:44
    |
395 |             return None
396 |
397 |     def _transform_returns_silver(self) -> Optional[DataFrame]:
    |                                            ^^^^^^^^^^^^^^^^^^^
398 |         """Transform returns to silver layer."""
399 |         try:
    |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/ingestion_pipeline.py:423:46
    |
421 |             return None
422 |
423 |     def _transform_inventory_silver(self) -> Optional[DataFrame]:
    |                                              ^^^^^^^^^^^^^^^^^^^
424 |         """Transform inventory to silver layer."""
425 |         try:
    |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/ingestion_pipeline.py:448:38
    |
446 |             return None
447 |
448 |     def _process_gold_layer(self) -> Dict[str, Any]:
    |                                      ^^^^
449 |         """Process data from silver to gold layer (business metrics)."""
450 |         logger.info("ðŸ† Processing gold layer aggregations")
    |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/ingestion_pipeline.py:515:43
    |
513 |         return metrics
514 |
515 |     def _create_customer_metrics(self) -> Optional[DataFrame]:
    |                                           ^^^^^^^^^^^^^^^^^^^
516 |         """Create customer business metrics."""
517 |         try:
    |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/ingestion_pipeline.py:537:42
    |
535 |             return None
536 |
537 |     def _create_product_metrics(self) -> Optional[DataFrame]:
    |                                          ^^^^^^^^^^^^^^^^^^^
538 |         """Create product business metrics."""
539 |         try:
    |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/ingestion_pipeline.py:558:40
    |
556 |             return None
557 |
558 |     def _create_order_metrics(self) -> Optional[DataFrame]:
    |                                        ^^^^^^^^^^^^^^^^^^^
559 |         """Create order business metrics."""
560 |         try:
    |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/ingestion_pipeline.py:581:41
    |
579 |             return None
580 |
581 |     def _create_return_metrics(self) -> Optional[DataFrame]:
    |                                         ^^^^^^^^^^^^^^^^^^^
582 |         """Create return business metrics."""
583 |         try:
    |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/ingestion_pipeline.py:601:44
    |
599 |             return None
600 |
601 |     def _create_inventory_metrics(self) -> Optional[DataFrame]:
    |                                            ^^^^^^^^^^^^^^^^^^^
602 |         """Create inventory business metrics."""
603 |         try:
    |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/ingestion_pipeline.py:621:40
    |
619 |             return None
620 |
621 |     def _optimize_performance(self) -> Dict[str, Any]:
    |                                        ^^^^
622 |         """Optimize table performance with Z-ordering and partitioning."""
623 |         logger.info("âš¡ Optimizing table performance")
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/ingestion_pipeline.py:681:49
    |
679 |             logger.warning(f"Failed to optimize {table_name}: {e}")
680 |
681 |     def _record_lineage(self, pipeline_metrics: Dict[str, Any]):
    |                                                 ^^^^
682 |         """Record data lineage for the pipeline run."""
683 |         try:
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/ingestion_pipeline.py:696:47
    |
694 |             logger.warning(f"Failed to record lineage: {e}")
695 |
696 |     def _save_pipeline_metrics(self, metrics: Dict[str, Any]):
    |                                               ^^^^
697 |         """Save pipeline metrics for monitoring."""
698 |         try:
    |
help: Replace with `dict`

I001 [*] Import block is un-sorted or un-formatted
 --> src/project_a/legacy/io_utils.py:1:1
  |
1 | / from __future__ import annotations
2 | | import logging
3 | | from typing import Optional, Sequence, Literal
4 | | from pyspark.sql import DataFrame, SparkSession
  | |_______________________________________________^
5 |
6 |   logger = logging.getLogger(__name__)
  |
help: Organize imports

UP035 [*] Import from `collections.abc` instead: `Sequence`
 --> src/project_a/legacy/io_utils.py:3:1
  |
1 | from __future__ import annotations
2 | import logging
3 | from typing import Optional, Sequence, Literal
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
4 | from pyspark.sql import DataFrame, SparkSession
  |
help: Import from `collections.abc`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/io_utils.py:38:19
   |
36 |     mode: Literal["overwrite", "append"] = "overwrite",
37 |     compression: str = "snappy",
38 |     partition_by: Optional[Sequence[str]] = None
   |                   ^^^^^^^^^^^^^^^^^^^^^^^
39 | ) -> None:
40 |     """Write DataFrame to Parquet format with safety checks."""
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/io_utils.py:52:19
   |
50 |     path: str,
51 |     mode: Literal["overwrite", "append"] = "overwrite",
52 |     partition_by: Optional[Sequence[str]] = None,
   |                   ^^^^^^^^^^^^^^^^^^^^^^^
53 |     merge_schema: bool = True,
54 |     strict_delta: bool = True
   |
help: Convert to `X | None`

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/jobs/dim_customer_scd2.py:11:1
   |
 9 |   """
10 |
11 | / import sys
12 | | import logging
13 | | import uuid
14 | | from pathlib import Path
15 | | from datetime import datetime
16 | | from typing import Dict, Any
   | |____________________________^
17 |
18 |   sys.path.insert(0, str(Path(__file__).parent.parent))
   |
help: Organize imports

F401 [*] `uuid` imported but unused
  --> src/project_a/legacy/jobs/dim_customer_scd2.py:13:8
   |
11 | import sys
12 | import logging
13 | import uuid
   |        ^^^^
14 | from pathlib import Path
15 | from datetime import datetime
   |
help: Remove unused import: `uuid`

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/legacy/jobs/dim_customer_scd2.py:16:1
   |
14 | from pathlib import Path
15 | from datetime import datetime
16 | from typing import Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
17 |
18 | sys.path.insert(0, str(Path(__file__).parent.parent))
   |

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/jobs/dim_customer_scd2.py:20:1
   |
18 |   sys.path.insert(0, str(Path(__file__).parent.parent))
19 |
20 | / from pyspark.sql import SparkSession, DataFrame
21 | | from pyspark.sql import functions as F
22 | | from delta.tables import DeltaTable
23 | |
24 | | from project_a.utils.spark_session import build_spark
25 | | from project_a.utils.path_resolver import resolve_path
26 | | from project_a.utils.io import read_delta, write_delta
27 | | from project_a.config_loader import load_config_resolved
28 | | from project_a.monitoring.lineage_decorator import lineage_job
29 | | from project_a.monitoring.metrics_collector import emit_rowcount
30 | | from project_a.utils.logging import setup_json_logging, get_trace_id
   | |____________________________________________________________________^
31 |
32 |   logging.basicConfig(level=logging.INFO)
   |
help: Organize imports

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/jobs/dim_customer_scd2.py:43:13
   |
41 | def build_dim_customer_scd2(
42 |     spark: SparkSession,
43 |     config: Dict[str, Any],
   |             ^^^^
44 |     run_date: str = None
45 | ) -> DataFrame:
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/dim_customer_scd2.py:48:1
   |
46 |     """
47 |     Build SCD Type-2 dim_customer from silver.customers.
48 |     
   | ^^^^
49 |     Args:
50 |         spark: SparkSession
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/dim_customer_scd2.py:53:1
   |
51 |         config: Configuration dict
52 |         run_date: Processing date
53 |         
   | ^^^^^^^^
54 |     Returns:
55 |         dim_customer DataFrame with SCD2 columns
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/dim_customer_scd2.py:59:1
   |
57 |     if run_date is None:
58 |         run_date = datetime.now().strftime("%Y-%m-%d")
59 |     
   | ^^^^
60 |     logger.info(f"ðŸš€ Building dim_customer SCD2 (run_date={run_date})")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/dim_customer_scd2.py:61:1
   |
60 |     logger.info(f"ðŸš€ Building dim_customer SCD2 (run_date={run_date})")
61 |     
   | ^^^^
62 |     # Read silver customers
63 |     silver_path = resolve_path("lake://silver", "customers", config=config)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/dim_customer_scd2.py:65:1
   |
63 |     silver_path = resolve_path("lake://silver", "customers", config=config)
64 |     src = read_delta(spark, silver_path)
65 |     
   | ^^^^
66 |     if src.isEmpty():
67 |         logger.warning("âš ï¸  No source data found")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/dim_customer_scd2.py:69:1
   |
67 |         logger.warning("âš ï¸  No source data found")
68 |         return spark.createDataFrame([], schema=spark.table("gold.dim_customer").schema if spark.catalog.tableExists("gold.dim_customeâ€¦
69 |     
   | ^^^^
70 |     # Prepare source with SCD2 columns
71 |     src_scd2 = src \
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/dim_customer_scd2.py:76:1
   |
74 |         .withColumn("valid_to", F.lit(None).cast("timestamp")) \
75 |         .withColumn("_surrogate_key", F.monotonically_increasing_id())  # Temporary SK
76 |     
   | ^^^^
77 |     # Get gold path
78 |     gold_path = resolve_path("lake://gold", "dim_customer", config=config)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/dim_customer_scd2.py:79:1
   |
77 |     # Get gold path
78 |     gold_path = resolve_path("lake://gold", "dim_customer", config=config)
79 |     
   | ^^^^
80 |     # Check if Delta table exists
81 |     try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/dim_customer_scd2.py:84:1
   |
82 |         delta_table = DeltaTable.forPath(spark, gold_path)
83 |         logger.info("ðŸ“Š Existing dim_customer table found, performing SCD2 MERGE")
84 |         
   | ^^^^^^^^
85 |         # SCD2 MERGE logic
86 |         # For existing records that changed, close current record and insert new
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/dim_customer_scd2.py:88:1
   |
86 |         # For existing records that changed, close current record and insert new
87 |         # For new records, just insert
88 |         
   | ^^^^^^^^
89 |         # Identify natural key (customer_id)
90 |         natural_key = "customer_id"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/dim_customer_scd2.py:91:1
   |
89 |         # Identify natural key (customer_id)
90 |         natural_key = "customer_id"
91 |         
   | ^^^^^^^^
92 |         # Perform MERGE
93 |         merge_result = delta_table.alias("target").merge(
   |
help: Remove whitespace from blank line

F841 Local variable `merge_result` is assigned to but never used
  --> src/project_a/legacy/jobs/dim_customer_scd2.py:93:9
   |
92 |         # Perform MERGE
93 |         merge_result = delta_table.alias("target").merge(
   |         ^^^^^^^^^^^^
94 |             src_scd2.alias("source"),
95 |             f"target.{natural_key} = source.{natural_key} AND target.is_current = true"
   |
help: Remove assignment to unused variable `merge_result`

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/dim_customer_scd2.py:104:1
    |
102 |             }
103 |         ).whenNotMatchedInsertAll().execute()
104 |         
    | ^^^^^^^^
105 |         logger.info("âœ… SCD2 MERGE completed")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/dim_customer_scd2.py:106:1
    |
105 |         logger.info("âœ… SCD2 MERGE completed")
106 |         
    | ^^^^^^^^
107 |     except Exception as e:
108 |         # Table doesn't exist, create initial table
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/dim_customer_scd2.py:110:1
    |
108 |         # Table doesn't exist, create initial table
109 |         logger.info(f"ðŸ“Š Creating new dim_customer table: {e}")
110 |         
    | ^^^^^^^^
111 |         # Generate surrogate keys (customer_sk)
112 |         # Use row_number over natural key for consistent SKs
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/dim_customer_scd2.py:114:1
    |
112 |         # Use row_number over natural key for consistent SKs
113 |         from pyspark.sql.window import Window
114 |         
    | ^^^^^^^^
115 |         window = Window.partitionBy("customer_id").orderBy(F.col("valid_from"))
116 |         src_with_sk = src_scd2 \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/dim_customer_scd2.py:119:1
    |
117 |             .withColumn("customer_sk", F.row_number().over(window)) \
118 |             .withColumn("customer_sk", F.col("customer_sk") + F.lit(1000000))  # Offset to avoid conflicts
119 |         
    | ^^^^^^^^
120 |         # Write initial table
121 |         write_delta(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/dim_customer_scd2.py:126:1
    |
124 |             mode="overwrite"
125 |         )
126 |         
    | ^^^^^^^^
127 |         logger.info("âœ… Initial dim_customer table created")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/dim_customer_scd2.py:128:1
    |
127 |         logger.info("âœ… Initial dim_customer table created")
128 |     
    | ^^^^
129 |     # Read final result
130 |     dim_customer = read_delta(spark, gold_path)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/dim_customer_scd2.py:131:1
    |
129 |     # Read final result
130 |     dim_customer = read_delta(spark, gold_path)
131 |     
    | ^^^^
132 |     # Metrics
133 |     current_count = dim_customer.filter(F.col("is_current") == True).count()
    |
help: Remove whitespace from blank line

E712 Avoid equality comparisons to `True`; use `F.col("is_current"):` for truth checks
   --> src/project_a/legacy/jobs/dim_customer_scd2.py:133:41
    |
132 |     # Metrics
133 |     current_count = dim_customer.filter(F.col("is_current") == True).count()
    |                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
134 |     total_count = dim_customer.count()
    |
help: Replace with `F.col("is_current")`

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/dim_customer_scd2.py:135:1
    |
133 |     current_count = dim_customer.filter(F.col("is_current") == True).count()
134 |     total_count = dim_customer.count()
135 |     
    | ^^^^
136 |     emit_rowcount("dim_customer_current", current_count, {"layer": "gold"}, config)
137 |     emit_rowcount("dim_customer_total", total_count, {"layer": "gold"}, config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/dim_customer_scd2.py:138:1
    |
136 |     emit_rowcount("dim_customer_current", current_count, {"layer": "gold"}, config)
137 |     emit_rowcount("dim_customer_total", total_count, {"layer": "gold"}, config)
138 |     
    | ^^^^
139 |     logger.info(f"âœ… dim_customer: {current_count:,} current records, {total_count:,} total (including history)")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/dim_customer_scd2.py:140:1
    |
139 |     logger.info(f"âœ… dim_customer: {current_count:,} current records, {total_count:,} total (including history)")
140 |     
    | ^^^^
141 |     return dim_customer
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/dim_customer_scd2.py:149:1
    |
147 |     setup_json_logging(level="INFO", include_trace_id=True)
148 |     logger.info(f"Job started (trace_id={trace_id})")
149 |     
    | ^^^^
150 |     # Load config
151 |     config_path = Path("config/prod.yaml")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/dim_customer_scd2.py:154:1
    |
152 |     if not config_path.exists():
153 |         config_path = Path("config/local.yaml")
154 |     
    | ^^^^
155 |     config = load_config_resolved(str(config_path))
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/dim_customer_scd2.py:156:1
    |
155 |     config = load_config_resolved(str(config_path))
156 |     
    | ^^^^
157 |     # Build Spark
158 |     spark = build_spark(config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/dim_customer_scd2.py:159:1
    |
157 |     # Build Spark
158 |     spark = build_spark(config)
159 |     
    | ^^^^
160 |     try:
161 |         # Build dim_customer
    |
help: Remove whitespace from blank line

F841 Local variable `dim_customer` is assigned to but never used
   --> src/project_a/legacy/jobs/dim_customer_scd2.py:162:9
    |
160 |     try:
161 |         # Build dim_customer
162 |         dim_customer = build_dim_customer_scd2(spark, config)
    |         ^^^^^^^^^^^^
163 |         
164 |         logger.info("ðŸŽ‰ dim_customer SCD2 job completed")
    |
help: Remove assignment to unused variable `dim_customer`

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/dim_customer_scd2.py:163:1
    |
161 |         # Build dim_customer
162 |         dim_customer = build_dim_customer_scd2(spark, config)
163 |         
    | ^^^^^^^^
164 |         logger.info("ðŸŽ‰ dim_customer SCD2 job completed")
165 |         return 0
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/dim_customer_scd2.py:166:1
    |
164 |         logger.info("ðŸŽ‰ dim_customer SCD2 job completed")
165 |         return 0
166 |         
    | ^^^^^^^^
167 |     except Exception as e:
168 |         logger.error(f"âŒ Job failed: {e}", exc_info=True)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/dim_customer_scd2.py:170:1
    |
168 |         logger.error(f"âŒ Job failed: {e}", exc_info=True)
169 |         return 1
170 |         
    | ^^^^^^^^
171 |     finally:
172 |         spark.stop()
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:5:1
   |
 3 |   """
 4 |
 5 | / import argparse
 6 | | import logging
 7 | | import os
 8 | | import sys
 9 | | from typing import Dict, Any
10 | |
11 | | from pyspark.sql import SparkSession, DataFrame
12 | | from pyspark.sql import functions as F
13 | | from pyspark.sql.functions import col, row_number, max as spark_max, when, lit
14 | | from pyspark.sql.window import Window
15 | | from datetime import date, timedelta
   | |____________________________________^
16 |
17 |   # Add src to path for imports
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:9:1
   |
 7 | import os
 8 | import sys
 9 | from typing import Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 |
11 | from pyspark.sql import SparkSession, DataFrame
   |

F401 [*] `typing.Dict` imported but unused
  --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:9:20
   |
 7 | import os
 8 | import sys
 9 | from typing import Dict, Any
   |                    ^^^^
10 |
11 | from pyspark.sql import SparkSession, DataFrame
   |
help: Remove unused import

F401 [*] `typing.Any` imported but unused
  --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:9:26
   |
 7 | import os
 8 | import sys
 9 | from typing import Dict, Any
   |                          ^^^
10 |
11 | from pyspark.sql import SparkSession, DataFrame
   |
help: Remove unused import

F401 [*] `pyspark.sql.functions.max` imported but unused
  --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:13:59
   |
11 | from pyspark.sql import SparkSession, DataFrame
12 | from pyspark.sql import functions as F
13 | from pyspark.sql.functions import col, row_number, max as spark_max, when, lit
   |                                                           ^^^^^^^^^
14 | from pyspark.sql.window import Window
15 | from datetime import date, timedelta
   |
help: Remove unused import

F401 [*] `pyspark.sql.functions.lit` imported but unused
  --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:13:76
   |
11 | from pyspark.sql import SparkSession, DataFrame
12 | from pyspark.sql import functions as F
13 | from pyspark.sql.functions import col, row_number, max as spark_max, when, lit
   |                                                                            ^^^
14 | from pyspark.sql.window import Window
15 | from datetime import date, timedelta
   |
help: Remove unused import

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:20:1
   |
18 |   sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))
19 |
20 | / from project_a.utils.spark_session import build_spark
21 | | from project_a.utils.config import load_conf
22 | | from project_a.utils.logging import setup_json_logging
   | |______________________________________________________^
23 |
24 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

F401 [*] `project_a.utils.spark_session.build_spark` imported but unused
  --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:20:43
   |
18 | sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))
19 |
20 | from project_a.utils.spark_session import build_spark
   |                                           ^^^^^^^^^^^
21 | from project_a.utils.config import load_conf
22 | from project_a.utils.logging import setup_json_logging
   |
help: Remove unused import: `project_a.utils.spark_session.build_spark`

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:30:1
   |
28 |     """
29 |     Deduplicate FX rates by (as_of_date, ccy) keeping the latest record.
30 |     
   | ^^^^
31 |     Args:
32 |         spark: Spark session
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:34:1
   |
32 |         spark: Spark session
33 |         bronze_df: Bronze FX rates DataFrame
34 |         
   | ^^^^^^^^
35 |     Returns:
36 |         Deduplicated DataFrame
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:39:1
   |
37 |     """
38 |     logger.info("Deduplicating FX rates by (as_of_date, ccy)")
39 |     
   | ^^^^
40 |     # FX rates freshness guard - check if rates are stale
41 |     latest = bronze_df.agg(F.max("as_of_date")).first()[0]
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:44:1
   |
42 |     if latest < (date.today() - timedelta(days=2)):
43 |         raise RuntimeError(f"FX rates stale: {latest}")
44 |     
   | ^^^^
45 |     # Define window for deduplication
46 |     window_spec = Window.partitionBy("as_of_date", "ccy").orderBy(col("ingestion_timestamp").desc())
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:47:1
   |
45 |     # Define window for deduplication
46 |     window_spec = Window.partitionBy("as_of_date", "ccy").orderBy(col("ingestion_timestamp").desc())
47 |     
   | ^^^^
48 |     # Add row number and filter latest records
49 |     deduplicated_df = bronze_df.withColumn(
   |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
  --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:50:19
   |
48 |     # Add row number and filter latest records
49 |     deduplicated_df = bronze_df.withColumn(
50 |         "row_num", 
   |                   ^
51 |         row_number().over(window_spec)
52 |     ).filter(col("row_num") == 1).drop("row_num")
   |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:53:1
   |
51 |         row_number().over(window_spec)
52 |     ).filter(col("row_num") == 1).drop("row_num")
53 |     
   | ^^^^
54 |     logger.info(f"Deduplication completed. Records: {deduplicated_df.count()}")
55 |     return deduplicated_df
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:61:1
   |
59 |     """
60 |     Add rate categories based on currency type.
61 |     
   | ^^^^
62 |     Args:
63 |         spark: Spark session
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:65:1
   |
63 |         spark: Spark session
64 |         df: FX rates DataFrame
65 |         
   | ^^^^^^^^
66 |     Returns:
67 |         DataFrame with rate categories
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:70:1
   |
68 |     """
69 |     logger.info("Adding rate categories")
70 |     
   | ^^^^
71 |     # Define major currencies
72 |     major_currencies = ["USD", "EUR", "GBP", "JPY", "CHF", "CAD", "AUD", "NZD"]
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:73:1
   |
71 |     # Define major currencies
72 |     major_currencies = ["USD", "EUR", "GBP", "JPY", "CHF", "CAD", "AUD", "NZD"]
73 |     
   | ^^^^
74 |     # Add category
75 |     df_with_category = df.withColumn(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:81:1
   |
79 |         .otherwise("exotic")
80 |     )
81 |     
   | ^^^^
82 |     logger.info("Rate categories added successfully")
83 |     return df_with_category
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:89:1
   |
87 |     """
88 |     Validate FX rates data quality.
89 |     
   | ^^^^
90 |     Args:
91 |         df: FX rates DataFrame
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:92:1
   |
90 |     Args:
91 |         df: FX rates DataFrame
92 |         
   | ^^^^^^^^
93 |     Returns:
94 |         True if validation passes, False otherwise
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:97:1
   |
95 |     """
96 |     logger.info("Validating FX rates data quality")
97 |     
   | ^^^^
98 |     # Check for null values in critical columns
99 |     null_checks = [
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:104:1
    |
102 |         df.filter(col("as_of_date").isNull()).count()
103 |     ]
104 |     
    | ^^^^
105 |     if any(null_checks):
106 |         logger.error(f"Found null values in critical columns: {null_checks}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:108:1
    |
106 |         logger.error(f"Found null values in critical columns: {null_checks}")
107 |         return False
108 |     
    | ^^^^
109 |     # Check for negative rates
110 |     negative_rates = df.filter(col("rate_to_base") <= 0).count()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:114:1
    |
112 |         logger.error(f"Found {negative_rates} negative or zero rates")
113 |         return False
114 |     
    | ^^^^
115 |     # Check for duplicate (as_of_date, ccy) combinations
116 |     total_count = df.count()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:121:1
    |
119 |         logger.error(f"Found duplicate (as_of_date, ccy) combinations: {total_count} total, {unique_count} unique")
120 |         return False
121 |     
    | ^^^^
122 |     logger.info("FX rates validation passed")
123 |     return True
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:131:1
    |
129 |     parser.add_argument("--config", required=True, help="Configuration file path")
130 |     parser.add_argument("--lake-root", required=True, help="Data lake root path")
131 |     
    | ^^^^
132 |     args = parser.parse_args()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:133:1
    |
132 |     args = parser.parse_args()
133 |     
    | ^^^^
134 |     # Setup logging
135 |     setup_json_logging()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:137:1
    |
135 |     setup_json_logging()
136 |     logger.info("Starting FX Bronze to Silver transformation job")
137 |     
    | ^^^^
138 |     # Load configuration
139 |     config = load_conf(args.config)
    |
help: Remove whitespace from blank line

F841 Local variable `config` is assigned to but never used
   --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:139:5
    |
138 |     # Load configuration
139 |     config = load_conf(args.config)
    |     ^^^^^^
140 |     lake_root = args.lake_root
    |
help: Remove assignment to unused variable `config`

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:141:1
    |
139 |     config = load_conf(args.config)
140 |     lake_root = args.lake_root
141 |     
    | ^^^^
142 |     # Create Spark session with Delta config
143 |     spark = get_spark_session(
    |
help: Remove whitespace from blank line

F821 Undefined name `get_spark_session`
   --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:143:13
    |
142 |     # Create Spark session with Delta config
143 |     spark = get_spark_session(
    |             ^^^^^^^^^^^^^^^^^
144 |         app_name="fx-bronze-to-silver",
145 |         config=get_delta_config()
    |

F821 Undefined name `get_delta_config`
   --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:145:16
    |
143 |     spark = get_spark_session(
144 |         app_name="fx-bronze-to-silver",
145 |         config=get_delta_config()
    |                ^^^^^^^^^^^^^^^^
146 |     )
    |

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:147:1
    |
145 |         config=get_delta_config()
146 |     )
147 |     
    | ^^^^
148 |     try:
149 |         # Read Bronze data
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:152:1
    |
150 |         bronze_path = f"{lake_root}/bronze/fx_rates"
151 |         bronze_df = spark.read.format("delta").load(bronze_path)
152 |         
    | ^^^^^^^^
153 |         logger.info(f"Read {bronze_df.count()} records from Bronze layer")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:154:1
    |
153 |         logger.info(f"Read {bronze_df.count()} records from Bronze layer")
154 |         
    | ^^^^^^^^
155 |         # Deduplicate data
156 |         deduplicated_df = deduplicate_fx_rates(spark, bronze_df)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:157:1
    |
155 |         # Deduplicate data
156 |         deduplicated_df = deduplicate_fx_rates(spark, bronze_df)
157 |         
    | ^^^^^^^^
158 |         # Add rate categories
159 |         silver_df = add_rate_categories(spark, deduplicated_df)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:160:1
    |
158 |         # Add rate categories
159 |         silver_df = add_rate_categories(spark, deduplicated_df)
160 |         
    | ^^^^^^^^
161 |         # Validate data quality
162 |         if not validate_fx_rates(silver_df):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:164:1
    |
162 |         if not validate_fx_rates(silver_df):
163 |             raise ValueError("Data quality validation failed")
164 |         
    | ^^^^^^^^
165 |         # Write to Silver layer
166 |         silver_path = f"{lake_root}/silver/fx_rates"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:172:1
    |
170 |             .option("mergeSchema", "true") \
171 |             .save(silver_path)
172 |         
    | ^^^^^^^^
173 |         logger.info(f"Successfully wrote {silver_df.count()} records to {silver_path}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:174:1
    |
173 |         logger.info(f"Successfully wrote {silver_df.count()} records to {silver_path}")
174 |         
    | ^^^^^^^^
175 |         # Show sample data
176 |         silver_df.show(5, truncate=False)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:177:1
    |
175 |         # Show sample data
176 |         silver_df.show(5, truncate=False)
177 |         
    | ^^^^^^^^
178 |         # Show data quality summary
179 |         silver_df.groupBy("rate_category").count().show()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/fx_bronze_to_silver.py:180:1
    |
178 |         # Show data quality summary
179 |         silver_df.groupBy("rate_category").count().show()
180 |         
    | ^^^^^^^^
181 |     except Exception as e:
182 |         logger.error(f"Job failed: {e}")
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/jobs/fx_to_bronze.py:5:1
   |
 3 |   """
 4 |
 5 | / import argparse
 6 | | import logging
 7 | | import os
 8 | | import sys
 9 | | from datetime import datetime, timedelta
10 | | from typing import Dict, Any, List
11 | |
12 | | from pyspark.sql import SparkSession
13 | | from pyspark.sql.functions import col, lit, current_timestamp, to_date
14 | | from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType
   | |_______________________________________________________________________________________^
15 |
16 |   # Add src to path for imports
   |
help: Organize imports

F401 [*] `datetime.timedelta` imported but unused
  --> src/project_a/legacy/jobs/fx_to_bronze.py:9:32
   |
 7 | import os
 8 | import sys
 9 | from datetime import datetime, timedelta
   |                                ^^^^^^^^^
10 | from typing import Dict, Any, List
   |
help: Remove unused import: `datetime.timedelta`

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/legacy/jobs/fx_to_bronze.py:10:1
   |
 8 | import sys
 9 | from datetime import datetime, timedelta
10 | from typing import Dict, Any, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
11 |
12 | from pyspark.sql import SparkSession
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/legacy/jobs/fx_to_bronze.py:10:1
   |
 8 | import sys
 9 | from datetime import datetime, timedelta
10 | from typing import Dict, Any, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
11 |
12 | from pyspark.sql import SparkSession
   |

F401 [*] `pyspark.sql.SparkSession` imported but unused
  --> src/project_a/legacy/jobs/fx_to_bronze.py:12:25
   |
10 | from typing import Dict, Any, List
11 |
12 | from pyspark.sql import SparkSession
   |                         ^^^^^^^^^^^^
13 | from pyspark.sql.functions import col, lit, current_timestamp, to_date
14 | from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType
   |
help: Remove unused import: `pyspark.sql.SparkSession`

F401 [*] `pyspark.sql.functions.col` imported but unused
  --> src/project_a/legacy/jobs/fx_to_bronze.py:13:35
   |
12 | from pyspark.sql import SparkSession
13 | from pyspark.sql.functions import col, lit, current_timestamp, to_date
   |                                   ^^^
14 | from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType
   |
help: Remove unused import

F401 [*] `pyspark.sql.functions.to_date` imported but unused
  --> src/project_a/legacy/jobs/fx_to_bronze.py:13:64
   |
12 | from pyspark.sql import SparkSession
13 | from pyspark.sql.functions import col, lit, current_timestamp, to_date
   |                                                                ^^^^^^^
14 | from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType
   |
help: Remove unused import

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/jobs/fx_to_bronze.py:19:1
   |
17 |   sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))
18 |
19 | / from project_a.utils.spark_session import build_spark
20 | | from project_a.utils.config import load_conf
21 | | from project_a.utils.logging import setup_json_logging
   | |______________________________________________________^
22 |
23 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

F401 [*] `project_a.utils.spark_session.build_spark` imported but unused
  --> src/project_a/legacy/jobs/fx_to_bronze.py:19:43
   |
17 | sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))
18 |
19 | from project_a.utils.spark_session import build_spark
   |                                           ^^^^^^^^^^^
20 | from project_a.utils.config import load_conf
21 | from project_a.utils.logging import setup_json_logging
   |
help: Remove unused import: `project_a.utils.spark_session.build_spark`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/legacy/jobs/fx_to_bronze.py:26:71
   |
26 | def fetch_fx_rates(base_currency: str = "USD", days_back: int = 7) -> List[Dict[str, Any]]:
   |                                                                       ^^^^
27 |     """
28 |     Legacy helper kept for compatibility. In this project, FX rates are
   |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/jobs/fx_to_bronze.py:26:76
   |
26 | def fetch_fx_rates(base_currency: str = "USD", days_back: int = 7) -> List[Dict[str, Any]]:
   |                                                                            ^^^^
27 |     """
28 |     Legacy helper kept for compatibility. In this project, FX rates are
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/fx_to_bronze.py:61:1
   |
59 |     parser.add_argument("--days-back", type=int, default=7, help="Days to fetch back")
60 |     parser.add_argument("--base-currency", default="USD", help="Base currency")
61 |     
   | ^^^^
62 |     args = parser.parse_args()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/fx_to_bronze.py:63:1
   |
62 |     args = parser.parse_args()
63 |     
   | ^^^^
64 |     # Setup logging
65 |     setup_json_logging()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/fx_to_bronze.py:67:1
   |
65 |     setup_json_logging()
66 |     logger.info("Starting FX to Bronze ingestion job")
67 |     
   | ^^^^
68 |     # Load configuration
69 |     config = load_conf(args.config)
   |
help: Remove whitespace from blank line

F841 Local variable `config` is assigned to but never used
  --> src/project_a/legacy/jobs/fx_to_bronze.py:69:5
   |
68 |     # Load configuration
69 |     config = load_conf(args.config)
   |     ^^^^^^
70 |     lake_root = args.lake_root
   |
help: Remove assignment to unused variable `config`

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/fx_to_bronze.py:71:1
   |
69 |     config = load_conf(args.config)
70 |     lake_root = args.lake_root
71 |     
   | ^^^^
72 |     # Create Spark session with Delta config
73 |     spark = get_spark_session(
   |
help: Remove whitespace from blank line

F821 Undefined name `get_spark_session`
  --> src/project_a/legacy/jobs/fx_to_bronze.py:73:13
   |
72 |     # Create Spark session with Delta config
73 |     spark = get_spark_session(
   |             ^^^^^^^^^^^^^^^^^
74 |         app_name="fx-to-bronze",
75 |         config=get_delta_config()
   |

F821 Undefined name `get_delta_config`
  --> src/project_a/legacy/jobs/fx_to_bronze.py:75:16
   |
73 |     spark = get_spark_session(
74 |         app_name="fx-to-bronze",
75 |         config=get_delta_config()
   |                ^^^^^^^^^^^^^^^^
76 |     )
   |

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/fx_to_bronze.py:77:1
   |
75 |         config=get_delta_config()
76 |     )
77 |     
   | ^^^^
78 |     try:
79 |         # Fetch FX rates
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/fx_to_bronze.py:81:1
   |
79 |         # Fetch FX rates
80 |         fx_rates = fetch_fx_rates(args.base_currency, args.days_back)
81 |         
   | ^^^^^^^^
82 |         if not fx_rates:
83 |             logger.warning("No FX rates fetched, exiting")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/fx_to_bronze.py:85:1
   |
83 |             logger.warning("No FX rates fetched, exiting")
84 |             return
85 |         
   | ^^^^^^^^
86 |         # Create DataFrame
87 |         df = spark.createDataFrame(fx_rates, create_fx_schema())
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/fx_to_bronze.py:88:1
   |
86 |         # Create DataFrame
87 |         df = spark.createDataFrame(fx_rates, create_fx_schema())
88 |         
   | ^^^^^^^^
89 |         # Add processing metadata
90 |         df = df.withColumn("_proc_date", lit(datetime.now().strftime("%Y-%m-%d")))
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/fx_to_bronze.py:92:1
   |
90 |         df = df.withColumn("_proc_date", lit(datetime.now().strftime("%Y-%m-%d")))
91 |         df = df.withColumn("_proc_timestamp", current_timestamp())
92 |         
   | ^^^^^^^^
93 |         # Write to Bronze layer
94 |         bronze_path = f"{lake_root}/bronze/fx_rates"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/fx_to_bronze.py:100:1
    |
 98 |             .option("mergeSchema", "true") \
 99 |             .save(bronze_path)
100 |         
    | ^^^^^^^^
101 |         logger.info(f"Successfully wrote {df.count()} records to {bronze_path}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/fx_to_bronze.py:102:1
    |
101 |         logger.info(f"Successfully wrote {df.count()} records to {bronze_path}")
102 |         
    | ^^^^^^^^
103 |         # Show sample data
104 |         df.show(5, truncate=False)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/fx_to_bronze.py:105:1
    |
103 |         # Show sample data
104 |         df.show(5, truncate=False)
105 |         
    | ^^^^^^^^
106 |     except Exception as e:
107 |         logger.error(f"Job failed: {e}")
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/jobs/gold_star_schema.py:13:1
   |
11 |   """
12 |
13 | / import sys
14 | | import logging
15 | | from pathlib import Path
16 | | from datetime import datetime
17 | | from typing import Dict, Any
   | |____________________________^
18 |
19 |   sys.path.insert(0, str(Path(__file__).parent.parent))
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/legacy/jobs/gold_star_schema.py:17:1
   |
15 | from pathlib import Path
16 | from datetime import datetime
17 | from typing import Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
18 |
19 | sys.path.insert(0, str(Path(__file__).parent.parent))
   |

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/jobs/gold_star_schema.py:21:1
   |
19 |   sys.path.insert(0, str(Path(__file__).parent.parent))
20 |
21 | / from pyspark.sql import SparkSession, DataFrame
22 | | from pyspark.sql import functions as F
23 | |
24 | | from project_a.utils.spark_session import build_spark
25 | | from project_a.utils.path_resolver import resolve_path
26 | | from project_a.utils.io import read_delta, write_delta
27 | | from project_a.config_loader import load_config_resolved
28 | | from project_a.monitoring.lineage_decorator import lineage_job
29 | | from project_a.monitoring.metrics_collector import emit_rowcount
30 | | from project_a.utils.logging import setup_json_logging
   | |______________________________________________________^
31 |
32 |   logging.basicConfig(level=logging.INFO)
   |
help: Organize imports

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/gold_star_schema.py:39:1
   |
37 |     """
38 |     Build dim_date dimension table.
39 |     
   | ^^^^
40 |     Args:
41 |         spark: SparkSession
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/gold_star_schema.py:44:1
   |
42 |         start_date: Start date YYYY-MM-DD
43 |         end_date: End date YYYY-MM-DD
44 |         
   | ^^^^^^^^
45 |     Returns:
46 |         dim_date DataFrame with surrogate key date_sk
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/gold_star_schema.py:49:1
   |
47 |     """
48 |     from datetime import datetime, timedelta
49 |     
   | ^^^^
50 |     start = datetime.strptime(start_date, "%Y-%m-%d")
51 |     end = datetime.strptime(end_date, "%Y-%m-%d")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/gold_star_schema.py:52:1
   |
50 |     start = datetime.strptime(start_date, "%Y-%m-%d")
51 |     end = datetime.strptime(end_date, "%Y-%m-%d")
52 |     
   | ^^^^
53 |     dates = []
54 |     current = start
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/gold_star_schema.py:56:1
   |
54 |     current = start
55 |     sk = 1
56 |     
   | ^^^^
57 |     while current <= end:
58 |         dates.append({
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/gold_star_schema.py:73:1
   |
71 |         current += timedelta(days=1)
72 |         sk += 1
73 |     
   | ^^^^
74 |     # Create DataFrame
75 |     dim_date = spark.createDataFrame(dates)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/gold_star_schema.py:76:1
   |
74 |     # Create DataFrame
75 |     dim_date = spark.createDataFrame(dates)
76 |     
   | ^^^^
77 |     logger.info(f"âœ… dim_date: {len(dates):,} dates created")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/gold_star_schema.py:78:1
   |
77 |     logger.info(f"âœ… dim_date: {len(dates):,} dates created")
78 |     
   | ^^^^
79 |     return dim_date
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/jobs/gold_star_schema.py:82:52
   |
82 | def build_dim_product(spark: SparkSession, config: Dict[str, Any]) -> DataFrame:
   |                                                    ^^^^
83 |     """
84 |     Build dim_product from silver products.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/gold_star_schema.py:85:1
   |
83 |     """
84 |     Build dim_product from silver products.
85 |     
   | ^^^^
86 |     Args:
87 |         spark: SparkSession
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/gold_star_schema.py:89:1
   |
87 |         spark: SparkSession
88 |         config: Configuration dict
89 |         
   | ^^^^^^^^
90 |     Returns:
91 |         dim_product DataFrame with surrogate key product_sk
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/gold_star_schema.py:95:1
   |
93 |     # Read silver products (or create from orders if products table doesn't exist)
94 |     silver_path = resolve_path("lake://silver", "products", config=config)
95 |     
   | ^^^^
96 |     try:
97 |         products = read_delta(spark, silver_path)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/gold_star_schema.py:98:1
    |
 96 |     try:
 97 |         products = read_delta(spark, silver_path)
 98 |         
    | ^^^^^^^^
 99 |         if products.isEmpty():
100 |             # If no products table, extract unique products from orders
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/gold_star_schema.py:103:1
    |
101 |             orders_path = resolve_path("lake://silver", "orders", config=config)
102 |             orders = read_delta(spark, orders_path)
103 |             
    | ^^^^^^^^^^^^
104 |             if "product_id" in orders.columns:
105 |                 products = orders.select("product_id", "product_name", "product_category") \
    |
help: Remove whitespace from blank line

W291 Trailing whitespace
   --> src/project_a/legacy/jobs/gold_star_schema.py:110:27
    |
108 |                 logger.warning("âš ï¸  No product data found, creating empty dim_product")
109 |                 return spark.createDataFrame([], schema=spark.sql("""
110 |                     SELECT 
    |                           ^
111 |                         0 as product_sk,
112 |                         '' as product_id,
    |
help: Remove trailing whitespace

E722 Do not use bare `except`
   --> src/project_a/legacy/jobs/gold_star_schema.py:118:5
    |
116 |                         0.0 as price
117 |                 """).schema)
118 |     except:
    |     ^^^^^^
119 |         logger.warning("âš ï¸  Products table not found, extracting from orders")
120 |         orders_path = resolve_path("lake://silver", "orders", config=config)
    |

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/gold_star_schema.py:122:1
    |
120 |         orders_path = resolve_path("lake://silver", "orders", config=config)
121 |         orders = read_delta(spark, orders_path)
122 |         
    | ^^^^^^^^
123 |         if "product_id" in orders.columns:
124 |             products = orders.select("product_id", "product_name", "product_category").distinct()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/gold_star_schema.py:127:1
    |
125 |         else:
126 |             return spark.createDataFrame([], schema=spark.sql("SELECT 0 as product_sk, '' as product_id").schema)
127 |     
    | ^^^^
128 |     # Add surrogate key
129 |     dim_product = products \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/gold_star_schema.py:133:1
    |
131 |         .withColumn("product_name", F.coalesce(F.col("product_name"), F.lit("UNKNOWN"))) \
132 |         .withColumn("product_category", F.coalesce(F.col("product_category"), F.lit("UNKNOWN")))
133 |     
    | ^^^^
134 |     logger.info(f"âœ… dim_product: {dim_product.count():,} products")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/gold_star_schema.py:135:1
    |
134 |     logger.info(f"âœ… dim_product: {dim_product.count():,} products")
135 |     
    | ^^^^
136 |     return dim_product
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/jobs/gold_star_schema.py:156:13
    |
154 | def build_gold_star_schema(
155 |     spark: SparkSession,
156 |     config: Dict[str, Any],
    |             ^^^^
157 |     run_date: str = None
158 | ) -> Dict[str, DataFrame]:
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/jobs/gold_star_schema.py:158:6
    |
156 |     config: Dict[str, Any],
157 |     run_date: str = None
158 | ) -> Dict[str, DataFrame]:
    |      ^^^^
159 |     """
160 |     Build complete star schema in Gold layer.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/legacy/jobs/gold_star_schema.py:161:1
    |
159 |     """
160 |     Build complete star schema in Gold layer.
161 |     
    | ^^^^
162 |     Args:
163 |         spark: SparkSession
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/jobs/gold_star_schema.py:166:1
    |
164 |         config: Configuration dict
165 |         run_date: Processing date
166 |         
    | ^^^^^^^^
167 |     Returns:
168 |         Dictionary with all gold tables
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/gold_star_schema.py:172:1
    |
170 |     if run_date is None:
171 |         run_date = datetime.now().strftime("%Y-%m-%d")
172 |     
    | ^^^^
173 |     logger.info(f"ðŸš€ Building gold star schema (run_date={run_date})")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/gold_star_schema.py:174:1
    |
173 |     logger.info(f"ðŸš€ Building gold star schema (run_date={run_date})")
174 |     
    | ^^^^
175 |     gold_root = resolve_path("lake://gold", config=config)
176 |     silver_root = resolve_path("lake://silver", config=config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/gold_star_schema.py:177:1
    |
175 |     gold_root = resolve_path("lake://gold", config=config)
176 |     silver_root = resolve_path("lake://silver", config=config)
177 |     
    | ^^^^
178 |     # 1. Build dim_date
179 |     logger.info("ðŸ“Š Building dim_date...")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/gold_star_schema.py:182:1
    |
180 |     dim_date = build_dim_date(spark)
181 |     write_delta(dim_date, f"{gold_root}/dim_date", mode="overwrite")
182 |     
    | ^^^^
183 |     # 2. Build dim_product
184 |     logger.info("ðŸ“Š Building dim_product...")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/gold_star_schema.py:187:1
    |
185 |     dim_product = build_dim_product(spark, config)
186 |     write_delta(dim_product, f"{gold_root}/dim_product", mode="overwrite")
187 |     
    | ^^^^
188 |     # 3. Read dim_customer (SCD2, already created)
189 |     logger.info("ðŸ“Š Reading dim_customer (SCD2)...")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/gold_star_schema.py:191:1
    |
189 |     logger.info("ðŸ“Š Reading dim_customer (SCD2)...")
190 |     dim_customer = read_delta(spark, f"{gold_root}/dim_customer")
191 |     
    | ^^^^
192 |     # Get current records only
193 |     dim_customer_current = dim_customer.filter(F.col("is_current") == True)
    |
help: Remove whitespace from blank line

E712 Avoid equality comparisons to `True`; use `F.col("is_current"):` for truth checks
   --> src/project_a/legacy/jobs/gold_star_schema.py:193:48
    |
192 |     # Get current records only
193 |     dim_customer_current = dim_customer.filter(F.col("is_current") == True)
    |                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
194 |     
195 |     # 4. Read silver fact data
    |
help: Replace with `F.col("is_current")`

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/gold_star_schema.py:194:1
    |
192 |     # Get current records only
193 |     dim_customer_current = dim_customer.filter(F.col("is_current") == True)
194 |     
    | ^^^^
195 |     # 4. Read silver fact data
196 |     logger.info("ðŸ“¥ Reading silver fact data...")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/gold_star_schema.py:199:1
    |
197 |     silver_orders = read_delta(spark, f"{silver_root}/orders")
198 |     silver_activity = read_delta(spark, f"{silver_root}/customer_activity")
199 |     
    | ^^^^
200 |     # 5. Build fact_sales with surrogate keys
201 |     logger.info("ðŸ“Š Building fact_sales...")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/gold_star_schema.py:202:1
    |
200 |     # 5. Build fact_sales with surrogate keys
201 |     logger.info("ðŸ“Š Building fact_sales...")
202 |     
    | ^^^^
203 |     # Join orders with dimensions
204 |     fact_sales = silver_orders \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/gold_star_schema.py:237:1
    |
235 |             F.col("_run_date")
236 |         )
237 |     
    | ^^^^
238 |     write_delta(
239 |         fact_sales,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/gold_star_schema.py:244:1
    |
242 |         partitionBy=["order_date"]
243 |     )
244 |     
    | ^^^^
245 |     logger.info(f"âœ… fact_sales: {fact_sales.count():,} records")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/gold_star_schema.py:246:1
    |
245 |     logger.info(f"âœ… fact_sales: {fact_sales.count():,} records")
246 |     
    | ^^^^
247 |     # 6. Build fact_behavior (optional)
248 |     logger.info("ðŸ“Š Building fact_behavior...")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/gold_star_schema.py:249:1
    |
247 |     # 6. Build fact_behavior (optional)
248 |     logger.info("ðŸ“Š Building fact_behavior...")
249 |     
    | ^^^^
250 |     fact_behavior = silver_activity \
251 |         .join(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/gold_star_schema.py:271:1
    |
269 |             F.col("_ingest_ts").alias("ingest_timestamp")
270 |         )
271 |     
    | ^^^^
272 |     write_delta(
273 |         fact_behavior,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/gold_star_schema.py:278:1
    |
276 |         partitionBy=["event_date"]
277 |     )
278 |     
    | ^^^^
279 |     logger.info(f"âœ… fact_behavior: {fact_behavior.count():,} records")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/gold_star_schema.py:280:1
    |
279 |     logger.info(f"âœ… fact_behavior: {fact_behavior.count():,} records")
280 |     
    | ^^^^
281 |     # Metrics
282 |     emit_rowcount("gold_fact_sales_total", fact_sales.count(), {"layer": "gold"}, config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/gold_star_schema.py:284:1
    |
282 |     emit_rowcount("gold_fact_sales_total", fact_sales.count(), {"layer": "gold"}, config)
283 |     emit_rowcount("gold_fact_behavior_total", fact_behavior.count(), {"layer": "gold"}, config)
284 |     
    | ^^^^
285 |     logger.info("ðŸŽ‰ Gold star schema build complete")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/gold_star_schema.py:286:1
    |
285 |     logger.info("ðŸŽ‰ Gold star schema build complete")
286 |     
    | ^^^^
287 |     return {
288 |         "dim_date": dim_date,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/gold_star_schema.py:299:1
    |
297 |     """Main entry point."""
298 |     setup_json_logging(level="INFO")
299 |     
    | ^^^^
300 |     config_path = Path("config/prod.yaml")
301 |     if not config_path.exists():
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/gold_star_schema.py:303:1
    |
301 |     if not config_path.exists():
302 |         config_path = Path("config/local.yaml")
303 |     
    | ^^^^
304 |     config = load_config_resolved(str(config_path))
305 |     spark = build_spark(config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/gold_star_schema.py:306:1
    |
304 |     config = load_config_resolved(str(config_path))
305 |     spark = build_spark(config)
306 |     
    | ^^^^
307 |     try:
308 |         build_gold_star_schema(spark, config)
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
 --> src/project_a/legacy/jobs/gold_writer.py:4:1
  |
2 |   Idempotent Gold Layer Writer with MERGE operations
3 |   """
4 | / import logging
5 | | from pyspark.sql import SparkSession
6 | | from delta.tables import DeltaTable
  | |___________________________________^
7 |
8 |   logger = logging.getLogger(__name__)
  |
help: Organize imports

W291 [*] Trailing whitespace
  --> src/project_a/legacy/jobs/gold_writer.py:10:74
   |
 8 | logger = logging.getLogger(__name__)
 9 |
10 | def publish_gold_idempotent(source_df, gold_path: str, key_columns: list, 
   |                                                                          ^
11 |                            merge_condition: str = None, spark_session=None):
12 |     """
   |
help: Remove trailing whitespace

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/gold_writer.py:14:1
   |
12 |     """
13 |     Publish data to Gold layer using MERGE operation for idempotency
14 |     
   | ^^^^
15 |     Args:
16 |         source_df: Source DataFrame to write
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/gold_writer.py:26:1
   |
24 |     else:
25 |         spark = spark_session
26 |     
   | ^^^^
27 |     try:
28 |         # Try to get existing Delta table
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/gold_writer.py:30:1
   |
28 |         # Try to get existing Delta table
29 |         target = DeltaTable.forPath(spark, gold_path)
30 |         
   | ^^^^^^^^
31 |         # Create merge condition if not provided
32 |         if merge_condition is None:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/gold_writer.py:34:1
   |
32 |         if merge_condition is None:
33 |             merge_condition = " AND ".join([f"t.{col} = s.{col}" for col in key_columns])
34 |         
   | ^^^^^^^^
35 |         logger.info(f"ðŸ”„ Merging data into existing Delta table at: {gold_path}")
36 |         logger.info(f"ðŸ“Š Source rows: {source_df.count()}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/gold_writer.py:38:1
   |
36 |         logger.info(f"ðŸ“Š Source rows: {source_df.count()}")
37 |         logger.info(f"ðŸ”‘ Merge keys: {key_columns}")
38 |         
   | ^^^^^^^^
39 |         # Perform MERGE operation
40 |         (target.alias("t")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/gold_writer.py:45:1
   |
43 |          .whenNotMatchedInsertAll()
44 |          .execute())
45 |         
   | ^^^^^^^^
46 |         # Get final count
47 |         final_count = spark.read.format("delta").load(gold_path).count()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/gold_writer.py:49:1
   |
47 |         final_count = spark.read.format("delta").load(gold_path).count()
48 |         logger.info(f"âœ… MERGE completed. Final table rows: {final_count}")
49 |         
   | ^^^^^^^^
50 |     except Exception as e:
51 |         # Table doesn't exist yet -> first write
   |
help: Remove whitespace from blank line

F841 [*] Local variable `e` is assigned to but never used
  --> src/project_a/legacy/jobs/gold_writer.py:50:25
   |
48 |         logger.info(f"âœ… MERGE completed. Final table rows: {final_count}")
49 |         
50 |     except Exception as e:
   |                         ^
51 |         # Table doesn't exist yet -> first write
52 |         logger.info(f"ðŸ“ Table doesn't exist, creating new Delta table at: {gold_path}")
   |
help: Remove assignment to unused variable `e`

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/gold_writer.py:53:1
   |
51 |         # Table doesn't exist yet -> first write
52 |         logger.info(f"ðŸ“ Table doesn't exist, creating new Delta table at: {gold_path}")
53 |         
   | ^^^^^^^^
54 |         (source_df.write
55 |          .format("delta")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/gold_writer.py:59:1
   |
57 |          .option("overwriteSchema", "true")
58 |          .save(gold_path))
59 |         
   | ^^^^^^^^
60 |         final_count = spark.read.format("delta").load(gold_path).count()
61 |         logger.info(f"âœ… New Delta table created with {final_count} rows")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/gold_writer.py:62:1
   |
60 |         final_count = spark.read.format("delta").load(gold_path).count()
61 |         logger.info(f"âœ… New Delta table created with {final_count} rows")
62 |     
   | ^^^^
63 |     # Log the write path and counts
64 |     logger.info(f"ðŸ“ Gold written to: {gold_path}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/gold_writer.py:66:1
   |
64 |     logger.info(f"ðŸ“ Gold written to: {gold_path}")
65 |     logger.info(f"ðŸ“Š Gold row count: {final_count}")
66 |     
   | ^^^^
67 |     return final_count
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/jobs/kafka_orders_stream.py:7:1
   |
 5 |   """
 6 |
 7 | / import os
 8 | | import sys
 9 | | import json
10 | | import logging
11 | | from typing import Optional
12 | | from pathlib import Path
   | |________________________^
13 |
14 |   # Add src to path
   |
help: Organize imports

F401 [*] `json` imported but unused
  --> src/project_a/legacy/jobs/kafka_orders_stream.py:9:8
   |
 7 | import os
 8 | import sys
 9 | import json
   |        ^^^^
10 | import logging
11 | from typing import Optional
   |
help: Remove unused import: `json`

F401 [*] `typing.Optional` imported but unused
  --> src/project_a/legacy/jobs/kafka_orders_stream.py:11:20
   |
 9 | import json
10 | import logging
11 | from typing import Optional
   |                    ^^^^^^^^
12 | from pathlib import Path
   |
help: Remove unused import: `typing.Optional`

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/jobs/kafka_orders_stream.py:17:1
   |
15 |   sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))
16 |
17 | / from pyspark.sql import SparkSession
18 | | from pyspark.sql.functions import (
19 | |     col, from_json, current_timestamp, lit, when, 
20 | |     to_timestamp, to_date, regexp_replace, split, expr
21 | | )
22 | | from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType
23 | | from pyspark.sql.utils import AnalysisException
24 | |
25 | | from project_a.utils.spark_session import build_spark
26 | | from project_a.utils.config import load_conf
27 | | from project_a.utils.logging import setup_json_logging
   | |______________________________________________________^
28 |
29 |   # Standardized checkpoint configuration
   |
help: Organize imports

F401 [*] `pyspark.sql.functions.when` imported but unused
  --> src/project_a/legacy/jobs/kafka_orders_stream.py:19:45
   |
17 | from pyspark.sql import SparkSession
18 | from pyspark.sql.functions import (
19 |     col, from_json, current_timestamp, lit, when, 
   |                                             ^^^^
20 |     to_timestamp, to_date, regexp_replace, split, expr
21 | )
   |
help: Remove unused import

W291 [*] Trailing whitespace
  --> src/project_a/legacy/jobs/kafka_orders_stream.py:19:50
   |
17 | from pyspark.sql import SparkSession
18 | from pyspark.sql.functions import (
19 |     col, from_json, current_timestamp, lit, when, 
   |                                                  ^
20 |     to_timestamp, to_date, regexp_replace, split, expr
21 | )
   |
help: Remove trailing whitespace

F401 [*] `pyspark.sql.functions.to_timestamp` imported but unused
  --> src/project_a/legacy/jobs/kafka_orders_stream.py:20:5
   |
18 | from pyspark.sql.functions import (
19 |     col, from_json, current_timestamp, lit, when, 
20 |     to_timestamp, to_date, regexp_replace, split, expr
   |     ^^^^^^^^^^^^
21 | )
22 | from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType
   |
help: Remove unused import

F401 [*] `pyspark.sql.functions.regexp_replace` imported but unused
  --> src/project_a/legacy/jobs/kafka_orders_stream.py:20:28
   |
18 | from pyspark.sql.functions import (
19 |     col, from_json, current_timestamp, lit, when, 
20 |     to_timestamp, to_date, regexp_replace, split, expr
   |                            ^^^^^^^^^^^^^^
21 | )
22 | from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType
   |
help: Remove unused import

F401 [*] `pyspark.sql.functions.split` imported but unused
  --> src/project_a/legacy/jobs/kafka_orders_stream.py:20:44
   |
18 | from pyspark.sql.functions import (
19 |     col, from_json, current_timestamp, lit, when, 
20 |     to_timestamp, to_date, regexp_replace, split, expr
   |                                            ^^^^^
21 | )
22 | from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType
   |
help: Remove unused import

F401 [*] `pyspark.sql.functions.expr` imported but unused
  --> src/project_a/legacy/jobs/kafka_orders_stream.py:20:51
   |
18 | from pyspark.sql.functions import (
19 |     col, from_json, current_timestamp, lit, when, 
20 |     to_timestamp, to_date, regexp_replace, split, expr
   |                                                   ^^^^
21 | )
22 | from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType
   |
help: Remove unused import

F401 [*] `pyspark.sql.types.TimestampType` imported but unused
  --> src/project_a/legacy/jobs/kafka_orders_stream.py:22:80
   |
20 |     to_timestamp, to_date, regexp_replace, split, expr
21 | )
22 | from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType
   |                                                                                ^^^^^^^^^^^^^
23 | from pyspark.sql.utils import AnalysisException
   |
help: Remove unused import: `pyspark.sql.types.TimestampType`

F401 [*] `pyspark.sql.utils.AnalysisException` imported but unused
  --> src/project_a/legacy/jobs/kafka_orders_stream.py:23:31
   |
21 | )
22 | from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType
23 | from pyspark.sql.utils import AnalysisException
   |                               ^^^^^^^^^^^^^^^^^
24 |
25 | from project_a.utils.spark_session import build_spark
   |
help: Remove unused import: `pyspark.sql.utils.AnalysisException`

F401 [*] `project_a.utils.spark_session.build_spark` imported but unused
  --> src/project_a/legacy/jobs/kafka_orders_stream.py:25:43
   |
23 | from pyspark.sql.utils import AnalysisException
24 |
25 | from project_a.utils.spark_session import build_spark
   |                                           ^^^^^^^^^^^
26 | from project_a.utils.config import load_conf
27 | from project_a.utils.logging import setup_json_logging
   |
help: Remove unused import: `project_a.utils.spark_session.build_spark`

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/kafka_orders_stream.py:38:1
   |
36 |     """
37 |     Get Kafka configuration from environment variables.
38 |     
   | ^^^^
39 |     Returns:
40 |         Dictionary with Kafka configuration
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/kafka_orders_stream.py:60:1
   |
58 |     """
59 |     Get the schema for orders events.
60 |     
   | ^^^^
61 |     Returns:
62 |         Spark StructType for orders events
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/kafka_orders_stream.py:77:1
   |
75 |     """
76 |     Process the orders stream from Kafka.
77 |     
   | ^^^^
78 |     Args:
79 |         spark: Spark session
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/kafka_orders_stream.py:83:1
   |
81 |     """
82 |     logger.info("Starting Kafka orders stream processing")
83 |     
   | ^^^^
84 |     # Get Kafka configuration
85 |     kafka_config = get_kafka_config()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/kafka_orders_stream.py:87:1
   |
85 |     kafka_config = get_kafka_config()
86 |     topic = os.getenv("KAFKA_TOPIC", "orders_events")
87 |     
   | ^^^^
88 |     # Get data lake configuration
89 |     lake_root = config["lake"]["root"]
   |
help: Remove whitespace from blank line

F841 Local variable `checkpoint_location` is assigned to but never used
  --> src/project_a/legacy/jobs/kafka_orders_stream.py:90:5
   |
88 |     # Get data lake configuration
89 |     lake_root = config["lake"]["root"]
90 |     checkpoint_location = f"{lake_root}/_checkpoints/orders_events"
   |     ^^^^^^^^^^^^^^^^^^^
91 |     dlq_location = f"{lake_root}/_errors/kafka/orders_events"
   |
help: Remove assignment to unused variable `checkpoint_location`

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/kafka_orders_stream.py:92:1
   |
90 |     checkpoint_location = f"{lake_root}/_checkpoints/orders_events"
91 |     dlq_location = f"{lake_root}/_errors/kafka/orders_events"
92 |     
   | ^^^^
93 |     # Define the orders schema
94 |     orders_schema = get_orders_schema()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/kafka_orders_stream.py:95:1
   |
93 |     # Define the orders schema
94 |     orders_schema = get_orders_schema()
95 |     
   | ^^^^
96 |     # Read from Kafka
97 |     kafka_df = spark \
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/kafka_orders_stream.py:107:1
    |
105 |         .option("startingOffsets", "latest") \
106 |         .load()
107 |     
    | ^^^^
108 |     # Parse JSON and validate schema
109 |     parsed_df = kafka_df.select(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/kafka_orders_stream.py:116:1
    |
114 |         col("offset")
115 |     )
116 |     
    | ^^^^
117 |     # Parse JSON with schema validation
118 |     orders_df = parsed_df.select(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/kafka_orders_stream.py:125:1
    |
123 |         from_json(col("json_value"), orders_schema).alias("parsed_data")
124 |     )
125 |     
    | ^^^^
126 |     # Extract fields and add processing metadata
127 |     processed_df = orders_df.select(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/kafka_orders_stream.py:141:1
    |
139 |         lit("kafka").alias("source")
140 |     )
141 |     
    | ^^^^
142 |     # Add watermark for deduplication and add proc_date
143 |     watermarked_df = processed_df.withWatermark("event_ts", "1 hour") \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/kafka_orders_stream.py:145:1
    |
143 |     watermarked_df = processed_df.withWatermark("event_ts", "1 hour") \
144 |         .withColumn("proc_date", to_date(col("event_ts")))
145 |     
    | ^^^^
146 |     # Separate valid and invalid records with business rules validation
147 |     valid_df = watermarked_df.filter(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/kafka_orders_stream.py:157:1
    |
155 |         col("status").isin(["PLACED", "PAID", "SHIPPED", "CANCELLED"])  # Status must be valid
156 |     )
157 |     
    | ^^^^
158 |     invalid_df = watermarked_df.filter(
159 |         col("order_id").isNull() |
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/kafka_orders_stream.py:168:1
    |
166 |         ~col("status").isin(["PLACED", "PAID", "SHIPPED", "CANCELLED"])  # Status must be valid
167 |     )
168 |     
    | ^^^^
169 |     # Write valid records to Delta table with standardized checkpointing
170 |     valid_query = valid_df.writeStream \
    |
help: Remove whitespace from blank line

F821 Undefined name `S3_LAKE_BUCKET`
   --> src/project_a/legacy/jobs/kafka_orders_stream.py:173:48
    |
171 |         .format("delta") \
172 |         .outputMode("append") \
173 |         .option("checkpointLocation", f"s3a://{S3_LAKE_BUCKET}/{S3_CHECKPOINT_PATH}/orders_stream/valid") \
    |                                                ^^^^^^^^^^^^^^
174 |         .option("path", f"{lake_root}/silver/orders_events") \
175 |         .trigger(processingTime="30 seconds") \
    |

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/kafka_orders_stream.py:177:1
    |
175 |         .trigger(processingTime="30 seconds") \
176 |         .start()
177 |     
    | ^^^^
178 |     # Write invalid records to DLQ with standardized checkpointing
179 |     dlq_query = invalid_df.writeStream \
    |
help: Remove whitespace from blank line

F821 Undefined name `S3_LAKE_BUCKET`
   --> src/project_a/legacy/jobs/kafka_orders_stream.py:182:48
    |
180 |         .format("delta") \
181 |         .outputMode("append") \
182 |         .option("checkpointLocation", f"s3a://{S3_LAKE_BUCKET}/{S3_CHECKPOINT_PATH}/orders_stream/dlq") \
    |                                                ^^^^^^^^^^^^^^
183 |         .option("path", f"{dlq_location}") \
184 |         .trigger(processingTime="30 seconds") \
    |

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/kafka_orders_stream.py:186:1
    |
184 |         .trigger(processingTime="30 seconds") \
185 |         .start()
186 |     
    | ^^^^
187 |     # Also write raw invalid records to S3 for debugging with standardized checkpointing
188 |     raw_invalid_df = parsed_df.filter(col("parsed_data").isNull())
    |
help: Remove whitespace from blank line

F821 Undefined name `S3_LAKE_BUCKET`
   --> src/project_a/legacy/jobs/kafka_orders_stream.py:192:48
    |
190 |         .format("delta") \
191 |         .outputMode("append") \
192 |         .option("checkpointLocation", f"s3a://{S3_LAKE_BUCKET}/{S3_CHECKPOINT_PATH}/orders_stream/raw_dlq") \
    |                                                ^^^^^^^^^^^^^^
193 |         .option("path", f"{dlq_location}/raw") \
194 |         .trigger(processingTime="30 seconds") \
    |

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/kafka_orders_stream.py:196:1
    |
194 |         .trigger(processingTime="30 seconds") \
195 |         .start()
196 |     
    | ^^^^
197 |     logger.info("Kafka stream processing started")
198 |     logger.info(f"Valid records: {lake_root}/silver/orders_events")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/kafka_orders_stream.py:201:1
    |
199 |     logger.info(f"Invalid records: {dlq_location}")
200 |     logger.info(f"Raw invalid records: {dlq_location}/raw")
201 |     
    | ^^^^
202 |     # Wait for termination
203 |     try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/kafka_orders_stream.py:217:1
    |
215 |     """Main function to run the Kafka orders stream job."""
216 |     import argparse
217 |     
    | ^^^^
218 |     parser = argparse.ArgumentParser(description="Process Kafka orders stream")
219 |     parser.add_argument("--config", required=True, help="Configuration file path")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/kafka_orders_stream.py:222:1
    |
220 |     parser.add_argument("--checkpoint", help="Checkpoint location override")
221 |     args = parser.parse_args()
222 |     
    | ^^^^
223 |     # Setup logging
224 |     setup_json_logging()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/kafka_orders_stream.py:227:1
    |
225 |     log_level = os.getenv("LOG_LEVEL", "INFO")
226 |     logging.getLogger().setLevel(getattr(logging, log_level.upper()))
227 |     
    | ^^^^
228 |     try:
229 |         # Load configuration
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/kafka_orders_stream.py:231:1
    |
229 |         # Load configuration
230 |         config = load_conf(args.config)
231 |         
    | ^^^^^^^^
232 |         # Create Spark session with Delta and Kafka support
233 |         spark = get_spark_session(
    |
help: Remove whitespace from blank line

F821 Undefined name `get_spark_session`
   --> src/project_a/legacy/jobs/kafka_orders_stream.py:233:17
    |
232 |         # Create Spark session with Delta and Kafka support
233 |         spark = get_spark_session(
    |                 ^^^^^^^^^^^^^^^^^
234 |             "KafkaOrdersStream",
235 |             extra_conf={
    |

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/kafka_orders_stream.py:242:1
    |
240 |             }
241 |         )
242 |         
    | ^^^^^^^^
243 |         # Process the stream
244 |         process_orders_stream(spark, config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/kafka_orders_stream.py:245:1
    |
243 |         # Process the stream
244 |         process_orders_stream(spark, config)
245 |         
    | ^^^^^^^^
246 |     except Exception as e:
247 |         logger.error(f"Kafka stream processing failed: {e}")
    |
help: Remove whitespace from blank line

W292 [*] No newline at end of file
   --> src/project_a/legacy/jobs/kafka_orders_stream.py:252:11
    |
251 | if __name__ == "__main__":
252 |     main()
    |           ^
    |
help: Add trailing newline

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/jobs/load_to_snowflake.py:7:1
   |
 5 |   """
 6 |
 7 | / import os
 8 | | import sys
 9 | | import logging
10 | | import argparse
11 | | from typing import Dict, Any, List
12 | | from pathlib import Path
   | |________________________^
13 |
14 |   # Add project root to path
   |
help: Organize imports

F401 [*] `os` imported but unused
 --> src/project_a/legacy/jobs/load_to_snowflake.py:7:8
  |
5 | """
6 |
7 | import os
  |        ^^
8 | import sys
9 | import logging
  |
help: Remove unused import: `os`

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/legacy/jobs/load_to_snowflake.py:11:1
   |
 9 | import logging
10 | import argparse
11 | from typing import Dict, Any, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
12 | from pathlib import Path
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/legacy/jobs/load_to_snowflake.py:11:1
   |
 9 | import logging
10 | import argparse
11 | from typing import Dict, Any, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
12 | from pathlib import Path
   |

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/jobs/load_to_snowflake.py:17:1
   |
15 |   sys.path.insert(0, str(Path(__file__).parent.parent.parent))
16 |
17 | / from pyspark.sql import SparkSession
18 | | from pyspark.sql.functions import lit, current_timestamp, col
19 | | from project_a.utils.spark_session import build_spark
20 | | from project_a.utils.config import load_conf
21 | | from project_a.utils.secrets import get_snowflake_credentials
   | |_____________________________________________________________^
22 |
23 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

F401 [*] `pyspark.sql.functions.lit` imported but unused
  --> src/project_a/legacy/jobs/load_to_snowflake.py:18:35
   |
17 | from pyspark.sql import SparkSession
18 | from pyspark.sql.functions import lit, current_timestamp, col
   |                                   ^^^
19 | from project_a.utils.spark_session import build_spark
20 | from project_a.utils.config import load_conf
   |
help: Remove unused import

F401 [*] `pyspark.sql.functions.col` imported but unused
  --> src/project_a/legacy/jobs/load_to_snowflake.py:18:59
   |
17 | from pyspark.sql import SparkSession
18 | from pyspark.sql.functions import lit, current_timestamp, col
   |                                                           ^^^
19 | from project_a.utils.spark_session import build_spark
20 | from project_a.utils.config import load_conf
   |
help: Remove unused import

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/jobs/load_to_snowflake.py:30:13
   |
28 |     delta_path: str,
29 |     snowflake_table: str,
30 |     config: Dict[str, Any],
   |             ^^^^
31 |     merge_keys: List[str] = None
32 | ) -> bool:
   |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/legacy/jobs/load_to_snowflake.py:31:17
   |
29 |     snowflake_table: str,
30 |     config: Dict[str, Any],
31 |     merge_keys: List[str] = None
   |                 ^^^^
32 | ) -> bool:
33 |     """
   |
help: Replace with `list`

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/load_to_snowflake.py:35:1
   |
33 |     """
34 |     Load Delta table to Snowflake with MERGE support.
35 |     
   | ^^^^
36 |     Args:
37 |         spark: SparkSession
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/load_to_snowflake.py:42:1
   |
40 |         config: Configuration dictionary
41 |         merge_keys: List of columns to use for MERGE (primary key)
42 |         
   | ^^^^^^^^
43 |     Returns:
44 |         True if successful
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/load_to_snowflake.py:47:1
   |
45 |     """
46 |     logger.info(f"Loading {delta_path} to Snowflake table {snowflake_table}")
47 |     
   | ^^^^
48 |     try:
49 |         # Get Snowflake credentials
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/load_to_snowflake.py:51:1
   |
49 |         # Get Snowflake credentials
50 |         sf_creds = get_snowflake_credentials(config)
51 |         
   | ^^^^^^^^
52 |         # Read Delta table
53 |         logger.info(f"Reading Delta table from {delta_path}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/load_to_snowflake.py:55:1
   |
53 |         logger.info(f"Reading Delta table from {delta_path}")
54 |         df = spark.read.format("delta").load(delta_path)
55 |         
   | ^^^^^^^^
56 |         record_count = df.count()
57 |         logger.info(f"Read {record_count:,} records from Delta table")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/load_to_snowflake.py:58:1
   |
56 |         record_count = df.count()
57 |         logger.info(f"Read {record_count:,} records from Delta table")
58 |         
   | ^^^^^^^^
59 |         # Add load metadata
60 |         df = df.withColumn("_load_timestamp", current_timestamp())
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/load_to_snowflake.py:61:1
   |
59 |         # Add load metadata
60 |         df = df.withColumn("_load_timestamp", current_timestamp())
61 |         
   | ^^^^^^^^
62 |         # Write to Snowflake
63 |         sf_options = {
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/load_to_snowflake.py:72:1
   |
70 |             "dbtable": snowflake_table.split('.')[-1]  # Table name only
71 |         }
72 |         
   | ^^^^^^^^
73 |         # If merge_keys provided, use MERGE mode
74 |         if merge_keys:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/load_to_snowflake.py:76:1
   |
74 |         if merge_keys:
75 |             logger.info(f"Using MERGE mode with keys: {merge_keys}")
76 |             
   | ^^^^^^^^^^^^
77 |             # Create temporary staging table
78 |             staging_table = f"{snowflake_table}_staging"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/load_to_snowflake.py:79:1
   |
77 |             # Create temporary staging table
78 |             staging_table = f"{snowflake_table}_staging"
79 |             
   | ^^^^^^^^^^^^
80 |             # Write to staging
81 |             df.write \
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/load_to_snowflake.py:87:1
   |
85 |                 .mode("overwrite") \
86 |                 .save()
87 |             
   | ^^^^^^^^^^^^
88 |             # Execute MERGE statement
89 |             merge_sql = _build_merge_sql(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/load_to_snowflake.py:95:1
   |
93 |                 columns=df.columns
94 |             )
95 |             
   | ^^^^^^^^^^^^
96 |             logger.info(f"Executing MERGE SQL: {merge_sql}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/load_to_snowflake.py:97:1
   |
96 |             logger.info(f"Executing MERGE SQL: {merge_sql}")
97 |             
   | ^^^^^^^^^^^^
98 |             # Execute via JDBC
99 |             sf_url = f"jdbc:snowflake://{sf_options['sfURL']}"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/load_to_snowflake.py:106:1
    |
104 |                 .option("query", merge_sql) \
105 |                 .load().collect()  # Execute query
106 |             
    | ^^^^^^^^^^^^
107 |             logger.info(f"âœ… MERGE completed for {snowflake_table}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/load_to_snowflake.py:108:1
    |
107 |             logger.info(f"âœ… MERGE completed for {snowflake_table}")
108 |             
    | ^^^^^^^^^^^^
109 |         else:
110 |             # Simple overwrite
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/load_to_snowflake.py:118:1
    |
116 |                 .mode("overwrite") \
117 |                 .save()
118 |             
    | ^^^^^^^^^^^^
119 |             logger.info(f"âœ… Overwrite completed for {snowflake_table}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/load_to_snowflake.py:120:1
    |
119 |             logger.info(f"âœ… Overwrite completed for {snowflake_table}")
120 |         
    | ^^^^^^^^
121 |         logger.info(f"âœ… Successfully loaded {record_count:,} records to {snowflake_table}")
122 |         return True
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/load_to_snowflake.py:123:1
    |
121 |         logger.info(f"âœ… Successfully loaded {record_count:,} records to {snowflake_table}")
122 |         return True
123 |         
    | ^^^^^^^^
124 |     except Exception as e:
125 |         logger.error(f"âŒ Failed to load to Snowflake: {e}")
    |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/legacy/jobs/load_to_snowflake.py:129:73
    |
129 | def _build_merge_sql(target_table: str, staging_table: str, merge_keys: List[str], columns: List[str]) -> str:
    |                                                                         ^^^^
130 |     """Build Snowflake MERGE SQL statement."""
131 |     target_alias = "target"
    |
help: Replace with `list`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/legacy/jobs/load_to_snowflake.py:129:93
    |
129 | def _build_merge_sql(target_table: str, staging_table: str, merge_keys: List[str], columns: List[str]) -> str:
    |                                                                                             ^^^^
130 |     """Build Snowflake MERGE SQL statement."""
131 |     target_alias = "target"
    |
help: Replace with `list`

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/load_to_snowflake.py:133:1
    |
131 |     target_alias = "target"
132 |     source_alias = "source"
133 |     
    | ^^^^
134 |     # Build ON clause
135 |     on_clause = " AND ".join([
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/load_to_snowflake.py:138:1
    |
136 |         f"{target_alias}.{key} = {source_alias}.{key}" for key in merge_keys
137 |     ])
138 |     
    | ^^^^
139 |     # Build UPDATE clause
140 |     update_cols = [col for col in columns if col not in merge_keys]
    |
help: Remove whitespace from blank line

F811 Redefinition of unused `col` from line 18
   --> src/project_a/legacy/jobs/load_to_snowflake.py:140:28
    |
139 |     # Build UPDATE clause
140 |     update_cols = [col for col in columns if col not in merge_keys]
    |                            ^^^
141 |     update_clause = ", ".join([
142 |         f"{col} = {source_alias}.{col}" for col in update_cols
    |
help: Remove definition: `col`

F811 Redefinition of unused `col` from line 18
   --> src/project_a/legacy/jobs/load_to_snowflake.py:142:45
    |
140 |     update_cols = [col for col in columns if col not in merge_keys]
141 |     update_clause = ", ".join([
142 |         f"{col} = {source_alias}.{col}" for col in update_cols
    |                                             ^^^
143 |     ])
    |
help: Remove definition: `col`

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/load_to_snowflake.py:144:1
    |
142 |         f"{col} = {source_alias}.{col}" for col in update_cols
143 |     ])
144 |     
    | ^^^^
145 |     # Build INSERT clause
146 |     insert_cols = ", ".join(columns)
    |
help: Remove whitespace from blank line

F811 Redefinition of unused `col` from line 18
   --> src/project_a/legacy/jobs/load_to_snowflake.py:147:60
    |
145 |     # Build INSERT clause
146 |     insert_cols = ", ".join(columns)
147 |     insert_values = ", ".join([f"{source_alias}.{col}" for col in columns])
    |                                                            ^^^
148 |     
149 |     merge_sql = f"""
    |
help: Remove definition: `col`

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/load_to_snowflake.py:148:1
    |
146 |     insert_cols = ", ".join(columns)
147 |     insert_values = ", ".join([f"{source_alias}.{col}" for col in columns])
148 |     
    | ^^^^
149 |     merge_sql = f"""
150 |         MERGE INTO {target_table} AS {target_alias}
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/load_to_snowflake.py:159:1
    |
157 |             VALUES ({insert_values})
158 |     """
159 |     
    | ^^^^
160 |     return merge_sql.strip()
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/jobs/load_to_snowflake.py:163:43
    |
163 | def load_gold_tables_to_snowflake(config: Dict[str, Any], tables: List[str] = None) -> bool:
    |                                           ^^^^
164 |     """
165 |     Load all Gold tables to Snowflake.
    |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/legacy/jobs/load_to_snowflake.py:163:67
    |
163 | def load_gold_tables_to_snowflake(config: Dict[str, Any], tables: List[str] = None) -> bool:
    |                                                                   ^^^^
164 |     """
165 |     Load all Gold tables to Snowflake.
    |
help: Replace with `list`

W293 Blank line contains whitespace
   --> src/project_a/legacy/jobs/load_to_snowflake.py:166:1
    |
164 |     """
165 |     Load all Gold tables to Snowflake.
166 |     
    | ^^^^
167 |     Args:
168 |         config: Configuration dictionary
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/jobs/load_to_snowflake.py:170:1
    |
168 |         config: Configuration dictionary
169 |         tables: List of table names to load (default: all gold tables)
170 |         
    | ^^^^^^^^
171 |     Returns:
172 |         True if successful
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/load_to_snowflake.py:180:1
    |
178 |             "sales_fact"
179 |         ]
180 |     
    | ^^^^
181 |     spark = build_spark(app_name="load_to_snowflake", config=config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/load_to_snowflake.py:182:1
    |
181 |     spark = build_spark(app_name="load_to_snowflake", config=config)
182 |     
    | ^^^^
183 |     gold_base = config.get("data_lake", {}).get("gold_path", "data/lakehouse_delta/gold")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/load_to_snowflake.py:184:1
    |
183 |     gold_base = config.get("data_lake", {}).get("gold_path", "data/lakehouse_delta/gold")
184 |     
    | ^^^^
185 |     # Table configurations
186 |     table_configs = {
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/load_to_snowflake.py:203:1
    |
201 |         }
202 |     }
203 |     
    | ^^^^
204 |     try:
205 |         for table_name in tables:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/load_to_snowflake.py:209:1
    |
207 |                 logger.warning(f"Table {table_name} not configured, skipping")
208 |                 continue
209 |             
    | ^^^^^^^^^^^^
210 |             table_config = table_configs[table_name]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/load_to_snowflake.py:211:1
    |
210 |             table_config = table_configs[table_name]
211 |             
    | ^^^^^^^^^^^^
212 |             success = load_delta_to_snowflake(
213 |                 spark=spark,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/load_to_snowflake.py:219:1
    |
217 |                 merge_keys=table_config.get("merge_keys")
218 |             )
219 |             
    | ^^^^^^^^^^^^
220 |             if not success:
221 |                 logger.error(f"Failed to load {table_name}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/load_to_snowflake.py:223:1
    |
221 |                 logger.error(f"Failed to load {table_name}")
222 |                 return False
223 |         
    | ^^^^^^^^
224 |         logger.info("âœ… All Gold tables loaded to Snowflake successfully")
225 |         return True
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/load_to_snowflake.py:226:1
    |
224 |         logger.info("âœ… All Gold tables loaded to Snowflake successfully")
225 |         return True
226 |         
    | ^^^^^^^^
227 |     finally:
228 |         spark.stop()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/load_to_snowflake.py:236:1
    |
234 |     parser.add_argument("--env", choices=["dev", "prod", "local"], default="prod", help="Environment")
235 |     parser.add_argument("--tables", nargs="+", help="Specific tables to load (default: all)")
236 |     
    | ^^^^
237 |     args = parser.parse_args()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/load_to_snowflake.py:238:1
    |
237 |     args = parser.parse_args()
238 |     
    | ^^^^
239 |     config = load_conf(args.config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/load_to_snowflake.py:240:1
    |
239 |     config = load_conf(args.config)
240 |     
    | ^^^^
241 |     success = load_gold_tables_to_snowflake(config, tables=args.tables)
242 |     sys.exit(0 if success else 1)
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/jobs/reconciliation_job.py:7:1
   |
 5 |   """
 6 |
 7 | / import logging
 8 | | from typing import Dict, Any, List, Tuple
 9 | | from pyspark.sql import SparkSession, DataFrame
10 | | from pyspark.sql.functions import count, sha2, concat_ws, col, sum as spark_sum
   | |_______________________________________________________________________________^
11 |
12 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/legacy/jobs/reconciliation_job.py:8:1
   |
 7 | import logging
 8 | from typing import Dict, Any, List, Tuple
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 9 | from pyspark.sql import SparkSession, DataFrame
10 | from pyspark.sql.functions import count, sha2, concat_ws, col, sum as spark_sum
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/legacy/jobs/reconciliation_job.py:8:1
   |
 7 | import logging
 8 | from typing import Dict, Any, List, Tuple
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 9 | from pyspark.sql import SparkSession, DataFrame
10 | from pyspark.sql.functions import count, sha2, concat_ws, col, sum as spark_sum
   |

UP035 `typing.Tuple` is deprecated, use `tuple` instead
  --> src/project_a/legacy/jobs/reconciliation_job.py:8:1
   |
 7 | import logging
 8 | from typing import Dict, Any, List, Tuple
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 9 | from pyspark.sql import SparkSession, DataFrame
10 | from pyspark.sql.functions import count, sha2, concat_ws, col, sum as spark_sum
   |

F401 [*] `typing.Tuple` imported but unused
  --> src/project_a/legacy/jobs/reconciliation_job.py:8:37
   |
 7 | import logging
 8 | from typing import Dict, Any, List, Tuple
   |                                     ^^^^^
 9 | from pyspark.sql import SparkSession, DataFrame
10 | from pyspark.sql.functions import count, sha2, concat_ws, col, sum as spark_sum
   |
help: Remove unused import: `typing.Tuple`

F401 [*] `pyspark.sql.functions.count` imported but unused
  --> src/project_a/legacy/jobs/reconciliation_job.py:10:35
   |
 8 | from typing import Dict, Any, List, Tuple
 9 | from pyspark.sql import SparkSession, DataFrame
10 | from pyspark.sql.functions import count, sha2, concat_ws, col, sum as spark_sum
   |                                   ^^^^^
11 |
12 | logger = logging.getLogger(__name__)
   |
help: Remove unused import: `pyspark.sql.functions.count`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/legacy/jobs/reconciliation_job.py:15:52
   |
15 | def compute_table_hash(df: DataFrame, key_columns: List[str] = None) -> str:
   |                                                    ^^^^
16 |     """
17 |     Compute hash sum of entire table for reconciliation.
   |
help: Replace with `list`

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/reconciliation_job.py:18:1
   |
16 |     """
17 |     Compute hash sum of entire table for reconciliation.
18 |     
   | ^^^^
19 |     Args:
20 |         df: DataFrame to hash
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/reconciliation_job.py:22:1
   |
20 |         df: DataFrame to hash
21 |         key_columns: Key columns (if None, uses all columns)
22 |         
   | ^^^^^^^^
23 |     Returns:
24 |         SHA-256 hash of table
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/reconciliation_job.py:28:1
   |
26 |     if key_columns is None:
27 |         key_columns = df.columns
28 |     
   | ^^^^
29 |     # Create hash from all key columns
30 |     hash_expr = sha2(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/reconciliation_job.py:34:1
   |
32 |         256
33 |     )
34 |     
   | ^^^^
35 |     # Sum all hashes
36 |     hash_sum = df.agg(spark_sum(hash_expr).alias("hash_sum")).collect()[0]["hash_sum"]
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/reconciliation_job.py:37:1
   |
35 |     # Sum all hashes
36 |     hash_sum = df.agg(spark_sum(hash_expr).alias("hash_sum")).collect()[0]["hash_sum"]
37 |     
   | ^^^^
38 |     return str(hash_sum) if hash_sum else ""
   |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/legacy/jobs/reconciliation_job.py:44:18
   |
42 |     source_df: DataFrame,
43 |     target_df: DataFrame,
44 |     key_columns: List[str],
   |                  ^^^^
45 |     source_name: str = "source",
46 |     target_name: str = "target"
   |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/jobs/reconciliation_job.py:47:6
   |
45 |     source_name: str = "source",
46 |     target_name: str = "target"
47 | ) -> Dict[str, Any]:
   |      ^^^^
48 |     """
49 |     Reconcile two tables by comparing row counts and hash sums.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/reconciliation_job.py:50:1
   |
48 |     """
49 |     Reconcile two tables by comparing row counts and hash sums.
50 |     
   | ^^^^
51 |     Args:
52 |         source_df: Source DataFrame
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/reconciliation_job.py:57:1
   |
55 |         source_name: Name of source system
56 |         target_name: Name of target system
57 |         
   | ^^^^^^^^
58 |     Returns:
59 |         Reconciliation results dictionary
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/reconciliation_job.py:62:1
   |
60 |     """
61 |     logger.info(f"Reconciling {source_name} â†” {target_name}")
62 |     
   | ^^^^
63 |     # Row counts
64 |     source_count = source_df.count()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/reconciliation_job.py:66:1
   |
64 |     source_count = source_df.count()
65 |     target_count = target_df.count()
66 |     
   | ^^^^
67 |     logger.info(f"  {source_name} rows: {source_count:,}")
68 |     logger.info(f"  {target_name} rows: {target_count:,}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/reconciliation_job.py:69:1
   |
67 |     logger.info(f"  {source_name} rows: {source_count:,}")
68 |     logger.info(f"  {target_name} rows: {target_count:,}")
69 |     
   | ^^^^
70 |     # Hash sums
71 |     source_hash = compute_table_hash(source_df, key_columns)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/reconciliation_job.py:73:1
   |
71 |     source_hash = compute_table_hash(source_df, key_columns)
72 |     target_hash = compute_table_hash(target_df, key_columns)
73 |     
   | ^^^^
74 |     logger.info(f"  {source_name} hash: {source_hash[:16]}...")
75 |     logger.info(f"  {target_name} hash: {target_hash[:16]}...")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/reconciliation_job.py:76:1
   |
74 |     logger.info(f"  {source_name} hash: {source_hash[:16]}...")
75 |     logger.info(f"  {target_name} hash: {target_hash[:16]}...")
76 |     
   | ^^^^
77 |     # Compare
78 |     row_count_match = source_count == target_count
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/reconciliation_job.py:80:1
   |
78 |     row_count_match = source_count == target_count
79 |     hash_match = source_hash == target_hash
80 |     
   | ^^^^
81 |     reconciliation_result = {
82 |         "source_name": source_name,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/reconciliation_job.py:92:1
   |
90 |         "reconciled": row_count_match and hash_match
91 |     }
92 |     
   | ^^^^
93 |     if reconciliation_result["reconciled"]:
94 |         logger.info("âœ… Reconciliation PASSED")
   |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
   --> src/project_a/legacy/jobs/reconciliation_job.py:100:26
    |
 98 |             logger.error(f"   Row count mismatch: {source_count:,} vs {target_count:,}")
 99 |         if not hash_match:
100 |             logger.error(f"   Hash mismatch detected")
    |                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
101 |     
102 |     return reconciliation_result
    |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/reconciliation_job.py:101:1
    |
 99 |         if not hash_match:
100 |             logger.error(f"   Hash mismatch detected")
101 |     
    | ^^^^
102 |     return reconciliation_result
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/jobs/reconciliation_job.py:109:13
    |
107 |     snowflake_table: str,
108 |     s3_path: str,
109 |     config: Dict[str, Any],
    |             ^^^^
110 |     key_columns: List[str]
111 | ) -> Dict[str, Any]:
    |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/legacy/jobs/reconciliation_job.py:110:18
    |
108 |     s3_path: str,
109 |     config: Dict[str, Any],
110 |     key_columns: List[str]
    |                  ^^^^
111 | ) -> Dict[str, Any]:
112 |     """
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/jobs/reconciliation_job.py:111:6
    |
109 |     config: Dict[str, Any],
110 |     key_columns: List[str]
111 | ) -> Dict[str, Any]:
    |      ^^^^
112 |     """
113 |     Reconcile Snowflake table with S3 Delta table.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/legacy/jobs/reconciliation_job.py:114:1
    |
112 |     """
113 |     Reconcile Snowflake table with S3 Delta table.
114 |     
    | ^^^^
115 |     Args:
116 |         spark: SparkSession
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/jobs/reconciliation_job.py:121:1
    |
119 |         config: Configuration dictionary
120 |         key_columns: Key columns for reconciliation
121 |         
    | ^^^^^^^^
122 |     Returns:
123 |         Reconciliation results
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/reconciliation_job.py:126:1
    |
124 |     """
125 |     logger.info(f"Reconciling Snowflake {snowflake_table} â†” S3 {s3_path}")
126 |     
    | ^^^^
127 |     # Read from Snowflake
128 |     from project_a.utils.secrets import get_snowflake_credentials
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/reconciliation_job.py:129:1
    |
127 |     # Read from Snowflake
128 |     from project_a.utils.secrets import get_snowflake_credentials
129 |     
    | ^^^^
130 |     sf_creds = get_snowflake_credentials(config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/reconciliation_job.py:131:1
    |
130 |     sf_creds = get_snowflake_credentials(config)
131 |     
    | ^^^^
132 |     source_df = spark.read \
133 |         .format("snowflake") \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/reconciliation_job.py:144:1
    |
142 |         }) \
143 |         .load()
144 |     
    | ^^^^
145 |     # Read from S3
146 |     target_df = spark.read.format("delta").load(s3_path)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/reconciliation_job.py:147:1
    |
145 |     # Read from S3
146 |     target_df = spark.read.format("delta").load(s3_path)
147 |     
    | ^^^^
148 |     return reconcile_tables(
149 |         source_df,
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/jobs/reconciliation_job.py:161:13
    |
159 |     redshift_table: str,
160 |     s3_path: str,
161 |     config: Dict[str, Any],
    |             ^^^^
162 |     key_columns: List[str]
163 | ) -> Dict[str, Any]:
    |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/legacy/jobs/reconciliation_job.py:162:18
    |
160 |     s3_path: str,
161 |     config: Dict[str, Any],
162 |     key_columns: List[str]
    |                  ^^^^
163 | ) -> Dict[str, Any]:
164 |     """
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/jobs/reconciliation_job.py:163:6
    |
161 |     config: Dict[str, Any],
162 |     key_columns: List[str]
163 | ) -> Dict[str, Any]:
    |      ^^^^
164 |     """
165 |     Reconcile Redshift table with S3 Delta table.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/legacy/jobs/reconciliation_job.py:166:1
    |
164 |     """
165 |     Reconcile Redshift table with S3 Delta table.
166 |     
    | ^^^^
167 |     Args:
168 |         spark: SparkSession
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/jobs/reconciliation_job.py:173:1
    |
171 |         config: Configuration dictionary
172 |         key_columns: Key columns
173 |         
    | ^^^^^^^^
174 |     Returns:
175 |         Reconciliation results
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/reconciliation_job.py:178:1
    |
176 |     """
177 |     logger.info(f"Reconciling Redshift {redshift_table} â†” S3 {s3_path}")
178 |     
    | ^^^^
179 |     # Read from Redshift
180 |     from project_a.utils.secrets import get_redshift_credentials
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/reconciliation_job.py:181:1
    |
179 |     # Read from Redshift
180 |     from project_a.utils.secrets import get_redshift_credentials
181 |     
    | ^^^^
182 |     rs_creds = get_redshift_credentials(config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/reconciliation_job.py:183:1
    |
182 |     rs_creds = get_redshift_credentials(config)
183 |     
    | ^^^^
184 |     source_df = spark.read \
185 |         .format("jdbc") \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/reconciliation_job.py:191:1
    |
189 |         .option("password", rs_creds.get('password')) \
190 |         .load()
191 |     
    | ^^^^
192 |     # Read from S3
193 |     target_df = spark.read.format("delta").load(s3_path)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/reconciliation_job.py:194:1
    |
192 |     # Read from S3
193 |     target_df = spark.read.format("delta").load(s3_path)
194 |     
    | ^^^^
195 |     return reconcile_tables(
196 |         source_df,
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
   --> src/project_a/legacy/jobs/reconciliation_job.py:205:5
    |
204 |   if __name__ == "__main__":
205 | /     import sys
206 | |     import argparse
207 | |     from pathlib import Path
    | |____________________________^
208 |       sys.path.insert(0, str(Path(__file__).parent.parent.parent))
    |
help: Organize imports

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/reconciliation_job.py:209:1
    |
207 |     from pathlib import Path
208 |     sys.path.insert(0, str(Path(__file__).parent.parent.parent))
209 |     
    | ^^^^
210 |     from project_a.utils.spark_session import build_spark
211 |     from project_a.utils.config import load_conf
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
   --> src/project_a/legacy/jobs/reconciliation_job.py:210:5
    |
208 |       sys.path.insert(0, str(Path(__file__).parent.parent.parent))
209 |       
210 | /     from project_a.utils.spark_session import build_spark
211 | |     from project_a.utils.config import load_conf
    | |________________________________________________^
212 |       
213 |       parser = argparse.ArgumentParser(description="Data reconciliation job")
    |
help: Organize imports

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/reconciliation_job.py:212:1
    |
210 |     from project_a.utils.spark_session import build_spark
211 |     from project_a.utils.config import load_conf
212 |     
    | ^^^^
213 |     parser = argparse.ArgumentParser(description="Data reconciliation job")
214 |     parser.add_argument("--source", choices=["snowflake", "redshift"], required=True, help="Source system")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/reconciliation_job.py:219:1
    |
217 |     parser.add_argument("--key-columns", nargs="+", required=True, help="Key columns for reconciliation")
218 |     parser.add_argument("--config", default="config/prod.yaml", help="Config file")
219 |     
    | ^^^^
220 |     args = parser.parse_args()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/reconciliation_job.py:221:1
    |
220 |     args = parser.parse_args()
221 |     
    | ^^^^
222 |     config = load_conf(args.config)
223 |     spark = build_spark(app_name="reconciliation", config=config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/reconciliation_job.py:224:1
    |
222 |     config = load_conf(args.config)
223 |     spark = build_spark(app_name="reconciliation", config=config)
224 |     
    | ^^^^
225 |     try:
226 |         if args.source == "snowflake":
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/reconciliation_job.py:245:1
    |
243 |             logger.error(f"Unknown source: {args.source}")
244 |             sys.exit(1)
245 |         
    | ^^^^^^^^
246 |         if not result["reconciled"]:
247 |             logger.error("âŒ Reconciliation FAILED")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/reconciliation_job.py:249:1
    |
247 |             logger.error("âŒ Reconciliation FAILED")
248 |             sys.exit(1)
249 |         
    | ^^^^^^^^
250 |         logger.info("âœ… Reconciliation PASSED")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/reconciliation_job.py:251:1
    |
250 |         logger.info("âœ… Reconciliation PASSED")
251 |         
    | ^^^^^^^^
252 |     finally:
253 |         spark.stop()
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:7:1
   |
 5 |   """
 6 |
 7 | / import argparse
 8 | | import logging
 9 | | import os
10 | | import sys
11 | | from typing import Dict, Any
12 | | from pathlib import Path
13 | |
14 | | from pyspark.sql import SparkSession
15 | | from pyspark.sql.functions import col, current_timestamp, lit, when, to_timestamp, regexp_replace
   | |_________________________________________________________________________________________________^
16 |
17 |   # Add src to path
   |
help: Organize imports

F401 [*] `os` imported but unused
  --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:9:8
   |
 7 | import argparse
 8 | import logging
 9 | import os
   |        ^^
10 | import sys
11 | from typing import Dict, Any
   |
help: Remove unused import: `os`

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:11:1
   |
 9 | import os
10 | import sys
11 | from typing import Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
12 | from pathlib import Path
   |

F401 [*] `typing.Dict` imported but unused
  --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:11:20
   |
 9 | import os
10 | import sys
11 | from typing import Dict, Any
   |                    ^^^^
12 | from pathlib import Path
   |
help: Remove unused import

F401 [*] `typing.Any` imported but unused
  --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:11:26
   |
 9 | import os
10 | import sys
11 | from typing import Dict, Any
   |                          ^^^
12 | from pathlib import Path
   |
help: Remove unused import

F401 [*] `pyspark.sql.functions.current_timestamp` imported but unused
  --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:15:40
   |
14 | from pyspark.sql import SparkSession
15 | from pyspark.sql.functions import col, current_timestamp, lit, when, to_timestamp, regexp_replace
   |                                        ^^^^^^^^^^^^^^^^^
16 |
17 | # Add src to path
   |
help: Remove unused import

F401 [*] `pyspark.sql.functions.lit` imported but unused
  --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:15:59
   |
14 | from pyspark.sql import SparkSession
15 | from pyspark.sql.functions import col, current_timestamp, lit, when, to_timestamp, regexp_replace
   |                                                           ^^^
16 |
17 | # Add src to path
   |
help: Remove unused import

F401 [*] `pyspark.sql.functions.when` imported but unused
  --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:15:64
   |
14 | from pyspark.sql import SparkSession
15 | from pyspark.sql.functions import col, current_timestamp, lit, when, to_timestamp, regexp_replace
   |                                                                ^^^^
16 |
17 | # Add src to path
   |
help: Remove unused import

F401 [*] `pyspark.sql.functions.regexp_replace` imported but unused
  --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:15:84
   |
14 | from pyspark.sql import SparkSession
15 | from pyspark.sql.functions import col, current_timestamp, lit, when, to_timestamp, regexp_replace
   |                                                                                    ^^^^^^^^^^^^^^
16 |
17 | # Add src to path
   |
help: Remove unused import

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:20:1
   |
18 |   sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))
19 |
20 | / from project_a.utils.spark_session import build_spark
21 | | from project_a.utils.config import load_conf
22 | | from project_a.utils.logging import setup_json_logging
   | |______________________________________________________^
23 |
24 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

F401 [*] `project_a.utils.spark_session.build_spark` imported but unused
  --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:20:43
   |
18 | sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))
19 |
20 | from project_a.utils.spark_session import build_spark
   |                                           ^^^^^^^^^^^
21 | from project_a.utils.config import load_conf
22 | from project_a.utils.logging import setup_json_logging
   |
help: Remove unused import: `project_a.utils.spark_session.build_spark`

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:30:1
   |
28 |     """
29 |     Transform Salesforce accounts from bronze to silver.
30 |     
   | ^^^^
31 |     Args:
32 |         spark: Spark session
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:34:1
   |
32 |         spark: Spark session
33 |         accounts_bronze: Bronze accounts DataFrame
34 |         
   | ^^^^^^^^
35 |     Returns:
36 |         Silver accounts DataFrame with schema: id, name, type, phone, billing_city, billing_state, billing_country, created_at, systemâ€¦
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:39:1
   |
37 |     """
38 |     logger.info("Transforming Salesforce accounts to silver layer")
39 |     
   | ^^^^
40 |     silver_accounts = accounts_bronze.select(
41 |         col("Id").alias("id"),
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:53:1
   |
51 |         col("id").isNotNull() & col("name").isNotNull()
52 |     )
53 |     
   | ^^^^
54 |     logger.info(f"Silver accounts created. Rows: {silver_accounts.count()}")
55 |     return silver_accounts
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:61:1
   |
59 |     """
60 |     Transform Salesforce leads from bronze to silver.
61 |     
   | ^^^^
62 |     Args:
63 |         spark: Spark session
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:65:1
   |
63 |         spark: Spark session
64 |         leads_bronze: Bronze leads DataFrame
65 |         
   | ^^^^^^^^
66 |     Returns:
67 |         Silver leads DataFrame with schema: id, first_name, last_name, company, status, email, phone, created_at, system_modstamp
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:70:1
   |
68 |     """
69 |     logger.info("Transforming Salesforce leads to silver layer")
70 |     
   | ^^^^
71 |     silver_leads = leads_bronze.select(
72 |         col("Id").alias("id"),
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:84:1
   |
82 |         col("id").isNotNull()
83 |     )
84 |     
   | ^^^^
85 |     logger.info(f"Silver leads created. Rows: {silver_leads.count()}")
86 |     return silver_leads
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:94:1
   |
92 |     parser.add_argument("--config", required=True, help="Configuration file path")
93 |     parser.add_argument("--lake-root", required=True, help="Data lake root path")
94 |     
   | ^^^^
95 |     args = parser.parse_args()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:96:1
   |
95 |     args = parser.parse_args()
96 |     
   | ^^^^
97 |     # Setup logging
98 |     setup_json_logging()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:100:1
    |
 98 |     setup_json_logging()
 99 |     logger.info("Starting Salesforce bronze to silver transformation")
100 |     
    | ^^^^
101 |     # Load configuration
102 |     config = load_conf(args.config)
    |
help: Remove whitespace from blank line

F841 Local variable `config` is assigned to but never used
   --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:102:5
    |
101 |     # Load configuration
102 |     config = load_conf(args.config)
    |     ^^^^^^
103 |     lake_root = args.lake_root
    |
help: Remove assignment to unused variable `config`

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:104:1
    |
102 |     config = load_conf(args.config)
103 |     lake_root = args.lake_root
104 |     
    | ^^^^
105 |     # Create Spark session with Delta config
106 |     spark = get_spark_session(
    |
help: Remove whitespace from blank line

F821 Undefined name `get_spark_session`
   --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:106:13
    |
105 |     # Create Spark session with Delta config
106 |     spark = get_spark_session(
    |             ^^^^^^^^^^^^^^^^^
107 |         app_name="salesforce-bronze-to-silver",
108 |         config=get_delta_config()
    |

F821 Undefined name `get_delta_config`
   --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:108:16
    |
106 |     spark = get_spark_session(
107 |         app_name="salesforce-bronze-to-silver",
108 |         config=get_delta_config()
    |                ^^^^^^^^^^^^^^^^
109 |     )
    |

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:110:1
    |
108 |         config=get_delta_config()
109 |     )
110 |     
    | ^^^^
111 |     try:
112 |         # Read Bronze layer data
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:114:1
    |
112 |         # Read Bronze layer data
113 |         logger.info("Reading Bronze layer data")
114 |         
    | ^^^^^^^^
115 |         # Read Salesforce accounts from bronze
116 |         accounts_bronze = spark.read.format("delta").load(f"{lake_root}/bronze/salesforce_accounts")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:117:1
    |
115 |         # Read Salesforce accounts from bronze
116 |         accounts_bronze = spark.read.format("delta").load(f"{lake_root}/bronze/salesforce_accounts")
117 |         
    | ^^^^^^^^
118 |         # Read Salesforce leads from bronze
119 |         leads_bronze = spark.read.format("delta").load(f"{lake_root}/bronze/salesforce_leads")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:120:1
    |
118 |         # Read Salesforce leads from bronze
119 |         leads_bronze = spark.read.format("delta").load(f"{lake_root}/bronze/salesforce_leads")
120 |         
    | ^^^^^^^^
121 |         # Transform to Silver layer
122 |         logger.info("Transforming to Silver layer")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:123:1
    |
121 |         # Transform to Silver layer
122 |         logger.info("Transforming to Silver layer")
123 |         
    | ^^^^^^^^
124 |         # Transform accounts
125 |         silver_accounts = transform_accounts_to_silver(spark, accounts_bronze)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:126:1
    |
124 |         # Transform accounts
125 |         silver_accounts = transform_accounts_to_silver(spark, accounts_bronze)
126 |         
    | ^^^^^^^^
127 |         # Transform leads
128 |         silver_leads = transform_leads_to_silver(spark, leads_bronze)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:129:1
    |
127 |         # Transform leads
128 |         silver_leads = transform_leads_to_silver(spark, leads_bronze)
129 |         
    | ^^^^^^^^
130 |         # Write Silver layer tables
131 |         logger.info("Writing Silver layer tables")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:132:1
    |
130 |         # Write Silver layer tables
131 |         logger.info("Writing Silver layer tables")
132 |         
    | ^^^^^^^^
133 |         # Write accounts
134 |         silver_accounts.write \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:139:1
    |
137 |             .option("mergeSchema", "true") \
138 |             .save(f"{lake_root}/silver/salesforce_accounts")
139 |         
    | ^^^^^^^^
140 |         # Write leads
141 |         silver_leads.write \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:146:1
    |
144 |             .option("mergeSchema", "true") \
145 |             .save(f"{lake_root}/silver/salesforce_leads")
146 |         
    | ^^^^^^^^
147 |         logger.info("Salesforce bronze to silver transformation completed successfully")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:148:1
    |
147 |         logger.info("Salesforce bronze to silver transformation completed successfully")
148 |         
    | ^^^^^^^^
149 |         # Show sample data
150 |         logger.info("Sample Silver layer data:")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_bronze_to_silver.py:153:1
    |
151 |         silver_accounts.show(5, truncate=False)
152 |         silver_leads.show(5, truncate=False)
153 |         
    | ^^^^^^^^
154 |     except Exception as e:
155 |         logger.error(f"Salesforce bronze to silver transformation failed: {e}")
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/jobs/salesforce_to_bronze.py:7:1
   |
 5 |   """
 6 |
 7 | / import os
 8 | | import sys
 9 | | import json
10 | | import logging
11 | | from typing import Dict, Any, Optional
12 | | from pathlib import Path
13 | | from datetime import datetime, timedelta
   | |________________________________________^
14 |
15 |   # Add src to path
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/legacy/jobs/salesforce_to_bronze.py:11:1
   |
 9 | import json
10 | import logging
11 | from typing import Dict, Any, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
12 | from pathlib import Path
13 | from datetime import datetime, timedelta
   |

F401 [*] `datetime.datetime` imported but unused
  --> src/project_a/legacy/jobs/salesforce_to_bronze.py:13:22
   |
11 | from typing import Dict, Any, Optional
12 | from pathlib import Path
13 | from datetime import datetime, timedelta
   |                      ^^^^^^^^
14 |
15 | # Add src to path
   |
help: Remove unused import

F401 [*] `datetime.timedelta` imported but unused
  --> src/project_a/legacy/jobs/salesforce_to_bronze.py:13:32
   |
11 | from typing import Dict, Any, Optional
12 | from pathlib import Path
13 | from datetime import datetime, timedelta
   |                                ^^^^^^^^^
14 |
15 | # Add src to path
   |
help: Remove unused import

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/jobs/salesforce_to_bronze.py:18:1
   |
16 |   sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))
17 |
18 | / from pyspark.sql import SparkSession
19 | | from pyspark.sql.functions import col, lit, current_timestamp, to_date, regexp_replace
20 | | from pyspark.sql.types import StructType, StructField, StringType, TimestampType
21 | |
22 | | from project_a.utils.spark_session import build_spark
23 | | from project_a.utils.config import load_conf
24 | | from project_a.utils.logging import setup_json_logging
   | |______________________________________________________^
25 |
26 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

F401 [*] `pyspark.sql.functions.col` imported but unused
  --> src/project_a/legacy/jobs/salesforce_to_bronze.py:19:35
   |
18 | from pyspark.sql import SparkSession
19 | from pyspark.sql.functions import col, lit, current_timestamp, to_date, regexp_replace
   |                                   ^^^
20 | from pyspark.sql.types import StructType, StructField, StringType, TimestampType
   |
help: Remove unused import

F401 [*] `pyspark.sql.functions.regexp_replace` imported but unused
  --> src/project_a/legacy/jobs/salesforce_to_bronze.py:19:73
   |
18 | from pyspark.sql import SparkSession
19 | from pyspark.sql.functions import col, lit, current_timestamp, to_date, regexp_replace
   |                                                                         ^^^^^^^^^^^^^^
20 | from pyspark.sql.types import StructType, StructField, StringType, TimestampType
   |
help: Remove unused import

F401 [*] `pyspark.sql.types.StructType` imported but unused
  --> src/project_a/legacy/jobs/salesforce_to_bronze.py:20:31
   |
18 | from pyspark.sql import SparkSession
19 | from pyspark.sql.functions import col, lit, current_timestamp, to_date, regexp_replace
20 | from pyspark.sql.types import StructType, StructField, StringType, TimestampType
   |                               ^^^^^^^^^^
21 |
22 | from project_a.utils.spark_session import build_spark
   |
help: Remove unused import

F401 [*] `pyspark.sql.types.StructField` imported but unused
  --> src/project_a/legacy/jobs/salesforce_to_bronze.py:20:43
   |
18 | from pyspark.sql import SparkSession
19 | from pyspark.sql.functions import col, lit, current_timestamp, to_date, regexp_replace
20 | from pyspark.sql.types import StructType, StructField, StringType, TimestampType
   |                                           ^^^^^^^^^^^
21 |
22 | from project_a.utils.spark_session import build_spark
   |
help: Remove unused import

F401 [*] `pyspark.sql.types.StringType` imported but unused
  --> src/project_a/legacy/jobs/salesforce_to_bronze.py:20:56
   |
18 | from pyspark.sql import SparkSession
19 | from pyspark.sql.functions import col, lit, current_timestamp, to_date, regexp_replace
20 | from pyspark.sql.types import StructType, StructField, StringType, TimestampType
   |                                                        ^^^^^^^^^^
21 |
22 | from project_a.utils.spark_session import build_spark
   |
help: Remove unused import

F401 [*] `pyspark.sql.types.TimestampType` imported but unused
  --> src/project_a/legacy/jobs/salesforce_to_bronze.py:20:68
   |
18 | from pyspark.sql import SparkSession
19 | from pyspark.sql.functions import col, lit, current_timestamp, to_date, regexp_replace
20 | from pyspark.sql.types import StructType, StructField, StringType, TimestampType
   |                                                                    ^^^^^^^^^^^^^
21 |
22 | from project_a.utils.spark_session import build_spark
   |
help: Remove unused import

F401 [*] `project_a.utils.spark_session.build_spark` imported but unused
  --> src/project_a/legacy/jobs/salesforce_to_bronze.py:22:43
   |
20 | from pyspark.sql.types import StructType, StructField, StringType, TimestampType
21 |
22 | from project_a.utils.spark_session import build_spark
   |                                           ^^^^^^^^^^^
23 | from project_a.utils.config import load_conf
24 | from project_a.utils.logging import setup_json_logging
   |
help: Remove unused import: `project_a.utils.spark_session.build_spark`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/jobs/salesforce_to_bronze.py:29:37
   |
29 | def get_salesforce_credentials() -> Dict[str, str]:
   |                                     ^^^^
30 |     """
31 |     Get Salesforce credentials from environment or AWS Secrets Manager.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/salesforce_to_bronze.py:32:1
   |
30 |     """
31 |     Get Salesforce credentials from environment or AWS Secrets Manager.
32 |     
   | ^^^^
33 |     Returns:
34 |         Dictionary with Salesforce credentials
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/salesforce_to_bronze.py:52:1
   |
50 |         except Exception as e:
51 |             logger.warning(f"Failed to get credentials from Secrets Manager: {e}")
52 |     
   | ^^^^
53 |     # Fall back to environment variables
54 |     return {
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/jobs/salesforce_to_bronze.py:62:45
   |
62 | def get_salesforce_connection_string(creds: Dict[str, str]) -> str:
   |                                             ^^^^
63 |     """
64 |     Build Salesforce JDBC connection string.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/salesforce_to_bronze.py:65:1
   |
63 |     """
64 |     Build Salesforce JDBC connection string.
65 |     
   | ^^^^
66 |     Args:
67 |         creds: Salesforce credentials dictionary
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/salesforce_to_bronze.py:68:1
   |
66 |     Args:
67 |         creds: Salesforce credentials dictionary
68 |         
   | ^^^^^^^^
69 |     Returns:
70 |         JDBC connection string
   |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/jobs/salesforce_to_bronze.py:80:82
   |
80 | def get_last_checkpoint(spark: SparkSession, table_name: str, lake_root: str) -> Optional[str]:
   |                                                                                  ^^^^^^^^^^^^^
81 |     """
82 |     Get the last checkpoint timestamp for incremental loading.
   |
help: Convert to `X | None`

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/salesforce_to_bronze.py:83:1
   |
81 |     """
82 |     Get the last checkpoint timestamp for incremental loading.
83 |     
   | ^^^^
84 |     Args:
85 |         spark: Spark session
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/salesforce_to_bronze.py:88:1
   |
86 |         table_name: Table name
87 |         lake_root: S3 root path of the data lake
88 |         
   | ^^^^^^^^
89 |     Returns:
90 |         Last checkpoint timestamp or None
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/salesforce_to_bronze.py:93:1
   |
91 |     """
92 |     checkpoint_path = f"{lake_root}/_checkpoints/salesforce/{table_name}"
93 |     
   | ^^^^
94 |     try:
95 |         # Try to read the last checkpoint
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:108:1
    |
106 |     """
107 |     Save the checkpoint timestamp for incremental loading.
108 |     
    | ^^^^
109 |     Args:
110 |         spark: Spark session
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:116:1
    |
114 |     """
115 |     checkpoint_path = f"{lake_root}/_checkpoints/salesforce/{table_name}"
116 |     
    | ^^^^
117 |     # Create checkpoint DataFrame
118 |     checkpoint_df = spark.createDataFrame(
    |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:119:30
    |
117 |     # Create checkpoint DataFrame
118 |     checkpoint_df = spark.createDataFrame(
119 |         [(checkpoint_time,)], 
    |                              ^
120 |         ["last_checkpoint"]
121 |     )
    |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:122:1
    |
120 |         ["last_checkpoint"]
121 |     )
122 |     
    | ^^^^
123 |     # Write checkpoint
124 |     checkpoint_df.write \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:128:1
    |
126 |         .mode("overwrite") \
127 |         .save(checkpoint_path)
128 |     
    | ^^^^
129 |     logger.info(f"Saved checkpoint for {table_name}: {checkpoint_time}")
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:140:1
    |
138 |     """
139 |     Extract data from Salesforce using JDBC.
140 |     
    | ^^^^
141 |     Args:
142 |         spark: Spark session
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:146:1
    |
144 |         soql_query: SOQL query to execute
145 |         connection_string: JDBC connection string
146 |         
    | ^^^^^^^^
147 |     Returns:
148 |         Spark DataFrame with Salesforce data
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:151:1
    |
149 |     """
150 |     logger.info(f"Extracting data from Salesforce {table_name}")
151 |     
    | ^^^^
152 |     try:
153 |         df = spark.read \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:159:1
    |
157 |             .option("driver", "com.salesforce.jdbc.SalesforceDriver") \
158 |             .load()
159 |         
    | ^^^^^^^^
160 |         logger.info(f"Successfully extracted {df.count()} records from {table_name}")
161 |         return df
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:162:1
    |
160 |         logger.info(f"Successfully extracted {df.count()} records from {table_name}")
161 |         return df
162 |         
    | ^^^^^^^^
163 |     except Exception as e:
164 |         logger.error(f"Failed to extract data from Salesforce {table_name}: {e}")
    |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:168:36
    |
168 | def get_lead_soql(last_checkpoint: Optional[str]) -> str:
    |                                    ^^^^^^^^^^^^^
169 |     """
170 |     Get SOQL query for Lead table with incremental logic.
    |
help: Convert to `X | None`

W293 Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:171:1
    |
169 |     """
170 |     Get SOQL query for Lead table with incremental logic.
171 |     
    | ^^^^
172 |     Args:
173 |         last_checkpoint: Last checkpoint timestamp
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:174:1
    |
172 |     Args:
173 |         last_checkpoint: Last checkpoint timestamp
174 |         
    | ^^^^^^^^
175 |     Returns:
176 |         SOQL query string
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:182:1
    |
180 |     else:
181 |         where_clause = "WHERE SystemModstamp >= LAST_N_DAYS:1"
182 |     
    | ^^^^
183 |     return f"""
184 |     SELECT 
    |
help: Remove whitespace from blank line

W291 Trailing whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:184:11
    |
183 |     return f"""
184 |     SELECT 
    |           ^
185 |         Id,
186 |         FirstName,
    |
help: Remove trailing whitespace

W291 Trailing whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:196:14
    |
194 |         LastModifiedDate,
195 |         SystemModstamp
196 |     FROM Lead 
    |              ^
197 |     {where_clause}
198 |     ORDER BY SystemModstamp
    |
help: Remove trailing whitespace

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:202:39
    |
202 | def get_account_soql(last_checkpoint: Optional[str]) -> str:
    |                                       ^^^^^^^^^^^^^
203 |     """
204 |     Get SOQL query for Account table with incremental logic.
    |
help: Convert to `X | None`

W293 Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:205:1
    |
203 |     """
204 |     Get SOQL query for Account table with incremental logic.
205 |     
    | ^^^^
206 |     Args:
207 |         last_checkpoint: Last checkpoint timestamp
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:208:1
    |
206 |     Args:
207 |         last_checkpoint: Last checkpoint timestamp
208 |         
    | ^^^^^^^^
209 |     Returns:
210 |         SOQL query string
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:216:1
    |
214 |     else:
215 |         where_clause = "WHERE SystemModstamp >= LAST_N_DAYS:1"
216 |     
    | ^^^^
217 |     return f"""
218 |     SELECT 
    |
help: Remove whitespace from blank line

W291 Trailing whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:218:11
    |
217 |     return f"""
218 |     SELECT 
    |           ^
219 |         Id,
220 |         Name,
    |
help: Remove trailing whitespace

W291 Trailing whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:233:17
    |
231 |         LastModifiedDate,
232 |         SystemModstamp
233 |     FROM Account 
    |                 ^
234 |     {where_clause}
235 |     ORDER BY SystemModstamp
    |
help: Remove trailing whitespace

W293 Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:242:1
    |
240 |     """
241 |     Add metadata columns to the DataFrame.
242 |     
    | ^^^^
243 |     Args:
244 |         df: Input DataFrame
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:247:1
    |
245 |         source: Source system name
246 |         table_name: Table name
247 |         
    | ^^^^^^^^
248 |     Returns:
249 |         DataFrame with metadata columns
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:260:1
    |
258 |     """
259 |     Write DataFrame to bronze layer with partitioning by date.
260 |     
    | ^^^^
261 |     Args:
262 |         df: Input DataFrame
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:267:1
    |
265 |     """
266 |     output_path = f"{lake_root}/bronze/salesforce/{table_name}"
267 |     
    | ^^^^
268 |     logger.info(f"Writing {table_name} to bronze layer: {output_path}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:269:1
    |
268 |     logger.info(f"Writing {table_name} to bronze layer: {output_path}")
269 |     
    | ^^^^
270 |     df.write \
271 |         .format("delta") \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:276:1
    |
274 |         .partitionBy("_proc_date") \
275 |         .save(output_path)
276 |     
    | ^^^^
277 |     logger.info(f"Successfully wrote {table_name} to bronze layer")
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:280:65
    |
280 | def process_salesforce_incremental(spark: SparkSession, config: Dict[str, Any]) -> None:
    |                                                                 ^^^^
281 |     """
282 |     Process Salesforce incremental data extraction.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:283:1
    |
281 |     """
282 |     Process Salesforce incremental data extraction.
283 |     
    | ^^^^
284 |     Args:
285 |         spark: Spark session
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:289:1
    |
287 |     """
288 |     logger.info("Starting Salesforce incremental data extraction")
289 |     
    | ^^^^
290 |     # Get credentials
291 |     creds = get_salesforce_credentials()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:293:1
    |
291 |     creds = get_salesforce_credentials()
292 |     connection_string = get_salesforce_connection_string(creds)
293 |     
    | ^^^^
294 |     # Get data lake configuration
295 |     lake_root = config["lake"]["root"]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:296:1
    |
294 |     # Get data lake configuration
295 |     lake_root = config["lake"]["root"]
296 |     
    | ^^^^
297 |     # Process Lead table
298 |     try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:304:1
    |
302 |         lead_df_with_meta = add_metadata_columns(lead_df, "salesforce", "lead")
303 |         write_to_bronze(lead_df_with_meta, "lead", lake_root)
304 |         
    | ^^^^^^^^
305 |         # Save checkpoint
306 |         if lead_df.count() > 0:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:309:1
    |
307 |             max_timestamp = lead_df.select("SystemModstamp").rdd.max()[0]
308 |             save_checkpoint(spark, "lead", lake_root, max_timestamp)
309 |             
    | ^^^^^^^^^^^^
310 |     except Exception as e:
311 |         logger.error(f"Failed to process Lead table: {e}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:313:1
    |
311 |         logger.error(f"Failed to process Lead table: {e}")
312 |         raise
313 |     
    | ^^^^
314 |     # Process Account table
315 |     try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:321:1
    |
319 |         account_df_with_meta = add_metadata_columns(account_df, "salesforce", "account")
320 |         write_to_bronze(account_df_with_meta, "account", lake_root)
321 |         
    | ^^^^^^^^
322 |         # Save checkpoint
323 |         if account_df.count() > 0:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:326:1
    |
324 |             max_timestamp = account_df.select("SystemModstamp").rdd.max()[0]
325 |             save_checkpoint(spark, "account", lake_root, max_timestamp)
326 |             
    | ^^^^^^^^^^^^
327 |     except Exception as e:
328 |         logger.error(f"Failed to process Account table: {e}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:330:1
    |
328 |         logger.error(f"Failed to process Account table: {e}")
329 |         raise
330 |     
    | ^^^^
331 |     logger.info("Salesforce incremental data extraction completed successfully")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:337:1
    |
335 |     """Main function to run the Salesforce incremental job."""
336 |     import argparse
337 |     
    | ^^^^
338 |     parser = argparse.ArgumentParser(description="Extract incremental data from Salesforce")
339 |     parser.add_argument("--config", required=True, help="Configuration file path")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:341:1
    |
339 |     parser.add_argument("--config", required=True, help="Configuration file path")
340 |     args = parser.parse_args()
341 |     
    | ^^^^
342 |     # Setup logging
343 |     setup_json_logging()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:346:1
    |
344 |     log_level = os.getenv("LOG_LEVEL", "INFO")
345 |     logging.getLogger().setLevel(getattr(logging, log_level.upper()))
346 |     
    | ^^^^
347 |     try:
348 |         # Load configuration
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:350:1
    |
348 |         # Load configuration
349 |         config = load_conf(args.config)
350 |         
    | ^^^^^^^^
351 |         # Create Spark session with Delta support
352 |         spark = get_spark_session(
    |
help: Remove whitespace from blank line

F821 Undefined name `get_spark_session`
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:352:17
    |
351 |         # Create Spark session with Delta support
352 |         spark = get_spark_session(
    |                 ^^^^^^^^^^^^^^^^^
353 |             "SalesforceToBronze",
354 |             extra_conf={
    |

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:360:1
    |
358 |             }
359 |         )
360 |         
    | ^^^^^^^^
361 |         # Process Salesforce data
362 |         process_salesforce_incremental(spark, config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:363:1
    |
361 |         # Process Salesforce data
362 |         process_salesforce_incremental(spark, config)
363 |         
    | ^^^^^^^^
364 |     except Exception as e:
365 |         logger.error(f"Salesforce extraction failed: {e}")
    |
help: Remove whitespace from blank line

W292 [*] No newline at end of file
   --> src/project_a/legacy/jobs/salesforce_to_bronze.py:370:11
    |
369 | if __name__ == "__main__":
370 |     main()
    |           ^
    |
help: Add trailing newline

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:7:1
   |
 5 |   """
 6 |
 7 | / import os
 8 | | import sys
 9 | | import logging
10 | | from typing import Dict, Any
11 | | from pathlib import Path
   | |________________________^
12 |
13 |   # Add src to path
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:10:1
   |
 8 | import sys
 9 | import logging
10 | from typing import Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
11 | from pathlib import Path
   |

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:16:1
   |
14 |   sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))
15 |
16 | / from pyspark.sql import SparkSession
17 | | from pyspark.sql.functions import col, lit, current_timestamp, when, coalesce
18 | | from delta.tables import DeltaTable
19 | |
20 | | from project_a.utils.spark_session import build_spark
21 | | from project_a.utils.config import load_conf
22 | | from project_a.utils.logging import setup_json_logging
   | |______________________________________________________^
23 |
24 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

F401 [*] `pyspark.sql.functions.lit` imported but unused
  --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:17:40
   |
16 | from pyspark.sql import SparkSession
17 | from pyspark.sql.functions import col, lit, current_timestamp, when, coalesce
   |                                        ^^^
18 | from delta.tables import DeltaTable
   |
help: Remove unused import

F401 [*] `pyspark.sql.functions.when` imported but unused
  --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:17:64
   |
16 | from pyspark.sql import SparkSession
17 | from pyspark.sql.functions import col, lit, current_timestamp, when, coalesce
   |                                                                ^^^^
18 | from delta.tables import DeltaTable
   |
help: Remove unused import

F401 [*] `pyspark.sql.functions.coalesce` imported but unused
  --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:17:70
   |
16 | from pyspark.sql import SparkSession
17 | from pyspark.sql.functions import col, lit, current_timestamp, when, coalesce
   |                                                                      ^^^^^^^^
18 | from delta.tables import DeltaTable
   |
help: Remove unused import

F401 [*] `project_a.utils.spark_session.build_spark` imported but unused
  --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:20:43
   |
18 | from delta.tables import DeltaTable
19 |
20 | from project_a.utils.spark_session import build_spark
   |                                           ^^^^^^^^^^^
21 | from project_a.utils.config import load_conf
22 | from project_a.utils.logging import setup_json_logging
   |
help: Remove unused import: `project_a.utils.spark_session.build_spark`

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:30:1
   |
28 |     """
29 |     Merge Snowflake orders data into silver layer.
30 |     
   | ^^^^
31 |     Args:
32 |         spark: Spark session
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:36:1
   |
34 |     """
35 |     logger.info("Merging Snowflake orders data into silver layer")
36 |     
   | ^^^^
37 |     # Read bronze data
38 |     bronze_orders_df = spark.read \
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:41:1
   |
39 |         .format("delta") \
40 |         .load(f"{lake_root}/bronze/snowflake/orders")
41 |     
   | ^^^^
42 |     # Read existing silver data (if exists)
43 |     silver_orders_path = f"{lake_root}/silver/orders"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:44:1
   |
42 |     # Read existing silver data (if exists)
43 |     silver_orders_path = f"{lake_root}/silver/orders"
44 |     
   | ^^^^
45 |     try:
46 |         # Check if silver table exists
   |
help: Remove whitespace from blank line

F841 Local variable `silver_orders_df` is assigned to but never used
  --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:47:9
   |
45 |     try:
46 |         # Check if silver table exists
47 |         silver_orders_df = spark.read.format("delta").load(silver_orders_path)
   |         ^^^^^^^^^^^^^^^^
48 |         silver_table_exists = True
49 |     except Exception:
   |
help: Remove assignment to unused variable `silver_orders_df`

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:52:1
   |
50 |         logger.info("Silver orders table does not exist, creating new one")
51 |         silver_table_exists = False
52 |     
   | ^^^^
53 |     if not silver_table_exists:
54 |         # Create new silver table
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:64:1
   |
62 |         # Merge data using Delta MERGE
63 |         silver_table = DeltaTable.forPath(spark, silver_orders_path)
64 |         
   | ^^^^^^^^
65 |         # Prepare bronze data for merge
66 |         bronze_orders_for_merge = bronze_orders_df.select(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:81:1
   |
79 |             current_timestamp().alias("_updated_at")
80 |         )
81 |         
   | ^^^^^^^^
82 |         # Perform MERGE operation
83 |         silver_table.alias("silver") \
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:117:1
    |
115 |             }) \
116 |             .execute()
117 |         
    | ^^^^^^^^
118 |         logger.info("Successfully merged orders data into silver layer")
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:124:1
    |
122 |     """
123 |     Merge Snowflake customers data into silver layer.
124 |     
    | ^^^^
125 |     Args:
126 |         spark: Spark session
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:130:1
    |
128 |     """
129 |     logger.info("Merging Snowflake customers data into silver layer")
130 |     
    | ^^^^
131 |     # Read bronze data
132 |     bronze_customers_df = spark.read \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:135:1
    |
133 |         .format("delta") \
134 |         .load(f"{lake_root}/bronze/snowflake/customers")
135 |     
    | ^^^^
136 |     # Read existing silver data (if exists)
137 |     silver_customers_path = f"{lake_root}/silver/customers"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:138:1
    |
136 |     # Read existing silver data (if exists)
137 |     silver_customers_path = f"{lake_root}/silver/customers"
138 |     
    | ^^^^
139 |     try:
140 |         # Check if silver table exists
    |
help: Remove whitespace from blank line

F841 Local variable `silver_customers_df` is assigned to but never used
   --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:141:9
    |
139 |     try:
140 |         # Check if silver table exists
141 |         silver_customers_df = spark.read.format("delta").load(silver_customers_path)
    |         ^^^^^^^^^^^^^^^^^^^
142 |         silver_table_exists = True
143 |     except Exception:
    |
help: Remove assignment to unused variable `silver_customers_df`

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:146:1
    |
144 |         logger.info("Silver customers table does not exist, creating new one")
145 |         silver_table_exists = False
146 |     
    | ^^^^
147 |     if not silver_table_exists:
148 |         # Create new silver table
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:158:1
    |
156 |         # Merge data using Delta MERGE
157 |         silver_table = DeltaTable.forPath(spark, silver_customers_path)
158 |         
    | ^^^^^^^^
159 |         # Prepare bronze data for merge
160 |         bronze_customers_for_merge = bronze_customers_df.select(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:174:1
    |
172 |             current_timestamp().alias("_updated_at")
173 |         )
174 |         
    | ^^^^^^^^
175 |         # Perform MERGE operation
176 |         silver_table.alias("silver") \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:208:1
    |
206 |             }) \
207 |             .execute()
208 |         
    | ^^^^^^^^
209 |         logger.info("Successfully merged customers data into silver layer")
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:215:1
    |
213 |     """
214 |     Merge Snowflake lineitems data into silver layer.
215 |     
    | ^^^^
216 |     Args:
217 |         spark: Spark session
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:221:1
    |
219 |     """
220 |     logger.info("Merging Snowflake lineitems data into silver layer")
221 |     
    | ^^^^
222 |     # Read bronze data
223 |     bronze_lineitems_df = spark.read \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:226:1
    |
224 |         .format("delta") \
225 |         .load(f"{lake_root}/bronze/snowflake/lineitems")
226 |     
    | ^^^^
227 |     # Read existing silver data (if exists)
228 |     silver_lineitems_path = f"{lake_root}/silver/lineitems"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:229:1
    |
227 |     # Read existing silver data (if exists)
228 |     silver_lineitems_path = f"{lake_root}/silver/lineitems"
229 |     
    | ^^^^
230 |     try:
231 |         # Check if silver table exists
    |
help: Remove whitespace from blank line

F841 Local variable `silver_lineitems_df` is assigned to but never used
   --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:232:9
    |
230 |     try:
231 |         # Check if silver table exists
232 |         silver_lineitems_df = spark.read.format("delta").load(silver_lineitems_path)
    |         ^^^^^^^^^^^^^^^^^^^
233 |         silver_table_exists = True
234 |     except Exception:
    |
help: Remove assignment to unused variable `silver_lineitems_df`

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:237:1
    |
235 |         logger.info("Silver lineitems table does not exist, creating new one")
236 |         silver_table_exists = False
237 |     
    | ^^^^
238 |     if not silver_table_exists:
239 |         # Create new silver table
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:249:1
    |
247 |         # Merge data using Delta MERGE
248 |         silver_table = DeltaTable.forPath(spark, silver_lineitems_path)
249 |         
    | ^^^^^^^^
250 |         # Prepare bronze data for merge
251 |         bronze_lineitems_for_merge = bronze_lineitems_df.select(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:273:1
    |
271 |             current_timestamp().alias("_updated_at")
272 |         )
273 |         
    | ^^^^^^^^
274 |         # Perform MERGE operation
275 |         silver_table.alias("silver") \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:321:1
    |
319 |             }) \
320 |             .execute()
321 |         
    | ^^^^^^^^
322 |         logger.info("Successfully merged lineitems data into silver layer")
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:325:58
    |
325 | def process_snowflake_merge(spark: SparkSession, config: Dict[str, Any]) -> None:
    |                                                          ^^^^
326 |     """
327 |     Process Snowflake bronze to silver merge.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:328:1
    |
326 |     """
327 |     Process Snowflake bronze to silver merge.
328 |     
    | ^^^^
329 |     Args:
330 |         spark: Spark session
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:334:1
    |
332 |     """
333 |     logger.info("Starting Snowflake bronze to silver merge")
334 |     
    | ^^^^
335 |     # Get data lake configuration
336 |     lake_root = config["lake"]["root"]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:337:1
    |
335 |     # Get data lake configuration
336 |     lake_root = config["lake"]["root"]
337 |     
    | ^^^^
338 |     # Merge each table
339 |     merge_orders_data(spark, lake_root)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:342:1
    |
340 |     merge_customers_data(spark, lake_root)
341 |     merge_lineitems_data(spark, lake_root)
342 |     
    | ^^^^
343 |     logger.info("Snowflake bronze to silver merge completed successfully")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:349:1
    |
347 |     """Main function to run the Snowflake merge job."""
348 |     import argparse
349 |     
    | ^^^^
350 |     parser = argparse.ArgumentParser(description="Merge Snowflake data from bronze to silver")
351 |     parser.add_argument("--config", required=True, help="Configuration file path")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:353:1
    |
351 |     parser.add_argument("--config", required=True, help="Configuration file path")
352 |     args = parser.parse_args()
353 |     
    | ^^^^
354 |     # Setup logging
355 |     setup_json_logging()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:358:1
    |
356 |     log_level = os.getenv("LOG_LEVEL", "INFO")
357 |     logging.getLogger().setLevel(getattr(logging, log_level.upper()))
358 |     
    | ^^^^
359 |     try:
360 |         # Load configuration
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:362:1
    |
360 |         # Load configuration
361 |         config = load_conf(args.config)
362 |         
    | ^^^^^^^^
363 |         # Create Spark session with Delta support
364 |         spark = get_spark_session(
    |
help: Remove whitespace from blank line

F821 Undefined name `get_spark_session`
   --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:364:17
    |
363 |         # Create Spark session with Delta support
364 |         spark = get_spark_session(
    |                 ^^^^^^^^^^^^^^^^^
365 |             "SnowflakeBronzeToSilverMerge",
366 |             extra_conf={
    |

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:372:1
    |
370 |             }
371 |         )
372 |         
    | ^^^^^^^^
373 |         # Process Snowflake merge
374 |         process_snowflake_merge(spark, config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:375:1
    |
373 |         # Process Snowflake merge
374 |         process_snowflake_merge(spark, config)
375 |         
    | ^^^^^^^^
376 |     except Exception as e:
377 |         logger.error(f"Snowflake merge failed: {e}")
    |
help: Remove whitespace from blank line

W292 [*] No newline at end of file
   --> src/project_a/legacy/jobs/snowflake_bronze_to_silver_merge.py:382:11
    |
381 | if __name__ == "__main__":
382 |     main()
    |           ^
    |
help: Add trailing newline

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/jobs/snowflake_to_bronze.py:7:1
   |
 5 |   """
 6 |
 7 | / import os
 8 | | import sys
 9 | | import logging
10 | | from typing import Dict, Any
11 | | from pathlib import Path
12 | | from datetime import datetime
   | |_____________________________^
13 |
14 |   # Add src to path
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/legacy/jobs/snowflake_to_bronze.py:10:1
   |
 8 | import sys
 9 | import logging
10 | from typing import Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
11 | from pathlib import Path
12 | from datetime import datetime
   |

F401 [*] `datetime.datetime` imported but unused
  --> src/project_a/legacy/jobs/snowflake_to_bronze.py:12:22
   |
10 | from typing import Dict, Any
11 | from pathlib import Path
12 | from datetime import datetime
   |                      ^^^^^^^^
13 |
14 | # Add src to path
   |
help: Remove unused import: `datetime.datetime`

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/jobs/snowflake_to_bronze.py:17:1
   |
15 |   sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))
16 |
17 | / from pyspark.sql import SparkSession
18 | | from pyspark.sql.functions import col, lit, current_timestamp, to_date
19 | |
20 | | from project_a.utils.spark_session import build_spark
21 | | from project_a.utils.config import load_conf
22 | | from project_a.utils.logging import setup_json_logging
   | |______________________________________________________^
23 |
24 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

F401 [*] `pyspark.sql.functions.col` imported but unused
  --> src/project_a/legacy/jobs/snowflake_to_bronze.py:18:35
   |
17 | from pyspark.sql import SparkSession
18 | from pyspark.sql.functions import col, lit, current_timestamp, to_date
   |                                   ^^^
19 |
20 | from project_a.utils.spark_session import build_spark
   |
help: Remove unused import: `pyspark.sql.functions.col`

F401 [*] `project_a.utils.spark_session.build_spark` imported but unused
  --> src/project_a/legacy/jobs/snowflake_to_bronze.py:20:43
   |
18 | from pyspark.sql.functions import col, lit, current_timestamp, to_date
19 |
20 | from project_a.utils.spark_session import build_spark
   |                                           ^^^^^^^^^^^
21 | from project_a.utils.config import load_conf
22 | from project_a.utils.logging import setup_json_logging
   |
help: Remove unused import: `project_a.utils.spark_session.build_spark`

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/snowflake_to_bronze.py:30:1
   |
28 |     """
29 |     Get Snowflake JDBC connection string from environment variables.
30 |     
   | ^^^^
31 |     Returns:
32 |         JDBC connection string
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/snowflake_to_bronze.py:40:1
   |
38 |     sf_schema = os.getenv("SF_SCHEMA", "TPCH_SF1")
39 |     sf_wh = os.getenv("SF_WH", "COMPUTE_WH")
40 |     
   | ^^^^
41 |     return (
42 |         f"jdbc:snowflake://{sf_url}/"
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/jobs/snowflake_to_bronze.py:47:29
   |
47 | def get_sample_queries() -> Dict[str, str]:
   |                             ^^^^
48 |     """
49 |     Get sample queries for Snowflake sample data.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/snowflake_to_bronze.py:50:1
   |
48 |     """
49 |     Get sample queries for Snowflake sample data.
50 |     
   | ^^^^
51 |     Returns:
52 |         Dictionary with table names and queries
   |
help: Remove whitespace from blank line

W291 Trailing whitespace
  --> src/project_a/legacy/jobs/snowflake_to_bronze.py:56:19
   |
54 |     return {
55 |         "orders": """
56 |             SELECT 
   |                   ^
57 |                 o_orderkey as order_id,
58 |                 o_custkey as customer_id,
   |
help: Remove trailing whitespace

W291 Trailing whitespace
  --> src/project_a/legacy/jobs/snowflake_to_bronze.py:66:24
   |
64 |                 o_shippriority as ship_priority,
65 |                 o_comment as comment
66 |             FROM orders 
   |                        ^
67 |             LIMIT 10000
68 |         """,
   |
help: Remove trailing whitespace

W291 Trailing whitespace
  --> src/project_a/legacy/jobs/snowflake_to_bronze.py:70:19
   |
68 |         """,
69 |         "customers": """
70 |             SELECT 
   |                   ^
71 |                 c_custkey as customer_id,
72 |                 c_name as name,
   |
help: Remove trailing whitespace

W291 Trailing whitespace
  --> src/project_a/legacy/jobs/snowflake_to_bronze.py:79:26
   |
77 |                 c_mktsegment as market_segment,
78 |                 c_comment as comment
79 |             FROM customer 
   |                          ^
80 |             LIMIT 1000
81 |         """,
   |
help: Remove trailing whitespace

W291 Trailing whitespace
  --> src/project_a/legacy/jobs/snowflake_to_bronze.py:83:19
   |
81 |         """,
82 |         "lineitems": """
83 |             SELECT 
   |                   ^
84 |                 l_orderkey as order_id,
85 |                 l_partkey as part_id,
   |
help: Remove trailing whitespace

W291 Trailing whitespace
   --> src/project_a/legacy/jobs/snowflake_to_bronze.py:100:26
    |
 98 |                 l_shipmode as ship_mode,
 99 |                 l_comment as comment
100 |             FROM lineitem 
    |                          ^
101 |             LIMIT 50000
102 |         """
    |
help: Remove trailing whitespace

W293 Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_to_bronze.py:114:1
    |
112 |     """
113 |     Extract data from Snowflake using JDBC.
114 |     
    | ^^^^
115 |     Args:
116 |         spark: Spark session
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_to_bronze.py:120:1
    |
118 |         query: SQL query to execute
119 |         connection_string: JDBC connection string
120 |         
    | ^^^^^^^^
121 |     Returns:
122 |         Spark DataFrame with Snowflake data
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_to_bronze.py:125:1
    |
123 |     """
124 |     logger.info(f"Extracting data from Snowflake {table_name}")
125 |     
    | ^^^^
126 |     try:
127 |         df = spark.read \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_to_bronze.py:133:1
    |
131 |             .option("driver", "net.snowflake.client.jdbc.SnowflakeDriver") \
132 |             .load()
133 |         
    | ^^^^^^^^
134 |         logger.info(f"Successfully extracted {df.count()} records from {table_name}")
135 |         return df
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_to_bronze.py:136:1
    |
134 |         logger.info(f"Successfully extracted {df.count()} records from {table_name}")
135 |         return df
136 |         
    | ^^^^^^^^
137 |     except Exception as e:
138 |         logger.error(f"Failed to extract data from Snowflake {table_name}: {e}")
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_to_bronze.py:145:1
    |
143 |     """
144 |     Add metadata columns to the DataFrame.
145 |     
    | ^^^^
146 |     Args:
147 |         df: Input DataFrame
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_to_bronze.py:150:1
    |
148 |         source: Source system name
149 |         table_name: Table name
150 |         
    | ^^^^^^^^
151 |     Returns:
152 |         DataFrame with metadata columns
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_to_bronze.py:163:1
    |
161 |     """
162 |     Write DataFrame to bronze layer with partitioning by date.
163 |     
    | ^^^^
164 |     Args:
165 |         df: Input DataFrame
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_to_bronze.py:170:1
    |
168 |     """
169 |     output_path = f"{lake_root}/bronze/snowflake/{table_name}"
170 |     
    | ^^^^
171 |     logger.info(f"Writing {table_name} to bronze layer: {output_path}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_to_bronze.py:172:1
    |
171 |     logger.info(f"Writing {table_name} to bronze layer: {output_path}")
172 |     
    | ^^^^
173 |     df.write \
174 |         .format("delta") \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_to_bronze.py:179:1
    |
177 |         .partitionBy("_proc_date") \
178 |         .save(output_path)
179 |     
    | ^^^^
180 |     logger.info(f"Successfully wrote {table_name} to bronze layer")
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/jobs/snowflake_to_bronze.py:183:61
    |
183 | def process_snowflake_backfill(spark: SparkSession, config: Dict[str, Any]) -> None:
    |                                                             ^^^^
184 |     """
185 |     Process Snowflake sample data backfill.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_to_bronze.py:186:1
    |
184 |     """
185 |     Process Snowflake sample data backfill.
186 |     
    | ^^^^
187 |     Args:
188 |         spark: Spark session
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_to_bronze.py:192:1
    |
190 |     """
191 |     logger.info("Starting Snowflake sample data backfill")
192 |     
    | ^^^^
193 |     # Get connection string
194 |     connection_string = get_snowflake_connection_string()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_to_bronze.py:195:1
    |
193 |     # Get connection string
194 |     connection_string = get_snowflake_connection_string()
195 |     
    | ^^^^
196 |     # Get data lake configuration
197 |     lake_root = config["lake"]["root"]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_to_bronze.py:198:1
    |
196 |     # Get data lake configuration
197 |     lake_root = config["lake"]["root"]
198 |     
    | ^^^^
199 |     # Get sample queries
200 |     queries = get_sample_queries()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_to_bronze.py:201:1
    |
199 |     # Get sample queries
200 |     queries = get_sample_queries()
201 |     
    | ^^^^
202 |     # Process each table
203 |     for table_name, query in queries.items():
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_to_bronze.py:211:1
    |
209 |             logger.error(f"Failed to process {table_name}: {e}")
210 |             raise
211 |     
    | ^^^^
212 |     logger.info("Snowflake sample data backfill completed successfully")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_to_bronze.py:218:1
    |
216 |     """Main function to run the Snowflake backfill job."""
217 |     import argparse
218 |     
    | ^^^^
219 |     parser = argparse.ArgumentParser(description="Extract sample data from Snowflake")
220 |     parser.add_argument("--config", required=True, help="Configuration file path")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_to_bronze.py:222:1
    |
220 |     parser.add_argument("--config", required=True, help="Configuration file path")
221 |     args = parser.parse_args()
222 |     
    | ^^^^
223 |     # Setup logging
224 |     setup_json_logging()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_to_bronze.py:227:1
    |
225 |     log_level = os.getenv("LOG_LEVEL", "INFO")
226 |     logging.getLogger().setLevel(getattr(logging, log_level.upper()))
227 |     
    | ^^^^
228 |     try:
229 |         # Load configuration
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_to_bronze.py:231:1
    |
229 |         # Load configuration
230 |         config = load_conf(args.config)
231 |         
    | ^^^^^^^^
232 |         # Create Spark session with Delta support
233 |         spark = get_spark_session(
    |
help: Remove whitespace from blank line

F821 Undefined name `get_spark_session`
   --> src/project_a/legacy/jobs/snowflake_to_bronze.py:233:17
    |
232 |         # Create Spark session with Delta support
233 |         spark = get_spark_session(
    |                 ^^^^^^^^^^^^^^^^^
234 |             "SnowflakeToBronze",
235 |             extra_conf={
    |

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_to_bronze.py:241:1
    |
239 |             }
240 |         )
241 |         
    | ^^^^^^^^
242 |         # Process Snowflake data
243 |         process_snowflake_backfill(spark, config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/snowflake_to_bronze.py:244:1
    |
242 |         # Process Snowflake data
243 |         process_snowflake_backfill(spark, config)
244 |         
    | ^^^^^^^^
245 |     except Exception as e:
246 |         logger.error(f"Snowflake extraction failed: {e}")
    |
help: Remove whitespace from blank line

W292 [*] No newline at end of file
   --> src/project_a/legacy/jobs/snowflake_to_bronze.py:251:11
    |
250 | if __name__ == "__main__":
251 |     main()
    |           ^
    |
help: Add trailing newline

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:7:1
   |
 5 |   """
 6 |
 7 | / import logging
 8 | | from typing import Dict, Any, List
 9 | | from pyspark.sql import SparkSession, DataFrame
10 | | from pyspark.sql.functions import (
11 | |     col, lit, current_timestamp, when, max as spark_max,
12 | |     row_number, rank, coalesce, lead, lag
13 | | )
14 | | from pyspark.sql.window import Window
15 | | from delta.tables import DeltaTable
   | |___________________________________^
16 |
17 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:8:1
   |
 7 | import logging
 8 | from typing import Dict, Any, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 9 | from pyspark.sql import SparkSession, DataFrame
10 | from pyspark.sql.functions import (
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:8:1
   |
 7 | import logging
 8 | from typing import Dict, Any, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 9 | from pyspark.sql import SparkSession, DataFrame
10 | from pyspark.sql.functions import (
   |

F401 [*] `typing.Dict` imported but unused
  --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:8:20
   |
 7 | import logging
 8 | from typing import Dict, Any, List
   |                    ^^^^
 9 | from pyspark.sql import SparkSession, DataFrame
10 | from pyspark.sql.functions import (
   |
help: Remove unused import

F401 [*] `typing.Any` imported but unused
  --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:8:26
   |
 7 | import logging
 8 | from typing import Dict, Any, List
   |                          ^^^
 9 | from pyspark.sql import SparkSession, DataFrame
10 | from pyspark.sql.functions import (
   |
help: Remove unused import

F401 [*] `pyspark.sql.functions.row_number` imported but unused
  --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:12:5
   |
10 | from pyspark.sql.functions import (
11 |     col, lit, current_timestamp, when, max as spark_max,
12 |     row_number, rank, coalesce, lead, lag
   |     ^^^^^^^^^^
13 | )
14 | from pyspark.sql.window import Window
   |
help: Remove unused import

F401 [*] `pyspark.sql.functions.rank` imported but unused
  --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:12:17
   |
10 | from pyspark.sql.functions import (
11 |     col, lit, current_timestamp, when, max as spark_max,
12 |     row_number, rank, coalesce, lead, lag
   |                 ^^^^
13 | )
14 | from pyspark.sql.window import Window
   |
help: Remove unused import

F401 [*] `pyspark.sql.functions.coalesce` imported but unused
  --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:12:23
   |
10 | from pyspark.sql.functions import (
11 |     col, lit, current_timestamp, when, max as spark_max,
12 |     row_number, rank, coalesce, lead, lag
   |                       ^^^^^^^^
13 | )
14 | from pyspark.sql.window import Window
   |
help: Remove unused import

F401 [*] `pyspark.sql.functions.lead` imported but unused
  --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:12:33
   |
10 | from pyspark.sql.functions import (
11 |     col, lit, current_timestamp, when, max as spark_max,
12 |     row_number, rank, coalesce, lead, lag
   |                                 ^^^^
13 | )
14 | from pyspark.sql.window import Window
   |
help: Remove unused import

F401 [*] `pyspark.sql.functions.lag` imported but unused
  --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:12:39
   |
10 | from pyspark.sql.functions import (
11 |     col, lit, current_timestamp, when, max as spark_max,
12 |     row_number, rank, coalesce, lead, lag
   |                                       ^^^
13 | )
14 | from pyspark.sql.window import Window
   |
help: Remove unused import

F401 [*] `pyspark.sql.window.Window` imported but unused
  --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:14:32
   |
12 |     row_number, rank, coalesce, lead, lag
13 | )
14 | from pyspark.sql.window import Window
   |                                ^^^^^^
15 | from delta.tables import DeltaTable
   |
help: Remove unused import: `pyspark.sql.window.Window`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:20:42
   |
20 | def hash_record(df: DataFrame, key_cols: List[str], hash_col_name: str = "_hash") -> DataFrame:
   |                                          ^^^^
21 |     """
22 |     Calculate hash of key columns to detect changes.
   |
help: Replace with `list`

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:23:1
   |
21 |     """
22 |     Calculate hash of key columns to detect changes.
23 |     
   | ^^^^
24 |     Args:
25 |         df: Input DataFrame
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:28:1
   |
26 |         key_cols: Columns to include in hash
27 |         hash_col_name: Name of hash column
28 |         
   | ^^^^^^^^
29 |     Returns:
30 |         DataFrame with hash column
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:32:5
   |
30 |         DataFrame with hash column
31 |     """
32 |     from pyspark.sql.functions import sha2, concat_ws
   |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
33 |     
34 |     # Create hash from key columns
   |
help: Organize imports

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:33:1
   |
31 |     """
32 |     from pyspark.sql.functions import sha2, concat_ws
33 |     
   | ^^^^
34 |     # Create hash from key columns
35 |     hash_expr = sha2(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:39:1
   |
37 |         256
38 |     )
39 |     
   | ^^^^
40 |     return df.withColumn(hash_col_name, hash_expr)
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:55:1
   |
53 |     """
54 |     Update customer dimension with SCD Type 2 logic.
55 |     
   | ^^^^
56 |     Process:
57 |     1. Add hash to source records
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:62:1
   |
60 |     4. Close old versions (set effective_to)
61 |     5. Insert new versions
62 |     
   | ^^^^
63 |     Args:
64 |         spark: SparkSession
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:72:1
   |
70 |         effective_to_col: Effective to date column
71 |         is_current_col: Is current record flag column
72 |         
   | ^^^^^^^^
73 |     Returns:
74 |         Number of records inserted/updated
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:77:1
   |
75 |     """
76 |     logger.info("Starting SCD2 update for customer dimension")
77 |     
   | ^^^^
78 |     # Add hash to source
79 |     source_with_hash = hash_record(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:84:1
   |
82 |         hash_col_name="_source_hash"
83 |     ).withColumn("_source_ts", current_timestamp())
84 |     
   | ^^^^
85 |     try:
86 |         # Read existing dimension table
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:89:1
   |
87 |         target = DeltaTable.forPath(spark, target_path)
88 |         existing_df = target.toDF()
89 |         
   | ^^^^^^^^
90 |         logger.info(f"Existing records: {existing_df.count():,}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:91:1
   |
90 |         logger.info(f"Existing records: {existing_df.count():,}")
91 |         
   | ^^^^^^^^
92 |         # Add hash to existing records
93 |         existing_with_hash = hash_record(
   |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
  --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:95:57
   |
93 |         existing_with_hash = hash_record(
94 |             existing_df,
95 |             key_cols=[col for col in existing_df.columns 
   |                                                         ^
96 |                      if col not in [key_column, version_col, effective_from_col, effective_to_col, is_current_col, "_hash"]],
97 |             hash_col_name="_target_hash"
   |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:99:1
    |
 97 |             hash_col_name="_target_hash"
 98 |         )
 99 |         
    | ^^^^^^^^
100 |         # Get current versions only
101 |         current_versions = existing_with_hash.filter(col(is_current_col) == lit(True))
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:102:1
    |
100 |         # Get current versions only
101 |         current_versions = existing_with_hash.filter(col(is_current_col) == lit(True))
102 |         
    | ^^^^^^^^
103 |         # Join source with current versions
104 |         comparison_df = source_with_hash.alias("source") \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:110:1
    |
108 |                 "full_outer"
109 |             )
110 |         
    | ^^^^^^^^
111 |         # Classify records
112 |         # New records: in source, not in target
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:116:1
    |
114 |             .filter(col("target.customer_id").isNull()) \
115 |             .select([col(f"source.{c}") for c in source_with_hash.columns if not c.startswith("_")])
116 |         
    | ^^^^^^^^
117 |         # Changed records: in both, but hash differs
118 |         changed_records = comparison_df \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:125:1
    |
123 |             ) \
124 |             .select([col(f"source.{c}") for c in source_with_hash.columns if not c.startswith("_")])
125 |         
    | ^^^^^^^^
126 |         # Unchanged records: hash matches (no action needed)
127 |         unchanged_count = comparison_df \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:132:1
    |
130 |             ) \
131 |             .count()
132 |         
    | ^^^^^^^^
133 |         logger.info(f"ðŸ“Š SCD2 Analysis:")
134 |         logger.info(f"   New records: {new_records.count():,}")
    |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:133:21
    |
131 |             .count()
132 |         
133 |         logger.info(f"ðŸ“Š SCD2 Analysis:")
    |                     ^^^^^^^^^^^^^^^^^^^^
134 |         logger.info(f"   New records: {new_records.count():,}")
135 |         logger.info(f"   Changed records: {changed_records.count():,}")
    |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:137:1
    |
135 |         logger.info(f"   Changed records: {changed_records.count():,}")
136 |         logger.info(f"   Unchanged records: {unchanged_count:,}")
137 |         
    | ^^^^^^^^
138 |         # Prepare new/changed records for insertion
139 |         records_to_insert = new_records.unionByName(changed_records, allowMissingColumns=True)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:140:1
    |
138 |         # Prepare new/changed records for insertion
139 |         records_to_insert = new_records.unionByName(changed_records, allowMissingColumns=True)
140 |         
    | ^^^^^^^^
141 |         if records_to_insert.count() == 0:
142 |             logger.info("âœ… No changes detected, SCD2 update complete")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:144:1
    |
142 |             logger.info("âœ… No changes detected, SCD2 update complete")
143 |             return 0
144 |         
    | ^^^^^^^^
145 |         # Add SCD2 metadata
146 |         records_with_scd2 = records_to_insert \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:151:1
    |
149 |             .withColumn(effective_to_col, lit(None).cast("timestamp")) \
150 |             .withColumn(is_current_col, lit(True))
151 |         
    | ^^^^^^^^
152 |         # For changed records, we need to:
153 |         # 1. Close old versions (set effective_to and is_current=False)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:155:1
    |
153 |         # 1. Close old versions (set effective_to and is_current=False)
154 |         # 2. Insert new versions
155 |         
    | ^^^^^^^^
156 |         # Close old versions for changed records
157 |         changed_customer_ids = changed_records.select(key_column).distinct()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:158:1
    |
156 |         # Close old versions for changed records
157 |         changed_customer_ids = changed_records.select(key_column).distinct()
158 |         
    | ^^^^^^^^
159 |         update_condition = f"target.{key_column} = source.{key_column} AND target.{is_current_col} = true"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:160:1
    |
159 |         update_condition = f"target.{key_column} = source.{key_column} AND target.{is_current_col} = true"
160 |         
    | ^^^^^^^^
161 |         (target.alias("target")
162 |          .merge(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:173:1
    |
171 |          )
172 |          .execute())
173 |         
    | ^^^^^^^^
174 |         logger.info("âœ… Closed old versions for changed records")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:175:1
    |
174 |         logger.info("âœ… Closed old versions for changed records")
175 |         
    | ^^^^^^^^
176 |         # Insert new versions
177 |         # Get next version numbers
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:181:1
    |
179 |             spark_max(version_col).alias("max_version")
180 |         )
181 |         
    | ^^^^^^^^
182 |         records_with_versions = records_with_scd2.alias("new") \
183 |             .join(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:194:1
    |
192 |             ) \
193 |             .drop("max_version")
194 |         
    | ^^^^^^^^
195 |         # Insert new records
196 |         records_with_versions.write \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:200:1
    |
198 |             .mode("append") \
199 |             .save(target_path)
200 |         
    | ^^^^^^^^
201 |         inserted_count = records_with_versions.count()
202 |         logger.info(f"âœ… Inserted {inserted_count:,} new/updated records")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:203:1
    |
201 |         inserted_count = records_with_versions.count()
202 |         logger.info(f"âœ… Inserted {inserted_count:,} new/updated records")
203 |         
    | ^^^^^^^^
204 |         return inserted_count
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:205:1
    |
204 |         return inserted_count
205 |         
    | ^^^^^^^^
206 |     except Exception as e:
207 |         # Table doesn't exist, create initial version
    |
help: Remove whitespace from blank line

F841 [*] Local variable `e` is assigned to but never used
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:206:25
    |
204 |         return inserted_count
205 |         
206 |     except Exception as e:
    |                         ^
207 |         # Table doesn't exist, create initial version
208 |         logger.info(f"Creating new SCD2 dimension table: {target_path}")
    |
help: Remove assignment to unused variable `e`

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:209:1
    |
207 |         # Table doesn't exist, create initial version
208 |         logger.info(f"Creating new SCD2 dimension table: {target_path}")
209 |         
    | ^^^^^^^^
210 |         initial_scd2 = source_with_hash \
211 |             .withColumn(version_col, lit(1)) \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:216:1
    |
214 |             .withColumn(is_current_col, lit(True)) \
215 |             .drop("_source_hash", "_source_ts")
216 |         
    | ^^^^^^^^
217 |         initial_scd2.write \
218 |             .format("delta") \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:221:1
    |
219 |             .mode("overwrite") \
220 |             .save(target_path)
221 |         
    | ^^^^^^^^
222 |         count = initial_scd2.count()
223 |         logger.info(f"âœ… Created initial SCD2 dimension with {count:,} records")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:224:1
    |
222 |         count = initial_scd2.count()
223 |         logger.info(f"âœ… Created initial SCD2 dimension with {count:,} records")
224 |         
    | ^^^^^^^^
225 |         return count
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:232:1
    |
230 |     from pathlib import Path
231 |     sys.path.insert(0, str(Path(__file__).parent.parent.parent))
232 |     
    | ^^^^
233 |     from project_a.utils.spark_session import build_spark
234 |     from project_a.utils.config import load_conf
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:233:5
    |
231 |       sys.path.insert(0, str(Path(__file__).parent.parent.parent))
232 |       
233 | /     from project_a.utils.spark_session import build_spark
234 | |     from project_a.utils.config import load_conf
    | |________________________________________________^
235 |       
236 |       config = load_conf("config/local.yaml")
    |
help: Organize imports

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:235:1
    |
233 |     from project_a.utils.spark_session import build_spark
234 |     from project_a.utils.config import load_conf
235 |     
    | ^^^^
236 |     config = load_conf("config/local.yaml")
237 |     spark = build_spark(app_name="scd2_customer_dim", config=config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:238:1
    |
236 |     config = load_conf("config/local.yaml")
237 |     spark = build_spark(app_name="scd2_customer_dim", config=config)
238 |     
    | ^^^^
239 |     # Load source data (from silver)
240 |     silver_path = config.get("data_lake", {}).get("silver_path", "data/lakehouse_delta/silver")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:242:1
    |
240 |     silver_path = config.get("data_lake", {}).get("silver_path", "data/lakehouse_delta/silver")
241 |     source_df = spark.read.format("delta").load(f"{silver_path}/customers")
242 |     
    | ^^^^
243 |     # Update dimension
244 |     gold_path = config.get("data_lake", {}).get("gold_path", "data/lakehouse_delta/gold")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:246:1
    |
244 |     gold_path = config.get("data_lake", {}).get("gold_path", "data/lakehouse_delta/gold")
245 |     target_path = f"{gold_path}/dim_customer_scd2"
246 |     
    | ^^^^
247 |     update_customer_dimension_scd2(spark, source_df, target_path)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/jobs/update_customer_dimension_scd2.py:248:1
    |
247 |     update_customer_dimension_scd2(spark, source_df, target_path)
248 |     
    | ^^^^
249 |     spark.stop()
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/lineage_tracker.py:5:1
   |
 3 |   """
 4 |
 5 | / import logging
 6 | | from datetime import datetime
 7 | | from typing import Dict, Any, List, Optional
 8 | | from dataclasses import dataclass, asdict
   | |_________________________________________^
 9 |
10 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
 --> src/project_a/legacy/lineage_tracker.py:7:1
  |
5 | import logging
6 | from datetime import datetime
7 | from typing import Dict, Any, List, Optional
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
8 | from dataclasses import dataclass, asdict
  |

UP035 `typing.List` is deprecated, use `list` instead
 --> src/project_a/legacy/lineage_tracker.py:7:1
  |
5 | import logging
6 | from datetime import datetime
7 | from typing import Dict, Any, List, Optional
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
8 | from dataclasses import dataclass, asdict
  |

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/lineage_tracker.py:21:15
   |
19 |     target_path: str
20 |     timestamp: datetime
21 |     metadata: Dict[str, Any]
   |               ^^^^
22 |     status: str = 'started'
23 |     error_message: Optional[str] = None
   |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/lineage_tracker.py:23:20
   |
21 |     metadata: Dict[str, Any]
22 |     status: str = 'started'
23 |     error_message: Optional[str] = None
   |                    ^^^^^^^^^^^^^
   |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/legacy/lineage_tracker.py:32:22
   |
31 |     def __init__(self):
32 |         self.events: List[LineageEvent] = []
   |                      ^^^^
33 |         self.current_event: Optional[LineageEvent] = None
   |
help: Replace with `list`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/lineage_tracker.py:33:29
   |
31 |     def __init__(self):
32 |         self.events: List[LineageEvent] = []
33 |         self.current_event: Optional[LineageEvent] = None
   |                             ^^^^^^^^^^^^^^^^^^^^^^
34 |
35 |     def start_event(self, event_type: str, source_path: str, target_path: str,
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/lineage_tracker.py:36:30
   |
35 |     def start_event(self, event_type: str, source_path: str, target_path: str,
36 |                    metadata: Dict[str, Any] = None) -> str:
   |                              ^^^^
37 |         """Start tracking a new lineage event."""
38 |         event_id = f"{event_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/lineage_tracker.py:63:38
   |
61 |                 break
62 |
63 |     def get_lineage_summary(self) -> Dict[str, Any]:
   |                                      ^^^^
64 |         """Get a summary of all lineage events."""
65 |         return {
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/lineage_tracker.py:71:39
   |
69 |         }
70 |
71 |     def _get_summary_by_type(self) -> Dict[str, int]:
   |                                       ^^^^
72 |         """Get count of events by type."""
73 |         summary = {}
   |
help: Replace with `dict`

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/load/write_idempotent.py:7:1
   |
 5 |   """
 6 |
 7 | / import os
 8 | | import uuid
 9 | | import logging
10 | | from datetime import datetime
11 | | from typing import Optional
12 | | from pathlib import Path
13 | | from pyspark.sql import DataFrame
14 | | from pyspark.sql.functions import lit, current_timestamp
   | |________________________________________________________^
15 |
16 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

F401 [*] `pathlib.Path` imported but unused
  --> src/project_a/legacy/load/write_idempotent.py:12:21
   |
10 | from datetime import datetime
11 | from typing import Optional
12 | from pathlib import Path
   |                     ^^^^
13 | from pyspark.sql import DataFrame
14 | from pyspark.sql.functions import lit, current_timestamp
   |
help: Remove unused import: `pathlib.Path`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/load/write_idempotent.py:19:24
   |
19 | def get_run_id(run_id: Optional[str] = None) -> str:
   |                        ^^^^^^^^^^^^^
20 |     """Generate or use provided run ID."""
21 |     if run_id:
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/load/write_idempotent.py:30:13
   |
28 |     output_path: str,
29 |     table_name: str,
30 |     run_id: Optional[str] = None,
   |             ^^^^^^^^^^^^^
31 |     partition_cols: list = None
32 | ) -> str:
   |
help: Convert to `X | None`

W293 Blank line contains whitespace
  --> src/project_a/legacy/load/write_idempotent.py:35:1
   |
33 |     """
34 |     Write to Bronze layer with idempotent staging workflow.
35 |     
   | ^^^^
36 |     Process:
37 |     1. Write to staging path: {output_path}/_staging/{run_id}/
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/load/write_idempotent.py:40:1
   |
38 |     2. Validate staging data
39 |     3. Move/rename to final path: {output_path}/dt={date}/
40 |     
   | ^^^^
41 |     Args:
42 |         df: DataFrame to write
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/load/write_idempotent.py:47:1
   |
45 |         run_id: Unique run identifier
46 |         partition_cols: Partition columns (e.g., ['dt', 'source'])
47 |         
   | ^^^^^^^^
48 |     Returns:
49 |         Final output path
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/load/write_idempotent.py:53:1
   |
51 |     run_id = get_run_id(run_id)
52 |     partition_cols = partition_cols or ['dt']
53 |     
   | ^^^^
54 |     # Staging path
55 |     staging_path = f"{output_path}/_staging/{run_id}"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/load/write_idempotent.py:56:1
   |
54 |     # Staging path
55 |     staging_path = f"{output_path}/_staging/{run_id}"
56 |     
   | ^^^^
57 |     # Add run metadata
58 |     df_with_metadata = df \
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/load/write_idempotent.py:61:1
   |
59 |         .withColumn("_run_id", lit(run_id)) \
60 |         .withColumn("_ingestion_ts", current_timestamp())
61 |     
   | ^^^^
62 |     logger.info(f"ðŸ“ Writing to staging: {staging_path}")
63 |     logger.info(f"   Run ID: {run_id}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/load/write_idempotent.py:65:1
   |
63 |     logger.info(f"   Run ID: {run_id}")
64 |     logger.info(f"   Records: {df_with_metadata.count():,}")
65 |     
   | ^^^^
66 |     # Write to staging
67 |     writer = df_with_metadata.write.format("delta").mode("overwrite")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/load/write_idempotent.py:68:1
   |
66 |     # Write to staging
67 |     writer = df_with_metadata.write.format("delta").mode("overwrite")
68 |     
   | ^^^^
69 |     if partition_cols:
70 |         writer = writer.partitionBy(*partition_cols)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/load/write_idempotent.py:71:1
   |
69 |     if partition_cols:
70 |         writer = writer.partitionBy(*partition_cols)
71 |     
   | ^^^^
72 |     writer.save(staging_path)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/load/write_idempotent.py:73:1
   |
72 |     writer.save(staging_path)
73 |     
   | ^^^^
74 |     logger.info(f"âœ… Staging write completed")
   |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
  --> src/project_a/legacy/load/write_idempotent.py:74:17
   |
72 |     writer.save(staging_path)
73 |     
74 |     logger.info(f"âœ… Staging write completed")
   |                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
75 |     
76 |     # Validate staging (optional: check row count, schema, etc.)
   |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/load/write_idempotent.py:75:1
   |
74 |     logger.info(f"âœ… Staging write completed")
75 |     
   | ^^^^
76 |     # Validate staging (optional: check row count, schema, etc.)
77 |     staging_df = df.sparkSession.read.format("delta").load(staging_path)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/load/write_idempotent.py:79:1
   |
77 |     staging_df = df.sparkSession.read.format("delta").load(staging_path)
78 |     staging_count = staging_df.count()
79 |     
   | ^^^^
80 |     if staging_count == 0:
81 |         logger.warning("âš ï¸ Staging data is empty, aborting final write")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/load/write_idempotent.py:83:1
   |
81 |         logger.warning("âš ï¸ Staging data is empty, aborting final write")
82 |         return staging_path
83 |     
   | ^^^^
84 |     # Move to final path (atomic operation)
85 |     # For Delta Lake, we can use OPTIMIZE + VACUUM pattern
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/load/write_idempotent.py:88:1
   |
86 |     # Or simply rename/copy files
87 |     final_path = f"{output_path}/dt={datetime.utcnow().strftime('%Y-%m-%d')}"
88 |     
   | ^^^^
89 |     logger.info(f"ðŸ“¦ Moving staging to final: {final_path}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/load/write_idempotent.py:90:1
   |
89 |     logger.info(f"ðŸ“¦ Moving staging to final: {final_path}")
90 |     
   | ^^^^
91 |     # Read from staging and write to final with append mode
92 |     staging_df.write \
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/load/write_idempotent.py:98:1
    |
 96 |         .partitionBy(*partition_cols) \
 97 |         .save(final_path)
 98 |     
    | ^^^^
 99 |     # Cleanup staging (optional: keep for audit)
100 |     # logger.info(f"ðŸ§¹ Cleaning up staging: {staging_path}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/load/write_idempotent.py:102:1
    |
100 |     # logger.info(f"ðŸ§¹ Cleaning up staging: {staging_path}")
101 |     # staging_df.sparkSession._jvm.org.apache.hadoop.fs.FileSystem.get(...).delete(...)
102 |     
    | ^^^^
103 |     logger.info(f"âœ… Idempotent write completed: {final_path}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/load/write_idempotent.py:104:1
    |
103 |     logger.info(f"âœ… Idempotent write completed: {final_path}")
104 |     
    | ^^^^
105 |     return final_path
    |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/load/write_idempotent.py:113:13
    |
111 |     table_name: str,
112 |     key_columns: list,
113 |     run_id: Optional[str] = None
    |             ^^^^^^^^^^^^^
114 | ) -> str:
115 |     """
    |
help: Convert to `X | None`

W293 Blank line contains whitespace
   --> src/project_a/legacy/load/write_idempotent.py:117:1
    |
115 |     """
116 |     Write to Silver layer with MERGE for idempotency.
117 |     
    | ^^^^
118 |     Uses Delta Lake MERGE to ensure idempotent upserts.
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/load/write_idempotent.py:119:1
    |
118 |     Uses Delta Lake MERGE to ensure idempotent upserts.
119 |     
    | ^^^^
120 |     Args:
121 |         df: DataFrame to write
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/load/write_idempotent.py:126:1
    |
124 |         key_columns: Primary key columns for MERGE
125 |         run_id: Unique run identifier
126 |         
    | ^^^^^^^^
127 |     Returns:
128 |         Final output path
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/load/write_idempotent.py:131:1
    |
129 |     """
130 |     from delta.tables import DeltaTable
131 |     
    | ^^^^
132 |     run_id = get_run_id(run_id)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/load/write_idempotent.py:133:1
    |
132 |     run_id = get_run_id(run_id)
133 |     
    | ^^^^
134 |     # Add run metadata
135 |     df_with_metadata = df \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/load/write_idempotent.py:138:1
    |
136 |         .withColumn("_run_id", lit(run_id)) \
137 |         .withColumn("_processed_ts", current_timestamp())
138 |     
    | ^^^^
139 |     spark = df.sparkSession
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/load/write_idempotent.py:140:1
    |
139 |     spark = df.sparkSession
140 |     
    | ^^^^
141 |     logger.info(f"ðŸ“ Writing to Silver: {output_path}")
142 |     logger.info(f"   Run ID: {run_id}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/load/write_idempotent.py:145:1
    |
143 |     logger.info(f"   Key columns: {key_columns}")
144 |     logger.info(f"   Records: {df_with_metadata.count():,}")
145 |     
    | ^^^^
146 |     try:
147 |         # Check if table exists
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/load/write_idempotent.py:149:1
    |
147 |         # Check if table exists
148 |         target = DeltaTable.forPath(spark, output_path)
149 |         
    | ^^^^^^^^
150 |         # Build merge condition
151 |         merge_condition = " AND ".join([
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/load/write_idempotent.py:154:1
    |
152 |             f"target.{key} = source.{key}" for key in key_columns
153 |         ])
154 |         
    | ^^^^^^^^
155 |         logger.info(f"ðŸ”„ MERGE operation: {merge_condition}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/load/write_idempotent.py:156:1
    |
155 |         logger.info(f"ðŸ”„ MERGE operation: {merge_condition}")
156 |         
    | ^^^^^^^^
157 |         # Perform MERGE
158 |         (target.alias("target")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/load/write_idempotent.py:163:1
    |
161 |          .whenNotMatchedInsertAll()
162 |          .execute())
163 |         
    | ^^^^^^^^
164 |         logger.info(f"âœ… MERGE completed for {table_name}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/load/write_idempotent.py:165:1
    |
164 |         logger.info(f"âœ… MERGE completed for {table_name}")
165 |         
    | ^^^^^^^^
166 |     except Exception as e:
167 |         # Table doesn't exist, create new
    |
help: Remove whitespace from blank line

F841 [*] Local variable `e` is assigned to but never used
   --> src/project_a/legacy/load/write_idempotent.py:166:25
    |
164 |         logger.info(f"âœ… MERGE completed for {table_name}")
165 |         
166 |     except Exception as e:
    |                         ^
167 |         # Table doesn't exist, create new
168 |         logger.info(f"ðŸ“ Creating new Silver table: {output_path}")
    |
help: Remove assignment to unused variable `e`

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/load/write_idempotent.py:169:1
    |
167 |         # Table doesn't exist, create new
168 |         logger.info(f"ðŸ“ Creating new Silver table: {output_path}")
169 |         
    | ^^^^^^^^
170 |         df_with_metadata.write \
171 |             .format("delta") \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/load/write_idempotent.py:175:1
    |
173 |             .option("overwriteSchema", "true") \
174 |             .save(output_path)
175 |         
    | ^^^^^^^^
176 |         logger.info(f"âœ… New Silver table created")
    |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
   --> src/project_a/legacy/load/write_idempotent.py:176:21
    |
174 |             .save(output_path)
175 |         
176 |         logger.info(f"âœ… New Silver table created")
    |                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
177 |     
178 |     return output_path
    |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/load/write_idempotent.py:177:1
    |
176 |         logger.info(f"âœ… New Silver table created")
177 |     
    | ^^^^
178 |     return output_path
    |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/load/write_idempotent.py:186:13
    |
184 |     table_name: str,
185 |     key_columns: list,
186 |     run_id: Optional[str] = None
    |             ^^^^^^^^^^^^^
187 | ) -> str:
188 |     """
    |
help: Convert to `X | None`

W293 Blank line contains whitespace
   --> src/project_a/legacy/load/write_idempotent.py:190:1
    |
188 |     """
189 |     Write to Gold layer with MERGE for idempotency.
190 |     
    | ^^^^
191 |     Args:
192 |         df: DataFrame to write
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/load/write_idempotent.py:197:1
    |
195 |         key_columns: Primary key columns
196 |         run_id: Unique run identifier
197 |         
    | ^^^^^^^^
198 |     Returns:
199 |         Final output path
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
 --> src/project_a/legacy/logging_config.py:1:1
  |
1 | / import json
2 | | import logging
3 | | import sys
4 | | import time
5 | | import uuid
6 | | from typing import Dict, Any
  | |____________________________^
  |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
 --> src/project_a/legacy/logging_config.py:6:1
  |
4 | import time
5 | import uuid
6 | from typing import Dict, Any
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  |

F401 [*] `typing.Dict` imported but unused
 --> src/project_a/legacy/logging_config.py:6:20
  |
4 | import time
5 | import uuid
6 | from typing import Dict, Any
  |                    ^^^^
  |
help: Remove unused import

F401 [*] `typing.Any` imported but unused
 --> src/project_a/legacy/logging_config.py:6:26
  |
4 | import time
5 | import uuid
6 | from typing import Dict, Any
  |                          ^^^
  |
help: Remove unused import

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/logging_config.py:26:1
   |
24 |     if run_id is None:
25 |         run_id = new_run_id()
26 |     
   | ^^^^
27 |     handler = logging.StreamHandler(sys.stdout)
28 |     handler.setFormatter(JsonFormatter())
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/logging_config.py:32:1
   |
30 |     root.setLevel(logging.INFO)
31 |     root.handlers = [handler]
32 |     
   | ^^^^
33 |     # Add run_id to all log records
34 |     logging.getLogger().info("Logging configured", extra={"run_id": run_id})
   |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
  --> src/project_a/legacy/logging_config.py:42:71
   |
42 | def log_metric(logger: logging.Logger, metric_name: str, value: float, 
   |                                                                       ^
43 |                run_id: str = None, **kwargs):
44 |     """Log a metric with structured format"""
   |
help: Remove trailing whitespace

I001 [*] Import block is un-sorted or un-formatted
 --> src/project_a/legacy/logging_setup.py:1:1
  |
1 | / import json
2 | | import logging
3 | | import os
4 | | import sys
5 | | import uuid
6 | | import time
7 | | from logging import LogRecord
  | |_____________________________^
  |
help: Organize imports

I001 [*] Import block is un-sorted or un-formatted
 --> src/project_a/legacy/metrics/ingest_pipeline_metrics.py:1:1
  |
1 | / import json
2 | | from pathlib import Path
3 | | from .metrics_exporter import PIPELINE_RUNS, ROWS_PROCESSED, STAGE_DURATION
  | |___________________________________________________________________________^
  |
help: Organize imports

UP015 [*] Unnecessary mode argument
  --> src/project_a/legacy/metrics/ingest_pipeline_metrics.py:9:21
   |
 7 |     if not path.exists():
 8 |         return
 9 |     with open(path, "r") as f:
   |                     ^^^
10 |         payload = json.load(f)
11 |     # Expecting e.g. {"stages":[{"name":"bronze_to_silver","rows":123,"duration_sec":4.5}, ...]}
   |
help: Remove mode argument

I001 [*] Import block is un-sorted or un-formatted
 --> src/project_a/legacy/metrics/metrics/metrics_exporter.py:1:1
  |
1 | from prometheus_client import Counter, Gauge, start_http_server
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  |
help: Organize imports

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
  --> src/project_a/legacy/metrics/metrics/metrics_exporter.py:23:9
   |
21 |             except OSError:
22 |                 continue
23 |         raise RuntimeError(f"Could not start metrics server on ports {port}-{port+9}")
   |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   |

I001 [*] Import block is un-sorted or un-formatted
 --> src/project_a/legacy/metrics/metrics/sink.py:1:1
  |
1 | / import time
2 | | from typing import Dict, Optional, List
  | |_______________________________________^
  |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
 --> src/project_a/legacy/metrics/metrics/sink.py:2:1
  |
1 | import time
2 | from typing import Dict, Optional, List
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  |

UP035 `typing.List` is deprecated, use `list` instead
 --> src/project_a/legacy/metrics/metrics/sink.py:2:1
  |
1 | import time
2 | from typing import Dict, Optional, List
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  |

F401 [*] `typing.List` imported but unused
 --> src/project_a/legacy/metrics/metrics/sink.py:2:36
  |
1 | import time
2 | from typing import Dict, Optional, List
  |                                    ^^^^
  |
help: Remove unused import: `typing.List`

UP006 [*] Use `dict` instead of `Dict` for type annotation
 --> src/project_a/legacy/metrics/metrics/sink.py:6:29
  |
5 | class MetricsSink:
6 |     def __init__(self, cfg: Dict, run_id: str = None):
  |                             ^^^^
7 |         self.cfg = cfg
8 |         self.run_id = run_id
  |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/metrics/metrics/sink.py:11:51
   |
 9 |         self.start_time = time.time()
10 |
11 |     def incr(self, name: str, value: float, dims: Optional[Dict[str, str]] = None):
   |                                                   ^^^^^^^^^^^^^^^^^^^^^^^^
12 |         pass
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/metrics/metrics/sink.py:11:60
   |
 9 |         self.start_time = time.time()
10 |
11 |     def incr(self, name: str, value: float, dims: Optional[Dict[str, str]] = None):
   |                                                            ^^^^
12 |         pass
   |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/metrics/metrics/sink.py:14:52
   |
12 |         pass
13 |
14 |     def gauge(self, name: str, value: float, dims: Optional[Dict[str, str]] = None):
   |                                                    ^^^^^^^^^^^^^^^^^^^^^^^^
15 |         pass
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/metrics/metrics/sink.py:14:61
   |
12 |         pass
13 |
14 |     def gauge(self, name: str, value: float, dims: Optional[Dict[str, str]] = None):
   |                                                             ^^^^
15 |         pass
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/metrics/metrics/sink.py:16:1
   |
14 |     def gauge(self, name: str, value: float, dims: Optional[Dict[str, str]] = None):
15 |         pass
16 |     
   | ^^^^
17 |     def timing(self, name: str, duration_ms: float, dims: Optional[Dict[str, str]] = None):
18 |         pass
   |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/metrics/metrics/sink.py:17:59
   |
15 |         pass
16 |     
17 |     def timing(self, name: str, duration_ms: float, dims: Optional[Dict[str, str]] = None):
   |                                                           ^^^^^^^^^^^^^^^^^^^^^^^^
18 |         pass
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/metrics/metrics/sink.py:17:68
   |
15 |         pass
16 |     
17 |     def timing(self, name: str, duration_ms: float, dims: Optional[Dict[str, str]] = None):
   |                                                                    ^^^^
18 |         pass
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/metrics/metrics/sink.py:19:1
   |
17 |     def timing(self, name: str, duration_ms: float, dims: Optional[Dict[str, str]] = None):
18 |         pass
19 |     
   | ^^^^
20 |     def pipeline_metric(self, stage: str, metric_type: str, value: float, 
21 |                        dims: Optional[Dict[str, str]] = None):
   |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
  --> src/project_a/legacy/metrics/metrics/sink.py:20:74
   |
18 |         pass
19 |     
20 |     def pipeline_metric(self, stage: str, metric_type: str, value: float, 
   |                                                                          ^
21 |                        dims: Optional[Dict[str, str]] = None):
22 |         """Log pipeline-specific metrics"""
   |
help: Remove trailing whitespace

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/metrics/metrics/sink.py:21:30
   |
20 |     def pipeline_metric(self, stage: str, metric_type: str, value: float, 
21 |                        dims: Optional[Dict[str, str]] = None):
   |                              ^^^^^^^^^^^^^^^^^^^^^^^^
22 |         """Log pipeline-specific metrics"""
23 |         metric_name = f"pipeline.{stage}.{metric_type}"
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/metrics/metrics/sink.py:21:39
   |
20 |     def pipeline_metric(self, stage: str, metric_type: str, value: float, 
21 |                        dims: Optional[Dict[str, str]] = None):
   |                                       ^^^^
22 |         """Log pipeline-specific metrics"""
23 |         metric_name = f"pipeline.{stage}.{metric_type}"
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/metrics/metrics/sink.py:25:1
   |
23 |         metric_name = f"pipeline.{stage}.{metric_type}"
24 |         self.gauge(metric_name, value, dims)
25 |     
   | ^^^^
26 |     def data_quality_metric(self, table: str, metric_type: str, value: float,
27 |                            dims: Optional[Dict[str, str]] = None):
   |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/metrics/metrics/sink.py:27:34
   |
26 |     def data_quality_metric(self, table: str, metric_type: str, value: float,
27 |                            dims: Optional[Dict[str, str]] = None):
   |                                  ^^^^^^^^^^^^^^^^^^^^^^^^
28 |         """Log data quality metrics"""
29 |         metric_name = f"dq.{table}.{metric_type}"
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/metrics/metrics/sink.py:27:43
   |
26 |     def data_quality_metric(self, table: str, metric_type: str, value: float,
27 |                            dims: Optional[Dict[str, str]] = None):
   |                                           ^^^^
28 |         """Log data quality metrics"""
29 |         metric_name = f"dq.{table}.{metric_type}"
   |
help: Replace with `dict`

W291 [*] Trailing whitespace
  --> src/project_a/legacy/metrics/metrics/sink.py:36:28
   |
34 |     def incr(self, name, value, dims=None):
35 |         metric_data = {
36 |             "metric": name, 
   |                            ^
37 |             "type": "counter", 
38 |             "value": value, 
   |
help: Remove trailing whitespace

W291 [*] Trailing whitespace
  --> src/project_a/legacy/metrics/metrics/sink.py:37:31
   |
35 |         metric_data = {
36 |             "metric": name, 
37 |             "type": "counter", 
   |                               ^
38 |             "value": value, 
39 |             "dims": dims or {},
   |
help: Remove trailing whitespace

W291 [*] Trailing whitespace
  --> src/project_a/legacy/metrics/metrics/sink.py:38:28
   |
36 |             "metric": name, 
37 |             "type": "counter", 
38 |             "value": value, 
   |                            ^
39 |             "dims": dims or {},
40 |             "run_id": self.run_id,
   |
help: Remove trailing whitespace

W291 [*] Trailing whitespace
  --> src/project_a/legacy/metrics/metrics/sink.py:47:28
   |
45 |     def gauge(self, name, value, dims=None):
46 |         metric_data = {
47 |             "metric": name, 
   |                            ^
48 |             "type": "gauge", 
49 |             "value": value, 
   |
help: Remove trailing whitespace

W291 [*] Trailing whitespace
  --> src/project_a/legacy/metrics/metrics/sink.py:48:29
   |
46 |         metric_data = {
47 |             "metric": name, 
48 |             "type": "gauge", 
   |                             ^
49 |             "value": value, 
50 |             "dims": dims or {},
   |
help: Remove trailing whitespace

W291 [*] Trailing whitespace
  --> src/project_a/legacy/metrics/metrics/sink.py:49:28
   |
47 |             "metric": name, 
48 |             "type": "gauge", 
49 |             "value": value, 
   |                            ^
50 |             "dims": dims or {},
51 |             "run_id": self.run_id,
   |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/metrics/metrics/sink.py:55:1
   |
53 |         }
54 |         print(f"METRIC: {metric_data}")
55 |     
   | ^^^^
56 |     def timing(self, name, duration_ms, dims=None):
57 |         metric_data = {
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/metrics/metrics/sink.py:66:1
   |
64 |         }
65 |         print(f"METRIC: {metric_data}")
66 |     
   | ^^^^
67 |     def pipeline_metric(self, stage, metric_type, value, dims=None):
68 |         """Log pipeline-specific metrics with enhanced context"""
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/metrics/metrics/sink.py:82:1
   |
80 |         }
81 |         print(f"METRIC: {metric_data}")
82 |     
   | ^^^^
83 |     def data_quality_metric(self, table, metric_type, value, dims=None):
84 |         """Log data quality metrics with enhanced context"""
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/metrics/metrics/sink.py:100:25
    |
100 | def create_metrics(cfg: Dict, run_id: str = None) -> MetricsSink:
    |                         ^^^^
101 |     sink = cfg.get("observability", {}).get("sink", "stdout")
102 |     if sink == "cloudwatch":
    |
help: Replace with `dict`

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/metrics_collector.py:6:1
   |
 4 |   """
 5 |
 6 | / import json
 7 | | import logging
 8 | | from datetime import datetime, timedelta
 9 | | from typing import Dict, Any
10 | | from pathlib import Path
11 | | import time
   | |___________^
12 |
13 |   # Prometheus client for metrics export
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/legacy/metrics_collector.py:9:1
   |
 7 | import logging
 8 | from datetime import datetime, timedelta
 9 | from typing import Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 | from pathlib import Path
11 | import time
   |

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/metrics_collector.py:15:5
   |
13 |   # Prometheus client for metrics export
14 |   try:
15 | /     from prometheus_client import (
16 | |         Counter, Gauge, Histogram, Summary,
17 | |         generate_latest, CONTENT_TYPE_LATEST,
18 | |         CollectorRegistry, multiprocess
19 | |     )
   | |_____^
20 |       PROMETHEUS_AVAILABLE = True
21 |   except ImportError:
   |
help: Organize imports

F401 `prometheus_client.CONTENT_TYPE_LATEST` imported but unused; consider using `importlib.util.find_spec` to test for availability
  --> src/project_a/legacy/metrics_collector.py:17:26
   |
15 |     from prometheus_client import (
16 |         Counter, Gauge, Histogram, Summary,
17 |         generate_latest, CONTENT_TYPE_LATEST,
   |                          ^^^^^^^^^^^^^^^^^^^
18 |         CollectorRegistry, multiprocess
19 |     )
   |
help: Remove unused import

F401 `prometheus_client.multiprocess` imported but unused; consider using `importlib.util.find_spec` to test for availability
  --> src/project_a/legacy/metrics_collector.py:18:28
   |
16 |         Counter, Gauge, Histogram, Summary,
17 |         generate_latest, CONTENT_TYPE_LATEST,
18 |         CollectorRegistry, multiprocess
   |                            ^^^^^^^^^^^^
19 |     )
20 |     PROMETHEUS_AVAILABLE = True
   |
help: Remove unused import

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/metrics_collector.py:147:43
    |
145 |         self.inventory_level = MockMetric()
146 |
147 |     def collect_pipeline_metrics(self) -> Dict[str, Any]:
    |                                           ^^^^
148 |         """Collect all pipeline metrics from JSON files."""
149 |         metrics_summary = {
    |
help: Replace with `dict`

UP015 [*] Unnecessary mode argument
   --> src/project_a/legacy/metrics_collector.py:180:45
    |
178 |             for metrics_file in metrics_files:
179 |                 try:
180 |                     with open(metrics_file, 'r') as f:
    |                                             ^^^
181 |                         metrics = json.load(f)
    |
help: Remove mode argument

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/metrics_collector.py:224:51
    |
222 |         return metrics_summary
223 |
224 |     def _update_prometheus_metrics(self, metrics: Dict[str, Any]):
    |                                                   ^^^^
225 |         """Update Prometheus metrics from pipeline metrics."""
226 |         try:
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/metrics_collector.py:303:45
    |
301 |             return f"# Error generating metrics: {e}"
302 |
303 |     def get_open_telemetry_metrics(self) -> Dict[str, Any]:
    |                                             ^^^^
304 |         """Get metrics in OpenTelemetry format."""
305 |         try:
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/metrics_collector.py:365:38
    |
363 |             return {"error": str(e)}
364 |
365 |     def get_metrics_summary(self) -> Dict[str, Any]:
    |                                      ^^^^
366 |         """Get a human-readable metrics summary."""
367 |         summary = self.collect_pipeline_metrics()
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/metrics_collector.py:406:48
    |
404 |             logger.error(f"Failed to export metrics: {e}")
405 |
406 |     def create_metrics_dashboard_data(self) -> Dict[str, Any]:
    |                                                ^^^^
407 |         """Create data for a metrics dashboard."""
408 |         try:
    |
help: Replace with `dict`

UP035 `typing.List` is deprecated, use `list` instead
 --> src/project_a/legacy/modeling.py:6:1
  |
4 | """
5 |
6 | from typing import List
  | ^^^^^^^^^^^^^^^^^^^^^^^
7 |
8 | from pyspark.sql import DataFrame, SparkSession
  |

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/modeling.py:6:1
   |
 4 |   """
 5 |
 6 | / from typing import List
 7 | |
 8 | | from pyspark.sql import DataFrame, SparkSession
 9 | | from pyspark.sql.functions import (col, concat_ws, date_format, dayofmonth,
10 | |                                    expr, lit, month, quarter,
11 | |                                    sha2, weekofyear, year)
   | |__________________________________________________________^
   |
help: Organize imports

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/legacy/modeling.py:15:33
   |
14 | def add_surrogate_key(
15 |     df: DataFrame, key_columns: List[str], surrogate_key_column: str
   |                                 ^^^^
16 | ) -> DataFrame:
17 |     """
   |
help: Replace with `list`

C405 Unnecessary list literal (rewrite as a set literal)
   --> src/project_a/legacy/modeling.py:115:38
    |
113 |     """
114 |     out = products_sk
115 |     if "category" in out.columns and set(["category"]).issubset(
    |                                      ^^^^^^^^^^^^^^^^^
116 |         set(dim_category.columns)
117 |     ):
    |
help: Rewrite as a set literal

C405 Unnecessary list literal (rewrite as a set literal)
   --> src/project_a/legacy/modeling.py:119:35
    |
117 |     ):
118 |         out = out.join(dim_category, ["category"], "left")
119 |     if "brand" in out.columns and set(["brand"]).issubset(set(dim_brand.columns)):
    |                                   ^^^^^^^^^^^^^^
120 |         out = out.join(dim_brand, ["brand"], "left")
121 |     return out
    |
help: Rewrite as a set literal

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/pipeline/pipeline/run_pipeline.py:23:1
   |
21 |       ]
22 |   """
23 | / import sys
24 | | import argparse
25 | | import logging
26 | | from pathlib import Path
27 | | from typing import Dict, Callable, Any
   | |______________________________________^
28 |
29 |   # Add src to path for imports
   |
help: Organize imports

UP035 [*] Import from `collections.abc` instead: `Callable`
  --> src/project_a/legacy/pipeline/pipeline/run_pipeline.py:27:1
   |
25 | import logging
26 | from pathlib import Path
27 | from typing import Dict, Callable, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
28 |
29 | # Add src to path for imports
   |
help: Import from `collections.abc`

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/legacy/pipeline/pipeline/run_pipeline.py:27:1
   |
25 | import logging
26 | from pathlib import Path
27 | from typing import Dict, Callable, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
28 |
29 | # Add src to path for imports
   |

F401 [*] `typing.Any` imported but unused
  --> src/project_a/legacy/pipeline/pipeline/run_pipeline.py:27:36
   |
25 | import logging
26 | from pathlib import Path
27 | from typing import Dict, Callable, Any
   |                                    ^^^
28 |
29 | # Add src to path for imports
   |
help: Remove unused import: `typing.Any`

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/pipeline/pipeline/run_pipeline.py:37:5
   |
35 |   try:
36 |       # Bronze ingestion jobs
37 | /     from jobs.ingest.fx_json_to_bronze import main as fx_json_to_bronze_main
38 | |     
39 | |     # Transform jobs
40 | |     from jobs.transform.bronze_to_silver import main as bronze_to_silver_main
41 | |     from jobs.gold.silver_to_gold import main as silver_to_gold_main
42 | |     
43 | |     # Publish jobs
44 | |     from jobs.publish_gold_to_snowflake import main as publish_gold_to_snowflake_main
   | |_____________________________________________________________________________________^
45 |       
46 |   except ImportError as e:
   |
help: Organize imports

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/pipeline/pipeline/run_pipeline.py:38:1
   |
36 |     # Bronze ingestion jobs
37 |     from jobs.ingest.fx_json_to_bronze import main as fx_json_to_bronze_main
38 |     
   | ^^^^
39 |     # Transform jobs
40 |     from jobs.transform.bronze_to_silver import main as bronze_to_silver_main
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/pipeline/pipeline/run_pipeline.py:42:1
   |
40 |     from jobs.transform.bronze_to_silver import main as bronze_to_silver_main
41 |     from jobs.gold.silver_to_gold import main as silver_to_gold_main
42 |     
   | ^^^^
43 |     # Publish jobs
44 |     from jobs.publish_gold_to_snowflake import main as publish_gold_to_snowflake_main
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/pipeline/pipeline/run_pipeline.py:45:1
   |
43 |     # Publish jobs
44 |     from jobs.publish_gold_to_snowflake import main as publish_gold_to_snowflake_main
45 |     
   | ^^^^
46 | except ImportError as e:
47 |     logger.warning(f"Some job imports failed: {e}. Continuing with available jobs.")
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/pipeline/pipeline/run_pipeline.py:63:10
   |
62 | # Job dispatcher map
63 | JOB_MAP: Dict[str, Callable] = {
   |          ^^^^
64 |     # Bronze ingestion
65 |     "fx_json_to_bronze": fx_json_to_bronze_main,
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/pipeline/pipeline/run_pipeline.py:66:1
   |
64 |     # Bronze ingestion
65 |     "fx_json_to_bronze": fx_json_to_bronze_main,
66 |     
   | ^^^^
67 |     # Transformations
68 |     "bronze_to_silver": bronze_to_silver_main,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/pipeline/pipeline/run_pipeline.py:70:1
   |
68 |     "bronze_to_silver": bronze_to_silver_main,
69 |     "silver_to_gold": silver_to_gold_main,
70 |     
   | ^^^^
71 |     # Publishing
72 |     "publish_gold_to_snowflake": publish_gold_to_snowflake_main,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/pipeline/pipeline/run_pipeline.py:97:1
   |
95 |         """
96 |     )
97 |     
   | ^^^^
98 |     parser.add_argument(
99 |         "--job",
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/pipeline/pipeline/run_pipeline.py:120:1
    |
118 |         help="Processing date (YYYY-MM-DD). Defaults to today."
119 |     )
120 |     
    | ^^^^
121 |     return parser.parse_args()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/pipeline/pipeline/run_pipeline.py:127:1
    |
125 |     """Main entry point - dispatches to appropriate job."""
126 |     args = parse_args()
127 |     
    | ^^^^
128 |     logging.basicConfig(
129 |         level=logging.INFO,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/pipeline/pipeline/run_pipeline.py:132:1
    |
130 |         format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
131 |     )
132 |     
    | ^^^^
133 |     logger.info(f"ðŸš€ Starting job: {args.job} (env={args.env}, config={args.config})")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/pipeline/pipeline/run_pipeline.py:134:1
    |
133 |     logger.info(f"ðŸš€ Starting job: {args.job} (env={args.env}, config={args.config})")
134 |     
    | ^^^^
135 |     if args.job not in JOB_MAP:
136 |         logger.error(f"âŒ Unknown job: {args.job}. Available: {list(JOB_MAP.keys())}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/pipeline/pipeline/run_pipeline.py:138:1
    |
136 |         logger.error(f"âŒ Unknown job: {args.job}. Available: {list(JOB_MAP.keys())}")
137 |         sys.exit(1)
138 |     
    | ^^^^
139 |     job_func = JOB_MAP[args.job]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/pipeline/pipeline/run_pipeline.py:140:1
    |
139 |     job_func = JOB_MAP[args.job]
140 |     
    | ^^^^
141 |     try:
142 |         # Call job with appropriate arguments
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/pipeline/pipeline/run_pipeline.py:145:1
    |
143 |         # Most jobs expect: --env, --config, optionally --run-date
144 |         import sys as job_sys
145 |         
    | ^^^^^^^^
146 |         # Build argument list for job
147 |         job_args = ["--env", args.env]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/pipeline/pipeline/run_pipeline.py:152:1
    |
150 |         if args.run_date:
151 |             job_args.extend(["--run-date", args.run_date])
152 |         
    | ^^^^^^^^
153 |         # Temporarily replace sys.argv for job execution
154 |         original_argv = job_sys.argv
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/pipeline/pipeline/run_pipeline.py:156:1
    |
154 |         original_argv = job_sys.argv
155 |         job_sys.argv = [args.job] + job_args
156 |         
    | ^^^^^^^^
157 |         try:
158 |             exit_code = job_func()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/pipeline/pipeline/run_pipeline.py:165:1
    |
163 |         finally:
164 |             job_sys.argv = original_argv
165 |             
    | ^^^^^^^^^^^^
166 |     except Exception as e:
167 |         logger.error(f"âŒ Job {args.job} failed: {e}", exc_info=True)
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/pipeline/scd2_customers.py:5:1
   |
 3 |   """
 4 |
 5 | / import logging
 6 | | from typing import Dict, Any
 7 | | from pyspark.sql import DataFrame, SparkSession
 8 | | from pyspark.sql.functions import col, current_timestamp, lit, when, coalesce, max as spark_max
   | |_______________________________________________________________________________________________^
 9 |
10 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
 --> src/project_a/legacy/pipeline/scd2_customers.py:6:1
  |
5 | import logging
6 | from typing import Dict, Any
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
7 | from pyspark.sql import DataFrame, SparkSession
8 | from pyspark.sql.functions import col, current_timestamp, lit, when, coalesce, max as spark_max
  |

F401 [*] `typing.Dict` imported but unused
 --> src/project_a/legacy/pipeline/scd2_customers.py:6:20
  |
5 | import logging
6 | from typing import Dict, Any
  |                    ^^^^
7 | from pyspark.sql import DataFrame, SparkSession
8 | from pyspark.sql.functions import col, current_timestamp, lit, when, coalesce, max as spark_max
  |
help: Remove unused import

F401 [*] `typing.Any` imported but unused
 --> src/project_a/legacy/pipeline/scd2_customers.py:6:26
  |
5 | import logging
6 | from typing import Dict, Any
  |                          ^^^
7 | from pyspark.sql import DataFrame, SparkSession
8 | from pyspark.sql.functions import col, current_timestamp, lit, when, coalesce, max as spark_max
  |
help: Remove unused import

F401 [*] `pyspark.sql.functions.when` imported but unused
  --> src/project_a/legacy/pipeline/scd2_customers.py:8:64
   |
 6 | from typing import Dict, Any
 7 | from pyspark.sql import DataFrame, SparkSession
 8 | from pyspark.sql.functions import col, current_timestamp, lit, when, coalesce, max as spark_max
   |                                                                ^^^^
 9 |
10 | logger = logging.getLogger(__name__)
   |
help: Remove unused import

F401 [*] `pyspark.sql.functions.coalesce` imported but unused
  --> src/project_a/legacy/pipeline/scd2_customers.py:8:70
   |
 6 | from typing import Dict, Any
 7 | from pyspark.sql import DataFrame, SparkSession
 8 | from pyspark.sql.functions import col, current_timestamp, lit, when, coalesce, max as spark_max
   |                                                                      ^^^^^^^^
 9 |
10 | logger = logging.getLogger(__name__)
   |
help: Remove unused import

F401 [*] `pyspark.sql.functions.max` imported but unused
  --> src/project_a/legacy/pipeline/scd2_customers.py:8:87
   |
 6 | from typing import Dict, Any
 7 | from pyspark.sql import DataFrame, SparkSession
 8 | from pyspark.sql.functions import col, current_timestamp, lit, when, coalesce, max as spark_max
   |                                                                                       ^^^^^^^^^
 9 |
10 | logger = logging.getLogger(__name__)
   |
help: Remove unused import

W291 [*] Trailing whitespace
  --> src/project_a/legacy/pipeline/scd2_customers.py:14:25
   |
13 | def scd2_merge_customers(
14 |     spark: SparkSession, 
   |                         ^
15 |     new_customers: DataFrame, 
16 |     existing_customers_path: str
   |
help: Remove trailing whitespace

W291 [*] Trailing whitespace
  --> src/project_a/legacy/pipeline/scd2_customers.py:15:30
   |
13 | def scd2_merge_customers(
14 |     spark: SparkSession, 
15 |     new_customers: DataFrame, 
   |                              ^
16 |     existing_customers_path: str
17 | ) -> DataFrame:
   |
help: Remove trailing whitespace

W293 Blank line contains whitespace
  --> src/project_a/legacy/pipeline/scd2_customers.py:20:1
   |
18 |     """
19 |     Perform SCD Type 2 merge for customers dimension.
20 |     
   | ^^^^
21 |     Args:
22 |         spark: Spark session
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/pipeline/scd2_customers.py:25:1
   |
23 |         new_customers: New customer data
24 |         existing_customers_path: Path to existing customer dimension
25 |         
   | ^^^^^^^^
26 |     Returns:
27 |         Updated customer dimension with SCD2 fields
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/pipeline/scd2_customers.py:30:1
   |
28 |     """
29 |     logger.info("Starting SCD2 merge for customers")
30 |     
   | ^^^^
31 |     try:
32 |         # Read existing customers if they exist
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/pipeline/scd2_customers.py:39:1
   |
37 |         logger.info("No existing customers table found, creating new one")
38 |         existing_customers = spark.createDataFrame([], new_customers.schema)
39 |     
   | ^^^^
40 |     # Add SCD2 fields to new data
41 |     new_customers_with_scd2 = new_customers.withColumn("start_ts", current_timestamp()) \
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/pipeline/scd2_customers.py:44:1
   |
42 |                                           .withColumn("end_ts", lit(None).cast("timestamp")) \
43 |                                           .withColumn("is_current", lit(True))
44 |     
   | ^^^^
45 |     # Perform Delta MERGE for SCD2
46 |     if existing_customers.count() > 0:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/pipeline/scd2_customers.py:90:1
   |
88 |             )
89 |         )
90 |         
   | ^^^^^^^^
91 |         # Close existing records for changed customers
92 |         changed_customers = existing_customers.alias("existing").join(
   |
help: Remove whitespace from blank line

E712 Avoid equality comparisons to `True`; use `col("existing.is_current"):` for truth checks
  --> src/project_a/legacy/pipeline/scd2_customers.py:95:14
   |
93 |             new_customers_with_scd2.alias("new"),
94 |             (col("existing.customer_id") == col("new.customer_id")) &
95 |             (col("existing.is_current") == True),
   |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
96 |             "inner"
97 |         ).select(
   |
help: Replace with `col("existing.is_current")`

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/pipeline/scd2_customers.py:115:1
    |
113 |             lit(False).alias("is_current")
114 |         )
115 |         
    | ^^^^^^^^
116 |         # Combine all records
117 |         final_customers = updated_customers.union(changed_customers)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/pipeline/scd2_customers.py:118:1
    |
116 |         # Combine all records
117 |         final_customers = updated_customers.union(changed_customers)
118 |         
    | ^^^^^^^^
119 |     else:
120 |         # No existing data, just use new data
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/pipeline/scd2_customers.py:122:1
    |
120 |         # No existing data, just use new data
121 |         final_customers = new_customers_with_scd2
122 |     
    | ^^^^
123 |     logger.info(f"SCD2 merge completed. Total rows: {final_customers.count()}")
124 |     return final_customers
    |
help: Remove whitespace from blank line

UP035 `typing.Dict` is deprecated, use `dict` instead
 --> src/project_a/legacy/pipeline_core.py:1:1
  |
1 | from typing import Dict
  | ^^^^^^^^^^^^^^^^^^^^^^^
2 | from pyspark.sql import DataFrame
3 | from .io.path_resolver import resolve
  |

I001 [*] Import block is un-sorted or un-formatted
 --> src/project_a/legacy/pipeline_core.py:1:1
  |
1 | / from typing import Dict
2 | | from pyspark.sql import DataFrame
3 | | from .io.path_resolver import resolve
4 | | from .dq.runner import run_yaml_policy
  | |______________________________________^
5 |
6 |   CFG = {}  # Global config, set by main
  |
help: Organize imports

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/pipeline_core.py:13:30
   |
13 | def run_pipeline(spark, cfg: Dict, run_id: str):
   |                              ^^^^
14 |     global CFG
15 |     CFG = cfg
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/pipeline_core.py:16:1
   |
14 |     global CFG
15 |     CFG = cfg
16 |     
   | ^^^^
17 |     # Extract
18 |     customers = spark.read.option("header", True).csv(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/pipeline_core.py:21:1
   |
19 |         resolve("lake://bronze/customers_raw", CFG)
20 |     )
21 |     
   | ^^^^
22 |     # Validate
23 |     dq = run_yaml_policy(customers, key_cols=["customer_id"], required_cols=["first_name"])
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/pipeline_core.py:40:5
   |
38 |       This function orchestrates the entire ETL pipeline execution.
39 |       """
40 | /     import logging
41 | |     from .utils.spark_session import build_spark
42 | |     from .utils.config import load_config
   | |_________________________________________^
43 |       
44 |       # Setup logging
   |
help: Organize imports

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/pipeline_core.py:43:1
   |
41 |     from .utils.spark_session import build_spark
42 |     from .utils.config import load_config
43 |     
   | ^^^^
44 |     # Setup logging
45 |     logging.basicConfig(level=logging.INFO)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/pipeline_core.py:47:1
   |
45 |     logging.basicConfig(level=logging.INFO)
46 |     logger = logging.getLogger(__name__)
47 |     
   | ^^^^
48 |     try:
49 |         # Load configuration
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/pipeline_core.py:51:1
   |
49 |         # Load configuration
50 |         config = load_config()
51 |         
   | ^^^^^^^^
52 |         # Build Spark session
53 |         spark = build_spark(config)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/pipeline_core.py:54:1
   |
52 |         # Build Spark session
53 |         spark = build_spark(config)
54 |         
   | ^^^^^^^^
55 |         # Run pipeline
56 |         run_pipeline(spark, config, "main_pipeline")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/pipeline_core.py:57:1
   |
55 |         # Run pipeline
56 |         run_pipeline(spark, config, "main_pipeline")
57 |         
   | ^^^^^^^^
58 |         logger.info("Pipeline execution completed successfully")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/pipeline_core.py:59:1
   |
58 |         logger.info("Pipeline execution completed successfully")
59 |         
   | ^^^^^^^^
60 |     except Exception as e:
61 |         logger.error(f"Pipeline execution failed: {str(e)}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/pipeline_core.py:63:1
   |
61 |         logger.error(f"Pipeline execution failed: {str(e)}")
62 |         raise
63 |     
   | ^^^^
64 |     finally:
65 |         if 'spark' in locals():
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/schema/validator.py:5:1
   |
 3 |   """
 4 |
 5 | / import json
 6 | | import logging
 7 | | from typing import Dict, Any, List, Optional
 8 | | from pathlib import Path
 9 | | from pyspark.sql import DataFrame, SparkSession
10 | | from pyspark.sql.types import StructType, StructField, DataType
   | |_______________________________________________________________^
11 |
12 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
 --> src/project_a/legacy/schema/validator.py:7:1
  |
5 | import json
6 | import logging
7 | from typing import Dict, Any, List, Optional
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
8 | from pathlib import Path
9 | from pyspark.sql import DataFrame, SparkSession
  |

UP035 `typing.List` is deprecated, use `list` instead
 --> src/project_a/legacy/schema/validator.py:7:1
  |
5 | import json
6 | import logging
7 | from typing import Dict, Any, List, Optional
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
8 | from pathlib import Path
9 | from pyspark.sql import DataFrame, SparkSession
  |

F401 [*] `typing.Dict` imported but unused
 --> src/project_a/legacy/schema/validator.py:7:20
  |
5 | import json
6 | import logging
7 | from typing import Dict, Any, List, Optional
  |                    ^^^^
8 | from pathlib import Path
9 | from pyspark.sql import DataFrame, SparkSession
  |
help: Remove unused import

F401 [*] `typing.Any` imported but unused
 --> src/project_a/legacy/schema/validator.py:7:26
  |
5 | import json
6 | import logging
7 | from typing import Dict, Any, List, Optional
  |                          ^^^
8 | from pathlib import Path
9 | from pyspark.sql import DataFrame, SparkSession
  |
help: Remove unused import

F401 [*] `typing.List` imported but unused
 --> src/project_a/legacy/schema/validator.py:7:31
  |
5 | import json
6 | import logging
7 | from typing import Dict, Any, List, Optional
  |                               ^^^^
8 | from pathlib import Path
9 | from pyspark.sql import DataFrame, SparkSession
  |
help: Remove unused import

F401 [*] `pathlib.Path` imported but unused
  --> src/project_a/legacy/schema/validator.py:8:21
   |
 6 | import logging
 7 | from typing import Dict, Any, List, Optional
 8 | from pathlib import Path
   |                     ^^^^
 9 | from pyspark.sql import DataFrame, SparkSession
10 | from pyspark.sql.types import StructType, StructField, DataType
   |
help: Remove unused import: `pathlib.Path`

F401 [*] `pyspark.sql.types.StructField` imported but unused
  --> src/project_a/legacy/schema/validator.py:10:43
   |
 8 | from pathlib import Path
 9 | from pyspark.sql import DataFrame, SparkSession
10 | from pyspark.sql.types import StructType, StructField, DataType
   |                                           ^^^^^^^^^^^
11 |
12 | logger = logging.getLogger(__name__)
   |
help: Remove unused import

F401 [*] `pyspark.sql.types.DataType` imported but unused
  --> src/project_a/legacy/schema/validator.py:10:56
   |
 8 | from pathlib import Path
 9 | from pyspark.sql import DataFrame, SparkSession
10 | from pyspark.sql.types import StructType, StructField, DataType
   |                                                        ^^^^^^^^
11 |
12 | logger = logging.getLogger(__name__)
   |
help: Remove unused import

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/schema/validator.py:17:1
   |
15 | class SchemaValidator:
16 |     """Schema validator for data lakehouse layers."""
17 |     
   | ^^^^
18 |     def __init__(self, spark: SparkSession):
19 |         self.spark = spark
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/schema/validator.py:20:1
   |
18 |     def __init__(self, spark: SparkSession):
19 |         self.spark = spark
20 |     
   | ^^^^
21 |     def validate_bronze_schema(self, df: DataFrame, table_name: str) -> bool:
22 |         """
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/schema/validator.py:24:1
   |
22 |         """
23 |         Bronze layer: permissive validation (log only).
24 |         
   | ^^^^^^^^
25 |         Args:
26 |             df: DataFrame to validate
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/schema/validator.py:28:1
   |
26 |             df: DataFrame to validate
27 |             table_name: Name of the table
28 |             
   | ^^^^^^^^^^^^
29 |         Returns:
30 |             Always True (permissive)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/schema/validator.py:35:1
   |
33 |         logger.info(f"Schema: {df.schema}")
34 |         return True
35 |     
   | ^^^^
36 |     def validate_silver_schema(self, df: DataFrame, table_name: str, existing_schema_path: Optional[str] = None) -> bool:
37 |         """
   |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/schema/validator.py:36:92
   |
34 |         return True
35 |     
36 |     def validate_silver_schema(self, df: DataFrame, table_name: str, existing_schema_path: Optional[str] = None) -> bool:
   |                                                                                            ^^^^^^^^^^^^^
37 |         """
38 |         Silver layer: must match previous schema (allow additive nullable columns).
   |
help: Convert to `X | None`

W293 Blank line contains whitespace
  --> src/project_a/legacy/schema/validator.py:39:1
   |
37 |         """
38 |         Silver layer: must match previous schema (allow additive nullable columns).
39 |         
   | ^^^^^^^^
40 |         Args:
41 |             df: DataFrame to validate
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/schema/validator.py:44:1
   |
42 |             table_name: Name of the table
43 |             existing_schema_path: Path to existing schema file
44 |             
   | ^^^^^^^^^^^^
45 |         Returns:
46 |             True if schema is compatible, False otherwise
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/schema/validator.py:49:1
   |
47 |         """
48 |         logger.info(f"Silver schema validation for {table_name}")
49 |         
   | ^^^^^^^^
50 |         if not existing_schema_path:
51 |             logger.info("No existing schema found, accepting new schema")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/schema/validator.py:53:1
   |
51 |             logger.info("No existing schema found, accepting new schema")
52 |             return True
53 |         
   | ^^^^^^^^
54 |         try:
55 |             # Load existing schema
   |
help: Remove whitespace from blank line

UP015 [*] Unnecessary mode argument
  --> src/project_a/legacy/schema/validator.py:56:45
   |
54 |         try:
55 |             # Load existing schema
56 |             with open(existing_schema_path, 'r') as f:
   |                                             ^^^
57 |                 existing_schema_dict = json.load(f)
   |
help: Remove mode argument

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/schema/validator.py:58:1
   |
56 |             with open(existing_schema_path, 'r') as f:
57 |                 existing_schema_dict = json.load(f)
58 |             
   | ^^^^^^^^^^^^
59 |             existing_schema = StructType.fromJson(existing_schema_dict)
60 |             current_schema = df.schema
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/schema/validator.py:61:1
   |
59 |             existing_schema = StructType.fromJson(existing_schema_dict)
60 |             current_schema = df.schema
61 |             
   | ^^^^^^^^^^^^
62 |             # Check compatibility
63 |             compatible = self._is_schema_compatible(existing_schema, current_schema)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/schema/validator.py:64:1
   |
62 |             # Check compatibility
63 |             compatible = self._is_schema_compatible(existing_schema, current_schema)
64 |             
   | ^^^^^^^^^^^^
65 |             if compatible:
66 |                 logger.info("Schema is compatible")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/schema/validator.py:69:1
   |
67 |             else:
68 |                 logger.error("Schema is not compatible - breaking changes detected")
69 |             
   | ^^^^^^^^^^^^
70 |             return compatible
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/schema/validator.py:71:1
   |
70 |             return compatible
71 |             
   | ^^^^^^^^^^^^
72 |         except Exception as e:
73 |             logger.error(f"Schema validation failed: {e}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/schema/validator.py:75:1
   |
73 |             logger.error(f"Schema validation failed: {e}")
74 |             return False
75 |     
   | ^^^^
76 |     def validate_gold_schema(self, df: DataFrame, table_name: str, contract_path: str) -> bool:
77 |         """
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/schema/validator.py:79:1
   |
77 |         """
78 |         Gold layer: enforce strict contract from JSON schema.
79 |         
   | ^^^^^^^^
80 |         Args:
81 |             df: DataFrame to validate
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/schema/validator.py:84:1
   |
82 |             table_name: Name of the table
83 |             contract_path: Path to schema contract JSON
84 |             
   | ^^^^^^^^^^^^
85 |         Returns:
86 |             True if schema matches contract, False otherwise
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/schema/validator.py:89:1
   |
87 |         """
88 |         logger.info(f"Gold schema validation for {table_name}")
89 |         
   | ^^^^^^^^
90 |         try:
91 |             # Load contract schema
   |
help: Remove whitespace from blank line

UP015 [*] Unnecessary mode argument
  --> src/project_a/legacy/schema/validator.py:92:38
   |
90 |         try:
91 |             # Load contract schema
92 |             with open(contract_path, 'r') as f:
   |                                      ^^^
93 |                 contract_schema_dict = json.load(f)
   |
help: Remove mode argument

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/schema/validator.py:94:1
   |
92 |             with open(contract_path, 'r') as f:
93 |                 contract_schema_dict = json.load(f)
94 |             
   | ^^^^^^^^^^^^
95 |             contract_schema = StructType.fromJson(contract_schema_dict)
96 |             current_schema = df.schema
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/schema/validator.py:97:1
   |
95 |             contract_schema = StructType.fromJson(contract_schema_dict)
96 |             current_schema = df.schema
97 |             
   | ^^^^^^^^^^^^
98 |             # Strict validation - schemas must match exactly
99 |             matches = self._schemas_match_exactly(contract_schema, current_schema)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/schema/validator.py:100:1
    |
 98 |             # Strict validation - schemas must match exactly
 99 |             matches = self._schemas_match_exactly(contract_schema, current_schema)
100 |             
    | ^^^^^^^^^^^^
101 |             if matches:
102 |                 logger.info("Schema matches contract")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/schema/validator.py:106:1
    |
104 |                 logger.error("Schema does not match contract")
105 |                 self._log_schema_differences(contract_schema, current_schema)
106 |             
    | ^^^^^^^^^^^^
107 |             return matches
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/schema/validator.py:108:1
    |
107 |             return matches
108 |             
    | ^^^^^^^^^^^^
109 |         except Exception as e:
110 |             logger.error(f"Contract validation failed: {e}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/schema/validator.py:112:1
    |
110 |             logger.error(f"Contract validation failed: {e}")
111 |             return False
112 |     
    | ^^^^
113 |     def _is_schema_compatible(self, existing: StructType, current: StructType) -> bool:
114 |         """Check if current schema is compatible with existing (additive changes only)."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/schema/validator.py:117:1
    |
115 |         existing_fields = {f.name: f for f in existing.fields}
116 |         current_fields = {f.name: f for f in current.fields}
117 |         
    | ^^^^^^^^
118 |         # Check for removed fields
119 |         removed_fields = set(existing_fields.keys()) - set(current_fields.keys())
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/schema/validator.py:123:1
    |
121 |             logger.error(f"Removed fields detected: {removed_fields}")
122 |             return False
123 |         
    | ^^^^^^^^
124 |         # Check for type changes
125 |         for field_name in existing_fields:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/schema/validator.py:129:1
    |
127 |                 existing_field = existing_fields[field_name]
128 |                 current_field = current_fields[field_name]
129 |                 
    | ^^^^^^^^^^^^^^^^
130 |                 if existing_field.dataType != current_field.dataType:
131 |                     logger.error(f"Type change detected for {field_name}: "
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/schema/validator.py:134:1
    |
132 |                                f"{existing_field.dataType} -> {current_field.dataType}")
133 |                     return False
134 |         
    | ^^^^^^^^
135 |         # New fields are allowed if they are nullable
136 |         new_fields = set(current_fields.keys()) - set(existing_fields.keys())
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/schema/validator.py:144:1
    |
142 |                 logger.error(f"New non-nullable field added: {field_name}")
143 |                 return False
144 |         
    | ^^^^^^^^
145 |         return True
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/schema/validator.py:146:1
    |
145 |         return True
146 |     
    | ^^^^
147 |     def _schemas_match_exactly(self, contract: StructType, current: StructType) -> bool:
148 |         """Check if schemas match exactly."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/schema/validator.py:150:1
    |
148 |         """Check if schemas match exactly."""
149 |         return contract == current
150 |     
    | ^^^^
151 |     def _log_schema_differences(self, contract: StructType, current: StructType) -> None:
152 |         """Log differences between contract and current schema."""
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
 --> src/project_a/legacy/schemas/returns_raw.py:1:1
  |
1 | / from pyspark.sql.types import (StructType, StructField, StringType, IntegerType,
2 | |                                DoubleType, TimestampType, DateType)
  | |___________________________________________________________________^
3 |
4 |   # Adjust fields to your real schema if different
  |
help: Organize imports

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/streaming/kafka_customer_events.py:6:1
   |
 4 |   Subscribes to Kafka topic and writes customer events to S3 bronze layer.
 5 |   """
 6 | / import os
 7 | | import logging
 8 | | from typing import Optional, Dict, Any
 9 | | from pyspark.sql import SparkSession
10 | | from pyspark.sql.functions import col, from_json
11 | | from pyspark.sql.types import (
12 | |     StructType, StructField, StringType, TimestampType, DoubleType
13 | | )
   | |_^
14 |
15 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/legacy/streaming/kafka_customer_events.py:8:1
   |
 6 | import os
 7 | import logging
 8 | from typing import Optional, Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 9 | from pyspark.sql import SparkSession
10 | from pyspark.sql.functions import col, from_json
   |

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/streaming/kafka_customer_events.py:18:23
   |
18 | def get_spark(config: Optional[Dict[str, Any]] = None):
   |                       ^^^^^^^^^^^^^^^^^^^^^^^^
19 |     """Get or create Spark session with streaming support."""
20 |     checkpoint_location = (
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/streaming/kafka_customer_events.py:18:32
   |
18 | def get_spark(config: Optional[Dict[str, Any]] = None):
   |                                ^^^^
19 |     """Get or create Spark session with streaming support."""
20 |     checkpoint_location = (
   |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/streaming/kafka_customer_events.py:33:18
   |
33 | def main(config: Optional[Dict[str, Any]] = None):
   |                  ^^^^^^^^^^^^^^^^^^^^^^^^
34 |     """Stream customer events from Kafka to Bronze S3."""
35 |     spark = get_spark(config)
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/streaming/kafka_customer_events.py:33:27
   |
33 | def main(config: Optional[Dict[str, Any]] = None):
   |                           ^^^^
34 |     """Stream customer events from Kafka to Bronze S3."""
35 |     spark = get_spark(config)
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/streaming/kafka_customer_events.py:36:1
   |
34 |     """Stream customer events from Kafka to Bronze S3."""
35 |     spark = get_spark(config)
36 |     
   | ^^^^
37 |     # Schema for customer events
38 |     schema = StructType([
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/streaming/kafka_customer_events.py:51:1
   |
49 |         StructField("revenue", DoubleType()),
50 |     ])
51 |     
   | ^^^^
52 |     # Configuration
53 |     kafka_brokers = os.getenv("KAFKA_BROKERS", "localhost:9092")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/streaming/kafka_customer_events.py:61:1
   |
59 |     )
60 |     topic = os.getenv("KAFKA_TOPIC", "customer_events")
61 |     
   | ^^^^
62 |     logger.info(f"Connecting to Kafka: {kafka_brokers}, topic: {topic}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/streaming/kafka_customer_events.py:63:1
   |
62 |     logger.info(f"Connecting to Kafka: {kafka_brokers}, topic: {topic}")
63 |     
   | ^^^^
64 |     # Read stream from Kafka
65 |     df = (spark.readStream
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/streaming/kafka_customer_events.py:72:1
   |
70 |         .option("failOnDataLoss", "false")  # Don't fail on data loss
71 |         .load())
72 |     
   | ^^^^
73 |     logger.info("Kafka stream connected")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/streaming/kafka_customer_events.py:74:1
   |
73 |     logger.info("Kafka stream connected")
74 |     
   | ^^^^
75 |     # Parse JSON and extract fields
76 |     json_df = (df
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/streaming/kafka_customer_events.py:82:1
   |
80 |         .filter(col("event_id").isNotNull())  # Filter out invalid records
81 |     )
82 |     
   | ^^^^
83 |     # Write stream to S3 Bronze with checkpointing
84 |     query = (json_df.writeStream
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/streaming/kafka_customer_events.py:91:1
   |
89 |         .partitionBy("event_ts")  # Partition by timestamp for performance
90 |         .start())
91 |     
   | ^^^^
92 |     logger.info("Stream started, writing to bronze/customer_events_stream/")
93 |     logger.info("Press Ctrl+C to stop")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/streaming/kafka_customer_events.py:94:1
   |
92 |     logger.info("Stream started, writing to bronze/customer_events_stream/")
93 |     logger.info("Press Ctrl+C to stop")
94 |     
   | ^^^^
95 |     # Wait for termination (or trigger.stop() in production)
96 |     query.awaitTermination()
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
 --> src/project_a/legacy/streaming_core.py:1:1
  |
1 | / import logging
2 | | from pyspark.sql import functions as F
3 | | from pyspark.sql import DataFrame
4 | | from .io.path_resolver import resolve
  | |_____________________________________^
  |
help: Organize imports

W293 Blank line contains whitespace
  --> src/project_a/legacy/streaming_core.py:15:1
   |
13 |     """
14 |     Write streaming data with enforced checkpoint and DLQ handling.
15 |     
   | ^^^^
16 |     Args:
17 |         df: Input DataFrame
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/streaming_core.py:21:1
   |
19 |         table_path_logical: Logical path for output table
20 |         checkpoint_logical: Logical path for checkpoint location
21 |     
   | ^^^^
22 |     Returns:
23 |         StreamingQuery object
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/streaming_core.py:28:1
   |
26 |     if "streaming" not in cfg:
27 |         raise ValueError("Streaming configuration is required")
28 |     
   | ^^^^
29 |     streaming_cfg = cfg["streaming"]
30 |     watermark = streaming_cfg.get("watermark", "1 hour")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/streaming_core.py:32:1
   |
30 |     watermark = streaming_cfg.get("watermark", "1 hour")
31 |     dlq_logical = streaming_cfg.get("dlq_path")
32 |     
   | ^^^^
33 |     if not dlq_logical:
34 |         raise ValueError("DLQ path is required in streaming configuration")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/streaming_core.py:35:1
   |
33 |     if not dlq_logical:
34 |         raise ValueError("DLQ path is required in streaming configuration")
35 |     
   | ^^^^
36 |     # Resolve paths
37 |     out_path = resolve(table_path_logical, cfg)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/streaming_core.py:40:1
   |
38 |     chk_path = resolve(checkpoint_logical, cfg)
39 |     dlq_path = resolve(dlq_logical, cfg)
40 |     
   | ^^^^
41 |     logger.info(f"Streaming write: {out_path}, checkpoint: {chk_path}, DLQ: {dlq_path}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/streaming_core.py:42:1
   |
41 |     logger.info(f"Streaming write: {out_path}, checkpoint: {chk_path}, DLQ: {dlq_path}")
42 |     
   | ^^^^
43 |     # Validate DataFrame has required columns
44 |     required_cols = ["event_time", "business_key"]
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/streaming_core.py:48:1
   |
46 |     if missing_cols:
47 |         raise ValueError(f"DataFrame missing required columns: {missing_cols}")
48 |     
   | ^^^^
49 |     # Apply watermark and deduplication
50 |     df_ev = df.withWatermark("event_time", watermark)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/streaming_core.py:51:1
   |
49 |     # Apply watermark and deduplication
50 |     df_ev = df.withWatermark("event_time", watermark)
51 |     
   | ^^^^
52 |     # Use dropDuplicatesWithinWatermark for Spark 3.5+ if available
53 |     try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/streaming_core.py:60:1
   |
58 |         df_ev = df_ev.dropDuplicates(["business_key", "event_time"])
59 |         logger.info("Using dropDuplicates for deduplication (fallback)")
60 |     
   | ^^^^
61 |     # Configure streaming write with strict error handling
62 |     query = (
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/streaming_core.py:73:1
   |
71 |         .start(out_path)
72 |     )
73 |     
   | ^^^^
74 |     # Log query details
75 |     logger.info(f"Started streaming query: {query.id}, status: {query.status['message']}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/streaming_core.py:76:1
   |
74 |     # Log query details
75 |     logger.info(f"Started streaming query: {query.id}, status: {query.status['message']}")
76 |     
   | ^^^^
77 |     return query
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/streaming_core.py:83:1
   |
81 |     """
82 |     Create a DLQ handler for malformed/late messages.
83 |     
   | ^^^^
84 |     Args:
85 |         cfg: Configuration dictionary
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/streaming_core.py:87:1
   |
85 |         cfg: Configuration dictionary
86 |         checkpoint_logical: Logical path for checkpoint location
87 |     
   | ^^^^
88 |     Returns:
89 |         Function to handle DLQ messages
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/streaming_core.py:94:1
   |
92 |     dlq_path = resolve(dlq_logical, cfg)
93 |     dlq_chk_path = resolve(checkpoint_logical, cfg) + "/dlq"
94 |     
   | ^^^^
95 |     def handle_dlq(malformed_df: DataFrame, error_reason: str):
96 |         """Handle malformed messages by writing to DLQ"""
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/streaming_core.py:101:1
    |
 99 |             dlq_df = malformed_df.withColumn("dlq_timestamp", F.current_timestamp()) \
100 |                                 .withColumn("error_reason", F.lit(error_reason))
101 |             
    | ^^^^^^^^^^^^
102 |             # Write to DLQ
103 |             dlq_query = (
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/streaming_core.py:111:1
    |
109 |                 .start(dlq_path)
110 |             )
111 |             
    | ^^^^^^^^^^^^
112 |             logger.warning(f"Writing {malformed_df.count()} malformed records to DLQ: {error_reason}")
113 |             return dlq_query
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/streaming_core.py:114:1
    |
112 |             logger.warning(f"Writing {malformed_df.count()} malformed records to DLQ: {error_reason}")
113 |             return dlq_query
114 |         
    | ^^^^^^^^
115 |         return None
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/streaming_core.py:116:1
    |
115 |         return None
116 |     
    | ^^^^
117 |     return handle_dlq
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/streaming_core.py:123:1
    |
121 |     """
122 |     Validate streaming configuration to ensure all required settings are present.
123 |     
    | ^^^^
124 |     Args:
125 |         cfg: Configuration dictionary
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/streaming_core.py:126:1
    |
124 |     Args:
125 |         cfg: Configuration dictionary
126 |     
    | ^^^^
127 |     Raises:
128 |         ValueError: If required configuration is missing
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/streaming_core.py:132:1
    |
130 |     if "streaming" not in cfg:
131 |         raise ValueError("Streaming configuration section is required")
132 |     
    | ^^^^
133 |     streaming_cfg = cfg["streaming"]
134 |     required_keys = ["watermark", "dlq_path"]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/streaming_core.py:136:1
    |
134 |     required_keys = ["watermark", "dlq_path"]
135 |     missing_keys = [key for key in required_keys if key not in streaming_cfg]
136 |     
    | ^^^^
137 |     if missing_keys:
138 |         raise ValueError(f"Missing required streaming configuration keys: {missing_keys}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/streaming_core.py:139:1
    |
137 |     if missing_keys:
138 |         raise ValueError(f"Missing required streaming configuration keys: {missing_keys}")
139 |     
    | ^^^^
140 |     # Validate watermark format
141 |     watermark = streaming_cfg["watermark"]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/streaming_core.py:144:1
    |
142 |     if not isinstance(watermark, str) or not any(unit in watermark for unit in ["hour", "minute", "second", "day"]):
143 |         raise ValueError(f"Invalid watermark format: {watermark}. Expected format like '1 hour', '30 minutes', etc.")
144 |     
    | ^^^^
145 |     logger.info("Streaming configuration validation passed")
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:10:1
   |
 8 |   """
 9 |
10 | / import logging
11 | | from typing import Dict, Any, Optional
12 | | from pyspark.sql import SparkSession, DataFrame, Window
13 | | from pyspark.sql import functions as F
14 | | from project_a.utils.path_resolver import resolve_path
15 | | from project_a.utils.io import read_delta, write_delta
16 | | from project_a.monitoring.lineage_decorator import lineage_job
17 | | from project_a.monitoring.metrics_collector import emit_rowcount, emit_duration
18 | | import time
   | |___________^
19 |
20 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:11:1
   |
10 | import logging
11 | from typing import Dict, Any, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
12 | from pyspark.sql import SparkSession, DataFrame, Window
13 | from pyspark.sql import functions as F
   |

F401 [*] `typing.Optional` imported but unused
  --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:11:31
   |
10 | import logging
11 | from typing import Dict, Any, Optional
   |                               ^^^^^^^^
12 | from pyspark.sql import SparkSession, DataFrame, Window
13 | from pyspark.sql import functions as F
   |
help: Remove unused import: `typing.Optional`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:38:13
   |
36 | def bronze_to_silver_multi_source(
37 |     spark: SparkSession,
38 |     config: Dict[str, Any],
   |             ^^^^
39 |     run_date: str = None
40 | ) -> Dict[str, DataFrame]:
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:40:6
   |
38 |     config: Dict[str, Any],
39 |     run_date: str = None
40 | ) -> Dict[str, DataFrame]:
   |      ^^^^
41 |     """
42 |     Transform bronze data from multiple sources to silver with canonicalization and joins.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:43:1
   |
41 |     """
42 |     Transform bronze data from multiple sources to silver with canonicalization and joins.
43 |     
   | ^^^^
44 |     Args:
45 |         spark: SparkSession
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:48:1
   |
46 |         config: Configuration dict
47 |         run_date: Processing date YYYY-MM-DD
48 |         
   | ^^^^^^^^
49 |     Returns:
50 |         Dictionary with silver DataFrames: {'customers', 'orders', 'customer_activity'}
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:55:1
   |
53 |         from datetime import datetime
54 |         run_date = datetime.now().strftime("%Y-%m-%d")
55 |     
   | ^^^^
56 |     logger.info(f"ðŸš€ Starting multi-source bronze to silver transformation (run_date={run_date})")
57 |     start_time = time.time()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:58:1
   |
56 |     logger.info(f"ðŸš€ Starting multi-source bronze to silver transformation (run_date={run_date})")
57 |     start_time = time.time()
58 |     
   | ^^^^
59 |     # Get bronze paths
60 |     bronze_root = resolve_path("lake://bronze", config=config)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:62:1
   |
60 |     bronze_root = resolve_path("lake://bronze", config=config)
61 |     silver_root = resolve_path("lake://silver", config=config)
62 |     
   | ^^^^
63 |     # 1. READ BRONZE DATA
64 |     logger.info("ðŸ“¥ Reading bronze data from multiple sources...")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:65:1
   |
63 |     # 1. READ BRONZE DATA
64 |     logger.info("ðŸ“¥ Reading bronze data from multiple sources...")
65 |     
   | ^^^^
66 |     orders_bronze = read_delta(spark, f"{bronze_root}/snowflake/orders")
67 |     customers_bronze = read_delta(spark, f"{bronze_root}/snowflake/customers")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:69:1
   |
67 |     customers_bronze = read_delta(spark, f"{bronze_root}/snowflake/customers")
68 |     behavior_bronze = read_delta(spark, f"{bronze_root}/redshift/behavior")
69 |     
   | ^^^^
70 |     logger.info(f"  Orders: {orders_bronze.count():,} rows")
71 |     logger.info(f"  Customers: {customers_bronze.count():,} rows")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:73:1
   |
71 |     logger.info(f"  Customers: {customers_bronze.count():,} rows")
72 |     logger.info(f"  Behavior: {behavior_bronze.count():,} rows")
73 |     
   | ^^^^
74 |     # 2. SILVER CUSTOMERS: Canonicalize + dedupe
75 |     logger.info("ðŸ”§ Creating silver.customers...")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:76:1
   |
74 |     # 2. SILVER CUSTOMERS: Canonicalize + dedupe
75 |     logger.info("ðŸ”§ Creating silver.customers...")
76 |     
   | ^^^^
77 |     silver_customers = customers_bronze \
78 |         .withColumn("effective_ts", F.current_timestamp()) \
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:80:1
   |
78 |         .withColumn("effective_ts", F.current_timestamp()) \
79 |         .withColumn("country", F.coalesce(F.col("country"), F.lit("UNKNOWN")))
80 |     
   | ^^^^
81 |     # Deduplicate by customer_id (keep most recent)
82 |     if "_ingest_ts" in silver_customers.columns:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:90:1
   |
88 |     else:
89 |         silver_customers = silver_customers.dropDuplicates(["customer_id"])
90 |     
   | ^^^^
91 |     logger.info(f"âœ… silver.customers: {silver_customers.count():,} unique customers")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:92:1
   |
91 |     logger.info(f"âœ… silver.customers: {silver_customers.count():,} unique customers")
92 |     
   | ^^^^
93 |     # 3. SILVER ORDERS: Canonicalize + dedupe
94 |     logger.info("ðŸ”§ Creating silver.orders...")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:95:1
   |
93 |     # 3. SILVER ORDERS: Canonicalize + dedupe
94 |     logger.info("ðŸ”§ Creating silver.orders...")
95 |     
   | ^^^^
96 |     silver_orders = orders_bronze \
97 |         .withColumn("order_date", F.to_date(F.coalesce(F.col("order_date"), F.col("event_ts")))) \
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:99:1
    |
 97 |         .withColumn("order_date", F.to_date(F.coalesce(F.col("order_date"), F.col("event_ts")))) \
 98 |         .withColumn("amount_usd", F.coalesce(F.col("total_amount"), F.col("amount"), F.lit(0.0)))
 99 |     
    | ^^^^
100 |     # Deduplicate by order_id
101 |     if "_ingest_ts" in silver_orders.columns:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:109:1
    |
107 |     else:
108 |         silver_orders = silver_orders.dropDuplicates(["order_id"])
109 |     
    | ^^^^
110 |     logger.info(f"âœ… silver.orders: {silver_orders.count():,} unique orders")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:111:1
    |
110 |     logger.info(f"âœ… silver.orders: {silver_orders.count():,} unique orders")
111 |     
    | ^^^^
112 |     # 4. SILVER CUSTOMER ACTIVITY: Canonicalize + dedupe
113 |     logger.info("ðŸ”§ Creating silver.customer_activity...")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:114:1
    |
112 |     # 4. SILVER CUSTOMER ACTIVITY: Canonicalize + dedupe
113 |     logger.info("ðŸ”§ Creating silver.customer_activity...")
114 |     
    | ^^^^
115 |     silver_customer_activity = behavior_bronze \
116 |         .withColumn("event_date", F.to_date(F.coalesce(F.col("event_ts"), F.col("timestamp")))) \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:119:1
    |
117 |         .withColumn("event_name", F.coalesce(F.col("event_name"), F.col("event_type"))) \
118 |         .withColumn("session_id", F.coalesce(F.col("session_id"), F.col("session_id")))
119 |     
    | ^^^^
120 |     # Deduplicate by event_id or composite key
121 |     if "event_id" in silver_customer_activity.columns:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:136:1
    |
134 |         if existing_key_cols:
135 |             silver_customer_activity = silver_customer_activity.dropDuplicates(existing_key_cols)
136 |     
    | ^^^^
137 |     logger.info(f"âœ… silver.customer_activity: {silver_customer_activity.count():,} unique events")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:138:1
    |
137 |     logger.info(f"âœ… silver.customer_activity: {silver_customer_activity.count():,} unique events")
138 |     
    | ^^^^
139 |     # 5. WRITE SILVER TABLES (partitioned by date)
140 |     logger.info("ðŸ’¾ Writing silver tables...")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:141:1
    |
139 |     # 5. WRITE SILVER TABLES (partitioned by date)
140 |     logger.info("ðŸ’¾ Writing silver tables...")
141 |     
    | ^^^^
142 |     write_delta(
143 |         silver_customers,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:148:1
    |
146 |         partitionBy=["country"]
147 |     )
148 |     
    | ^^^^
149 |     write_delta(
150 |         silver_orders,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:155:1
    |
153 |         partitionBy=["order_date"]
154 |     )
155 |     
    | ^^^^
156 |     write_delta(
157 |         silver_customer_activity,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:162:1
    |
160 |         partitionBy=["event_date"]
161 |     )
162 |     
    | ^^^^
163 |     # 6. METRICS
164 |     duration_ms = (time.time() - start_time) * 1000
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:165:1
    |
163 |     # 6. METRICS
164 |     duration_ms = (time.time() - start_time) * 1000
165 |     
    | ^^^^
166 |     emit_rowcount("silver_customers_total", silver_customers.count(), {
167 |         "layer": "silver",
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:170:1
    |
168 |         "table": "customers"
169 |     }, config)
170 |     
    | ^^^^
171 |     emit_rowcount("silver_orders_total", silver_orders.count(), {
172 |         "layer": "silver",
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:175:1
    |
173 |         "table": "orders"
174 |     }, config)
175 |     
    | ^^^^
176 |     emit_rowcount("silver_activity_total", silver_customer_activity.count(), {
177 |         "layer": "silver",
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:180:1
    |
178 |         "table": "customer_activity"
179 |     }, config)
180 |     
    | ^^^^
181 |     emit_duration("silver_transformation_duration", duration_ms, {
182 |         "stage": "bronze_to_silver"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:184:1
    |
182 |         "stage": "bronze_to_silver"
183 |     }, config)
184 |     
    | ^^^^
185 |     logger.info(f"âœ… Multi-source silver transformation complete in {duration_ms:.0f}ms")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:186:1
    |
185 |     logger.info(f"âœ… Multi-source silver transformation complete in {duration_ms:.0f}ms")
186 |     
    | ^^^^
187 |     return {
188 |         "customers": silver_customers,
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:202:1
    |
200 |     """
201 |     Join silver sources on customer_id to create unified customer view.
202 |     
    | ^^^^
203 |     Args:
204 |         spark: SparkSession
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:208:1
    |
206 |         silver_orders: Orders silver table
207 |         silver_behavior: Behavior/activity silver table
208 |         
    | ^^^^^^^^
209 |     Returns:
210 |         Joined DataFrame with all customer-related data
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:213:1
    |
211 |     """
212 |     logger.info("ðŸ”— Joining silver sources on customer_id...")
213 |     
    | ^^^^
214 |     # Join orders with customers
215 |     orders_enriched = silver_orders \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:221:1
    |
219 |             how="left"
220 |         )
221 |     
    | ^^^^
222 |     # Join behavior with customers (for customer attributes)
223 |     behavior_enriched = silver_behavior \
    |
help: Remove whitespace from blank line

F841 Local variable `behavior_enriched` is assigned to but never used
   --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:223:5
    |
222 |     # Join behavior with customers (for customer attributes)
223 |     behavior_enriched = silver_behavior \
    |     ^^^^^^^^^^^^^^^^^
224 |         .join(
225 |             silver_customers.select("customer_id", "country", "email"),
    |
help: Remove assignment to unused variable `behavior_enriched`

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:229:1
    |
227 |             how="left"
228 |         )
229 |     
    | ^^^^
230 |     # Optional: Create fact combining orders + behavior by date
231 |     # This would be done in silver_to_gold typically
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:232:1
    |
230 |     # Optional: Create fact combining orders + behavior by date
231 |     # This would be done in silver_to_gold typically
232 |     
    | ^^^^
233 |     logger.info(f"âœ… Joined data: {orders_enriched.count():,} enriched orders")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/bronze_to_silver_multi_source.py:234:1
    |
233 |     logger.info(f"âœ… Joined data: {orders_enriched.count():,} enriched orders")
234 |     
    | ^^^^
235 |     return orders_enriched
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
 --> src/project_a/legacy/transform/enrich_with_fx.py:2:1
  |
1 |   """Enrich data with FX rates."""
2 | / import logging
3 | | from pyspark.sql import SparkSession, DataFrame
4 | | from pyspark.sql.functions import col, join
  | |___________________________________________^
5 |
6 |   logger = logging.getLogger(__name__)
  |
help: Organize imports

F401 [*] `pyspark.sql.functions.join` imported but unused
 --> src/project_a/legacy/transform/enrich_with_fx.py:4:40
  |
2 | import logging
3 | from pyspark.sql import SparkSession, DataFrame
4 | from pyspark.sql.functions import col, join
  |                                        ^^^^
5 |
6 | logger = logging.getLogger(__name__)
  |
help: Remove unused import: `pyspark.sql.functions.join`

W293 Blank line contains whitespace
  --> src/project_a/legacy/transform/enrich_with_fx.py:17:1
   |
15 |     """
16 |     Enrich orders/data with FX rates for currency conversion.
17 |     
   | ^^^^
18 |     Args:
19 |         spark: SparkSession object
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/transform/enrich_with_fx.py:23:1
   |
21 |         fx_rates_df: FX rates DataFrame
22 |         **kwargs: Additional arguments
23 |         
   | ^^^^^^^^
24 |     Returns:
25 |         Enriched DataFrame with FX conversions
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/transform/enrich_with_fx.py:28:1
   |
26 |     """
27 |     logger.info("Enriching data with FX rates")
28 |     
   | ^^^^
29 |     try:
30 |         # Join with FX rates on date
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/transform/enrich_with_fx.py:40:1
   |
38 |             # Fallback: add dummy FX columns if no date match
39 |             df_enriched = df
40 |         
   | ^^^^^^^^
41 |         logger.info(f"Successfully enriched {df_enriched.count()} records with FX rates")
42 |         return df_enriched
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/transform/enrich_with_fx.py:43:1
   |
41 |         logger.info(f"Successfully enriched {df_enriched.count()} records with FX rates")
42 |         return df_enriched
43 |         
   | ^^^^^^^^
44 |     except Exception as e:
45 |         logger.error(f"Failed to enrich with FX rates: {e}")
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/transform/incremental_customer_dim_upsert.py:10:1
   |
 8 |   """
 9 |
10 | / import sys
11 | | import logging
12 | | from datetime import datetime
13 | | from typing import Dict, Any, Optional
14 | | from pyspark.sql import SparkSession, DataFrame
15 | | from pyspark.sql.functions import (
16 | |     col, max as spark_max, current_timestamp, to_timestamp,
17 | |     coalesce, lit, when, row_number
18 | | )
19 | | from pyspark.sql.window import Window
20 | | from delta.tables import DeltaTable
   | |___________________________________^
21 |
22 |   # Add project root to path
   |
help: Organize imports

F401 [*] `datetime.datetime` imported but unused
  --> src/project_a/legacy/transform/incremental_customer_dim_upsert.py:12:22
   |
10 | import sys
11 | import logging
12 | from datetime import datetime
   |                      ^^^^^^^^
13 | from typing import Dict, Any, Optional
14 | from pyspark.sql import SparkSession, DataFrame
   |
help: Remove unused import: `datetime.datetime`

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/legacy/transform/incremental_customer_dim_upsert.py:13:1
   |
11 | import logging
12 | from datetime import datetime
13 | from typing import Dict, Any, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
14 | from pyspark.sql import SparkSession, DataFrame
15 | from pyspark.sql.functions import (
   |

F401 [*] `typing.Optional` imported but unused
  --> src/project_a/legacy/transform/incremental_customer_dim_upsert.py:13:31
   |
11 | import logging
12 | from datetime import datetime
13 | from typing import Dict, Any, Optional
   |                               ^^^^^^^^
14 | from pyspark.sql import SparkSession, DataFrame
15 | from pyspark.sql.functions import (
   |
help: Remove unused import: `typing.Optional`

F401 [*] `pyspark.sql.functions.when` imported but unused
  --> src/project_a/legacy/transform/incremental_customer_dim_upsert.py:17:20
   |
15 | from pyspark.sql.functions import (
16 |     col, max as spark_max, current_timestamp, to_timestamp,
17 |     coalesce, lit, when, row_number
   |                    ^^^^
18 | )
19 | from pyspark.sql.window import Window
   |
help: Remove unused import: `pyspark.sql.functions.when`

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/transform/incremental_customer_dim_upsert.py:25:1
   |
23 |   sys.path.insert(0, str(__file__).rsplit('/', 3)[0])
24 |
25 | / from project_a.utils.watermark import load_watermark, save_watermark
26 | | from project_a.utils.spark_session import build_spark
27 | | from project_a.utils.config import load_config
   | |______________________________________________^
28 |
29 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

W293 Blank line contains whitespace
  --> src/project_a/legacy/transform/incremental_customer_dim_upsert.py:35:1
   |
33 |     """
34 |     Deduplicate DataFrame keeping only the latest record per key based on timestamp.
35 |     
   | ^^^^
36 |     Args:
37 |         df: Input DataFrame
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/transform/incremental_customer_dim_upsert.py:40:1
   |
38 |         key_col: Column to use as business key
39 |         ts_col: Timestamp column for determining latest
40 |         
   | ^^^^^^^^
41 |     Returns:
42 |         Deduplicated DataFrame
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/transform/incremental_customer_dim_upsert.py:45:1
   |
43 |     """
44 |     window_spec = Window.partitionBy(key_col).orderBy(col(ts_col).desc())
45 |     
   | ^^^^
46 |     return df.withColumn("_row_num", row_number().over(window_spec)) \
47 |              .filter(col("_row_num") == 1) \
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/transform/incremental_customer_dim_upsert.py:53:13
   |
51 | def incremental_customer_dim_upsert(
52 |     spark: SparkSession,
53 |     config: Dict[str, Any],
   |             ^^^^
54 |     watermark_checkpoint_path: str = "data/checkpoints"
55 | ) -> None:
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/legacy/transform/incremental_customer_dim_upsert.py:58:1
   |
56 |     """
57 |     Perform incremental upsert into customer dimension table using Delta Lake MERGE.
58 |     
   | ^^^^
59 |     Args:
60 |         spark: Spark session
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/transform/incremental_customer_dim_upsert.py:65:1
   |
63 |     """
64 |     logger.info("Starting incremental customer dimension upsert")
65 |     
   | ^^^^
66 |     # Get paths from config
67 |     bronze_base = config.get("lake", {}).get("bronze", {}).get("base", "s3://bucket/bronze")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/transform/incremental_customer_dim_upsert.py:69:1
   |
67 |     bronze_base = config.get("lake", {}).get("bronze", {}).get("base", "s3://bucket/bronze")
68 |     silver_base = config.get("lake", {}).get("silver", {}).get("base", "s3://bucket/silver")
69 |     
   | ^^^^
70 |     bronze_crm_path = f"{bronze_base}/crm/contacts"
71 |     silver_customer_path = f"{silver_base}/dim_customer"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/transform/incremental_customer_dim_upsert.py:72:1
   |
70 |     bronze_crm_path = f"{bronze_base}/crm/contacts"
71 |     silver_customer_path = f"{silver_base}/dim_customer"
72 |     
   | ^^^^
73 |     # Load watermark
74 |     watermark_ts = load_watermark(watermark_checkpoint_path, "customer_dim")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/transform/incremental_customer_dim_upsert.py:76:1
   |
74 |     watermark_ts = load_watermark(watermark_checkpoint_path, "customer_dim")
75 |     logger.info(f"Loaded watermark: {watermark_ts}")
76 |     
   | ^^^^
77 |     # Read new batch from bronze
78 |     logger.info(f"Reading bronze data from: {bronze_crm_path}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/transform/incremental_customer_dim_upsert.py:80:1
   |
78 |     logger.info(f"Reading bronze data from: {bronze_crm_path}")
79 |     bronze_df = spark.read.format("delta").load(bronze_crm_path)
80 |     
   | ^^^^
81 |     # Filter by watermark if exists
82 |     if watermark_ts:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/transform/incremental_customer_dim_upsert.py:87:1
   |
85 |     else:
86 |         logger.info("No watermark found, processing all records")
87 |     
   | ^^^^
88 |     # Deduplicate by customer_id + last_modified_date (assuming these columns exist)
89 |     # Map CRM contact fields to customer dimension structure
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/incremental_customer_dim_upsert.py:104:1
    |
102 |         col("_source_system")
103 |     )
104 |     
    | ^^^^
105 |     # Deduplicate within this batch
106 |     customer_updates = deduplicate_by_latest(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/incremental_customer_dim_upsert.py:111:1
    |
109 |         ts_col="last_modified_date"
110 |     )
111 |     
    | ^^^^
112 |     # Cache the updates since we'll use it multiple times
113 |     customer_updates.cache()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/incremental_customer_dim_upsert.py:116:1
    |
114 |     record_count = customer_updates.count()
115 |     logger.info(f"Prepared {record_count} customer records for merge")
116 |     
    | ^^^^
117 |     # Check if silver table exists
118 |     try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/incremental_customer_dim_upsert.py:125:1
    |
123 |         table_exists = False
124 |         logger.info("Silver customer table does not exist, creating new table")
125 |     
    | ^^^^
126 |     if table_exists:
127 |         # Perform Delta Lake MERGE
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/incremental_customer_dim_upsert.py:129:1
    |
127 |         # Perform Delta Lake MERGE
128 |         merge_condition = "target.customer_id = source.customer_id"
129 |         
    | ^^^^^^^^
130 |         silver_table.alias("target").merge(
131 |             customer_updates.alias("source"),
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/incremental_customer_dim_upsert.py:164:1
    |
162 |             }
163 |         ).execute()
164 |         
    | ^^^^^^^^
165 |         logger.info("MERGE operation completed successfully")
166 |     else:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/incremental_customer_dim_upsert.py:176:1
    |
174 |                        .partitionBy("_source_system") \
175 |                        .save(silver_customer_path)
176 |         
    | ^^^^^^^^
177 |         logger.info("Created new silver customer dimension table")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/incremental_customer_dim_upsert.py:178:1
    |
177 |         logger.info("Created new silver customer dimension table")
178 |     
    | ^^^^
179 |     # Update watermark to latest ingestion timestamp
180 |     if record_count > 0:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/incremental_customer_dim_upsert.py:186:1
    |
184 |             save_watermark(watermark_checkpoint_path, "customer_dim", watermark_iso)
185 |             logger.info(f"Updated watermark to: {watermark_iso}")
186 |     
    | ^^^^
187 |     # Unpersist cached DataFrame
188 |     customer_updates.unpersist()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/incremental_customer_dim_upsert.py:189:1
    |
187 |     # Unpersist cached DataFrame
188 |     customer_updates.unpersist()
189 |     
    | ^^^^
190 |     logger.info("Incremental customer dimension upsert completed successfully")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/incremental_customer_dim_upsert.py:196:1
    |
194 |     """Main entry point."""
195 |     import argparse
196 |     
    | ^^^^
197 |     parser = argparse.ArgumentParser(description="Incremental customer dimension upsert")
198 |     parser.add_argument("--config", default="config/dev.yaml", help="Configuration file")
    |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
   --> src/project_a/legacy/transform/incremental_customer_dim_upsert.py:199:73
    |
197 |     parser = argparse.ArgumentParser(description="Incremental customer dimension upsert")
198 |     parser.add_argument("--config", default="config/dev.yaml", help="Configuration file")
199 |     parser.add_argument("--checkpoint-path", default="data/checkpoints", 
    |                                                                         ^
200 |                        help="Watermark checkpoint path")
201 |     args = parser.parse_args()
    |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/incremental_customer_dim_upsert.py:202:1
    |
200 |                        help="Watermark checkpoint path")
201 |     args = parser.parse_args()
202 |     
    | ^^^^
203 |     spark = build_spark("IncrementalCustomerDimUpsert")
204 |     config = load_config(args.config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/incremental_customer_dim_upsert.py:205:1
    |
203 |     spark = build_spark("IncrementalCustomerDimUpsert")
204 |     config = load_config(args.config)
205 |     
    | ^^^^
206 |     try:
207 |         incremental_customer_dim_upsert(spark, config, args.checkpoint_path)
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/transform/pii_masking.py:6:1
   |
 4 |   Provides UDFs for masking sensitive data in Silver/Gold layers.
 5 |   """
 6 | / import logging
 7 | | from pyspark.sql.functions import udf
 8 | | from pyspark.sql.types import StringType, IntegerType
   | |_____________________________________________________^
 9 |
10 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

F401 [*] `pyspark.sql.types.IntegerType` imported but unused
  --> src/project_a/legacy/transform/pii_masking.py:8:43
   |
 6 | import logging
 7 | from pyspark.sql.functions import udf
 8 | from pyspark.sql.types import StringType, IntegerType
   |                                           ^^^^^^^^^^^
 9 |
10 | logger = logging.getLogger(__name__)
   |
help: Remove unused import: `pyspark.sql.types.IntegerType`

W291 [*] Trailing whitespace
  --> src/project_a/legacy/transform/pii_masking.py:65:72
   |
65 | def apply_pii_masking(df, email_col=None, phone_col=None, ssn_col=None, 
   |                                                                        ^
66 |                       name_col=None, ip_col=None):
67 |     """
   |
help: Remove trailing whitespace

W293 Blank line contains whitespace
  --> src/project_a/legacy/transform/pii_masking.py:69:1
   |
67 |     """
68 |     Apply PII masking to specified columns in DataFrame.
69 |     
   | ^^^^
70 |     Args:
71 |         df: Input DataFrame
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/transform/pii_masking.py:77:1
   |
75 |         name_col: Column name for name masking
76 |         ip_col: Column name for IP masking
77 |         
   | ^^^^^^^^
78 |     Returns:
79 |         DataFrame with PII masked
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/transform/pii_masking.py:80:1
   |
78 |     Returns:
79 |         DataFrame with PII masked
80 |         
   | ^^^^^^^^
81 |     Example:
82 |         masked_df = apply_pii_masking(
   |
help: Remove whitespace from blank line

W291 Trailing whitespace
  --> src/project_a/legacy/transform/pii_masking.py:83:16
   |
81 |     Example:
82 |         masked_df = apply_pii_masking(
83 |             df, 
   |                ^
84 |             email_col="email",
85 |             phone_col="phone_number"
   |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/transform/pii_masking.py:89:1
   |
87 |     """
88 |     from pyspark.sql import functions as F
89 |     
   | ^^^^
90 |     df_masked = df
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/transform/pii_masking.py:91:1
   |
90 |     df_masked = df
91 |     
   | ^^^^
92 |     if email_col and email_col in df.columns:
93 |         logger.info(f"Masking PII: {email_col}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/transform/pii_masking.py:95:1
   |
93 |         logger.info(f"Masking PII: {email_col}")
94 |         df_masked = df_masked.withColumn(email_col, mask_email(F.col(email_col)))
95 |     
   | ^^^^
96 |     if phone_col and phone_col in df.columns:
97 |         logger.info(f"Masking PII: {phone_col}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/pii_masking.py:99:1
    |
 97 |         logger.info(f"Masking PII: {phone_col}")
 98 |         df_masked = df_masked.withColumn(phone_col, mask_phone(F.col(phone_col)))
 99 |     
    | ^^^^
100 |     if ssn_col and ssn_col in df.columns:
101 |         logger.info(f"Masking PII: {ssn_col}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/pii_masking.py:103:1
    |
101 |         logger.info(f"Masking PII: {ssn_col}")
102 |         df_masked = df_masked.withColumn(ssn_col, mask_ssn(F.col(ssn_col)))
103 |     
    | ^^^^
104 |     if name_col and name_col in df.columns:
105 |         logger.info(f"Masking PII: {name_col}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/pii_masking.py:107:1
    |
105 |         logger.info(f"Masking PII: {name_col}")
106 |         df_masked = df_masked.withColumn(name_col, mask_name(F.col(name_col)))
107 |     
    | ^^^^
108 |     if ip_col and ip_col in df.columns:
109 |         logger.info(f"Masking PII: {ip_col}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/transform/pii_masking.py:111:1
    |
109 |         logger.info(f"Masking PII: {ip_col}")
110 |         df_masked = df_masked.withColumn(ip_col, mask_ip(F.col(ip_col)))
111 |     
    | ^^^^
112 |     return df_masked
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/utils/dlq_handler.py:7:1
   |
 5 |   """
 6 |
 7 | / import logging
 8 | | from datetime import datetime
 9 | | from typing import Dict, Any, Optional
10 | | from pyspark.sql import DataFrame, SparkSession
11 | | from pyspark.sql.functions import lit, current_timestamp, col
12 | | import json
   | |___________^
13 |
14 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/legacy/utils/dlq_handler.py:9:1
   |
 7 | import logging
 8 | from datetime import datetime
 9 | from typing import Dict, Any, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 | from pyspark.sql import DataFrame, SparkSession
11 | from pyspark.sql.functions import lit, current_timestamp, col
   |

F401 [*] `pyspark.sql.functions.col` imported but unused
  --> src/project_a/legacy/utils/dlq_handler.py:11:59
   |
 9 | from typing import Dict, Any, Optional
10 | from pyspark.sql import DataFrame, SparkSession
11 | from pyspark.sql.functions import lit, current_timestamp, col
   |                                                           ^^^
12 | import json
   |
help: Remove unused import: `pyspark.sql.functions.col`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/legacy/utils/dlq_handler.py:23:13
   |
21 |     error_reason: str,
22 |     error_type: str,
23 |     config: Dict[str, Any],
   |             ^^^^
24 |     processing_date: Optional[str] = None
25 | ) -> None:
   |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/legacy/utils/dlq_handler.py:24:22
   |
22 |     error_type: str,
23 |     config: Dict[str, Any],
24 |     processing_date: Optional[str] = None
   |                      ^^^^^^^^^^^^^
25 | ) -> None:
26 |     """
   |
help: Convert to `X | None`

W293 Blank line contains whitespace
  --> src/project_a/legacy/utils/dlq_handler.py:28:1
   |
26 |     """
27 |     Write rejected records to Dead Letter Queue (DLQ).
28 |     
   | ^^^^
29 |     Args:
30 |         spark: SparkSession
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/utils/dlq_handler.py:41:1
   |
39 |         logger.info("No records to write to DLQ")
40 |         return
41 |     
   | ^^^^
42 |     # Get DLQ path from config
43 |     dlq_base = config.get("data_lake", {}).get("dlq_prefix", "s3://bucket/_errors")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/utils/dlq_handler.py:46:1
   |
44 |     if not dlq_base.startswith("s3://"):
45 |         dlq_base = f"s3://{dlq_base}"
46 |     
   | ^^^^
47 |     # Set processing date
48 |     if processing_date is None:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/utils/dlq_handler.py:50:1
   |
48 |     if processing_date is None:
49 |         processing_date = datetime.now().strftime("%Y-%m-%d")
50 |     
   | ^^^^
51 |     # DLQ path structure: s3://bucket/_errors/{dataset}/dt={date}/
52 |     dlq_path = f"{dlq_base}/{dataset_name}/dt={processing_date}/"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/utils/dlq_handler.py:53:1
   |
51 |     # DLQ path structure: s3://bucket/_errors/{dataset}/dt={date}/
52 |     dlq_path = f"{dlq_base}/{dataset_name}/dt={processing_date}/"
53 |     
   | ^^^^
54 |     # Add error metadata columns
55 |     rejected_with_metadata = rejected_df.withColumn(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/utils/dlq_handler.py:64:1
   |
62 |         "_dlq_processing_date", lit(processing_date)
63 |     )
64 |     
   | ^^^^
65 |     # Write rejected records
66 |     logger.warning(f"Writing {rejected_df.count()} rejected records to DLQ: {dlq_path}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/utils/dlq_handler.py:73:1
   |
71 |         .partitionBy("_dlq_processing_date") \
72 |         .save(f"{dlq_path}rejected_rows")
73 |     
   | ^^^^
74 |     # Write error metadata JSON
75 |     error_metadata = {
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/utils/dlq_handler.py:84:1
   |
82 |         "dlq_path": dlq_path
83 |     }
84 |     
   | ^^^^
85 |     # Save metadata (in real implementation, would use boto3 to write JSON to S3)
86 |     logger.info(f"Error metadata: {json.dumps(error_metadata, indent=2)}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/utils/dlq_handler.py:87:1
   |
85 |     # Save metadata (in real implementation, would use boto3 to write JSON to S3)
86 |     logger.info(f"Error metadata: {json.dumps(error_metadata, indent=2)}")
87 |     
   | ^^^^
88 |     # Create success marker
89 |     success_df = spark.createDataFrame(
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/legacy/utils/dlq_handler.py:104:13
    |
102 |     dataset_name: str,
103 |     reason: str,
104 |     config: Dict[str, Any],
    |             ^^^^
105 |     processing_date: Optional[str] = None
106 | ) -> None:
    |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/utils/dlq_handler.py:105:22
    |
103 |     reason: str,
104 |     config: Dict[str, Any],
105 |     processing_date: Optional[str] = None
    |                      ^^^^^^^^^^^^^
106 | ) -> None:
107 |     """
    |
help: Convert to `X | None`

W293 Blank line contains whitespace
   --> src/project_a/legacy/utils/dlq_handler.py:109:1
    |
107 |     """
108 |     Move data to quarantine for investigation.
109 |     
    | ^^^^
110 |     Args:
111 |         spark: SparkSession
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/utils/dlq_handler.py:120:1
    |
118 |     if df.isEmpty():
119 |         return
120 |     
    | ^^^^
121 |     quarantine_base = config.get("data_lake", {}).get("quarantine_prefix", "s3://bucket/_quarantine")
122 |     if not quarantine_base.startswith("s3://"):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/utils/dlq_handler.py:124:1
    |
122 |     if not quarantine_base.startswith("s3://"):
123 |         quarantine_base = f"s3://{quarantine_base}"
124 |     
    | ^^^^
125 |     if processing_date is None:
126 |         processing_date = datetime.now().strftime("%Y-%m-%d")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/utils/dlq_handler.py:127:1
    |
125 |     if processing_date is None:
126 |         processing_date = datetime.now().strftime("%Y-%m-%d")
127 |     
    | ^^^^
128 |     quarantine_path = f"{quarantine_base}/{dataset_name}/dt={processing_date}/{reason}/"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/utils/dlq_handler.py:129:1
    |
128 |     quarantine_path = f"{quarantine_base}/{dataset_name}/dt={processing_date}/{reason}/"
129 |     
    | ^^^^
130 |     logger.warning(f"Quarantining {df.count()} records: {quarantine_path}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/utils/dlq_handler.py:131:1
    |
130 |     logger.warning(f"Quarantining {df.count()} records: {quarantine_path}")
131 |     
    | ^^^^
132 |     df.withColumn("_quarantine_reason", lit(reason)) \
133 |       .withColumn("_quarantine_timestamp", current_timestamp()) \
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/legacy/utils/freshness_guards.py:5:1
   |
 3 |   """
 4 |
 5 | / import logging
 6 | | from datetime import datetime, timedelta, date
 7 | | from typing import Dict, Any, Optional
 8 | | from pyspark.sql import DataFrame
 9 | | from pyspark.sql.functions import col, max as spark_max, current_date
   | |_____________________________________________________________________^
10 |
11 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
 --> src/project_a/legacy/utils/freshness_guards.py:7:1
  |
5 | import logging
6 | from datetime import datetime, timedelta, date
7 | from typing import Dict, Any, Optional
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
8 | from pyspark.sql import DataFrame
9 | from pyspark.sql.functions import col, max as spark_max, current_date
  |

F401 [*] `typing.Dict` imported but unused
 --> src/project_a/legacy/utils/freshness_guards.py:7:20
  |
5 | import logging
6 | from datetime import datetime, timedelta, date
7 | from typing import Dict, Any, Optional
  |                    ^^^^
8 | from pyspark.sql import DataFrame
9 | from pyspark.sql.functions import col, max as spark_max, current_date
  |
help: Remove unused import

F401 [*] `typing.Any` imported but unused
 --> src/project_a/legacy/utils/freshness_guards.py:7:26
  |
5 | import logging
6 | from datetime import datetime, timedelta, date
7 | from typing import Dict, Any, Optional
  |                          ^^^
8 | from pyspark.sql import DataFrame
9 | from pyspark.sql.functions import col, max as spark_max, current_date
  |
help: Remove unused import

F401 [*] `pyspark.sql.functions.current_date` imported but unused
  --> src/project_a/legacy/utils/freshness_guards.py:9:58
   |
 7 | from typing import Dict, Any, Optional
 8 | from pyspark.sql import DataFrame
 9 | from pyspark.sql.functions import col, max as spark_max, current_date
   |                                                          ^^^^^^^^^^^^
10 |
11 | logger = logging.getLogger(__name__)
   |
help: Remove unused import: `pyspark.sql.functions.current_date`

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/utils/freshness_guards.py:16:1
   |
14 | class FreshnessGuard:
15 |     """Base class for source freshness validation."""
16 |     
   | ^^^^
17 |     def __init__(self, max_age_days: int = 2):
18 |         self.max_age_days = max_age_days
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/utils/freshness_guards.py:19:1
   |
17 |     def __init__(self, max_age_days: int = 2):
18 |         self.max_age_days = max_age_days
19 |     
   | ^^^^
20 |     def check_freshness(self, df: DataFrame, date_column: str) -> bool:
21 |         """
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/utils/freshness_guards.py:23:1
   |
21 |         """
22 |         Check if data is fresh enough.
23 |         
   | ^^^^^^^^
24 |         Args:
25 |             df: DataFrame to check
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/legacy/utils/freshness_guards.py:27:1
   |
25 |             df: DataFrame to check
26 |             date_column: Name of the date column
27 |             
   | ^^^^^^^^^^^^
28 |         Returns:
29 |             True if data is fresh, False otherwise
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/utils/freshness_guards.py:36:1
   |
34 |                 logger.warning(f"No data found in {date_column}")
35 |                 return False
36 |             
   | ^^^^^^^^^^^^
37 |             # Convert to date if it's a datetime
38 |             if isinstance(latest_date, datetime):
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/utils/freshness_guards.py:40:1
   |
38 |             if isinstance(latest_date, datetime):
39 |                 latest_date = latest_date.date()
40 |             
   | ^^^^^^^^^^^^
41 |             cutoff_date = date.today() - timedelta(days=self.max_age_days)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/utils/freshness_guards.py:42:1
   |
41 |             cutoff_date = date.today() - timedelta(days=self.max_age_days)
42 |             
   | ^^^^^^^^^^^^
43 |             if latest_date < cutoff_date:
44 |                 logger.error(f"Data is stale: latest {date_column} = {latest_date}, cutoff = {cutoff_date}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/utils/freshness_guards.py:46:1
   |
44 |                 logger.error(f"Data is stale: latest {date_column} = {latest_date}, cutoff = {cutoff_date}")
45 |                 return False
46 |             
   | ^^^^^^^^^^^^
47 |             logger.info(f"Data is fresh: latest {date_column} = {latest_date}")
48 |             return True
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/utils/freshness_guards.py:49:1
   |
47 |             logger.info(f"Data is fresh: latest {date_column} = {latest_date}")
48 |             return True
49 |             
   | ^^^^^^^^^^^^
50 |         except Exception as e:
51 |             logger.error(f"Error checking freshness: {e}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/utils/freshness_guards.py:57:1
   |
55 | class HubSpotFreshnessGuard(FreshnessGuard):
56 |     """HubSpot-specific freshness guard."""
57 |     
   | ^^^^
58 |     def __init__(self, max_age_hours: int = 24):
59 |         super().__init__(max_age_days=1)  # 1 day for HubSpot
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/utils/freshness_guards.py:61:1
   |
59 |         super().__init__(max_age_days=1)  # 1 day for HubSpot
60 |         self.max_age_hours = max_age_hours
61 |     
   | ^^^^
62 |     def check_freshness(self, df: DataFrame, timestamp_column: str = "last_modified") -> bool:
63 |         """Check HubSpot data freshness."""
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/utils/freshness_guards.py:69:1
   |
67 |                 logger.warning("No HubSpot data found")
68 |                 return False
69 |             
   | ^^^^^^^^^^^^
70 |             # Convert to datetime if needed
71 |             if isinstance(latest_timestamp, str):
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/utils/freshness_guards.py:73:1
   |
71 |             if isinstance(latest_timestamp, str):
72 |                 latest_timestamp = datetime.fromisoformat(latest_timestamp.replace('Z', '+00:00'))
73 |             
   | ^^^^^^^^^^^^
74 |             cutoff_time = datetime.now() - timedelta(hours=self.max_age_hours)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/utils/freshness_guards.py:75:1
   |
74 |             cutoff_time = datetime.now() - timedelta(hours=self.max_age_hours)
75 |             
   | ^^^^^^^^^^^^
76 |             if latest_timestamp < cutoff_time:
77 |                 logger.error(f"HubSpot data is stale: latest = {latest_timestamp}, cutoff = {cutoff_time}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/utils/freshness_guards.py:79:1
   |
77 |                 logger.error(f"HubSpot data is stale: latest = {latest_timestamp}, cutoff = {cutoff_time}")
78 |                 return False
79 |             
   | ^^^^^^^^^^^^
80 |             logger.info(f"HubSpot data is fresh: latest = {latest_timestamp}")
81 |             return True
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/utils/freshness_guards.py:82:1
   |
80 |             logger.info(f"HubSpot data is fresh: latest = {latest_timestamp}")
81 |             return True
82 |             
   | ^^^^^^^^^^^^
83 |         except Exception as e:
84 |             logger.error(f"Error checking HubSpot freshness: {e}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/utils/freshness_guards.py:90:1
   |
88 | class SnowflakeFreshnessGuard(FreshnessGuard):
89 |     """Snowflake-specific freshness guard with Airflow macro support."""
90 |     
   | ^^^^
91 |     def __init__(self, max_age_days: int = 2):
92 |         super().__init__(max_age_days)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/legacy/utils/freshness_guards.py:93:1
   |
91 |     def __init__(self, max_age_days: int = 2):
92 |         super().__init__(max_age_days)
93 |     
   | ^^^^
94 |     def check_freshness(self, df: DataFrame, date_column: str = "event_ts") -> bool:
95 |         """Check Snowflake data freshness."""
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/utils/freshness_guards.py:102:1
    |
100 | class KafkaFreshnessGuard(FreshnessGuard):
101 |     """Kafka-specific freshness guard using checkpoints."""
102 |     
    | ^^^^
103 |     def __init__(self, max_age_minutes: int = 60):
104 |         super().__init__(max_age_days=0)  # Kafka is real-time
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/utils/freshness_guards.py:106:1
    |
104 |         super().__init__(max_age_days=0)  # Kafka is real-time
105 |         self.max_age_minutes = max_age_minutes
106 |     
    | ^^^^
107 |     def check_freshness(self, df: DataFrame, timestamp_column: str = "event_timestamp") -> bool:
108 |         """Check Kafka data freshness."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/utils/freshness_guards.py:114:1
    |
112 |                 logger.warning("No Kafka data found")
113 |                 return False
114 |             
    | ^^^^^^^^^^^^
115 |             cutoff_time = datetime.now() - timedelta(minutes=self.max_age_minutes)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/utils/freshness_guards.py:116:1
    |
115 |             cutoff_time = datetime.now() - timedelta(minutes=self.max_age_minutes)
116 |             
    | ^^^^^^^^^^^^
117 |             if latest_timestamp < cutoff_time:
118 |                 logger.error(f"Kafka data is stale: latest = {latest_timestamp}, cutoff = {cutoff_time}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/utils/freshness_guards.py:120:1
    |
118 |                 logger.error(f"Kafka data is stale: latest = {latest_timestamp}, cutoff = {cutoff_time}")
119 |                 return False
120 |             
    | ^^^^^^^^^^^^
121 |             logger.info(f"Kafka data is fresh: latest = {latest_timestamp}")
122 |             return True
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/utils/freshness_guards.py:123:1
    |
121 |             logger.info(f"Kafka data is fresh: latest = {latest_timestamp}")
122 |             return True
123 |             
    | ^^^^^^^^^^^^
124 |         except Exception as e:
125 |             logger.error(f"Error checking Kafka freshness: {e}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/utils/freshness_guards.py:131:1
    |
129 | class FXFreshnessGuard(FreshnessGuard):
130 |     """FX rates freshness guard."""
131 |     
    | ^^^^
132 |     def __init__(self, max_age_days: int = 2):
133 |         super().__init__(max_age_days)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/utils/freshness_guards.py:134:1
    |
132 |     def __init__(self, max_age_days: int = 2):
133 |         super().__init__(max_age_days)
134 |     
    | ^^^^
135 |     def check_freshness(self, df: DataFrame, date_column: str = "fx_date") -> bool:
136 |         """Check FX rates freshness."""
    |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
   --> src/project_a/legacy/utils/freshness_guards.py:141:60
    |
141 | def apply_freshness_window(df: DataFrame, source_type: str, 
    |                                                            ^
142 |                           date_column: str, last_success_ts: Optional[datetime] = None) -> DataFrame:
143 |     """
    |
help: Remove trailing whitespace

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/legacy/utils/freshness_guards.py:142:62
    |
141 | def apply_freshness_window(df: DataFrame, source_type: str, 
142 |                           date_column: str, last_success_ts: Optional[datetime] = None) -> DataFrame:
    |                                                              ^^^^^^^^^^^^^^^^^^
143 |     """
144 |     Apply freshness window filtering to DataFrame.
    |
help: Convert to `X | None`

W293 Blank line contains whitespace
   --> src/project_a/legacy/utils/freshness_guards.py:145:1
    |
143 |     """
144 |     Apply freshness window filtering to DataFrame.
145 |     
    | ^^^^
146 |     Args:
147 |         df: Input DataFrame
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/legacy/utils/freshness_guards.py:151:1
    |
149 |         date_column: Name of the date/timestamp column
150 |         last_success_ts: Last successful processing timestamp
151 |         
    | ^^^^^^^^
152 |     Returns:
153 |         Filtered DataFrame
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/utils/freshness_guards.py:169:1
    |
167 |     else:
168 |         cutoff = last_success_ts
169 |     
    | ^^^^
170 |     # Filter data based on cutoff
171 |     filtered_df = df.filter(col(date_column) >= cutoff)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/legacy/utils/freshness_guards.py:172:1
    |
170 |     # Filter data based on cutoff
171 |     filtered_df = df.filter(col(date_column) >= cutoff)
172 |     
    | ^^^^
173 |     logger.info(f"Applied freshness window to {source_type}: {cutoff}")
174 |     return filtered_df
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/monitoring/lineage_emitter.py:8:1
   |
 6 |   """
 7 |
 8 | / import logging
 9 | | from datetime import datetime, timezone
10 | | from pathlib import Path
11 | | from typing import Any, Optional, Dict, List
12 | | import json
13 | | import urllib.error
14 | | import urllib.request
   | |_____________________^
15 |
16 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/monitoring/lineage_emitter.py:11:1
   |
 9 | from datetime import datetime, timezone
10 | from pathlib import Path
11 | from typing import Any, Optional, Dict, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
12 | import json
13 | import urllib.error
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/monitoring/lineage_emitter.py:11:1
   |
 9 | from datetime import datetime, timezone
10 | from pathlib import Path
11 | from typing import Any, Optional, Dict, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
12 | import json
13 | import urllib.error
   |

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/monitoring/lineage_emitter.py:24:32
   |
22 |     """
23 |
24 |     def __init__(self, config: Dict[str, Any]):
   |                                ^^^^
25 |         """
26 |         Initialize lineage emitter from config.
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/monitoring/lineage_emitter.py:47:27
   |
45 |             self.enabled = False
46 |
47 |     def _headers(self) -> Dict[str, str]:
   |                           ^^^^
48 |         """Get HTTP headers for API requests."""
49 |         headers = {"Content-Type": "application/json"}
   |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/monitoring/lineage_emitter.py:65:17
   |
63 |         job_name: str,
64 |         run_id: str,
65 |         inputs: List[str],
   |                 ^^^^
66 |         outputs: List[str],
67 |         status: str = "SUCCESS",
   |
help: Replace with `list`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/monitoring/lineage_emitter.py:66:18
   |
64 |         run_id: str,
65 |         inputs: List[str],
66 |         outputs: List[str],
   |                  ^^^^
67 |         status: str = "SUCCESS",
68 |         error_message: Optional[str] = None,
   |
help: Replace with `list`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/monitoring/lineage_emitter.py:68:24
   |
66 |         outputs: List[str],
67 |         status: str = "SUCCESS",
68 |         error_message: Optional[str] = None,
   |                        ^^^^^^^^^^^^^
69 |         metadata: Optional[Dict[str, Any]] = None,
70 |     ) -> None:
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/monitoring/lineage_emitter.py:69:19
   |
67 |         status: str = "SUCCESS",
68 |         error_message: Optional[str] = None,
69 |         metadata: Optional[Dict[str, Any]] = None,
   |                   ^^^^^^^^^^^^^^^^^^^^^^^^
70 |     ) -> None:
71 |         """
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/monitoring/lineage_emitter.py:69:28
   |
67 |         status: str = "SUCCESS",
68 |         error_message: Optional[str] = None,
69 |         metadata: Optional[Dict[str, Any]] = None,
   |                            ^^^^
70 |     ) -> None:
71 |         """
   |
help: Replace with `dict`

F821 Undefined name `config`
   --> src/project_a/monitoring/lineage_emitter.py:120:51
    |
118 | â€¦             input_name
119 | â€¦             if input_name.startswith(("s3://", "file://", "s3a://"))
120 | â€¦             else f"s3://{(config or self.config if hasattr(self, 'config') and self.config else {}).get('buckets', {}).get('lake', â€¦
    |                             ^^^^^^
121 | â€¦         ),
122 | â€¦     }
    |

F821 Undefined name `config`
   --> src/project_a/monitoring/lineage_emitter.py:137:51
    |
135 | â€¦             output_name
136 | â€¦             if output_name.startswith(("s3://", "file://", "s3a://"))
137 | â€¦             else f"s3://{(config or self.config if hasattr(self, 'config') and self.config else {}).get('buckets', {}).get('lake', â€¦
    |                             ^^^^^^
138 | â€¦         ),
139 | â€¦     }
    |

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/monitoring/lineage_emitter.py:178:46
    |
178 | def load_lineage_config(config_path: str) -> Dict[str, Any]:
    |                                              ^^^^
179 |     """
180 |     Load lineage configuration from YAML file (local or S3).
    |
help: Replace with `dict`

F821 Undefined name `import_error`
  --> src/project_a/pipeline/run_pipeline.py:54:55
   |
52 |     class DummyModule:
53 |         def main(self, args):
54 |             logger.error(f"Job module not available: {import_error}")
   |                                                       ^^^^^^^^^^^^
55 |             sys.exit(1)
   |

I001 [*] Import block is un-sorted or un-formatted
 --> src/project_a/pyspark_interview_project/dq/__init__.py:5:1
  |
3 |   """
4 |
5 | / from .runner import run_yaml_policy, print_dq_summary
6 | | from .rules import DataQualityRule, NotNullRule, UniqueRule, ExpressionRule
  | |___________________________________________________________________________^
7 |
8 |   __all__ = [
  |
help: Organize imports

W291 [*] Trailing whitespace
  --> src/project_a/pyspark_interview_project/dq/__init__.py:10:24
   |
 8 | __all__ = [
 9 |     "run_yaml_policy",
10 |     "print_dq_summary", 
   |                        ^
11 |     "DataQualityRule",
12 |     "NotNullRule",
   |
help: Remove trailing whitespace

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/dq/gate.py:8:1
   |
 6 |   """
 7 |
 8 | / import logging
 9 | | from typing import Dict, Any, List, Optional
10 | | from pyspark.sql import SparkSession, DataFrame
11 | |
12 | | from project_a.dq.ge_runner import GERunner
   | |___________________________________________^
13 |
14 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/dq/gate.py:9:1
   |
 8 | import logging
 9 | from typing import Dict, Any, List, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 | from pyspark.sql import SparkSession, DataFrame
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/pyspark_interview_project/dq/gate.py:9:1
   |
 8 | import logging
 9 | from typing import Dict, Any, List, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 | from pyspark.sql import SparkSession, DataFrame
   |

F401 [*] `typing.List` imported but unused
  --> src/project_a/pyspark_interview_project/dq/gate.py:9:31
   |
 8 | import logging
 9 | from typing import Dict, Any, List, Optional
   |                               ^^^^
10 | from pyspark.sql import SparkSession, DataFrame
   |
help: Remove unused import

F401 [*] `typing.Optional` imported but unused
  --> src/project_a/pyspark_interview_project/dq/gate.py:9:37
   |
 8 | import logging
 9 | from typing import Dict, Any, List, Optional
   |                                     ^^^^^^^^
10 | from pyspark.sql import SparkSession, DataFrame
   |
help: Remove unused import

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/gate.py:20:1
   |
18 |     """
19 |     Data Quality Gate that blocks pipeline on critical failures.
20 |     
   | ^^^^
21 |     Features:
22 |     - Runs GE suites from config/dq.yaml
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/gate.py:27:1
   |
25 |     - Emits metrics
26 |     """
27 |     
   | ^^^^
28 |     def __init__(self, config: Dict[str, Any]):
29 |         """
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/dq/gate.py:28:32
   |
26 |     """
27 |     
28 |     def __init__(self, config: Dict[str, Any]):
   |                                ^^^^
29 |         """
30 |         Initialize DQ Gate.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/gate.py:31:1
   |
29 |         """
30 |         Initialize DQ Gate.
31 |         
   | ^^^^^^^^
32 |         Args:
33 |             config: Configuration dict
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/gate.py:37:1
   |
35 |         self.config = config
36 |         self.ge_runner = GERunner(config)
37 |     
   | ^^^^
38 |     def check_and_block(
39 |         self,
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/dq/gate.py:45:10
   |
43 |         layer: str = "silver",
44 |         execution_date: str = None
45 |     ) -> Dict[str, Any]:
   |          ^^^^
46 |         """
47 |         Run DQ suite and block pipeline on critical failures.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/gate.py:48:1
   |
46 |         """
47 |         Run DQ suite and block pipeline on critical failures.
48 |         
   | ^^^^^^^^
49 |         Args:
50 |             spark: SparkSession
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/gate.py:55:1
   |
53 |             layer: Data layer (bronze, silver, gold)
54 |             execution_date: Processing date
55 |             
   | ^^^^^^^^^^^^
56 |         Returns:
57 |             DQ results dictionary
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/gate.py:58:1
   |
56 |         Returns:
57 |             DQ results dictionary
58 |             
   | ^^^^^^^^^^^^
59 |         Raises:
60 |             ValueError: If critical expectations fail
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/gate.py:63:1
   |
61 |         """
62 |         logger.info(f"ðŸ” Running DQ gate: {layer}.{suite_name}")
63 |         
   | ^^^^^^^^
64 |         try:
65 |             # Run GE suite
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/gate.py:83:1
   |
81 |                 "error": str(e)
82 |             }
83 |         
   | ^^^^^^^^
84 |         # Check for critical failures
85 |         critical_failures = results.get("critical_failures", 0)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/gate.py:87:1
   |
85 |         critical_failures = results.get("critical_failures", 0)
86 |         warnings = results.get("warnings", 0)
87 |         
   | ^^^^^^^^
88 |         if critical_failures > 0:
89 |             error_msg = f"âŒ DQ Gate FAILED: {critical_failures} critical expectation(s) failed for {layer}.{suite_name}"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/gate.py:93:1
   |
91 |             logger.error(f"DQ Results: {results}")
92 |             raise ValueError(error_msg)
93 |         
   | ^^^^^^^^
94 |         if warnings > 0:
95 |             logger.warning(f"âš ï¸  DQ Gate: {warnings} warning(s) for {layer}.{suite_name}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/gate.py:96:1
   |
94 |         if warnings > 0:
95 |             logger.warning(f"âš ï¸  DQ Gate: {warnings} warning(s) for {layer}.{suite_name}")
96 |         
   | ^^^^^^^^
97 |         logger.info(f"âœ… DQ Gate PASSED: {layer}.{suite_name}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/gate.py:98:1
   |
97 |         logger.info(f"âœ… DQ Gate PASSED: {layer}.{suite_name}")
98 |         
   | ^^^^^^^^
99 |         return results
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/dq/gate.py:106:13
    |
104 |     df: DataFrame,
105 |     suite_name: str,
106 |     config: Dict[str, Any],
    |             ^^^^
107 |     layer: str = "silver"
108 | ) -> None:
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/gate.py:111:1
    |
109 |     """
110 |     Convenience function to enforce DQ gate.
111 |     
    | ^^^^
112 |     Args:
113 |         spark: SparkSession
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/gate.py:118:1
    |
116 |         config: Configuration dict
117 |         layer: Data layer
118 |         
    | ^^^^^^^^
119 |     Raises:
120 |         ValueError: If DQ fails
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/dq/ge_runner.py:7:1
   |
 5 |   """
 6 |
 7 | / import logging
 8 | | import json
 9 | | from typing import Dict, Any, List
10 | | from datetime import datetime
11 | | from pathlib import Path
   | |________________________^
12 |
13 |   try:
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/dq/ge_runner.py:9:1
   |
 7 | import logging
 8 | import json
 9 | from typing import Dict, Any, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 | from datetime import datetime
11 | from pathlib import Path
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/pyspark_interview_project/dq/ge_runner.py:9:1
   |
 7 | import logging
 8 | import json
 9 | from typing import Dict, Any, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 | from datetime import datetime
11 | from pathlib import Path
   |

F401 [*] `typing.List` imported but unused
  --> src/project_a/pyspark_interview_project/dq/ge_runner.py:9:31
   |
 7 | import logging
 8 | import json
 9 | from typing import Dict, Any, List
   |                               ^^^^
10 | from datetime import datetime
11 | from pathlib import Path
   |
help: Remove unused import: `typing.List`

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/dq/ge_runner.py:21:1
   |
19 |     ge = None
20 |
21 | from pyspark.sql import SparkSession, DataFrame
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
22 |
23 | logger = logging.getLogger(__name__)
   |
help: Organize imports

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/ge_runner.py:28:1
   |
26 | class GERunner:
27 |     """Great Expectations runner with DQ breaker support."""
28 |     
   | ^^^^
29 |     def __init__(self, config: Dict[str, Any]):
30 |         """
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/dq/ge_runner.py:29:32
   |
27 |     """Great Expectations runner with DQ breaker support."""
28 |     
29 |     def __init__(self, config: Dict[str, Any]):
   |                                ^^^^
30 |         """
31 |         Initialize GE runner.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/ge_runner.py:32:1
   |
30 |         """
31 |         Initialize GE runner.
32 |         
   | ^^^^^^^^
33 |         Args:
34 |             config: Configuration dictionary
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/ge_runner.py:37:1
   |
35 |         """
36 |         self.config = config
37 |         
   | ^^^^^^^^
38 |         # Load DQ config from file
39 |         self.dq_config = self._load_dq_config(config)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/ge_runner.py:43:1
   |
41 |         # Use config-based DQ results path
42 |         self.dq_results_path = config.get('paths', {}).get('dq_results_root') or config.get('data_lake', {}).get('dq_results_path', 'sâ€¦
43 |     
   | ^^^^
44 |     def _load_dq_config(self, config: Dict[str, Any]) -> Dict[str, Any]:
45 |         """Load DQ configuration from config/dq.yaml."""
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/dq/ge_runner.py:44:39
   |
42 |         self.dq_results_path = config.get('paths', {}).get('dq_results_root') or config.get('data_lake', {}).get('dq_results_path', 'sâ€¦
43 |     
44 |     def _load_dq_config(self, config: Dict[str, Any]) -> Dict[str, Any]:
   |                                       ^^^^
45 |         """Load DQ configuration from config/dq.yaml."""
46 |         import yaml
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/dq/ge_runner.py:44:58
   |
42 |         self.dq_results_path = config.get('paths', {}).get('dq_results_root') or config.get('data_lake', {}).get('dq_results_path', 'sâ€¦
43 |     
44 |     def _load_dq_config(self, config: Dict[str, Any]) -> Dict[str, Any]:
   |                                                          ^^^^
45 |         """Load DQ configuration from config/dq.yaml."""
46 |         import yaml
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/ge_runner.py:47:1
   |
45 |         """Load DQ configuration from config/dq.yaml."""
46 |         import yaml
47 |         
   | ^^^^^^^^
48 |         try:
49 |             config_path = Path("config/dq.yaml")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/ge_runner.py:55:1
   |
53 |         except Exception as e:
54 |             logger.warning(f"Could not load DQ config: {e}")
55 |         
   | ^^^^^^^^
56 |         # Fallback to inline config
57 |         return config.get('dq', {})
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/ge_runner.py:58:1
   |
56 |         # Fallback to inline config
57 |         return config.get('dq', {})
58 |     
   | ^^^^
59 |     def run_suite(
60 |         self,
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/dq/ge_runner.py:66:10
   |
64 |         layer: str,
65 |         execution_date: str = None
66 |     ) -> Dict[str, Any]:
   |          ^^^^
67 |         """
68 |         Run GE expectations suite.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/ge_runner.py:69:1
   |
67 |         """
68 |         Run GE expectations suite.
69 |         
   | ^^^^^^^^
70 |         Args:
71 |             spark: SparkSession
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/ge_runner.py:76:1
   |
74 |             layer: Layer name (bronze, silver, gold)
75 |             execution_date: Execution date
76 |             
   | ^^^^^^^^^^^^
77 |         Returns:
78 |             Results dictionary with pass/fail status
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/ge_runner.py:83:1
   |
81 |             logger.warning("Great Expectations not available, skipping DQ checks")
82 |             return {"passed": True, "skipped": True}
83 |         
   | ^^^^^^^^
84 |         if execution_date is None:
85 |             execution_date = datetime.now().strftime("%Y-%m-%d")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/ge_runner.py:86:1
   |
84 |         if execution_date is None:
85 |             execution_date = datetime.now().strftime("%Y-%m-%d")
86 |         
   | ^^^^^^^^
87 |         logger.info(f"Running DQ suite: {suite_name} (layer: {layer})")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/ge_runner.py:88:1
   |
87 |         logger.info(f"Running DQ suite: {suite_name} (layer: {layer})")
88 |         
   | ^^^^^^^^
89 |         # Convert to GE dataset
90 |         ge_df = SparkDFDataset(df)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/ge_runner.py:91:1
   |
89 |         # Convert to GE dataset
90 |         ge_df = SparkDFDataset(df)
91 |         
   | ^^^^^^^^
92 |         # Load suite from config
93 |         suite_config = self._load_suite_config(suite_name, layer)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/ge_runner.py:94:1
   |
92 |         # Load suite from config
93 |         suite_config = self._load_suite_config(suite_name, layer)
94 |         
   | ^^^^^^^^
95 |         if not suite_config:
96 |             logger.warning(f"Suite config not found: {suite_name}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/ge_runner.py:98:1
    |
 96 |             logger.warning(f"Suite config not found: {suite_name}")
 97 |             return {"passed": True, "skipped": True}
 98 |         
    | ^^^^^^^^
 99 |         results = {
100 |             "suite_name": suite_name,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/ge_runner.py:109:1
    |
107 |             "warning_failures": 0
108 |         }
109 |         
    | ^^^^^^^^
110 |         # Run expectations
111 |         for expectation in suite_config.get('expectations', []):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/ge_runner.py:114:1
    |
112 |             exp_result = self._run_expectation(ge_df, expectation)
113 |             results["expectations"].append(exp_result)
114 |             
    | ^^^^^^^^^^^^
115 |             if not exp_result.get("passed"):
116 |                 results["passed"] = False
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/ge_runner.py:117:1
    |
115 |             if not exp_result.get("passed"):
116 |                 results["passed"] = False
117 |                 
    | ^^^^^^^^^^^^^^^^
118 |                 # Check if critical
119 |                 if exp_result.get("expectation_type") in self.critical_rules:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/ge_runner.py:123:1
    |
121 |                 else:
122 |                     results["warning_failures"] += 1
123 |         
    | ^^^^^^^^
124 |         # Write results
125 |         self._write_results(results, suite_name, execution_date)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/ge_runner.py:126:1
    |
124 |         # Write results
125 |         self._write_results(results, suite_name, execution_date)
126 |         
    | ^^^^^^^^
127 |         # Emit to CloudWatch
128 |         self._emit_metrics(results, suite_name)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/ge_runner.py:129:1
    |
127 |         # Emit to CloudWatch
128 |         self._emit_metrics(results, suite_name)
129 |         
    | ^^^^^^^^
130 |         # DQ Breaker: Fail if critical violations
131 |         if results["critical_failures"] > 0:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/ge_runner.py:135:1
    |
133 |             logger.error(error_msg)
134 |             raise ValueError(error_msg)
135 |         
    | ^^^^^^^^
136 |         logger.info(f"âœ… DQ suite passed: {suite_name} ({len(results['expectations'])} expectations)")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/ge_runner.py:137:1
    |
136 |         logger.info(f"âœ… DQ suite passed: {suite_name} ({len(results['expectations'])} expectations)")
137 |         
    | ^^^^^^^^
138 |         return results
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/ge_runner.py:139:1
    |
138 |         return results
139 |     
    | ^^^^
140 |     def _load_suite_config(self, suite_name: str, layer: str) -> Dict[str, Any]:
141 |         """Load suite configuration from dq.yaml."""
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/dq/ge_runner.py:140:66
    |
138 |         return results
139 |     
140 |     def _load_suite_config(self, suite_name: str, layer: str) -> Dict[str, Any]:
    |                                                                  ^^^^
141 |         """Load suite configuration from dq.yaml."""
142 |         layer_config = self.dq_config.get(layer, {})
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/ge_runner.py:144:1
    |
142 |         layer_config = self.dq_config.get(layer, {})
143 |         return layer_config.get(suite_name, {})
144 |     
    | ^^^^
145 |     def _run_expectation(self, ge_df, expectation: Dict[str, Any]) -> Dict[str, Any]:
146 |         """Run a single expectation."""
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/dq/ge_runner.py:145:52
    |
143 |         return layer_config.get(suite_name, {})
144 |     
145 |     def _run_expectation(self, ge_df, expectation: Dict[str, Any]) -> Dict[str, Any]:
    |                                                    ^^^^
146 |         """Run a single expectation."""
147 |         exp_type = expectation.get('type')
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/dq/ge_runner.py:145:71
    |
143 |         return layer_config.get(suite_name, {})
144 |     
145 |     def _run_expectation(self, ge_df, expectation: Dict[str, Any]) -> Dict[str, Any]:
    |                                                                       ^^^^
146 |         """Run a single expectation."""
147 |         exp_type = expectation.get('type')
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/ge_runner.py:149:1
    |
147 |         exp_type = expectation.get('type')
148 |         kwargs = expectation.get('kwargs', {})
149 |         
    | ^^^^^^^^
150 |         try:
151 |             # Map to GE method
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/ge_runner.py:156:1
    |
154 |                 logger.warning(f"Unknown expectation type: {exp_type}")
155 |                 return {"passed": True, "skipped": True, "expectation_type": exp_type}
156 |             
    | ^^^^^^^^^^^^
157 |             result = method(**kwargs)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/ge_runner.py:158:1
    |
157 |             result = method(**kwargs)
158 |             
    | ^^^^^^^^^^^^
159 |             return {
160 |                 "expectation_type": exp_type,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/ge_runner.py:172:1
    |
170 |                 "error": str(e)
171 |             }
172 |     
    | ^^^^
173 |     def _write_results(self, results: Dict[str, Any], suite_name: str, execution_date: str):
174 |         """Write DQ results to S3."""
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/dq/ge_runner.py:173:39
    |
171 |             }
172 |     
173 |     def _write_results(self, results: Dict[str, Any], suite_name: str, execution_date: str):
    |                                       ^^^^
174 |         """Write DQ results to S3."""
175 |         results_json = json.dumps(results, indent=2, default=str)
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/ge_runner.py:176:1
    |
174 |         """Write DQ results to S3."""
175 |         results_json = json.dumps(results, indent=2, default=str)
176 |         
    | ^^^^^^^^
177 |         # Write to S3 or local
178 |         results_path = f"{self.dq_results_path}/dt={execution_date}/{suite_name}.json"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/ge_runner.py:179:1
    |
177 |         # Write to S3 or local
178 |         results_path = f"{self.dq_results_path}/dt={execution_date}/{suite_name}.json"
179 |         
    | ^^^^^^^^
180 |         try:
181 |             if results_path.startswith("s3://"):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/ge_runner.py:191:1
    |
189 |                 with open(results_path, 'w') as f:
190 |                     f.write(results_json)
191 |             
    | ^^^^^^^^^^^^
192 |             logger.info(f"DQ results written to: {results_path}")
193 |         except Exception as e:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/ge_runner.py:195:1
    |
193 |         except Exception as e:
194 |             logger.warning(f"Failed to write DQ results: {e}")
195 |     
    | ^^^^
196 |     def _emit_metrics(self, results: Dict[str, Any], suite_name: str):
197 |         """Emit DQ metrics to CloudWatch."""
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/dq/ge_runner.py:196:38
    |
194 |             logger.warning(f"Failed to write DQ results: {e}")
195 |     
196 |     def _emit_metrics(self, results: Dict[str, Any], suite_name: str):
    |                                      ^^^^
197 |         """Emit DQ metrics to CloudWatch."""
198 |         try:
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/ge_runner.py:200:1
    |
198 |         try:
199 |             from project_a.monitoring.metrics_collector import emit_rowcount
200 |             
    | ^^^^^^^^^^^^
201 |             emit_rowcount(
202 |                 "dq_failed_records",
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:7:1
   |
 5 |   """
 6 |
 7 | / import logging
 8 | | import sys
 9 | | from typing import Dict, Any, Optional
10 | | from pathlib import Path
11 | | import json
   | |___________^
12 |
13 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

F401 [*] `sys` imported but unused
  --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:8:8
   |
 7 | import logging
 8 | import sys
   |        ^^^
 9 | from typing import Dict, Any, Optional
10 | from pathlib import Path
   |
help: Remove unused import: `sys`

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:9:1
   |
 7 | import logging
 8 | import sys
 9 | from typing import Dict, Any, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 | from pathlib import Path
11 | import json
   |

F401 [*] `json` imported but unused
  --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:11:8
   |
 9 | from typing import Dict, Any, Optional
10 | from pathlib import Path
11 | import json
   |        ^^^^
12 |
13 | logger = logging.getLogger(__name__)
   |
help: Remove unused import: `json`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:19:1
   |
17 |     """
18 |     Production-ready GE runner with fail-fast behavior.
19 |     
   | ^^^^
20 |     Features:
21 |     - Fails job immediately on validation failures
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:26:1
   |
24 |     - Stores suite versions under version control
25 |     """
26 |     
   | ^^^^
27 |     def __init__(self, context_root: Optional[Path] = None):
28 |         """
   |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:27:38
   |
25 |     """
26 |     
27 |     def __init__(self, context_root: Optional[Path] = None):
   |                                      ^^^^^^^^^^^^^^
28 |         """
29 |         Initialize GE runner.
   |
help: Convert to `X | None`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:30:1
   |
28 |         """
29 |         Initialize GE runner.
30 |         
   | ^^^^^^^^
31 |         Args:
32 |             context_root: Root directory for GE context (default: project root)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:36:1
   |
34 |         self.context_root = context_root or Path.cwd()
35 |         self.context = None
36 |         
   | ^^^^^^^^
37 |     def init_context(self) -> None:
38 |         """Initialize Great Expectations context."""
   |
help: Remove whitespace from blank line

F401 `great_expectations` imported but unused; consider using `importlib.util.find_spec` to test for availability
  --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:40:42
   |
38 |         """Initialize Great Expectations context."""
39 |         try:
40 |             import great_expectations as gx
   |                                          ^^
41 |             from great_expectations.data_context import DataContext
   |
help: Remove unused import: `great_expectations`

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:42:1
   |
40 |             import great_expectations as gx
41 |             from great_expectations.data_context import DataContext
42 |             
   | ^^^^^^^^^^^^
43 |             context_path = self.context_root / "great_expectations"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:44:1
   |
43 |             context_path = self.context_root / "great_expectations"
44 |             
   | ^^^^^^^^^^^^
45 |             if context_path.exists():
46 |                 self.context = DataContext(str(context_path))
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:54:1
   |
52 |                 )
53 |                 self.context = None
54 |                 
   | ^^^^^^^^^^^^^^^^
55 |         except ImportError:
56 |             logger.error(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:64:1
   |
62 |             logger.error(f"Failed to initialize GE context: {e}")
63 |             self.context = None
64 |     
   | ^^^^
65 |     def run_checkpoint(
66 |         self,
   |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:68:24
   |
66 |         self,
67 |         checkpoint_name: str,
68 |         batch_request: Optional[Dict[str, Any]] = None,
   |                        ^^^^^^^^^^^^^^^^^^^^^^^^
69 |         fail_on_error: bool = True
70 |     ) -> Dict[str, Any]:
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:68:33
   |
66 |         self,
67 |         checkpoint_name: str,
68 |         batch_request: Optional[Dict[str, Any]] = None,
   |                                 ^^^^
69 |         fail_on_error: bool = True
70 |     ) -> Dict[str, Any]:
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:70:10
   |
68 |         batch_request: Optional[Dict[str, Any]] = None,
69 |         fail_on_error: bool = True
70 |     ) -> Dict[str, Any]:
   |          ^^^^
71 |         """
72 |         Run a GE checkpoint with fail-fast behavior.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:73:1
   |
71 |         """
72 |         Run a GE checkpoint with fail-fast behavior.
73 |         
   | ^^^^^^^^
74 |         Args:
75 |             checkpoint_name: Name of the checkpoint to run
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:78:1
   |
76 |             batch_request: Optional batch request configuration
77 |             fail_on_error: Whether to raise exception on validation failure
78 |             
   | ^^^^^^^^^^^^
79 |         Returns:
80 |             Dictionary with validation results
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:81:1
   |
79 |         Returns:
80 |             Dictionary with validation results
81 |             
   | ^^^^^^^^^^^^
82 |         Raises:
83 |             RuntimeError: If validation fails and fail_on_error is True
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:92:1
   |
90 |                 "message": "GE not configured"
91 |             }
92 |         
   | ^^^^^^^^
93 |         try:
94 |             logger.info(f"Running GE checkpoint: {checkpoint_name}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:95:1
   |
93 |         try:
94 |             logger.info(f"Running GE checkpoint: {checkpoint_name}")
95 |             
   | ^^^^^^^^^^^^
96 |             # Get checkpoint
97 |             checkpoint = self.context.get_checkpoint(checkpoint_name)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:98:1
    |
 96 |             # Get checkpoint
 97 |             checkpoint = self.context.get_checkpoint(checkpoint_name)
 98 |             
    | ^^^^^^^^^^^^
 99 |             # Run checkpoint
100 |             if batch_request:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:104:1
    |
102 |             else:
103 |                 result = checkpoint.run()
104 |             
    | ^^^^^^^^^^^^
105 |             # Extract results
106 |             success = result.get("success", False)
    |
help: Remove whitespace from blank line

F841 Local variable `validation_results` is assigned to but never used
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:107:13
    |
105 |             # Extract results
106 |             success = result.get("success", False)
107 |             validation_results = result.get("run_results", {})
    |             ^^^^^^^^^^^^^^^^^^
108 |             
109 |             # Build summary
    |
help: Remove assignment to unused variable `validation_results`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:108:1
    |
106 |             success = result.get("success", False)
107 |             validation_results = result.get("run_results", {})
108 |             
    | ^^^^^^^^^^^^
109 |             # Build summary
110 |             summary = self._build_summary(result)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:111:1
    |
109 |             # Build summary
110 |             summary = self._build_summary(result)
111 |             
    | ^^^^^^^^^^^^
112 |             # Log summary
113 |             logger.info(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:124:1
    |
122 |                 }
123 |             )
124 |             
    | ^^^^^^^^^^^^
125 |             # Generate data docs link
126 |             data_docs_url = self._get_data_docs_url(result)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:129:1
    |
127 |             if data_docs_url:
128 |                 logger.info(f"Data Docs available at: {data_docs_url}")
129 |             
    | ^^^^^^^^^^^^
130 |             # Fail fast if validation failed
131 |             if not success and fail_on_error:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:139:1
    |
137 |                 )
138 |                 logger.error(error_msg)
139 |                 
    | ^^^^^^^^^^^^^^^^
140 |                 # Log specific failures
141 |                 for failure in summary["failures"]:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:145:1
    |
143 |                         f"  - {failure['expectation_type']}: {failure['kwargs']}"
144 |                     )
145 |                 
    | ^^^^^^^^^^^^^^^^
146 |                 raise RuntimeError(error_msg)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:147:1
    |
146 |                 raise RuntimeError(error_msg)
147 |             
    | ^^^^^^^^^^^^
148 |             return {
149 |                 "success": success,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:153:1
    |
151 |                 "data_docs_url": data_docs_url
152 |             }
153 |             
    | ^^^^^^^^^^^^
154 |         except Exception as e:
155 |             logger.error(f"GE checkpoint execution failed: {e}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:162:1
    |
160 |                 "error": str(e)
161 |             }
162 |     
    | ^^^^
163 |     def _build_summary(self, result: Dict[str, Any]) -> Dict[str, Any]:
164 |         """
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:163:38
    |
161 |             }
162 |     
163 |     def _build_summary(self, result: Dict[str, Any]) -> Dict[str, Any]:
    |                                      ^^^^
164 |         """
165 |         Build compact summary from GE results.
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:163:57
    |
161 |             }
162 |     
163 |     def _build_summary(self, result: Dict[str, Any]) -> Dict[str, Any]:
    |                                                         ^^^^
164 |         """
165 |         Build compact summary from GE results.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:166:1
    |
164 |         """
165 |         Build compact summary from GE results.
166 |         
    | ^^^^^^^^
167 |         Args:
168 |             result: GE checkpoint result
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:169:1
    |
167 |         Args:
168 |             result: GE checkpoint result
169 |             
    | ^^^^^^^^^^^^
170 |         Returns:
171 |             Summary dictionary
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:180:1
    |
178 |             "failures": []
179 |         }
180 |         
    | ^^^^^^^^
181 |         try:
182 |             run_results = result.get("run_results", {})
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:183:1
    |
181 |         try:
182 |             run_results = result.get("run_results", {})
183 |             
    | ^^^^^^^^^^^^
184 |             for run_id, run_result in run_results.items():
185 |                 validation_result = run_result.get("validation_result", {})
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:186:1
    |
184 |             for run_id, run_result in run_results.items():
185 |                 validation_result = run_result.get("validation_result", {})
186 |                 
    | ^^^^^^^^^^^^^^^^
187 |                 # Get statistics
188 |                 statistics = validation_result.get("statistics", {})
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:195:1
    |
193 |                     "successful_expectations", 0
194 |                 )
195 |                 
    | ^^^^^^^^^^^^^^^^
196 |                 # Get specific failures
197 |                 results = validation_result.get("results", [])
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:210:1
    |
208 |                             "result": exp_result.get("result", {})
209 |                         })
210 |                 
    | ^^^^^^^^^^^^^^^^
211 |                 # Add validation result summary
212 |                 summary["validation_results"].append({
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:222:1
    |
220 |                     )
221 |                 })
222 |                 
    | ^^^^^^^^^^^^^^^^
223 |         except Exception as e:
224 |             logger.warning(f"Failed to build detailed summary: {e}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:225:1
    |
223 |         except Exception as e:
224 |             logger.warning(f"Failed to build detailed summary: {e}")
225 |         
    | ^^^^^^^^
226 |         return summary
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:227:1
    |
226 |         return summary
227 |     
    | ^^^^
228 |     def _get_data_docs_url(self, result: Dict[str, Any]) -> Optional[str]:
229 |         """
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:228:42
    |
226 |         return summary
227 |     
228 |     def _get_data_docs_url(self, result: Dict[str, Any]) -> Optional[str]:
    |                                          ^^^^
229 |         """
230 |         Extract data docs URL from result.
    |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:228:61
    |
226 |         return summary
227 |     
228 |     def _get_data_docs_url(self, result: Dict[str, Any]) -> Optional[str]:
    |                                                             ^^^^^^^^^^^^^
229 |         """
230 |         Extract data docs URL from result.
    |
help: Convert to `X | None`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:231:1
    |
229 |         """
230 |         Extract data docs URL from result.
231 |         
    | ^^^^^^^^
232 |         Args:
233 |             result: GE checkpoint result
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:234:1
    |
232 |         Args:
233 |             result: GE checkpoint result
234 |             
    | ^^^^^^^^^^^^
235 |         Returns:
236 |             Data docs URL or None
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:249:1
    |
247 |         except Exception:
248 |             pass
249 |         
    | ^^^^^^^^
250 |         return None
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:251:1
    |
250 |         return None
251 |     
    | ^^^^
252 |     def validate_dataframe(
253 |         self,
    |
help: Remove whitespace from blank line

F821 Undefined name `pyspark`
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:254:14
    |
252 |     def validate_dataframe(
253 |         self,
254 |         df: "pyspark.sql.DataFrame",
    |              ^^^^^^^
255 |         expectation_suite_name: str,
256 |         fail_on_error: bool = True
    |

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:257:10
    |
255 |         expectation_suite_name: str,
256 |         fail_on_error: bool = True
257 |     ) -> Dict[str, Any]:
    |          ^^^^
258 |         """
259 |         Validate a PySpark DataFrame against an expectation suite.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:260:1
    |
258 |         """
259 |         Validate a PySpark DataFrame against an expectation suite.
260 |         
    | ^^^^^^^^
261 |         Args:
262 |             df: PySpark DataFrame to validate
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:265:1
    |
263 |             expectation_suite_name: Name of expectation suite
264 |             fail_on_error: Whether to raise exception on failure
265 |             
    | ^^^^^^^^^^^^
266 |         Returns:
267 |             Dictionary with validation results
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:276:1
    |
274 |                 "message": "GE not configured"
275 |             }
276 |         
    | ^^^^^^^^
277 |         try:
278 |             # Create batch request
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:288:1
    |
286 |                 }
287 |             }
288 |             
    | ^^^^^^^^^^^^
289 |             # Get expectation suite
290 |             suite = self.context.get_expectation_suite(expectation_suite_name)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:291:1
    |
289 |             # Get expectation suite
290 |             suite = self.context.get_expectation_suite(expectation_suite_name)
291 |             
    | ^^^^^^^^^^^^
292 |             # Run validation
293 |             validator = self.context.get_validator(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:297:1
    |
295 |                 expectation_suite=suite
296 |             )
297 |             
    | ^^^^^^^^^^^^
298 |             result = validator.validate()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:299:1
    |
298 |             result = validator.validate()
299 |             
    | ^^^^^^^^^^^^
300 |             # Check success
301 |             success = result.get("success", False)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:303:1
    |
301 |             success = result.get("success", False)
302 |             statistics = result.get("statistics", {})
303 |             
    | ^^^^^^^^^^^^
304 |             # Log results
305 |             logger.info(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:318:1
    |
316 |                 }
317 |             )
318 |             
    | ^^^^^^^^^^^^
319 |             # Fail fast if needed
320 |             if not success and fail_on_error:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:331:1
    |
329 |                 logger.error(error_msg)
330 |                 raise RuntimeError(error_msg)
331 |             
    | ^^^^^^^^^^^^
332 |             return {
333 |                 "success": success,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:336:1
    |
334 |                 "statistics": statistics
335 |             }
336 |             
    | ^^^^^^^^^^^^
337 |         except Exception as e:
338 |             logger.error(f"DataFrame validation failed: {e}")
    |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:349:19
    |
347 | def run_dq_checkpoint(
348 |     checkpoint_name: str,
349 |     context_root: Optional[Path] = None,
    |                   ^^^^^^^^^^^^^^
350 |     fail_on_error: bool = True
351 | ) -> Dict[str, Any]:
    |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:351:6
    |
349 |     context_root: Optional[Path] = None,
350 |     fail_on_error: bool = True
351 | ) -> Dict[str, Any]:
    |      ^^^^
352 |     """
353 |     Convenience function to run a GE checkpoint with fail-fast behavior.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:354:1
    |
352 |     """
353 |     Convenience function to run a GE checkpoint with fail-fast behavior.
354 |     
    | ^^^^
355 |     Args:
356 |         checkpoint_name: Name of checkpoint to run
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/great_expectations_runner.py:359:1
    |
357 |         context_root: Root directory for GE context
358 |         fail_on_error: Whether to fail job on validation failure
359 |         
    | ^^^^^^^^
360 |     Returns:
361 |         Dictionary with validation results
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/dq/rules.py:5:1
   |
 3 |   """
 4 |
 5 | / import logging
 6 | | from typing import Dict, Any, List
 7 | | from pyspark.sql import DataFrame, SparkSession
 8 | | from pyspark.sql.functions import col, count, isnan, when, sum as spark_sum
   | |___________________________________________________________________________^
 9 |
10 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
 --> src/project_a/pyspark_interview_project/dq/rules.py:6:1
  |
5 | import logging
6 | from typing import Dict, Any, List
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
7 | from pyspark.sql import DataFrame, SparkSession
8 | from pyspark.sql.functions import col, count, isnan, when, sum as spark_sum
  |

UP035 `typing.List` is deprecated, use `list` instead
 --> src/project_a/pyspark_interview_project/dq/rules.py:6:1
  |
5 | import logging
6 | from typing import Dict, Any, List
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
7 | from pyspark.sql import DataFrame, SparkSession
8 | from pyspark.sql.functions import col, count, isnan, when, sum as spark_sum
  |

F401 [*] `pyspark.sql.SparkSession` imported but unused
 --> src/project_a/pyspark_interview_project/dq/rules.py:7:36
  |
5 | import logging
6 | from typing import Dict, Any, List
7 | from pyspark.sql import DataFrame, SparkSession
  |                                    ^^^^^^^^^^^^
8 | from pyspark.sql.functions import col, count, isnan, when, sum as spark_sum
  |
help: Remove unused import: `pyspark.sql.SparkSession`

F401 [*] `pyspark.sql.functions.count` imported but unused
  --> src/project_a/pyspark_interview_project/dq/rules.py:8:40
   |
 6 | from typing import Dict, Any, List
 7 | from pyspark.sql import DataFrame, SparkSession
 8 | from pyspark.sql.functions import col, count, isnan, when, sum as spark_sum
   |                                        ^^^^^
 9 |
10 | logger = logging.getLogger(__name__)
   |
help: Remove unused import

F401 [*] `pyspark.sql.functions.isnan` imported but unused
  --> src/project_a/pyspark_interview_project/dq/rules.py:8:47
   |
 6 | from typing import Dict, Any, List
 7 | from pyspark.sql import DataFrame, SparkSession
 8 | from pyspark.sql.functions import col, count, isnan, when, sum as spark_sum
   |                                               ^^^^^
 9 |
10 | logger = logging.getLogger(__name__)
   |
help: Remove unused import

F401 [*] `pyspark.sql.functions.when` imported but unused
  --> src/project_a/pyspark_interview_project/dq/rules.py:8:54
   |
 6 | from typing import Dict, Any, List
 7 | from pyspark.sql import DataFrame, SparkSession
 8 | from pyspark.sql.functions import col, count, isnan, when, sum as spark_sum
   |                                                      ^^^^
 9 |
10 | logger = logging.getLogger(__name__)
   |
help: Remove unused import

F401 [*] `pyspark.sql.functions.sum` imported but unused
  --> src/project_a/pyspark_interview_project/dq/rules.py:8:67
   |
 6 | from typing import Dict, Any, List
 7 | from pyspark.sql import DataFrame, SparkSession
 8 | from pyspark.sql.functions import col, count, isnan, when, sum as spark_sum
   |                                                                   ^^^^^^^^^
 9 |
10 | logger = logging.getLogger(__name__)
   |
help: Remove unused import

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/rules.py:15:1
   |
13 | class DataQualityRule:
14 |     """Base class for data quality rules."""
15 |     
   | ^^^^
16 |     def __init__(self, name: str, severity: str):
17 |         self.name = name
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/rules.py:19:1
   |
17 |         self.name = name
18 |         self.severity = severity
19 |     
   | ^^^^
20 |     def check(self, df: DataFrame) -> Dict[str, Any]:
21 |         """Check rule against DataFrame."""
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/dq/rules.py:20:39
   |
18 |         self.severity = severity
19 |     
20 |     def check(self, df: DataFrame) -> Dict[str, Any]:
   |                                       ^^^^
21 |         """Check rule against DataFrame."""
22 |         raise NotImplementedError
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/rules.py:27:1
   |
25 | class NotNullRule(DataQualityRule):
26 |     """Check for null values in specified column."""
27 |     
   | ^^^^
28 |     def __init__(self, name: str, column: str, severity: str):
29 |         super().__init__(name, severity)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/rules.py:31:1
   |
29 |         super().__init__(name, severity)
30 |         self.column = column
31 |     
   | ^^^^
32 |     def check(self, df: DataFrame) -> Dict[str, Any]:
33 |         """Check for null values."""
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/dq/rules.py:32:39
   |
30 |         self.column = column
31 |     
32 |     def check(self, df: DataFrame) -> Dict[str, Any]:
   |                                       ^^^^
33 |         """Check for null values."""
34 |         null_count = df.filter(col(self.column).isNull()).count()
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/rules.py:36:1
   |
34 |         null_count = df.filter(col(self.column).isNull()).count()
35 |         total_count = df.count()
36 |         
   | ^^^^^^^^
37 |         return {
38 |             "rule_name": self.name,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/rules.py:51:1
   |
49 | class UniqueRule(DataQualityRule):
50 |     """Check for unique values in specified columns."""
51 |     
   | ^^^^
52 |     def __init__(self, name: str, columns: List[str], severity: str):
53 |         super().__init__(name, severity)
   |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/pyspark_interview_project/dq/rules.py:52:44
   |
50 |     """Check for unique values in specified columns."""
51 |     
52 |     def __init__(self, name: str, columns: List[str], severity: str):
   |                                            ^^^^
53 |         super().__init__(name, severity)
54 |         self.columns = columns
   |
help: Replace with `list`

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/rules.py:55:1
   |
53 |         super().__init__(name, severity)
54 |         self.columns = columns
55 |     
   | ^^^^
56 |     def check(self, df: DataFrame) -> Dict[str, Any]:
57 |         """Check for unique values."""
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/dq/rules.py:56:39
   |
54 |         self.columns = columns
55 |     
56 |     def check(self, df: DataFrame) -> Dict[str, Any]:
   |                                       ^^^^
57 |         """Check for unique values."""
58 |         total_count = df.count()
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/rules.py:61:1
   |
59 |         unique_count = df.select(*self.columns).dropDuplicates().count()
60 |         duplicate_count = total_count - unique_count
61 |         
   | ^^^^^^^^
62 |         return {
63 |             "rule_name": self.name,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/rules.py:77:1
   |
75 | class ExpressionRule(DataQualityRule):
76 |     """Check custom SQL expression."""
77 |     
   | ^^^^
78 |     def __init__(self, name: str, sql: str, severity: str):
79 |         super().__init__(name, severity)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/rules.py:81:1
   |
79 |         super().__init__(name, severity)
80 |         self.sql = sql
81 |     
   | ^^^^
82 |     def check(self, df: DataFrame) -> Dict[str, Any]:
83 |         """Check custom expression."""
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/dq/rules.py:82:39
   |
80 |         self.sql = sql
81 |     
82 |     def check(self, df: DataFrame) -> Dict[str, Any]:
   |                                       ^^^^
83 |         """Check custom expression."""
84 |         try:
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/rules.py:87:1
   |
85 |             # Create temporary view for SQL
86 |             df.createOrReplaceTempView("dq_check")
87 |             
   | ^^^^^^^^^^^^
88 |             # Execute SQL expression
89 |             result_df = df.sparkSession.sql(f"SELECT COUNT(*) as total FROM dq_check WHERE NOT ({self.sql})")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/rules.py:92:1
   |
90 |             failure_count = result_df.collect()[0]["total"]
91 |             total_count = df.count()
92 |             
   | ^^^^^^^^^^^^
93 |             return {
94 |                 "rule_name": self.name,
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/dq/rules.py:115:42
    |
115 | def create_rule_from_config(rule_config: Dict[str, Any]) -> DataQualityRule:
    |                                          ^^^^
116 |     """Create rule instance from configuration."""
117 |     rule_type = rule_config["type"]
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/rules.py:120:1
    |
118 |     name = rule_config["name"]
119 |     severity = rule_config["severity"]
120 |     
    | ^^^^
121 |     if rule_type == "not_null":
122 |         return NotNullRule(name, rule_config["column"], severity)
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/dq/runner.py:5:1
   |
 3 |   """
 4 |
 5 | / import json
 6 | | import logging
 7 | | import yaml
 8 | | from typing import Dict, Any, List
 9 | | from pathlib import Path
10 | | from pyspark.sql import DataFrame, SparkSession
11 | | from dataclasses import dataclass
12 | |
13 | | from .rules import create_rule_from_config
   | |__________________________________________^
14 |
15 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

F401 [*] `json` imported but unused
 --> src/project_a/pyspark_interview_project/dq/runner.py:5:8
  |
3 | """
4 |
5 | import json
  |        ^^^^
6 | import logging
7 | import yaml
  |
help: Remove unused import: `json`

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/dq/runner.py:8:1
   |
 6 | import logging
 7 | import yaml
 8 | from typing import Dict, Any, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 9 | from pathlib import Path
10 | from pyspark.sql import DataFrame, SparkSession
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/pyspark_interview_project/dq/runner.py:8:1
   |
 6 | import logging
 7 | import yaml
 8 | from typing import Dict, Any, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 9 | from pathlib import Path
10 | from pyspark.sql import DataFrame, SparkSession
   |

F401 [*] `pathlib.Path` imported but unused
  --> src/project_a/pyspark_interview_project/dq/runner.py:9:21
   |
 7 | import yaml
 8 | from typing import Dict, Any, List
 9 | from pathlib import Path
   |                     ^^^^
10 | from pyspark.sql import DataFrame, SparkSession
11 | from dataclasses import dataclass
   |
help: Remove unused import: `pathlib.Path`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/pyspark_interview_project/dq/runner.py:22:15
   |
20 |     """Data quality result container."""
21 |     passed: bool
22 |     failures: List[str] = None
   |               ^^^^
23 |     warnings: List[str] = None
   |
help: Replace with `list`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/pyspark_interview_project/dq/runner.py:23:15
   |
21 |     passed: bool
22 |     failures: List[str] = None
23 |     warnings: List[str] = None
   |               ^^^^
24 |     
25 |     def __post_init__(self):
   |
help: Replace with `list`

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/runner.py:24:1
   |
22 |     failures: List[str] = None
23 |     warnings: List[str] = None
24 |     
   | ^^^^
25 |     def __post_init__(self):
26 |         if self.failures is None:
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/runner.py:35:1
   |
33 |     """
34 |     Run a data quality suite against a table.
35 |     
   | ^^^^
36 |     Args:
37 |         suite_name: Name of the DQ suite to run
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/runner.py:40:1
   |
38 |         table_path: Path to the Delta table
39 |         spark: SparkSession (optional)
40 |         
   | ^^^^^^^^
41 |     Returns:
42 |         DQResult object with pass/fail status
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/runner.py:45:1
   |
43 |     """
44 |     logger.info(f"Running DQ suite: {suite_name} on table: {table_path}")
45 |     
   | ^^^^
46 |     try:
47 |         # For now, create a simple mock DQ result
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/runner.py:57:1
   |
55 |         else:
56 |             return DQResult(passed=True, failures=[], warnings=[])
57 |             
   | ^^^^^^^^^^^^
58 |     except Exception as e:
59 |         logger.error(f"DQ suite {suite_name} failed: {str(e)}")
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/dq/runner.py:63:44
   |
63 | def run_yaml_policy(df: DataFrame, policy: Dict[str, Any], key_cols: List[str] = None) -> Dict[str, Any]:
   |                                            ^^^^
64 |     """
65 |     Run data quality policy with flexible arguments.
   |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/pyspark_interview_project/dq/runner.py:63:70
   |
63 | def run_yaml_policy(df: DataFrame, policy: Dict[str, Any], key_cols: List[str] = None) -> Dict[str, Any]:
   |                                                                      ^^^^
64 |     """
65 |     Run data quality policy with flexible arguments.
   |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/dq/runner.py:63:91
   |
63 | def run_yaml_policy(df: DataFrame, policy: Dict[str, Any], key_cols: List[str] = None) -> Dict[str, Any]:
   |                                                                                           ^^^^
64 |     """
65 |     Run data quality policy with flexible arguments.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/runner.py:66:1
   |
64 |     """
65 |     Run data quality policy with flexible arguments.
66 |     
   | ^^^^
67 |     Args:
68 |         df: DataFrame to check
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/runner.py:71:1
   |
69 |         policy: Policy configuration dictionary
70 |         key_cols: Optional list of key columns for uniqueness checks
71 |         
   | ^^^^^^^^
72 |     Returns:
73 |         JSON summary of data quality results
   |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
  --> src/project_a/pyspark_interview_project/dq/runner.py:75:17
   |
73 |         JSON summary of data quality results
74 |     """
75 |     logger.info(f"Running data quality policy")
   |                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
76 |     
77 |     # Handle both dict and file path inputs
   |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/runner.py:76:1
   |
74 |     """
75 |     logger.info(f"Running data quality policy")
76 |     
   | ^^^^
77 |     # Handle both dict and file path inputs
78 |     if isinstance(policy, str):
   |
help: Remove whitespace from blank line

UP015 [*] Unnecessary mode argument
  --> src/project_a/pyspark_interview_project/dq/runner.py:79:27
   |
77 |     # Handle both dict and file path inputs
78 |     if isinstance(policy, str):
79 |         with open(policy, 'r') as f:
   |                           ^^^
80 |             policy = yaml.safe_load(f)
   |
help: Remove mode argument

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/runner.py:81:1
   |
79 |         with open(policy, 'r') as f:
80 |             policy = yaml.safe_load(f)
81 |     
   | ^^^^
82 |     table_name = policy.get("table", "unknown_table")
83 |     rules_config = policy.get("rules", [])
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/runner.py:84:1
   |
82 |     table_name = policy.get("table", "unknown_table")
83 |     rules_config = policy.get("rules", [])
84 |     
   | ^^^^
85 |     # Use key_cols if provided, else try policy.get("unique")
86 |     unique_cols = key_cols or policy.get("unique", [])
   |
help: Remove whitespace from blank line

F841 Local variable `unique_cols` is assigned to but never used
  --> src/project_a/pyspark_interview_project/dq/runner.py:86:5
   |
85 |     # Use key_cols if provided, else try policy.get("unique")
86 |     unique_cols = key_cols or policy.get("unique", [])
   |     ^^^^^^^^^^^
87 |     
88 |     logger.info(f"Running DQ checks for table: {table_name}")
   |
help: Remove assignment to unused variable `unique_cols`

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/runner.py:87:1
   |
85 |     # Use key_cols if provided, else try policy.get("unique")
86 |     unique_cols = key_cols or policy.get("unique", [])
87 |     
   | ^^^^
88 |     logger.info(f"Running DQ checks for table: {table_name}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/runner.py:89:1
   |
88 |     logger.info(f"Running DQ checks for table: {table_name}")
89 |     
   | ^^^^
90 |     # Execute rules
91 |     results = []
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/dq/runner.py:94:1
   |
92 |     error_count = 0
93 |     warning_count = 0
94 |     
   | ^^^^
95 |     for rule_config in rules_config:
96 |         try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/runner.py:100:1
    |
 98 |             result = rule.check(df)
 99 |             results.append(result)
100 |             
    | ^^^^^^^^^^^^
101 |             if result["severity"] == "error" and not result["passed"]:
102 |                 error_count += 1
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/runner.py:105:1
    |
103 |             elif result["severity"] == "warn" and not result["passed"]:
104 |                 warning_count += 1
105 |                 
    | ^^^^^^^^^^^^^^^^
106 |             logger.info(f"Rule '{rule.name}': {'PASSED' if result['passed'] else 'FAILED'}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/runner.py:107:1
    |
106 |             logger.info(f"Rule '{rule.name}': {'PASSED' if result['passed'] else 'FAILED'}")
107 |             
    | ^^^^^^^^^^^^
108 |         except Exception as e:
109 |             logger.error(f"Rule '{rule_config['name']}' failed with error: {e}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/runner.py:118:1
    |
116 |             })
117 |             error_count += 1
118 |     
    | ^^^^
119 |     # Create summary
120 |     summary = {
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/runner.py:130:1
    |
128 |         "results": results
129 |     }
130 |     
    | ^^^^
131 |     logger.info(f"DQ Policy Summary: {summary['overall_status']} "
132 |                f"({summary['passed_rules']}/{summary['total_rules']} rules passed)")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/runner.py:133:1
    |
131 |     logger.info(f"DQ Policy Summary: {summary['overall_status']} "
132 |                f"({summary['passed_rules']}/{summary['total_rules']} rules passed)")
133 |     
    | ^^^^
134 |     return summary
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/dq/runner.py:137:31
    |
137 | def print_dq_summary(summary: Dict[str, Any]) -> None:
    |                               ^^^^
138 |     """Print formatted data quality summary."""
139 |     print("\n" + "="*60)
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/runner.py:147:1
    |
145 |     print(f"Errors: {summary['error_count']}, Warnings: {summary['warning_count']}")
146 |     print("-"*60)
147 |     
    | ^^^^
148 |     for result in summary["results"]:
149 |         status = "âœ“ PASS" if result.get("passed", False) else "âœ— FAIL"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/runner.py:151:1
    |
149 |         status = "âœ“ PASS" if result.get("passed", False) else "âœ— FAIL"
150 |         print(f"{status} {result['rule_name']} ({result['rule_type']})")
151 |         
    | ^^^^^^^^
152 |         if not result.get("passed", False):
153 |             if "failure_rate" in result:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/runner.py:157:1
    |
155 |             if "error" in result:
156 |                 print(f"    Error: {result['error']}")
157 |     
    | ^^^^
158 |     print("="*60)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/dq/runner.py:159:1
    |
158 |     print("="*60)
159 |     
    | ^^^^
160 |     # Exit with non-zero code if there are errors
161 |     if summary["error_count"] > 0:
    |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
   --> src/project_a/pyspark_interview_project/dq/runner.py:165:15
    |
163 |         exit(1)
164 |     else:
165 |         print(f"\nâœ… Data quality check PASSED")
    |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |
help: Remove extraneous `f` prefix

W291 Trailing whitespace
 --> src/project_a/pyspark_interview_project/extract/__init__.py:5:47
  |
4 | Consolidated module-based architecture:
5 | - BaseExtractor: Base class for all extractors  
  |                                               ^^
6 | - SalesforceExtractor, CRMExtractor, SnowflakeExtractor, RedshiftExtractor, FXRatesExtractor, HubSpotExtractor
7 | - Factory function: get_extractor() for easy access
  |
help: Remove trailing whitespace

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/extract/__init__.py:11:1
   |
 9 |   """
10 |
11 | / from .base_extractor import (
12 | |     BaseExtractor,
13 | |     SalesforceExtractor,
14 | |     CRMExtractor,
15 | |     SnowflakeExtractor,
16 | |     RedshiftExtractor,
17 | |     FXRatesExtractor,
18 | |     HubSpotExtractor,
19 | |     get_extractor,
20 | |     extract_incremental
21 | | )
22 | |
23 | | # Legacy wrapper functions for backward compatibility
24 | | from .snowflake_orders import extract_snowflake_orders
25 | | from .redshift_behavior import extract_redshift_behavior
26 | | from .kafka_orders_stream import get_kafka_stream
   | |_________________________________________________^
27 |
28 |   # Wrapper for old extract_fx_rates function
   |
help: Organize imports

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/extract/__init__.py:31:5
   |
29 |   def extract_fx_rates(spark, config, **kwargs):
30 |       """Legacy wrapper for FX rates extraction."""
31 | /     from pyspark.sql import SparkSession
32 | |     from typing import Dict, Any
   | |________________________________^
33 |       
34 |       extractor = FXRatesExtractor(config)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/extract/__init__.py:32:5
   |
30 |     """Legacy wrapper for FX rates extraction."""
31 |     from pyspark.sql import SparkSession
32 |     from typing import Dict, Any
   |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
33 |     
34 |     extractor = FXRatesExtractor(config)
   |

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/__init__.py:33:1
   |
31 |     from pyspark.sql import SparkSession
32 |     from typing import Dict, Any
33 |     
   | ^^^^
34 |     extractor = FXRatesExtractor(config)
35 |     return extractor.extract_with_metrics(spark, **kwargs)
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/extract/base_extractor.py:12:1
   |
10 |   """
11 |
12 | / import logging
13 | | from abc import ABC, abstractmethod
14 | | from typing import Dict, Any, Optional
15 | | from datetime import datetime
16 | | from pyspark.sql import SparkSession, DataFrame
17 | | from pyspark.sql.functions import lit, current_timestamp, col
18 | |
19 | | from project_a.utils.state_store import get_state_store
20 | | from project_a.monitoring.metrics_collector import emit_rowcount, emit_duration
21 | | from project_a.monitoring.lineage_decorator import lineage_job
22 | | import time
   | |___________^
23 |
24 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/extract/base_extractor.py:14:1
   |
12 | import logging
13 | from abc import ABC, abstractmethod
14 | from typing import Dict, Any, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
15 | from datetime import datetime
16 | from pyspark.sql import SparkSession, DataFrame
   |

F401 [*] `project_a.monitoring.lineage_decorator.lineage_job` imported but unused
  --> src/project_a/pyspark_interview_project/extract/base_extractor.py:21:52
   |
19 | from project_a.utils.state_store import get_state_store
20 | from project_a.monitoring.metrics_collector import emit_rowcount, emit_duration
21 | from project_a.monitoring.lineage_decorator import lineage_job
   |                                                    ^^^^^^^^^^^
22 | import time
   |
help: Remove unused import: `project_a.monitoring.lineage_decorator.lineage_job`

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/base_extractor.py:29:1
   |
27 | class BaseExtractor(ABC):
28 |     """Base class for all extractors."""
29 |     
   | ^^^^
30 |     def __init__(self, source_name: str, table_name: str, config: Dict[str, Any]):
31 |         """
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/extract/base_extractor.py:30:67
   |
28 |     """Base class for all extractors."""
29 |     
30 |     def __init__(self, source_name: str, table_name: str, config: Dict[str, Any]):
   |                                                                   ^^^^
31 |         """
32 |         Initialize extractor.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/base_extractor.py:33:1
   |
31 |         """
32 |         Initialize extractor.
33 |         
   | ^^^^^^^^
34 |         Args:
35 |             source_name: Source identifier (e.g., 'salesforce', 'snowflake')
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/base_extractor.py:44:1
   |
42 |         self.state_store = get_state_store(config)
43 |         self.environment = config.get('environment', 'local')
44 |     
   | ^^^^
45 |     def _read_local_csv(self, spark: SparkSession, path: str) -> DataFrame:
46 |         """Read CSV file for local development."""
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/base_extractor.py:49:1
   |
47 |         logger.info(f"Reading local CSV: {path}")
48 |         return spark.read.option("header", "true").option("inferSchema", "true").csv(path)
49 |     
   | ^^^^
50 |     def _add_metadata(self, df: DataFrame) -> DataFrame:
51 |         """Add standard metadata columns to DataFrame."""
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/base_extractor.py:57:1
   |
55 |             .withColumn("record_table", lit(self.table_name)) \
56 |             .withColumn("ingest_timestamp", current_timestamp())
57 |     
   | ^^^^
58 |     def _get_watermark(self) -> Optional[datetime]:
59 |         """Get watermark for incremental loading."""
   |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/extract/base_extractor.py:58:33
   |
56 |             .withColumn("ingest_timestamp", current_timestamp())
57 |     
58 |     def _get_watermark(self) -> Optional[datetime]:
   |                                 ^^^^^^^^^^^^^^^^^^
59 |         """Get watermark for incremental loading."""
60 |         watermark_str = self.state_store.get_watermark(f"{self.source_name}_{self.table_name}")
   |
help: Convert to `X | None`

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/base_extractor.py:64:1
   |
62 |             return datetime.fromisoformat(watermark_str.replace('Z', '+00:00'))
63 |         return None
64 |     
   | ^^^^
65 |     def _set_watermark(self, value: datetime):
66 |         """Set watermark after successful extraction."""
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/base_extractor.py:68:1
   |
66 |         """Set watermark after successful extraction."""
67 |         self.state_store.set_watermark(f"{self.source_name}_{self.table_name}", value.isoformat())
68 |     
   | ^^^^
69 |     @abstractmethod
70 |     def extract(self, spark: SparkSession, **kwargs) -> DataFrame:
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/base_extractor.py:73:1
   |
71 |         """
72 |         Extract data from source. Must be implemented by subclasses.
73 |         
   | ^^^^^^^^
74 |         Args:
75 |             spark: SparkSession
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/base_extractor.py:77:1
   |
75 |             spark: SparkSession
76 |             **kwargs: Additional arguments
77 |             
   | ^^^^^^^^^^^^
78 |         Returns:
79 |             DataFrame with extracted data
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/base_extractor.py:82:1
   |
80 |         """
81 |         pass
82 |     
   | ^^^^
83 |     def extract_with_metrics(self, spark: SparkSession, **kwargs) -> DataFrame:
84 |         """
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/base_extractor.py:86:1
   |
84 |         """
85 |         Extract data and emit metrics.
86 |         
   | ^^^^^^^^
87 |         Args:
88 |             spark: SparkSession
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/base_extractor.py:90:1
   |
88 |             spark: SparkSession
89 |             **kwargs: Additional arguments
90 |             
   | ^^^^^^^^^^^^
91 |         Returns:
92 |             DataFrame with extracted data
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/base_extractor.py:95:1
   |
93 |         """
94 |         start_time = time.time()
95 |         
   | ^^^^^^^^
96 |         try:
97 |             df = self.extract(spark, **kwargs)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:98:1
    |
 96 |         try:
 97 |             df = self.extract(spark, **kwargs)
 98 |             
    | ^^^^^^^^^^^^
 99 |             # Add metadata
100 |             df = self._add_metadata(df)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:101:1
    |
 99 |             # Add metadata
100 |             df = self._add_metadata(df)
101 |             
    | ^^^^^^^^^^^^
102 |             # Emit metrics
103 |             record_count = df.count()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:105:1
    |
103 |             record_count = df.count()
104 |             duration_ms = (time.time() - start_time) * 1000
105 |             
    | ^^^^^^^^^^^^
106 |             emit_rowcount("records_extracted", record_count, {
107 |                 "source": self.source_name,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:110:1
    |
108 |                 "table": self.table_name
109 |             }, self.config)
110 |             
    | ^^^^^^^^^^^^
111 |             emit_duration("extraction_duration", duration_ms, {
112 |                 "source": self.source_name
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:114:1
    |
112 |                 "source": self.source_name
113 |             }, self.config)
114 |             
    | ^^^^^^^^^^^^
115 |             logger.info(f"âœ… Extracted {record_count:,} records from {self.source_name}.{self.table_name}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:116:1
    |
115 |             logger.info(f"âœ… Extracted {record_count:,} records from {self.source_name}.{self.table_name}")
116 |             
    | ^^^^^^^^^^^^
117 |             return df
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:118:1
    |
117 |             return df
118 |             
    | ^^^^^^^^^^^^
119 |         except Exception as e:
120 |             logger.error(f"âŒ Failed to extract {self.source_name}.{self.table_name}: {e}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:126:1
    |
124 | class SalesforceExtractor(BaseExtractor):
125 |     """Extractor for Salesforce objects (consolidates all Salesforce extractors)."""
126 |     
    | ^^^^
127 |     def __init__(self, table_name: str, config: Dict[str, Any]):
128 |         super().__init__("salesforce", table_name, config)
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:127:49
    |
125 |     """Extractor for Salesforce objects (consolidates all Salesforce extractors)."""
126 |     
127 |     def __init__(self, table_name: str, config: Dict[str, Any]):
    |                                                 ^^^^
128 |         super().__init__("salesforce", table_name, config)
129 |         self.local_csv_path = f"aws/data/salesforce/salesforce_{table_name}_ready.csv"
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:130:1
    |
128 |         super().__init__("salesforce", table_name, config)
129 |         self.local_csv_path = f"aws/data/salesforce/salesforce_{table_name}_ready.csv"
130 |     
    | ^^^^
131 |     def extract(self, spark: SparkSession, **kwargs) -> DataFrame:
132 |         """Extract Salesforce data."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:139:1
    |
137 |             # For now, return empty DataFrame with expected schema
138 |             return self._create_empty_dataframe(spark)
139 |     
    | ^^^^
140 |     def _create_empty_dataframe(self, spark: SparkSession) -> DataFrame:
141 |         """Create empty DataFrame with Salesforce schema."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:150:1
    |
148 | class CRMExtractor(BaseExtractor):
149 |     """Extractor for CRM data (accounts, contacts, opportunities)."""
150 |     
    | ^^^^
151 |     def __init__(self, table_name: str, config: Dict[str, Any]):
152 |         super().__init__("crm", table_name, config)
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:151:49
    |
149 |     """Extractor for CRM data (accounts, contacts, opportunities)."""
150 |     
151 |     def __init__(self, table_name: str, config: Dict[str, Any]):
    |                                                 ^^^^
152 |         super().__init__("crm", table_name, config)
153 |         self.local_csv_path = f"aws/data/crm/{table_name}.csv"
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:154:1
    |
152 |         super().__init__("crm", table_name, config)
153 |         self.local_csv_path = f"aws/data/crm/{table_name}.csv"
154 |     
    | ^^^^
155 |     def extract(self, spark: SparkSession, **kwargs) -> DataFrame:
156 |         """Extract CRM data."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:162:1
    |
160 |             # Production: Use CRM API (Salesforce/HubSpot)
161 |             return self._create_empty_dataframe(spark)
162 |     
    | ^^^^
163 |     def _create_empty_dataframe(self, spark: SparkSession) -> DataFrame:
164 |         """Create empty DataFrame with CRM schema."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:172:1
    |
170 | class HubSpotExtractor(BaseExtractor):
171 |     """Extractor for HubSpot CRM data with incremental support."""
172 |     
    | ^^^^
173 |     def __init__(self, table_name: str, config: Dict[str, Any]):
174 |         super().__init__("hubspot", table_name, config)
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:173:49
    |
171 |     """Extractor for HubSpot CRM data with incremental support."""
172 |     
173 |     def __init__(self, table_name: str, config: Dict[str, Any]):
    |                                                 ^^^^
174 |         super().__init__("hubspot", table_name, config)
175 |         self.local_csv_path = f"aws/data/crm/{table_name}.csv"
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:176:1
    |
174 |         super().__init__("hubspot", table_name, config)
175 |         self.local_csv_path = f"aws/data/crm/{table_name}.csv"
176 |     
    | ^^^^
177 |     def extract(self, spark: SparkSession, since_ts: Optional[datetime] = None, **kwargs) -> DataFrame:
178 |         """Extract HubSpot data with incremental support."""
    |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:177:54
    |
175 |         self.local_csv_path = f"aws/data/crm/{table_name}.csv"
176 |     
177 |     def extract(self, spark: SparkSession, since_ts: Optional[datetime] = None, **kwargs) -> DataFrame:
    |                                                      ^^^^^^^^^^^^^^^^^^
178 |         """Extract HubSpot data with incremental support."""
179 |         if since_ts is None:
    |
help: Convert to `X | None`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:181:1
    |
179 |         if since_ts is None:
180 |             since_ts = self._get_watermark()
181 |         
    | ^^^^^^^^
182 |         if self.environment == 'local':
183 |             df = self._read_local_csv(spark, self.local_csv_path)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:190:1
    |
188 |             # Production: Use HubSpot API
189 |             df = self._extract_from_api(spark, since_ts)
190 |         
    | ^^^^^^^^
191 |         # Update watermark
192 |         if df.count() > 0:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:197:1
    |
195 |             if latest_ts:
196 |                 self._set_watermark(latest_ts)
197 |         
    | ^^^^^^^^
198 |         return df
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:199:1
    |
198 |         return df
199 |     
    | ^^^^
200 |     def _extract_from_api(self, spark: SparkSession, since_ts: Optional[datetime]) -> DataFrame:
201 |         """Extract from HubSpot API."""
    |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:200:64
    |
198 |         return df
199 |     
200 |     def _extract_from_api(self, spark: SparkSession, since_ts: Optional[datetime]) -> DataFrame:
    |                                                                ^^^^^^^^^^^^^^^^^^
201 |         """Extract from HubSpot API."""
202 |         from pyspark.sql.types import StructType
    |
help: Convert to `X | None`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:209:1
    |
207 | class SnowflakeExtractor(BaseExtractor):
208 |     """Extractor for Snowflake data with incremental support."""
209 |     
    | ^^^^
210 |     def __init__(self, table_name: str, config: Dict[str, Any]):
211 |         super().__init__("snowflake", table_name, config)
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:210:49
    |
208 |     """Extractor for Snowflake data with incremental support."""
209 |     
210 |     def __init__(self, table_name: str, config: Dict[str, Any]):
    |                                                 ^^^^
211 |         super().__init__("snowflake", table_name, config)
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:212:1
    |
210 |     def __init__(self, table_name: str, config: Dict[str, Any]):
211 |         super().__init__("snowflake", table_name, config)
212 |     
    | ^^^^
213 |     def extract(self, spark: SparkSession, since_ts: Optional[datetime] = None, **kwargs) -> DataFrame:
214 |         """Extract Snowflake data with watermark support."""
    |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:213:54
    |
211 |         super().__init__("snowflake", table_name, config)
212 |     
213 |     def extract(self, spark: SparkSession, since_ts: Optional[datetime] = None, **kwargs) -> DataFrame:
    |                                                      ^^^^^^^^^^^^^^^^^^
214 |         """Extract Snowflake data with watermark support."""
215 |         # Get watermark if not provided
    |
help: Convert to `X | None`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:218:1
    |
216 |         if since_ts is None:
217 |             since_ts = self._get_watermark()
218 |         
    | ^^^^^^^^
219 |         if self.environment == 'local':
220 |             sample_path = self.config.get('paths', {}).get(f'snowflake_{self.table_name}')
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:234:1
    |
232 |                 .option("query", query) \
233 |                 .load()
234 |         
    | ^^^^^^^^
235 |         # Update watermark
236 |         if df.count() > 0:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:241:1
    |
239 |             if latest_ts:
240 |                 self._set_watermark(latest_ts)
241 |         
    | ^^^^^^^^
242 |         return df
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:243:1
    |
242 |         return df
243 |     
    | ^^^^
244 |     def _build_query(self, since_ts: Optional[datetime]) -> str:
245 |         """Build Snowflake query with optional watermark filter."""
    |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:244:38
    |
242 |         return df
243 |     
244 |     def _build_query(self, since_ts: Optional[datetime]) -> str:
    |                                      ^^^^^^^^^^^^^^^^^^
245 |         """Build Snowflake query with optional watermark filter."""
246 |         table_name_upper = self.table_name.upper()
    |
help: Convert to `X | None`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:254:1
    |
252 | class RedshiftExtractor(BaseExtractor):
253 |     """Extractor for Redshift data with incremental support."""
254 |     
    | ^^^^
255 |     def __init__(self, table_name: str, config: Dict[str, Any]):
256 |         super().__init__("redshift", table_name, config)
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:255:49
    |
253 |     """Extractor for Redshift data with incremental support."""
254 |     
255 |     def __init__(self, table_name: str, config: Dict[str, Any]):
    |                                                 ^^^^
256 |         super().__init__("redshift", table_name, config)
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:257:1
    |
255 |     def __init__(self, table_name: str, config: Dict[str, Any]):
256 |         super().__init__("redshift", table_name, config)
257 |     
    | ^^^^
258 |     def extract(self, spark: SparkSession, since_ts: Optional[datetime] = None, **kwargs) -> DataFrame:
259 |         """Extract Redshift data with watermark support."""
    |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:258:54
    |
256 |         super().__init__("redshift", table_name, config)
257 |     
258 |     def extract(self, spark: SparkSession, since_ts: Optional[datetime] = None, **kwargs) -> DataFrame:
    |                                                      ^^^^^^^^^^^^^^^^^^
259 |         """Extract Redshift data with watermark support."""
260 |         if since_ts is None:
    |
help: Convert to `X | None`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:262:1
    |
260 |         if since_ts is None:
261 |             since_ts = self._get_watermark()
262 |         
    | ^^^^^^^^
263 |         if self.environment == 'local':
264 |             sample_path = self.config.get('paths', {}).get(f'redshift_{self.table_name}')
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:278:1
    |
276 |                 .option("query", query) \
277 |                 .load()
278 |         
    | ^^^^^^^^
279 |         # Update watermark
280 |         if df.count() > 0:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:285:1
    |
283 |             if latest_ts:
284 |                 self._set_watermark(latest_ts)
285 |         
    | ^^^^^^^^
286 |         return df
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:287:1
    |
286 |         return df
287 |     
    | ^^^^
288 |     def _build_query(self, since_ts: Optional[datetime]) -> str:
289 |         """Build Redshift query with optional watermark filter."""
    |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:288:38
    |
286 |         return df
287 |     
288 |     def _build_query(self, since_ts: Optional[datetime]) -> str:
    |                                      ^^^^^^^^^^^^^^^^^^
289 |         """Build Redshift query with optional watermark filter."""
290 |         if since_ts:
    |
help: Convert to `X | None`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:297:1
    |
295 | class FXRatesExtractor(BaseExtractor):
296 |     """Extractor for FX rates with REST API support."""
297 |     
    | ^^^^
298 |     def __init__(self, config: Dict[str, Any]):
299 |         super().__init__("fx_rates", "rates", config)
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:298:32
    |
296 |     """Extractor for FX rates with REST API support."""
297 |     
298 |     def __init__(self, config: Dict[str, Any]):
    |                                ^^^^
299 |         super().__init__("fx_rates", "rates", config)
300 |         self.api_url = config.get('data_sources', {}).get('fx_rates', {}).get('api_url')
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:302:1
    |
300 |         self.api_url = config.get('data_sources', {}).get('fx_rates', {}).get('api_url')
301 |         self.api_key = config.get('data_sources', {}).get('fx_rates', {}).get('api_key')
302 |     
    | ^^^^
303 |     def extract(self, spark: SparkSession, date: str = None, **kwargs) -> DataFrame:
304 |         """Extract FX rates from REST API or CSV."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:308:1
    |
306 |             from datetime import datetime
307 |             date = datetime.now().strftime("%Y-%m-%d")
308 |         
    | ^^^^^^^^
309 |         if self.environment == 'local':
310 |             csv_path = self.config.get('paths', {}).get('fx_rates', "data/fx_rates_historical_730_days.csv")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:318:1
    |
316 |             # Production: Call REST API
317 |             df = self._extract_from_rest_api(spark, date)
318 |         
    | ^^^^^^^^
319 |         return df
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:320:1
    |
319 |         return df
320 |     
    | ^^^^
321 |     def _extract_from_rest_api(self, spark: SparkSession, date: str) -> DataFrame:
322 |         """
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:361:62
    |
360 | # Factory function for easy access
361 | def get_extractor(source_name: str, table_name: str, config: Dict[str, Any]) -> BaseExtractor:
    |                                                              ^^^^
362 |     """Factory function to get appropriate extractor."""
363 |     if source_name == "salesforce":
    |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:384:19
    |
382 |     source_name: str,
383 |     read_fn,
384 |     read_options: Optional[Dict[str, Any]] = None,
    |                   ^^^^^^^^^^^^^^^^^^^^^^^^
385 | ) -> DataFrame:
386 |     """
    |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:384:28
    |
382 |     source_name: str,
383 |     read_fn,
384 |     read_options: Optional[Dict[str, Any]] = None,
    |                            ^^^^
385 | ) -> DataFrame:
386 |     """
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:388:1
    |
386 |     """
387 |     Extract incrementally from a source using updated_at watermark.
388 |     
    | ^^^^
389 |     This is a legacy standalone function. For new code, use BaseExtractor classes
390 |     which have built-in incremental support via watermarks.
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:391:1
    |
389 |     This is a legacy standalone function. For new code, use BaseExtractor classes
390 |     which have built-in incremental support via watermarks.
391 |     
    | ^^^^
392 |     Args:
393 |         spark: Spark session
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:397:1
    |
395 |         read_fn: Callable that returns a DataFrame when invoked as read_fn(spark, **read_options)
396 |         read_options: Optional kwargs for read_fn
397 |         
    | ^^^^^^^^
398 |     Returns:
399 |         Filtered DataFrame containing only rows with updated_at > watermark
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:401:5
    |
399 |           Filtered DataFrame containing only rows with updated_at > watermark
400 |       """
401 | /     from pyspark.sql.functions import max as spark_max
402 | |     from datetime import timezone
    | |_________________________________^
403 |       
404 |       read_options = read_options or {}
    |
help: Organize imports

F401 [*] `datetime.timezone` imported but unused
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:402:26
    |
400 |     """
401 |     from pyspark.sql.functions import max as spark_max
402 |     from datetime import timezone
    |                          ^^^^^^^^
403 |     
404 |     read_options = read_options or {}
    |
help: Remove unused import: `datetime.timezone`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:403:1
    |
401 |     from pyspark.sql.functions import max as spark_max
402 |     from datetime import timezone
403 |     
    | ^^^^
404 |     read_options = read_options or {}
405 |     logger.info(f"Starting incremental extract for {source_name}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:406:1
    |
404 |     read_options = read_options or {}
405 |     logger.info(f"Starting incremental extract for {source_name}")
406 |     
    | ^^^^
407 |     # Get watermark from state store
408 |     state_store = get_state_store(config={})  # Will use default config
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:413:1
    |
411 |     if watermark_str:
412 |         watermark = datetime.fromisoformat(watermark_str.replace('Z', '+00:00'))
413 |     
    | ^^^^
414 |     # Load raw df via provided function
415 |     raw_df: DataFrame = read_fn(spark, **read_options)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:418:1
    |
416 |     if "updated_at" not in raw_df.columns:
417 |         raise ValueError("Input DataFrame must contain 'updated_at' column for incremental processing")
418 |     
    | ^^^^
419 |     if watermark:
420 |         from pyspark.sql.types import TimestampType
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:426:1
    |
424 |         df = raw_df
425 |         logger.info("No watermark found, processing all records")
426 |     
    | ^^^^
427 |     # Compute new watermark
428 |     if not df.isEmpty():
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/base_extractor.py:435:1
    |
433 |     else:
434 |         logger.info("No new records to update watermark")
435 |     
    | ^^^^
436 |     return df
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/extract/fx_json_reader.py:6:1
   |
 4 |   Reads JSON Lines (NDJSON) from S3, enforces schema, cleans nulls, and normalizes.
 5 |   """
 6 | / from __future__ import annotations
 7 | |
 8 | | import logging
 9 | | from typing import Dict, Any
10 | | from pyspark.sql import SparkSession, DataFrame
11 | | from pyspark.sql import functions as F
   | |______________________________________^
12 |
13 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/extract/fx_json_reader.py:9:1
   |
 8 | import logging
 9 | from typing import Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 | from pyspark.sql import SparkSession, DataFrame
11 | from pyspark.sql import functions as F
   |

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/extract/fx_json_reader.py:18:13
   |
16 | def read_fx_json(
17 |     spark: SparkSession,
18 |     config: Dict[str, Any],
   |             ^^^^
19 | ) -> DataFrame:
20 |     """
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/fx_json_reader.py:37:1
   |
35 |     raw_path = fx_cfg.get("raw_path", "")
36 |     files = fx_cfg.get("files", {})
37 |     
   | ^^^^
38 |     if not raw_path:
39 |         raise ValueError("FX raw_path not found in config")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/fx_json_reader.py:40:1
   |
38 |     if not raw_path:
39 |         raise ValueError("FX raw_path not found in config")
40 |     
   | ^^^^
41 |     # Construct full path to JSON file
42 |     json_file = files.get("daily_rates_json", "fx_rates_historical.json")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/fx_json_reader.py:44:1
   |
42 |     json_file = files.get("daily_rates_json", "fx_rates_historical.json")
43 |     json_path = f"{raw_path.rstrip('/')}/{json_file}"
44 |     
   | ^^^^
45 |     logger.info(f"ðŸ“¥ Reading FX JSON from: {json_path}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/fx_json_reader.py:46:1
   |
45 |     logger.info(f"ðŸ“¥ Reading FX JSON from: {json_path}")
46 |     
   | ^^^^
47 |     # Read JSON with explicit schema (no inference)
48 |     from project_a.schemas.bronze_schemas import FX_RATES_SCHEMA
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/fx_json_reader.py:49:1
   |
47 |     # Read JSON with explicit schema (no inference)
48 |     from project_a.schemas.bronze_schemas import FX_RATES_SCHEMA
49 |     
   | ^^^^
50 |     df = spark.read.schema(FX_RATES_SCHEMA).json(json_path)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/fx_json_reader.py:51:1
   |
50 |     df = spark.read.schema(FX_RATES_SCHEMA).json(json_path)
51 |     
   | ^^^^
52 |     initial_count = df.count()
53 |     logger.info(f"ðŸ“Š Read {initial_count:,} raw JSON records")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/fx_json_reader.py:54:1
   |
52 |     initial_count = df.count()
53 |     logger.info(f"ðŸ“Š Read {initial_count:,} raw JSON records")
54 |     
   | ^^^^
55 |     # Normalize column names defensively (handle variations)
56 |     if "ccy" in df.columns and "base_ccy" not in df.columns:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/fx_json_reader.py:62:1
   |
60 |     if "currency" in df.columns and "quote_ccy" not in df.columns:
61 |         df = df.withColumnRenamed("currency", "quote_ccy")
62 |     
   | ^^^^
63 |     # Standardize and clean
64 |     df = df.withColumn(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/fx_json_reader.py:83:1
   |
81 |         F.coalesce(F.col("source"), F.lit("fx-json-demo"))
82 |     )
83 |     
   | ^^^^
84 |     # Drop obviously bad rows
85 |     df = df.filter(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/fx_json_reader.py:94:1
   |
92 |         (F.length(F.trim(F.col("quote_ccy"))) > 0)
93 |     )
94 |     
   | ^^^^
95 |     # Select standardized columns
96 |     df_clean = df.select(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/fx_json_reader.py:106:1
    |
104 |         F.coalesce(F.col("mid_rate"), F.col("rate")).alias("mid_rate")
105 |     )
106 |     
    | ^^^^
107 |     final_count = df_clean.count()
108 |     logger.info(f"âœ… Cleaned FX data: {final_count:,} records (dropped {initial_count - final_count:,} invalid)")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/fx_json_reader.py:109:1
    |
107 |     final_count = df_clean.count()
108 |     logger.info(f"âœ… Cleaned FX data: {final_count:,} records (dropped {initial_count - final_count:,} invalid)")
109 |     
    | ^^^^
110 |     return df_clean
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/fx_json_reader.py:162:1
    |
160 |                 logger.debug("JSON not found at %s: %s", json_path, e2)
161 |                 continue
162 |         
    | ^^^^^^^^
163 |         # If JSON failed, try CSV
164 |         if df_raw is None:
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:7:1
   |
 5 |   """
 6 |
 7 | / import logging
 8 | | import os
 9 | | from typing import Dict, Any
10 | | from pyspark.sql import SparkSession, DataFrame
11 | | from pyspark.sql.functions import col, from_json, current_timestamp, lit
12 | | from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType
   | |_________________________________________________________________________________________________________^
13 |
14 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:9:1
   |
 7 | import logging
 8 | import os
 9 | from typing import Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 | from pyspark.sql import SparkSession, DataFrame
11 | from pyspark.sql.functions import col, from_json, current_timestamp, lit
   |

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:19:13
   |
17 | def get_kafka_stream(
18 |     spark: SparkSession,
19 |     config: Dict[str, Any],
   |             ^^^^
20 |     topic: str = "orders_events"
21 | ) -> DataFrame:
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:24:1
   |
22 |     """
23 |     Create Spark Structured Streaming DataFrame from Kafka.
24 |     
   | ^^^^
25 |     Args:
26 |         spark: SparkSession
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:29:1
   |
27 |         config: Configuration dictionary
28 |         topic: Kafka topic name
29 |         
   | ^^^^^^^^
30 |     Returns:
31 |         Streaming DataFrame
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:34:1
   |
32 |     """
33 |     kafka_config = config.get('data_sources', {}).get('kafka', {})
34 |     
   | ^^^^
35 |     # Kafka connection options
36 |     kafka_options = {
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:42:1
   |
40 |         "failOnDataLoss": "false"
41 |     }
42 |     
   | ^^^^
43 |     # For Confluent Cloud or MSK
44 |     if kafka_config.get('security_protocol') == 'SASL_SSL':
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:50:1
   |
48 |             "kafka.sasl.jaas.config": kafka_config.get('jaas_config')
49 |         })
50 |     
   | ^^^^
51 |     logger.info(f"Connecting to Kafka topic: {topic}")
52 |     logger.info(f"Bootstrap servers: {kafka_options['kafka.bootstrap.servers']}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:53:1
   |
51 |     logger.info(f"Connecting to Kafka topic: {topic}")
52 |     logger.info(f"Bootstrap servers: {kafka_options['kafka.bootstrap.servers']}")
53 |     
   | ^^^^
54 |     # Read stream
55 |     stream_df = spark \
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:60:1
   |
58 |         .options(**kafka_options) \
59 |         .load()
60 |     
   | ^^^^
61 |     return stream_df
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:67:1
   |
65 |     """
66 |     Parse Kafka messages (assumed JSON format).
67 |     
   | ^^^^
68 |     Args:
69 |         stream_df: Raw Kafka stream DataFrame
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:71:1
   |
69 |         stream_df: Raw Kafka stream DataFrame
70 |         schema: Expected schema for parsed data
71 |         
   | ^^^^^^^^
72 |     Returns:
73 |         Parsed DataFrame
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:89:1
   |
87 |         "data.*"
88 |     )
89 |     
   | ^^^^
90 |     return parsed_df
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:101:1
    |
 99 |     """
100 |     Write Kafka stream to Bronze layer with checkpointing.
101 |     
    | ^^^^
102 |     Args:
103 |         stream_df: Parsed streaming DataFrame
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:107:1
    |
105 |         checkpoint_path: Checkpoint location for recovery
106 |         trigger_interval: Trigger interval (e.g., "10 seconds", "1 minute")
107 |         
    | ^^^^^^^^
108 |     Returns:
109 |         StreamingQuery object
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:113:1
    |
111 |     logger.info(f"Writing Kafka stream to Bronze: {output_path}")
112 |     logger.info(f"Checkpoint: {checkpoint_path}")
113 |     
    | ^^^^
114 |     # Add metadata
115 |     stream_with_metadata = stream_df \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:118:1
    |
116 |         .withColumn("_ingestion_ts", current_timestamp()) \
117 |         .withColumn("_source", lit("kafka"))
118 |     
    | ^^^^
119 |     # Write stream
120 |     query = stream_with_metadata \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:129:1
    |
127 |         .trigger(processingTime=trigger_interval) \
128 |         .start()
129 |     
    | ^^^^
130 |     logger.info(f"âœ… Started streaming query: {query.id}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:131:1
    |
130 |     logger.info(f"âœ… Started streaming query: {query.id}")
131 |     
    | ^^^^
132 |     return query
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:137:13
    |
135 | def stream_orders_from_kafka(
136 |     spark: SparkSession,
137 |     config: Dict[str, Any],
    |             ^^^^
138 |     topic: str = "orders_events"
139 | ) -> None:
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:142:1
    |
140 |     """
141 |     Main function to stream orders from Kafka to Bronze.
142 |     
    | ^^^^
143 |     Args:
144 |         spark: SparkSession
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:149:1
    |
147 |     """
148 |     logger.info(f"ðŸš€ Starting Kafka streaming for topic: {topic}")
149 |     
    | ^^^^
150 |     # Get stream
151 |     kafka_stream = get_kafka_stream(spark, config, topic)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:152:1
    |
150 |     # Get stream
151 |     kafka_stream = get_kafka_stream(spark, config, topic)
152 |     
    | ^^^^
153 |     # Define schema for orders events (should match Avro schema)
154 |     orders_schema = StructType([
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:162:1
    |
160 |         StructField("timestamp", TimestampType(), True)
161 |     ])
162 |     
    | ^^^^
163 |     # Parse messages
164 |     parsed_stream = parse_kafka_messages(kafka_stream, orders_schema)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:165:1
    |
163 |     # Parse messages
164 |     parsed_stream = parse_kafka_messages(kafka_stream, orders_schema)
165 |     
    | ^^^^
166 |     # Write to Bronze
167 |     bronze_path = config.get("data_lake", {}).get("bronze_path", "data/lakehouse_delta/bronze")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:169:1
    |
167 |     bronze_path = config.get("data_lake", {}).get("bronze_path", "data/lakehouse_delta/bronze")
168 |     output_path = f"{bronze_path}/kafka/{topic}"
169 |     
    | ^^^^
170 |     checkpoint_path = config.get("data_lake", {}).get("checkpoint_path", "data/checkpoints/kafka")
171 |     checkpoint_path = f"{checkpoint_path}/{topic}"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:172:1
    |
170 |     checkpoint_path = config.get("data_lake", {}).get("checkpoint_path", "data/checkpoints/kafka")
171 |     checkpoint_path = f"{checkpoint_path}/{topic}"
172 |     
    | ^^^^
173 |     query = write_kafka_stream_to_bronze(
174 |         parsed_stream,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:179:1
    |
177 |         trigger_interval="10 seconds"
178 |     )
179 |     
    | ^^^^
180 |     # Wait for termination (or run in background)
181 |     logger.info("Streaming query started. Waiting for termination...")
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:187:14
    |
185 | def save_kafka_offsets(
186 |     spark: SparkSession,
187 |     offsets: Dict[str, Any],
    |              ^^^^
188 |     offset_path: str
189 | ):
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:192:1
    |
190 |     """
191 |     Save Kafka offsets for manual tracking/resume.
192 |     
    | ^^^^
193 |     Args:
194 |         spark: SparkSession
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:200:1
    |
198 |     import json
199 |     from datetime import datetime
200 |     
    | ^^^^
201 |     offset_data = {
202 |         "timestamp": datetime.utcnow().isoformat(),
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:205:1
    |
203 |         "offsets": offsets
204 |     }
205 |     
    | ^^^^
206 |     # Write to S3 or local
207 |     if offset_path.startswith("s3://"):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:223:1
    |
221 |     from pathlib import Path
222 |     sys.path.insert(0, str(Path(__file__).parent.parent.parent))
223 |     
    | ^^^^
224 |     from project_a.utils.spark_session import build_spark
225 |     from project_a.utils.config import load_conf
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
   --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:224:5
    |
222 |       sys.path.insert(0, str(Path(__file__).parent.parent.parent))
223 |       
224 | /     from project_a.utils.spark_session import build_spark
225 | |     from project_a.utils.config import load_conf
    | |________________________________________________^
226 |       
227 |       config = load_conf("config/local.yaml")
    |
help: Organize imports

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:226:1
    |
224 |     from project_a.utils.spark_session import build_spark
225 |     from project_a.utils.config import load_conf
226 |     
    | ^^^^
227 |     config = load_conf("config/local.yaml")
228 |     spark = build_spark(app_name="kafka_streaming", config=config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/kafka_orders_stream.py:229:1
    |
227 |     config = load_conf("config/local.yaml")
228 |     spark = build_spark(app_name="kafka_streaming", config=config)
229 |     
    | ^^^^
230 |     try:
231 |         stream_orders_from_kafka(spark, config)
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/extract/redshift_behavior.py:2:1
   |
 1 |   """Extract Redshift customer behavior data with CDC/incremental support."""
 2 | / import logging
 3 | | import argparse
 4 | | from typing import Dict, Any, Optional
 5 | | from datetime import datetime
 6 | | from pyspark.sql import SparkSession, DataFrame
 7 | | from pyspark.sql.functions import lit, current_timestamp, col
 8 | | from project_a.utils.watermark_utils import get_watermark, upsert_watermark, get_latest_timestamp_from_df
 9 | | from project_a.monitoring.lineage_decorator import lineage_job
10 | | from project_a.monitoring.metrics_collector import emit_rowcount, emit_duration
11 | | import time
   | |___________^
12 |
13 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
 --> src/project_a/pyspark_interview_project/extract/redshift_behavior.py:4:1
  |
2 | import logging
3 | import argparse
4 | from typing import Dict, Any, Optional
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
5 | from datetime import datetime
6 | from pyspark.sql import SparkSession, DataFrame
  |

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/extract/redshift_behavior.py:23:13
   |
21 | def extract_redshift_behavior(
22 |     spark: SparkSession,
23 |     config: Dict[str, Any],
   |             ^^^^
24 |     since_ts: Optional[datetime] = None,
25 |     **kwargs
   |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/extract/redshift_behavior.py:24:15
   |
22 |     spark: SparkSession,
23 |     config: Dict[str, Any],
24 |     since_ts: Optional[datetime] = None,
   |               ^^^^^^^^^^^^^^^^^^
25 |     **kwargs
26 | ) -> DataFrame:
   |
help: Convert to `X | None`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/redshift_behavior.py:29:1
   |
27 |     """
28 |     Extract customer behavior from Redshift source with incremental support.
29 |     
   | ^^^^
30 |     Args:
31 |         spark: SparkSession object
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/redshift_behavior.py:35:1
   |
33 |         since_ts: Optional timestamp to filter records (if None, uses watermark)
34 |         **kwargs: Additional arguments
35 |         
   | ^^^^^^^^
36 |     Returns:
37 |         DataFrame with Redshift customer behavior data
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/redshift_behavior.py:41:1
   |
39 |     logger.info("Extracting Redshift customer behavior data")
40 |     start_time = time.time()
41 |     
   | ^^^^
42 |     # Get watermark if not provided
43 |     if since_ts is None:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/redshift_behavior.py:50:1
   |
48 |         else:
49 |             logger.info("No watermark found, performing full load")
50 |     
   | ^^^^
51 |     try:
52 |         # For local dev, use sample data
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/redshift_behavior.py:62:1
   |
60 |             # In AWS, use Redshift JDBC connection with incremental query
61 |             redshift_config = config.get('data_sources', {}).get('redshift', {})
62 |             
   | ^^^^^^^^^^^^
63 |             # Build query with watermark filter
64 |             table_name = redshift_config.get('table', 'customer_behavior')
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/redshift_behavior.py:75:1
   |
73 |                 query = f"SELECT * FROM {table_name} ORDER BY event_ts"
74 |                 logger.info("Full load: no watermark filter")
75 |             
   | ^^^^^^^^^^^^
76 |             df = spark.read \
77 |                 .format("jdbc") \
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/redshift_behavior.py:83:1
   |
81 |                 .option("password", redshift_config.get('password')) \
82 |                 .load()
83 |         
   | ^^^^^^^^
84 |         # Add metadata columns
85 |         df = df.withColumn("record_source", lit("redshift")) \
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/redshift_behavior.py:88:1
   |
86 |                .withColumn("record_table", lit("customer_behavior")) \
87 |                .withColumn("_ingestion_ts", current_timestamp())
88 |         
   | ^^^^^^^^
89 |         record_count = df.count()
90 |         duration_ms = (time.time() - start_time) * 1000
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/redshift_behavior.py:91:1
   |
89 |         record_count = df.count()
90 |         duration_ms = (time.time() - start_time) * 1000
91 |         
   | ^^^^^^^^
92 |         logger.info(f"Successfully extracted {record_count:,} Redshift behavior records")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/redshift_behavior.py:93:1
   |
92 |         logger.info(f"Successfully extracted {record_count:,} Redshift behavior records")
93 |         
   | ^^^^^^^^
94 |         # Emit metrics
95 |         emit_rowcount("records_extracted", record_count, {"source": "redshift", "table": "customer_behavior"}, config)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/redshift_behavior.py:97:1
   |
95 |         emit_rowcount("records_extracted", record_count, {"source": "redshift", "table": "customer_behavior"}, config)
96 |         emit_duration("extraction_duration", duration_ms, {"source": "redshift"}, config)
97 |         
   | ^^^^^^^^
98 |         # Update watermark if records were loaded
99 |         if record_count > 0:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/redshift_behavior.py:103:1
    |
101 |             if latest_ts:
102 |                 upsert_watermark("redshift_behavior", latest_ts, config, spark)
103 |         
    | ^^^^^^^^
104 |         return df
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/redshift_behavior.py:105:1
    |
104 |         return df
105 |         
    | ^^^^^^^^
106 |     except Exception as e:
107 |         logger.error(f"Failed to extract Redshift behavior: {e}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/redshift_behavior.py:117:1
    |
115 |     parser.add_argument("--config", default="config/local.yaml", help="Config file path")
116 |     args = parser.parse_args()
117 |     
    | ^^^^
118 |     since_ts = None
119 |     if args.since_ts:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/redshift_behavior.py:121:1
    |
119 |     if args.since_ts:
120 |         since_ts = datetime.fromisoformat(args.since_ts.replace('Z', '+00:00'))
121 |     
    | ^^^^
122 |     # Run extraction (would need SparkSession setup)
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/extract/snowflake_orders.py:2:1
   |
 1 |   """Extract Snowflake orders data with CDC/incremental support."""
 2 | / import logging
 3 | | import argparse
 4 | | from typing import Dict, Any, Optional
 5 | | from datetime import datetime
 6 | | from pyspark.sql import SparkSession, DataFrame
 7 | | from pyspark.sql.functions import lit, current_timestamp, col
 8 | | from project_a.utils.watermark_utils import get_watermark, upsert_watermark, get_latest_timestamp_from_df
 9 | | from project_a.utils.secrets import get_snowflake_credentials
10 | | from project_a.monitoring.lineage_decorator import lineage_job
11 | | from project_a.monitoring.metrics_collector import emit_rowcount, emit_duration
12 | | import time
   | |___________^
13 |
14 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
 --> src/project_a/pyspark_interview_project/extract/snowflake_orders.py:4:1
  |
2 | import logging
3 | import argparse
4 | from typing import Dict, Any, Optional
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
5 | from datetime import datetime
6 | from pyspark.sql import SparkSession, DataFrame
  |

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/extract/snowflake_orders.py:24:13
   |
22 | def extract_snowflake_orders(
23 |     spark: SparkSession,
24 |     config: Dict[str, Any],
   |             ^^^^
25 |     since_ts: Optional[datetime] = None,
26 |     **kwargs
   |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/extract/snowflake_orders.py:25:15
   |
23 |     spark: SparkSession,
24 |     config: Dict[str, Any],
25 |     since_ts: Optional[datetime] = None,
   |               ^^^^^^^^^^^^^^^^^^
26 |     **kwargs
27 | ) -> DataFrame:
   |
help: Convert to `X | None`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/snowflake_orders.py:30:1
   |
28 |     """
29 |     Extract orders from Snowflake source with incremental support.
30 |     
   | ^^^^
31 |     Args:
32 |         spark: SparkSession object
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/snowflake_orders.py:36:1
   |
34 |         since_ts: Optional timestamp to filter records (if None, uses watermark)
35 |         **kwargs: Additional arguments
36 |         
   | ^^^^^^^^
37 |     Returns:
38 |         DataFrame with Snowflake orders data
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/snowflake_orders.py:42:1
   |
40 |     logger.info("Extracting Snowflake orders data")
41 |     start_time = time.time()
42 |     
   | ^^^^
43 |     # Get watermark if not provided
44 |     if since_ts is None:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/snowflake_orders.py:51:1
   |
49 |         else:
50 |             logger.info("No watermark found, performing full load")
51 |     
   | ^^^^
52 |     try:
53 |         # For local dev, use sample data
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/snowflake_orders.py:74:1
   |
72 |                 "sfWarehouse": snowflake_config.get("warehouse"),
73 |             }
74 |             
   | ^^^^^^^^^^^^
75 |             # Build query with watermark filter
76 |             if since_ts:
   |
help: Remove whitespace from blank line

W291 Trailing whitespace
  --> src/project_a/pyspark_interview_project/extract/snowflake_orders.py:78:41
   |
76 |             if since_ts:
77 |                 query = f"""
78 |                     SELECT * FROM ORDERS 
   |                                         ^
79 |                     WHERE LAST_MODIFIED_TS > '{since_ts.isoformat()}'
80 |                     ORDER BY LAST_MODIFIED_TS
   |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/snowflake_orders.py:86:1
   |
84 |                 query = "SELECT * FROM ORDERS ORDER BY LAST_MODIFIED_TS"
85 |                 logger.info("Full load: no watermark filter")
86 |             
   | ^^^^^^^^^^^^
87 |             df = spark.read \
88 |                 .format("snowflake") \
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/snowflake_orders.py:92:1
   |
90 |                 .option("query", query) \
91 |                 .load()
92 |         
   | ^^^^^^^^
93 |         # Add metadata columns
94 |         df = df.withColumn("record_source", lit("snowflake")) \
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/extract/snowflake_orders.py:97:1
   |
95 |                .withColumn("record_table", lit("orders")) \
96 |                .withColumn("_ingestion_ts", current_timestamp())
97 |         
   | ^^^^^^^^
98 |         record_count = df.count()
99 |         duration_ms = (time.time() - start_time) * 1000
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/snowflake_orders.py:100:1
    |
 98 |         record_count = df.count()
 99 |         duration_ms = (time.time() - start_time) * 1000
100 |         
    | ^^^^^^^^
101 |         logger.info(f"Successfully extracted {record_count:,} Snowflake orders")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/snowflake_orders.py:102:1
    |
101 |         logger.info(f"Successfully extracted {record_count:,} Snowflake orders")
102 |         
    | ^^^^^^^^
103 |         # Emit metrics
104 |         emit_rowcount("records_extracted", record_count, {"source": "snowflake", "table": "orders"}, config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/snowflake_orders.py:106:1
    |
104 |         emit_rowcount("records_extracted", record_count, {"source": "snowflake", "table": "orders"}, config)
105 |         emit_duration("extraction_duration", duration_ms, {"source": "snowflake"}, config)
106 |         
    | ^^^^^^^^
107 |         # Update watermark if records were loaded
108 |         if record_count > 0:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/snowflake_orders.py:112:1
    |
110 |             if latest_ts:
111 |                 upsert_watermark("snowflake_orders", latest_ts, config, spark)
112 |         
    | ^^^^^^^^
113 |         return df
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/snowflake_orders.py:114:1
    |
113 |         return df
114 |         
    | ^^^^^^^^
115 |     except Exception as e:
116 |         logger.error(f"Failed to extract Snowflake orders: {e}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/snowflake_orders.py:126:1
    |
124 |     parser.add_argument("--config", default="config/local.yaml", help="Config file path")
125 |     args = parser.parse_args()
126 |     
    | ^^^^
127 |     since_ts = None
128 |     if args.since_ts:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/extract/snowflake_orders.py:130:1
    |
128 |     if args.since_ts:
129 |         since_ts = datetime.fromisoformat(args.since_ts.replace('Z', '+00:00'))
130 |     
    | ^^^^
131 |     # Run extraction (would need SparkSession setup)
132 |     # extract_snowflake_orders(spark, config, since_ts=since_ts)
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
 --> src/project_a/pyspark_interview_project/io/__init__.py:1:1
  |
1 | / from .path_resolver import resolve
2 | | from .write_table import write_table, read_table
  | |________________________________________________^
3 |   # Optional imports - may not be available
4 |   try:
  |
help: Organize imports

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/io/delta_writer.py:8:1
   |
 6 |   """
 7 |
 8 | / import logging
 9 | | import os
10 | | import shutil
11 | | from pathlib import Path
12 | | from typing import Optional, List
13 | | from pyspark.sql import SparkSession, DataFrame
   | |_______________________________________________^
14 |
15 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

F401 [*] `pathlib.Path` imported but unused
  --> src/project_a/pyspark_interview_project/io/delta_writer.py:11:21
   |
 9 | import os
10 | import shutil
11 | from pathlib import Path
   |                     ^^^^
12 | from typing import Optional, List
13 | from pyspark.sql import SparkSession, DataFrame
   |
help: Remove unused import: `pathlib.Path`

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/pyspark_interview_project/io/delta_writer.py:12:1
   |
10 | import shutil
11 | from pathlib import Path
12 | from typing import Optional, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
13 | from pyspark.sql import SparkSession, DataFrame
   |

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/delta_writer.py:21:1
   |
19 |     """
20 |     Determine write format based on environment.
21 |     
   | ^^^^
22 |     Args:
23 |         config: Configuration dictionary
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/delta_writer.py:24:1
   |
22 |     Args:
23 |         config: Configuration dictionary
24 |         
   | ^^^^^^^^
25 |     Returns:
26 |         "delta" for AWS/EMR, "parquet" for local
   |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/io/delta_writer.py:51:19
   |
49 |     table_name: str,
50 |     config: dict,
51 |     partition_by: Optional[List[str]] = None,
   |                   ^^^^^^^^^^^^^^^^^^^
52 |     mode: str = "overwrite"
53 | ) -> None:
   |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/pyspark_interview_project/io/delta_writer.py:51:28
   |
49 |     table_name: str,
50 |     config: dict,
51 |     partition_by: Optional[List[str]] = None,
   |                            ^^^^
52 |     mode: str = "overwrite"
53 | ) -> None:
   |
help: Replace with `list`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/delta_writer.py:56:1
   |
54 |     """
55 |     Write DataFrame to Delta or Parquet format.
56 |     
   | ^^^^
57 |     Args:
58 |         df: DataFrame to write
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/delta_writer.py:67:1
   |
65 |     write_format = get_write_format(config)
66 |     target_path = f"{path.rstrip('/')}/{table_name}"
67 |     
   | ^^^^
68 |     logger.info(f"Writing {table_name} to {target_path} (format: {write_format}, mode: {mode})")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/delta_writer.py:69:1
   |
68 |     logger.info(f"Writing {table_name} to {target_path} (format: {write_format}, mode: {mode})")
69 |     
   | ^^^^
70 |     # For local Parquet writes, delete existing directory first
71 |     if write_format == "parquet" and target_path.startswith("file://"):
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/delta_writer.py:76:1
   |
74 |         # Relative path - assume local
75 |         delete_if_exists_local(target_path)
76 |     
   | ^^^^
77 |     writer = df.write.format(write_format).mode(mode)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/delta_writer.py:78:1
   |
77 |     writer = df.write.format(write_format).mode(mode)
78 |     
   | ^^^^
79 |     # Add partitioning if specified
80 |     if partition_by:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/delta_writer.py:89:1
   |
87 |         else:
88 |             logger.warning(f"Partition columns {partition_by} not found, skipping partitioning")
89 |     
   | ^^^^
90 |     # Write
91 |     try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/delta_writer.py:95:1
   |
93 |         # Spark will write an empty parquet file with schema metadata
94 |         writer.save(target_path)
95 |         
   | ^^^^^^^^
96 |         # Verify write succeeded (especially for empty DataFrames)
97 |         if write_format == "parquet":
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/delta_writer.py:106:1
    |
104 |                 os.makedirs(clean_path, exist_ok=True)
105 |                 # Spark should have written schema metadata, but if not, we'll rely on the DataFrame schema
106 |         
    | ^^^^^^^^
107 |         logger.info(f"âœ… Successfully wrote {table_name} to {target_path}")
108 |     except Exception as e:
    |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/io/delta_writer.py:118:17
    |
116 |     table_name: str,
117 |     config: dict,
118 |     z_order_by: Optional[List[str]] = None
    |                 ^^^^^^^^^^^^^^^^^^^
119 | ) -> None:
120 |     """
    |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/pyspark_interview_project/io/delta_writer.py:118:26
    |
116 |     table_name: str,
117 |     config: dict,
118 |     z_order_by: Optional[List[str]] = None
    |                          ^^^^
119 | ) -> None:
120 |     """
    |
help: Replace with `list`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/delta_writer.py:122:1
    |
120 |     """
121 |     Optimize Delta table (Z-ORDER and OPTIMIZE).
122 |     
    | ^^^^
123 |     Args:
124 |         spark: SparkSession
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/delta_writer.py:131:1
    |
129 |     """
130 |     write_format = get_write_format(config)
131 |     
    | ^^^^
132 |     if write_format != "delta":
133 |         logger.info(f"Skipping OPTIMIZE for {table_name} (not Delta format)")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/delta_writer.py:135:1
    |
133 |         logger.info(f"Skipping OPTIMIZE for {table_name} (not Delta format)")
134 |         return
135 |     
    | ^^^^
136 |     try:
137 |         from delta.tables import DeltaTable
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/delta_writer.py:138:1
    |
136 |     try:
137 |         from delta.tables import DeltaTable
138 |         
    | ^^^^^^^^
139 |         table_path = f"{path.rstrip('/')}/{table_name}"
140 |         delta_table = DeltaTable.forPath(spark, table_path)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/delta_writer.py:141:1
    |
139 |         table_path = f"{path.rstrip('/')}/{table_name}"
140 |         delta_table = DeltaTable.forPath(spark, table_path)
141 |         
    | ^^^^^^^^
142 |         # OPTIMIZE
143 |         logger.info(f"Optimizing {table_name}...")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/delta_writer.py:145:1
    |
143 |         logger.info(f"Optimizing {table_name}...")
144 |         delta_table.optimize().executeCompaction()
145 |         
    | ^^^^^^^^
146 |         # Z-ORDER if specified
147 |         if z_order_by:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/delta_writer.py:153:1
    |
151 |                 logger.info(f"Z-ORDERing {table_name} by: {valid_z_order}")
152 |                 delta_table.optimize().executeZOrderBy(*valid_z_order)
153 |         
    | ^^^^^^^^
154 |         logger.info(f"âœ… Optimized {table_name}")
155 |     except ImportError:
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/delta_writer.py:170:1
    |
168 |     """
169 |     VACUUM Delta table to remove old files.
170 |     
    | ^^^^
171 |     Args:
172 |         spark: SparkSession
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/delta_writer.py:179:1
    |
177 |     """
178 |     write_format = get_write_format(config)
179 |     
    | ^^^^
180 |     if write_format != "delta":
181 |         logger.info(f"Skipping VACUUM for {table_name} (not Delta format)")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/delta_writer.py:183:1
    |
181 |         logger.info(f"Skipping VACUUM for {table_name} (not Delta format)")
182 |         return
183 |     
    | ^^^^
184 |     try:
185 |         from delta.tables import DeltaTable
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/delta_writer.py:186:1
    |
184 |     try:
185 |         from delta.tables import DeltaTable
186 |         
    | ^^^^^^^^
187 |         table_path = f"{path.rstrip('/')}/{table_name}"
188 |         delta_table = DeltaTable.forPath(spark, table_path)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/delta_writer.py:189:1
    |
187 |         table_path = f"{path.rstrip('/')}/{table_name}"
188 |         delta_table = DeltaTable.forPath(spark, table_path)
189 |         
    | ^^^^^^^^
190 |         logger.info(f"VACUUMing {table_name} (retention: {retention_hours}h)...")
191 |         delta_table.vacuum(retentionHours=retention_hours)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/delta_writer.py:192:1
    |
190 |         logger.info(f"VACUUMing {table_name} (retention: {retention_hours}h)...")
191 |         delta_table.vacuum(retentionHours=retention_hours)
192 |         
    | ^^^^^^^^
193 |         logger.info(f"âœ… VACUUMed {table_name}")
194 |     except ImportError:
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
 --> src/project_a/pyspark_interview_project/io/path_resolver.py:1:1
  |
1 | / from __future__ import annotations
2 | | from typing import Dict
  | |_______________________^
  |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
 --> src/project_a/pyspark_interview_project/io/path_resolver.py:2:1
  |
1 | from __future__ import annotations
2 | from typing import Dict
  | ^^^^^^^^^^^^^^^^^^^^^^^
  |

UP006 [*] Use `dict` instead of `Dict` for type annotation
 --> src/project_a/pyspark_interview_project/io/path_resolver.py:5:28
  |
5 | def resolve(uri: str, cfg: Dict[str, dict]) -> str:
  |                            ^^^^
6 |     """
7 |     Map logical lake:// URIs to cloud paths.
  |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/path_resolver.py:9:1
   |
 7 |     Map logical lake:// URIs to cloud paths.
 8 |     Example: lake://bronze/returns -> s3a://... or abfss://...
 9 |     
   | ^^^^
10 |     Ensures consistent path resolution across environments.
11 |     """
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/path_resolver.py:14:1
   |
12 |     if not uri.startswith("lake://"):
13 |         return uri
14 |     
   | ^^^^
15 |     # Get lake root from config
16 |     lake_root = cfg.get("paths", {}).get("lake_root", "")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/path_resolver.py:25:1
   |
23 |         else:
24 |             lake_root = f"s3a://{bucket}"  # Default to S3 for now
25 |     
   | ^^^^
26 |     # Ensure lake_root doesn't end with slash
27 |     root = lake_root.rstrip("/")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/path_resolver.py:28:1
   |
26 |     # Ensure lake_root doesn't end with slash
27 |     root = lake_root.rstrip("/")
28 |     
   | ^^^^
29 |     # Remove lake:// prefix and build path
30 |     relative_path = uri.replace('lake://', '', 1)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/path_resolver.py:32:1
   |
30 |     relative_path = uri.replace('lake://', '', 1)
31 |     full_path = f"{root}/{relative_path}".rstrip("/")
32 |     
   | ^^^^
33 |     return full_path
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/io/publish.py:7:1
   |
 5 |   """
 6 |
 7 | / import logging
 8 | | from typing import Dict, Any, Optional, List
 9 | | from pyspark.sql import SparkSession, DataFrame
10 | | from pyspark.sql.functions import current_timestamp
11 | |
12 | | from project_a.utils.secrets import get_snowflake_credentials, get_redshift_credentials
   | |_______________________________________________________________________________________^
13 |
14 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/io/publish.py:8:1
   |
 7 | import logging
 8 | from typing import Dict, Any, Optional, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 9 | from pyspark.sql import SparkSession, DataFrame
10 | from pyspark.sql.functions import current_timestamp
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/pyspark_interview_project/io/publish.py:8:1
   |
 7 | import logging
 8 | from typing import Dict, Any, Optional, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 9 | from pyspark.sql import SparkSession, DataFrame
10 | from pyspark.sql.functions import current_timestamp
   |

F401 [*] `pyspark.sql.SparkSession` imported but unused
  --> src/project_a/pyspark_interview_project/io/publish.py:9:25
   |
 7 | import logging
 8 | from typing import Dict, Any, Optional, List
 9 | from pyspark.sql import SparkSession, DataFrame
   |                         ^^^^^^^^^^^^
10 | from pyspark.sql.functions import current_timestamp
   |
help: Remove unused import: `pyspark.sql.SparkSession`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/io/publish.py:21:11
   |
19 |     table_name: str,
20 |     mode: str = "merge",
21 |     keys: Optional[List[str]] = None,
   |           ^^^^^^^^^^^^^^^^^^^
22 |     config: Optional[Dict[str, Any]] = None
23 | ) -> bool:
   |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/pyspark_interview_project/io/publish.py:21:20
   |
19 |     table_name: str,
20 |     mode: str = "merge",
21 |     keys: Optional[List[str]] = None,
   |                    ^^^^
22 |     config: Optional[Dict[str, Any]] = None
23 | ) -> bool:
   |
help: Replace with `list`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/io/publish.py:22:13
   |
20 |     mode: str = "merge",
21 |     keys: Optional[List[str]] = None,
22 |     config: Optional[Dict[str, Any]] = None
   |             ^^^^^^^^^^^^^^^^^^^^^^^^
23 | ) -> bool:
24 |     """
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/io/publish.py:22:22
   |
20 |     mode: str = "merge",
21 |     keys: Optional[List[str]] = None,
22 |     config: Optional[Dict[str, Any]] = None
   |                      ^^^^
23 | ) -> bool:
24 |     """
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/publish.py:26:1
   |
24 |     """
25 |     Publish DataFrame to Snowflake table.
26 |     
   | ^^^^
27 |     Args:
28 |         df: DataFrame to publish
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/publish.py:33:1
   |
31 |         keys: Primary key columns for MERGE operation
32 |         config: Configuration dictionary
33 |         
   | ^^^^^^^^
34 |     Returns:
35 |         True if successful
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/publish.py:38:1
   |
36 |     """
37 |     logger.info(f"Publishing to Snowflake: {table_name} (mode={mode})")
38 |     
   | ^^^^
39 |     creds = get_snowflake_credentials(config or {})
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/publish.py:40:1
   |
39 |     creds = get_snowflake_credentials(config or {})
40 |     
   | ^^^^
41 |     # Build Snowflake options
42 |     account = creds.get('account', '').replace('.snowflakecomputing.com', '')
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/publish.py:51:1
   |
49 |         "sfWarehouse": creds.get("warehouse"),
50 |     }
51 |     
   | ^^^^
52 |     df = df.withColumn("_load_ts", current_timestamp())
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/publish.py:53:1
   |
52 |     df = df.withColumn("_load_ts", current_timestamp())
53 |     
   | ^^^^
54 |     if mode == "merge" and keys:
55 |         # Use staging table + MERGE pattern
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/publish.py:57:1
   |
55 |         # Use staging table + MERGE pattern
56 |         staging_table = f"{table_name}_STAGING"
57 |         
   | ^^^^^^^^
58 |         # Write to staging
59 |         df.write \
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/publish.py:65:1
   |
63 |             .mode("overwrite") \
64 |             .save()
65 |         
   | ^^^^^^^^
66 |         # Build MERGE SQL
67 |         columns = df.columns
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/publish.py:72:1
   |
70 |         insert_vals = ", ".join([f"s.{c}" for c in columns])
71 |         on_clause = " AND ".join([f"t.{k} = s.{k}" for k in keys])
72 |         
   | ^^^^^^^^
73 |         merge_sql = f"""
74 |             MERGE INTO {table_name} t
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/publish.py:80:1
   |
78 |             WHEN NOT MATCHED THEN INSERT ({insert_cols}) VALUES ({insert_vals})
79 |         """.strip()
80 |         
   | ^^^^^^^^
81 |         # Execute MERGE via JDBC
82 |         jdbc_url = (
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/publish.py:88:1
   |
86 |             f"warehouse={sf_options['sfWarehouse']}"
87 |         )
88 |         
   | ^^^^^^^^
89 |         spark = df.sparkSession
90 |         spark.read \
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/publish.py:99:1
    |
 97 |             .load() \
 98 |             .collect()
 99 |         
    | ^^^^^^^^
100 |         # Cleanup staging
101 |         spark.read \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/publish.py:110:1
    |
108 |             .load() \
109 |             .collect()
110 |         
    | ^^^^^^^^
111 |         logger.info(f"âœ… MERGE completed for {table_name}")
112 |     else:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/publish.py:120:1
    |
118 |             .mode(mode) \
119 |             .save()
120 |         
    | ^^^^^^^^
121 |         logger.info(f"âœ… {mode} completed for {table_name}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/publish.py:122:1
    |
121 |         logger.info(f"âœ… {mode} completed for {table_name}")
122 |     
    | ^^^^
123 |     return True
    |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/io/publish.py:130:13
    |
128 |     table_name: str,
129 |     mode: str = "append",
130 |     config: Optional[Dict[str, Any]] = None
    |             ^^^^^^^^^^^^^^^^^^^^^^^^
131 | ) -> bool:
132 |     """
    |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/io/publish.py:130:22
    |
128 |     table_name: str,
129 |     mode: str = "append",
130 |     config: Optional[Dict[str, Any]] = None
    |                      ^^^^
131 | ) -> bool:
132 |     """
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/publish.py:134:1
    |
132 |     """
133 |     Publish DataFrame to Redshift table.
134 |     
    | ^^^^
135 |     Args:
136 |         df: DataFrame to publish
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/publish.py:140:1
    |
138 |         mode: Write mode ("append", "overwrite")
139 |         config: Configuration dictionary
140 |         
    | ^^^^^^^^
141 |     Returns:
142 |         True if successful
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/publish.py:145:1
    |
143 |     """
144 |     logger.info(f"Publishing to Redshift: {table_name} (mode={mode})")
145 |     
    | ^^^^
146 |     creds = get_redshift_credentials(config or {})
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/publish.py:147:1
    |
146 |     creds = get_redshift_credentials(config or {})
147 |     
    | ^^^^
148 |     # Build JDBC URL
149 |     jdbc_url = (
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/publish.py:153:1
    |
151 |         f"{creds.get('database')}"
152 |     )
153 |     
    | ^^^^
154 |     df = df.withColumn("_load_ts", current_timestamp())
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/publish.py:155:1
    |
154 |     df = df.withColumn("_load_ts", current_timestamp())
155 |     
    | ^^^^
156 |     # Write to Redshift
157 |     df.write \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/publish.py:166:1
    |
164 |         .mode(mode) \
165 |         .save()
166 |     
    | ^^^^
167 |     logger.info(f"âœ… Published {df.count():,} rows to Redshift {table_name}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/publish.py:168:1
    |
167 |     logger.info(f"âœ… Published {df.count():,} rows to Redshift {table_name}")
168 |     
    | ^^^^
169 |     return True
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:7:1
   |
 5 |   """
 6 |
 7 | / import logging
 8 | | from typing import Dict, Any, Optional, List
 9 | | from pyspark.sql import DataFrame, SparkSession
   | |_______________________________________________^
10 |
11 |   # Optional import - secrets module may not exist
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
 --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:8:1
  |
7 | import logging
8 | from typing import Dict, Any, Optional, List
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
9 | from pyspark.sql import DataFrame, SparkSession
  |

UP035 `typing.List` is deprecated, use `list` instead
 --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:8:1
  |
7 | import logging
8 | from typing import Dict, Any, Optional, List
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
9 | from pyspark.sql import DataFrame, SparkSession
  |

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:25:13
   |
23 |     df: DataFrame,
24 |     table_name: str,
25 |     config: Dict[str, Any],
   |             ^^^^
26 |     mode: str = "append",
27 |     pk: Optional[List[str]] = None
   |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:27:9
   |
25 |     config: Dict[str, Any],
26 |     mode: str = "append",
27 |     pk: Optional[List[str]] = None
   |         ^^^^^^^^^^^^^^^^^^^
28 | ) -> bool:
29 |     """
   |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:27:18
   |
25 |     config: Dict[str, Any],
26 |     mode: str = "append",
27 |     pk: Optional[List[str]] = None
   |                  ^^^^
28 | ) -> bool:
29 |     """
   |
help: Replace with `list`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:31:1
   |
29 |     """
30 |     Write DataFrame to Snowflake with optional MERGE support.
31 |     
   | ^^^^
32 |     Args:
33 |         spark: SparkSession
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:39:1
   |
37 |         mode: Write mode ("append", "overwrite", "merge")
38 |         pk: List of primary key columns for MERGE operation
39 |         
   | ^^^^^^^^
40 |     Returns:
41 |         True if successful
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:42:1
   |
40 |     Returns:
41 |         True if successful
42 |         
   | ^^^^^^^^
43 |     Example:
44 |         write_df_to_snowflake(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:50:1
   |
48 |     """
49 |     logger.info(f"Writing DataFrame to Snowflake table: {table_name} (mode={mode})")
50 |     
   | ^^^^
51 |     try:
52 |         # Get Snowflake credentials from config
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:54:1
   |
52 |         # Get Snowflake credentials from config
53 |         sf_creds = get_snowflake_credentials(config)
54 |         
   | ^^^^^^^^
55 |         # Build Snowflake connection options
56 |         account = sf_creds.get('account', '').replace('.snowflakecomputing.com', '')
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:66:1
   |
64 |             "dbtable": table_name.split('.')[-1]  # Table name only
65 |         }
66 |         
   | ^^^^^^^^
67 |         record_count = df.count()
68 |         logger.info(f"Writing {record_count:,} records to {table_name}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:69:1
   |
67 |         record_count = df.count()
68 |         logger.info(f"Writing {record_count:,} records to {table_name}")
69 |         
   | ^^^^^^^^
70 |         # Handle different write modes
71 |         if mode == "merge" and pk:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:81:1
   |
79 |                 .mode(mode) \
80 |                 .save()
81 |             
   | ^^^^^^^^^^^^
82 |             logger.info(f"âœ… Successfully wrote {record_count:,} records to {table_name} (mode={mode})")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:83:1
   |
82 |             logger.info(f"âœ… Successfully wrote {record_count:,} records to {table_name} (mode={mode})")
83 |         
   | ^^^^^^^^
84 |         return True
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:85:1
   |
84 |         return True
85 |         
   | ^^^^^^^^
86 |     except Exception as e:
87 |         logger.error(f"âŒ Failed to write to Snowflake {table_name}: {e}")
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:95:17
   |
93 |     df: DataFrame,
94 |     table_name: str,
95 |     sf_options: Dict[str, Any],
   |                 ^^^^
96 |     pk: List[str]
97 | ) -> None:
   |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:96:9
   |
94 |     table_name: str,
95 |     sf_options: Dict[str, Any],
96 |     pk: List[str]
   |         ^^^^
97 | ) -> None:
98 |     """
   |
help: Replace with `list`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:100:1
    |
 98 |     """
 99 |     Write DataFrame to Snowflake using MERGE for idempotent upserts.
100 |     
    | ^^^^
101 |     Args:
102 |         spark: SparkSession
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:110:1
    |
108 |     # Create staging table
109 |     staging_table = f"{table_name}_staging"
110 |     
    | ^^^^
111 |     logger.info(f"Writing to staging table: {staging_table}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:112:1
    |
111 |     logger.info(f"Writing to staging table: {staging_table}")
112 |     
    | ^^^^
113 |     # Write to staging
114 |     df.write \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:120:1
    |
118 |         .mode("overwrite") \
119 |         .save()
120 |     
    | ^^^^
121 |     # Build MERGE SQL
122 |     merge_sql = _build_snowflake_merge_sql(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:128:1
    |
126 |         columns=df.columns
127 |     )
128 |     
    | ^^^^
129 |     logger.info(f"Executing MERGE SQL on {table_name}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:130:1
    |
129 |     logger.info(f"Executing MERGE SQL on {table_name}")
130 |     
    | ^^^^
131 |     # Execute MERGE via JDBC
132 |     sf_url = f"jdbc:snowflake://{sf_options['sfURL']}"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:133:1
    |
131 |     # Execute MERGE via JDBC
132 |     sf_url = f"jdbc:snowflake://{sf_options['sfURL']}"
133 |     
    | ^^^^
134 |     spark.read.format("jdbc") \
135 |         .option("url", sf_url) \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:140:1
    |
138 |         .option("query", merge_sql) \
139 |         .load().collect()  # Execute query
140 |     
    | ^^^^
141 |     logger.info(f"âœ… MERGE completed for {table_name}")
    |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:147:9
    |
145 |     target_table: str,
146 |     staging_table: str,
147 |     pk: List[str],
    |         ^^^^
148 |     columns: List[str]
149 | ) -> str:
    |
help: Replace with `list`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:148:14
    |
146 |     staging_table: str,
147 |     pk: List[str],
148 |     columns: List[str]
    |              ^^^^
149 | ) -> str:
150 |     """
    |
help: Replace with `list`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:152:1
    |
150 |     """
151 |     Build Snowflake MERGE SQL statement.
152 |     
    | ^^^^
153 |     Args:
154 |         target_table: Target table name
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:158:1
    |
156 |         pk: Primary key columns
157 |         columns: All columns to merge
158 |         
    | ^^^^^^^^
159 |     Returns:
160 |         MERGE SQL statement
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:164:1
    |
162 |     target_alias = "target"
163 |     source_alias = "source"
164 |     
    | ^^^^
165 |     # ON clause
166 |     on_clause = " AND ".join([
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:169:1
    |
167 |         f"{target_alias}.{key} = {source_alias}.{key}" for key in pk
168 |     ])
169 |     
    | ^^^^
170 |     # UPDATE clause (all columns except PK)
171 |     update_cols = [col for col in columns if col not in pk]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:175:1
    |
173 |         f"{col} = {source_alias}.{col}" for col in update_cols
174 |     ])
175 |     
    | ^^^^
176 |     # INSERT clause
177 |     insert_cols = ", ".join(columns)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:179:1
    |
177 |     insert_cols = ", ".join(columns)
178 |     insert_values = ", ".join([f"{source_alias}.{col}" for col in columns])
179 |     
    | ^^^^
180 |     merge_sql = f"""
181 |         MERGE INTO {target_table} AS {target_alias}
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/snowflake_writer.py:190:1
    |
188 |             VALUES ({insert_values})
189 |     """
190 |     
    | ^^^^
191 |     return merge_sql.strip()
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/io/write_table.py:8:1
   |
 6 |   """
 7 |
 8 | / import logging
 9 | | from typing import Dict, Any, Optional
10 | | from pyspark.sql import DataFrame, SparkSession
   | |_______________________________________________^
11 |
12 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/io/write_table.py:9:1
   |
 8 | import logging
 9 | from typing import Dict, Any, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 | from pyspark.sql import DataFrame, SparkSession
   |

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/io/write_table.py:19:10
   |
17 |     table_name: str,
18 |     mode: str = "overwrite",
19 |     cfg: Optional[Dict[str, Any]] = None,
   |          ^^^^^^^^^^^^^^^^^^^^^^^^
20 |     partition_cols: Optional[list] = None,
21 |     spark: Optional[SparkSession] = None
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/io/write_table.py:19:19
   |
17 |     table_name: str,
18 |     mode: str = "overwrite",
19 |     cfg: Optional[Dict[str, Any]] = None,
   |                   ^^^^
20 |     partition_cols: Optional[list] = None,
21 |     spark: Optional[SparkSession] = None
   |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/io/write_table.py:20:21
   |
18 |     mode: str = "overwrite",
19 |     cfg: Optional[Dict[str, Any]] = None,
20 |     partition_cols: Optional[list] = None,
   |                     ^^^^^^^^^^^^^^
21 |     spark: Optional[SparkSession] = None
22 | ) -> None:
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/io/write_table.py:21:12
   |
19 |     cfg: Optional[Dict[str, Any]] = None,
20 |     partition_cols: Optional[list] = None,
21 |     spark: Optional[SparkSession] = None
   |            ^^^^^^^^^^^^^^^^^^^^^^
22 | ) -> None:
23 |     """
   |
help: Convert to `X | None`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/write_table.py:25:1
   |
23 |     """
24 |     Write DataFrame to table using configured storage format.
25 |     
   | ^^^^
26 |     Supports:
27 |     - Iceberg: Writes to Glue catalog using Iceberg format
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/write_table.py:30:1
   |
28 |     - Delta: Writes to Delta Lake format
29 |     - Parquet: Writes plain Parquet files to S3/local path
30 |     
   | ^^^^
31 |     Args:
32 |         df: DataFrame to write
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/write_table.py:39:1
   |
37 |         partition_cols: Optional list of partition column names
38 |         spark: SparkSession (optional, will infer from df)
39 |     
   | ^^^^
40 |     Examples:
41 |         # Iceberg write (registered in Glue)
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/write_table.py:43:1
   |
41 |         # Iceberg write (registered in Glue)
42 |         write_table(df, "silver.customer_behavior", cfg={"storage": {"format": "iceberg", "catalog": "glue_catalog"}})
43 |         
   | ^^^^^^^^
44 |         # Delta write (to S3 path)
45 |         write_table(df, "s3://bucket/silver/customer_behavior", cfg={"storage": {"format": "delta"}})
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/write_table.py:46:1
   |
44 |         # Delta write (to S3 path)
45 |         write_table(df, "s3://bucket/silver/customer_behavior", cfg={"storage": {"format": "delta"}})
46 |         
   | ^^^^^^^^
47 |         # Parquet write (to S3 path)
48 |         write_table(df, "s3://bucket/silver/customer_behavior", cfg={"storage": {"format": "parquet"}})
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/write_table.py:52:1
   |
50 |     if cfg is None:
51 |         cfg = {}
52 |     
   | ^^^^
53 |     storage = cfg.get("storage", {})
54 |     fmt = storage.get("format", "parquet")
   |
help: Remove whitespace from blank line

F841 Local variable `warehouse` is assigned to but never used
  --> src/project_a/pyspark_interview_project/io/write_table.py:57:5
   |
55 |     catalog = storage.get("catalog", "glue_catalog")
56 |     # Use config-based warehouse path
57 |     warehouse = (
   |     ^^^^^^^^^
58 |         storage.get("warehouse")
59 |         or cfg.get('paths', {}).get('silver_root', 's3://my-etl-lake-demo')
   |
help: Remove assignment to unused variable `warehouse`

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/write_table.py:63:1
   |
61 |     )
62 |     data_lake = cfg.get("data_lake", {})
63 |     
   | ^^^^
64 |     logger.info(f"Writing table '{table_name}' using format '{fmt}'")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/write_table.py:65:1
   |
64 |     logger.info(f"Writing table '{table_name}' using format '{fmt}'")
65 |     
   | ^^^^
66 |     if fmt == "iceberg":
67 |         # Iceberg write: table_name expected as "layer.table_name" (e.g., "silver.customer_behavior")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/write_table.py:69:1
   |
67 |         # Iceberg write: table_name expected as "layer.table_name" (e.g., "silver.customer_behavior")
68 |         full_table_name = f"{catalog}.{table_name}"
69 |         
   | ^^^^^^^^
70 |         logger.info(f"Writing to Iceberg table: {full_table_name}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/write_table.py:71:1
   |
70 |         logger.info(f"Writing to Iceberg table: {full_table_name}")
71 |         
   | ^^^^^^^^
72 |         try:
73 |             writer = df.writeTo(full_table_name).using("iceberg")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/write_table.py:74:1
   |
72 |         try:
73 |             writer = df.writeTo(full_table_name).using("iceberg")
74 |             
   | ^^^^^^^^^^^^
75 |             # Set overwrite mode
76 |             if mode == "overwrite":
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/write_table.py:83:1
   |
81 |             else:
82 |                 writer = writer.createOrReplace()
83 |             
   | ^^^^^^^^^^^^
84 |             # Apply partitioning if specified
85 |             if partition_cols:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/io/write_table.py:87:1
   |
85 |             if partition_cols:
86 |                 writer = writer.partitionedBy(*partition_cols)
87 |             
   | ^^^^^^^^^^^^
88 |             writer.execute()
89 |             logger.info(f"Successfully wrote to Iceberg table: {full_table_name}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/write_table.py:99:1
    |
 97 |                 logger.error(f"Iceberg write failed completely: {e2}")
 98 |                 raise
 99 |         
    | ^^^^^^^^
100 |     elif fmt == "delta":
101 |         # Delta Lake write: table_name is S3 path
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/write_table.py:110:1
    |
108 |             else:
109 |                 base_path = data_lake.get("bronze_path", "data/lakehouse_delta/bronze")
110 |             
    | ^^^^^^^^^^^^
111 |             # Extract table name from "layer.table" format
112 |             if "." in table_name:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/write_table.py:114:1
    |
112 |             if "." in table_name:
113 |                 table_name = table_name.split(".", 1)[1]
114 |             
    | ^^^^^^^^^^^^
115 |             delta_path = f"{base_path}/{table_name}"
116 |         else:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/write_table.py:118:1
    |
116 |         else:
117 |             delta_path = table_name
118 |         
    | ^^^^^^^^
119 |         logger.info(f"Writing to Delta table: {delta_path}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/write_table.py:120:1
    |
119 |         logger.info(f"Writing to Delta table: {delta_path}")
120 |         
    | ^^^^^^^^
121 |         writer = df.write.format("delta").mode(mode)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/write_table.py:122:1
    |
121 |         writer = df.write.format("delta").mode(mode)
122 |         
    | ^^^^^^^^
123 |         if mode == "overwrite":
124 |             writer = writer.option("mergeSchema", "true")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/write_table.py:125:1
    |
123 |         if mode == "overwrite":
124 |             writer = writer.option("mergeSchema", "true")
125 |         
    | ^^^^^^^^
126 |         if partition_cols:
127 |             writer = writer.partitionBy(*partition_cols)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/write_table.py:128:1
    |
126 |         if partition_cols:
127 |             writer = writer.partitionBy(*partition_cols)
128 |         
    | ^^^^^^^^
129 |         writer.save(delta_path)
130 |         logger.info(f"Successfully wrote to Delta table: {delta_path}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/write_table.py:131:1
    |
129 |         writer.save(delta_path)
130 |         logger.info(f"Successfully wrote to Delta table: {delta_path}")
131 |         
    | ^^^^^^^^
132 |     else:
133 |         # Plain Parquet write: table_name is S3/local path
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/write_table.py:142:1
    |
140 |             else:
141 |                 base_path = data_lake.get("bronze_path", "data/lakehouse_delta/bronze")
142 |             
    | ^^^^^^^^^^^^
143 |             # Extract table name from "layer.table" format
144 |             if "." in table_name:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/write_table.py:146:1
    |
144 |             if "." in table_name:
145 |                 table_name = table_name.split(".", 1)[1]
146 |             
    | ^^^^^^^^^^^^
147 |             parquet_path = f"{base_path}/{table_name}"
148 |         else:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/write_table.py:150:1
    |
148 |         else:
149 |             parquet_path = table_name
150 |         
    | ^^^^^^^^
151 |         logger.info(f"Writing to Parquet: {parquet_path}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/write_table.py:152:1
    |
151 |         logger.info(f"Writing to Parquet: {parquet_path}")
152 |         
    | ^^^^^^^^
153 |         writer = df.write.mode(mode).format("parquet").option("compression", "snappy")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/write_table.py:154:1
    |
153 |         writer = df.write.mode(mode).format("parquet").option("compression", "snappy")
154 |         
    | ^^^^^^^^
155 |         if partition_cols:
156 |             writer = writer.partitionBy(*partition_cols)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/write_table.py:157:1
    |
155 |         if partition_cols:
156 |             writer = writer.partitionBy(*partition_cols)
157 |         
    | ^^^^^^^^
158 |         writer.save(parquet_path)
159 |         logger.info(f"Successfully wrote to Parquet: {parquet_path}")
    |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/io/write_table.py:164:10
    |
162 | def read_table(
163 |     table_name: str,
164 |     cfg: Optional[Dict[str, Any]] = None,
    |          ^^^^^^^^^^^^^^^^^^^^^^^^
165 |     spark: Optional[SparkSession] = None
166 | ) -> DataFrame:
    |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/io/write_table.py:164:19
    |
162 | def read_table(
163 |     table_name: str,
164 |     cfg: Optional[Dict[str, Any]] = None,
    |                   ^^^^
165 |     spark: Optional[SparkSession] = None
166 | ) -> DataFrame:
    |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/io/write_table.py:165:12
    |
163 |     table_name: str,
164 |     cfg: Optional[Dict[str, Any]] = None,
165 |     spark: Optional[SparkSession] = None
    |            ^^^^^^^^^^^^^^^^^^^^^^
166 | ) -> DataFrame:
167 |     """
    |
help: Convert to `X | None`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/write_table.py:169:1
    |
167 |     """
168 |     Read table using configured storage format.
169 |     
    | ^^^^
170 |     Args:
171 |         table_name: Table name (e.g., "silver.customer_behavior" for Iceberg,
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/write_table.py:175:1
    |
173 |         cfg: Configuration dictionary
174 |         spark: SparkSession (required for Iceberg, optional for others)
175 |     
    | ^^^^
176 |     Returns:
177 |         DataFrame from the table
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/write_table.py:181:1
    |
179 |     if cfg is None:
180 |         cfg = {}
181 |     
    | ^^^^
182 |     if spark is None:
183 |         raise ValueError("SparkSession is required for reading tables")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/write_table.py:184:1
    |
182 |     if spark is None:
183 |         raise ValueError("SparkSession is required for reading tables")
184 |     
    | ^^^^
185 |     storage = cfg.get("storage", {})
186 |     fmt = storage.get("format", "parquet")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/write_table.py:189:1
    |
187 |     catalog = storage.get("catalog", "glue_catalog")
188 |     data_lake = cfg.get("data_lake", {})
189 |     
    | ^^^^
190 |     logger.info(f"Reading table '{table_name}' using format '{fmt}'")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/write_table.py:191:1
    |
190 |     logger.info(f"Reading table '{table_name}' using format '{fmt}'")
191 |     
    | ^^^^
192 |     if fmt == "iceberg":
193 |         # Iceberg read
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/write_table.py:197:1
    |
195 |         logger.info(f"Reading from Iceberg table: {full_table_name}")
196 |         return spark.table(full_table_name)
197 |         
    | ^^^^^^^^
198 |     elif fmt == "delta":
199 |         # Delta read
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/write_table.py:207:1
    |
205 |             else:
206 |                 base_path = data_lake.get("bronze_path", "data/lakehouse_delta/bronze")
207 |             
    | ^^^^^^^^^^^^
208 |             if "." in table_name:
209 |                 table_name = table_name.split(".", 1)[1]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/write_table.py:210:1
    |
208 |             if "." in table_name:
209 |                 table_name = table_name.split(".", 1)[1]
210 |             
    | ^^^^^^^^^^^^
211 |             delta_path = f"{base_path}/{table_name}"
212 |         else:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/write_table.py:214:1
    |
212 |         else:
213 |             delta_path = table_name
214 |         
    | ^^^^^^^^
215 |         logger.info(f"Reading from Delta table: {delta_path}")
216 |         return spark.read.format("delta").load(delta_path)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/write_table.py:217:1
    |
215 |         logger.info(f"Reading from Delta table: {delta_path}")
216 |         return spark.read.format("delta").load(delta_path)
217 |         
    | ^^^^^^^^
218 |     else:
219 |         # Parquet read
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/write_table.py:227:1
    |
225 |             else:
226 |                 base_path = data_lake.get("bronze_path", "data/lakehouse_delta/bronze")
227 |             
    | ^^^^^^^^^^^^
228 |             if "." in table_name:
229 |                 table_name = table_name.split(".", 1)[1]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/write_table.py:230:1
    |
228 |             if "." in table_name:
229 |                 table_name = table_name.split(".", 1)[1]
230 |             
    | ^^^^^^^^^^^^
231 |             parquet_path = f"{base_path}/{table_name}"
232 |         else:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/io/write_table.py:234:1
    |
232 |         else:
233 |             parquet_path = table_name
234 |         
    | ^^^^^^^^
235 |         logger.info(f"Reading from Parquet: {parquet_path}")
236 |         return spark.read.format("parquet").load(parquet_path)
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/lineage/openlineage_emitter.py:9:1
   |
 7 |   """
 8 |
 9 | / import os
10 | | import logging
11 | | from datetime import datetime
12 | | from typing import List, Dict, Any, Optional
13 | |
14 | | import json
15 | | import urllib.error
16 | | import urllib.request
   | |_____________________^
17 |
18 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/pyspark_interview_project/lineage/openlineage_emitter.py:12:1
   |
10 | import logging
11 | from datetime import datetime
12 | from typing import List, Dict, Any, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
13 |
14 | import json
   |

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/lineage/openlineage_emitter.py:12:1
   |
10 | import logging
11 | from datetime import datetime
12 | from typing import List, Dict, Any, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
13 |
14 | import json
   |

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/lineage/openlineage_emitter.py:21:33
   |
21 | def _get_endpoint(cfg: dict) -> Optional[str]:
   |                                 ^^^^^^^^^^^^^
22 |     url = os.getenv("OPENLINEAGE_URL") or cfg.get("lineage", {}).get("url")
23 |     return url.rstrip("/") if url else None
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/lineage/openlineage_emitter.py:52:49
   |
52 | def build_dataset(name: str, namespace: str) -> Dict[str, Any]:
   |                                                 ^^^^
53 |     return {"namespace": namespace, "name": name}
   |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/pyspark_interview_project/lineage/openlineage_emitter.py:59:13
   |
57 |     cfg: dict,
58 |     job_name: str,
59 |     inputs: List[str],
   |             ^^^^
60 |     outputs: List[str],
61 |     event_type: str,
   |
help: Replace with `list`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/pyspark_interview_project/lineage/openlineage_emitter.py:60:14
   |
58 |     job_name: str,
59 |     inputs: List[str],
60 |     outputs: List[str],
   |              ^^^^
61 |     event_type: str,
62 |     run_id: Optional[str] = None,
   |
help: Replace with `list`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/lineage/openlineage_emitter.py:62:13
   |
60 |     outputs: List[str],
61 |     event_type: str,
62 |     run_id: Optional[str] = None,
   |             ^^^^^^^^^^^^^
63 |     metadata: Optional[Dict[str, Any]] = None,
64 | ) -> None:
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/lineage/openlineage_emitter.py:63:15
   |
61 |     event_type: str,
62 |     run_id: Optional[str] = None,
63 |     metadata: Optional[Dict[str, Any]] = None,
   |               ^^^^^^^^^^^^^^^^^^^^^^^^
64 | ) -> None:
65 |     ns = _get_namespace(cfg)
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/lineage/openlineage_emitter.py:63:24
   |
61 |     event_type: str,
62 |     run_id: Optional[str] = None,
63 |     metadata: Optional[Dict[str, Any]] = None,
   |                        ^^^^
64 | ) -> None:
65 |     ns = _get_namespace(cfg)
   |
help: Replace with `dict`

F841 Local variable `ns` is assigned to but never used
  --> src/project_a/pyspark_interview_project/lineage/openlineage_emitter.py:81:5
   |
80 | def emit_bronze_to_silver(cfg: dict, table: str, run_id: str) -> None:
81 |     ns = _get_namespace(cfg)
   |     ^^
82 |     emit_job_event(
83 |         cfg,
   |
help: Remove assignment to unused variable `ns`

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/monitoring.py:8:1
   |
 6 |   """
 7 |
 8 | / import logging
 9 | | import threading
10 | | import time
11 | | from contextlib import contextmanager
12 | | from dataclasses import asdict, dataclass, field
13 | | from datetime import datetime, timedelta
14 | | from typing import Any, Callable, Dict, List, Optional
15 | |
16 | | import psutil
17 | | import urllib.error
18 | | import urllib.request
19 | | from pyspark.sql import SparkSession
   | |____________________________________^
20 |
21 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 [*] Import from `collections.abc` instead: `Callable`
  --> src/project_a/pyspark_interview_project/monitoring.py:14:1
   |
12 | from dataclasses import asdict, dataclass, field
13 | from datetime import datetime, timedelta
14 | from typing import Any, Callable, Dict, List, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
15 |
16 | import psutil
   |
help: Import from `collections.abc`

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/monitoring.py:14:1
   |
12 | from dataclasses import asdict, dataclass, field
13 | from datetime import datetime, timedelta
14 | from typing import Any, Callable, Dict, List, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
15 |
16 | import psutil
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/pyspark_interview_project/monitoring.py:14:1
   |
12 | from dataclasses import asdict, dataclass, field
13 | from datetime import datetime, timedelta
14 | from typing import Any, Callable, Dict, List, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
15 |
16 | import psutil
   |

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/monitoring.py:30:15
   |
28 |     pipeline_name: str
29 |     start_time: datetime
30 |     end_time: Optional[datetime] = None
   |               ^^^^^^^^^^^^^^^^^^
31 |     status: str = "running"
32 |     total_records_processed: int = 0
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/monitoring.py:38:20
   |
36 |     error_count: int = 0
37 |     warning_count: int = 0
38 |     stage_metrics: Dict[str, Any] = field(default_factory=dict)
   |                    ^^^^
39 |
40 |     def to_dict(self) -> Dict[str, Any]:
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/monitoring.py:40:26
   |
38 |     stage_metrics: Dict[str, Any] = field(default_factory=dict)
39 |
40 |     def to_dict(self) -> Dict[str, Any]:
   |                          ^^^^
41 |         """Convert to dictionary for serialization."""
42 |         data = asdict(self)
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/monitoring.py:58:14
   |
56 |     timestamp: datetime
57 |     source: str
58 |     context: Dict[str, Any] = None
   |              ^^^^
59 |     acknowledged: bool = False
60 |     acknowledged_by: Optional[str] = None
   |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/monitoring.py:60:22
   |
58 |     context: Dict[str, Any] = None
59 |     acknowledged: bool = False
60 |     acknowledged_by: Optional[str] = None
   |                      ^^^^^^^^^^^^^
61 |     acknowledged_at: Optional[datetime] = None
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/monitoring.py:61:22
   |
59 |     acknowledged: bool = False
60 |     acknowledged_by: Optional[str] = None
61 |     acknowledged_at: Optional[datetime] = None
   |                      ^^^^^^^^^^^^^^^^^^
62 |
63 |     def __post_init__(self):
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/monitoring.py:73:23
   |
71 |     def __init__(self, spark: SparkSession):
72 |         self.spark = spark
73 |         self.metrics: Dict[str, PipelineMetrics] = {}
   |                       ^^^^
74 |         self._lock = threading.Lock()
   |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/monitoring.py:114:57
    |
112 |                         setattr(self.metrics[pipeline_id], key, value)
113 |
114 |     def get_pipeline_metrics(self, pipeline_id: str) -> Optional[PipelineMetrics]:
    |                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^
115 |         """Get metrics for a specific pipeline."""
116 |         return self.metrics.get(pipeline_id)
    |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/pyspark_interview_project/monitoring.py:118:34
    |
116 |         return self.metrics.get(pipeline_id)
117 |
118 |     def get_all_metrics(self) -> List[PipelineMetrics]:
    |                                  ^^^^
119 |         """Get all pipeline metrics."""
120 |         return list(self.metrics.values())
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/monitoring.py:122:41
    |
120 |         return list(self.metrics.values())
121 |
122 |     def collect_system_metrics(self) -> Dict[str, float]:
    |                                         ^^^^
123 |         """Collect system-level metrics."""
124 |         try:
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/monitoring.py:158:29
    |
156 |     def __init__(self, spark: SparkSession):
157 |         self.spark = spark
158 |         self.health_checks: Dict[str, Callable] = {}
    |                             ^^^^
159 |         self.register_default_checks()
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/monitoring.py:172:36
    |
170 |         self.health_checks[name] = check_function
171 |
172 |     def run_health_checks(self) -> Dict[str, Dict[str, Any]]:
    |                                    ^^^^
173 |         """Run all registered health checks."""
174 |         results = {}
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/monitoring.py:172:46
    |
170 |         self.health_checks[name] = check_function
171 |
172 |     def run_health_checks(self) -> Dict[str, Dict[str, Any]]:
    |                                              ^^^^
173 |         """Run all registered health checks."""
174 |         results = {}
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/monitoring.py:194:39
    |
192 |         return results
193 |
194 |     def _check_spark_session(self) -> Dict[str, Any]:
    |                                       ^^^^
195 |         """Check Spark session health."""
196 |         try:
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/monitoring.py:214:38
    |
212 |             return {"healthy": False, "error": str(e)}
213 |
214 |     def _check_data_sources(self) -> Dict[str, Any]:
    |                                      ^^^^
215 |         """Check data source accessibility."""
216 |         # This would check actual data sources in production
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/monitoring.py:224:38
    |
222 |         }
223 |
224 |     def _check_data_quality(self) -> Dict[str, Any]:
    |                                      ^^^^
225 |         """Check data quality metrics."""
226 |         # This would run actual DQ checks in production
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/monitoring.py:233:37
    |
231 |         }
232 |
233 |     def _check_performance(self) -> Dict[str, Any]:
    |                                     ^^^^
234 |         """Check performance metrics."""
235 |         try:
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/monitoring.py:261:32
    |
259 |     """Manages alerts and notifications."""
260 |
261 |     def __init__(self, config: Dict[str, Any]):
    |                                ^^^^
262 |         self.config = config
263 |         self.alerts: List[Alert] = []
    |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/pyspark_interview_project/monitoring.py:263:22
    |
261 |     def __init__(self, config: Dict[str, Any]):
262 |         self.config = config
263 |         self.alerts: List[Alert] = []
    |                      ^^^^
264 |         self.alert_handlers: Dict[str, Callable] = {}
265 |         self._setup_handlers()
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/monitoring.py:264:30
    |
262 |         self.config = config
263 |         self.alerts: List[Alert] = []
264 |         self.alert_handlers: Dict[str, Callable] = {}
    |                              ^^^^
265 |         self._setup_handlers()
    |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/monitoring.py:290:18
    |
288 |         message: str,
289 |         source: str,
290 |         context: Optional[Dict[str, Any]] = None,
    |                  ^^^^^^^^^^^^^^^^^^^^^^^^
291 |     ) -> Alert:
292 |         """Create a new alert."""
    |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/monitoring.py:290:27
    |
288 |         message: str,
289 |         source: str,
290 |         context: Optional[Dict[str, Any]] = None,
    |                           ^^^^
291 |     ) -> Alert:
292 |         """Create a new alert."""
    |
help: Replace with `dict`

F821 Undefined name `json`
   --> src/project_a/pyspark_interview_project/monitoring.py:357:20
    |
356 |         try:
357 |             data = json.dumps(payload).encode("utf-8")
    |                    ^^^^
358 |             headers = {"Content-Type": "application/json"}
359 |             req = urllib.request.Request(webhook_url, data=data, headers=headers, method="POST")
    |

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/monitoring.py:372:25
    |
371 |     def get_alerts(
372 |         self, severity: Optional[str] = None, acknowledged: Optional[bool] = None
    |                         ^^^^^^^^^^^^^
373 |     ) -> List[Alert]:
374 |         """Get filtered alerts."""
    |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/monitoring.py:372:61
    |
371 |     def get_alerts(
372 |         self, severity: Optional[str] = None, acknowledged: Optional[bool] = None
    |                                                             ^^^^^^^^^^^^^^
373 |     ) -> List[Alert]:
374 |         """Get filtered alerts."""
    |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/pyspark_interview_project/monitoring.py:373:10
    |
371 |     def get_alerts(
372 |         self, severity: Optional[str] = None, acknowledged: Optional[bool] = None
373 |     ) -> List[Alert]:
    |          ^^^^
374 |         """Get filtered alerts."""
375 |         filtered_alerts = self.alerts
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/monitoring.py:401:53
    |
399 |     """Main monitoring orchestrator."""
400 |
401 |     def __init__(self, spark: SparkSession, config: Dict[str, Any]):
    |                                                     ^^^^
402 |         self.spark = spark
403 |         self.config = config
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/monitoring.py:434:35
    |
432 |             raise
433 |
434 |     def run_health_check(self) -> Dict[str, Any]:
    |                                   ^^^^
435 |         """Run comprehensive health check."""
436 |         if not self.monitoring_enabled:
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/monitoring.py:479:38
    |
477 |             }
478 |
479 |     def get_metrics_summary(self) -> Dict[str, Any]:
    |                                      ^^^^
480 |         """Get summary of all metrics."""
481 |         if not self.monitoring_enabled:
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/monitoring.py:548:49
    |
547 | # Utility functions for easy monitoring
548 | def create_monitor(spark: SparkSession, config: Dict[str, Any]) -> PipelineMonitor:
    |                                                 ^^^^
549 |     """Create a monitoring instance."""
550 |     return PipelineMonitor(spark, config)
    |
help: Replace with `dict`

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/monitoring/__init__.py:4:1
   |
 2 |   Monitoring and observability package for ETL pipeline.
 3 |   """
 4 | / from .metrics import (
 5 | |     put_metric,
 6 | |     emit_duration,
 7 | |     emit_count,
 8 | |     emit_size,
 9 | |     emit_dq_metrics,
10 | | )
   | |_^
11 |   try:
12 |       from .lineage_emitter import (
   |
help: Organize imports

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/monitoring/__init__.py:12:5
   |
10 |   )
11 |   try:
12 | /     from .lineage_emitter import (
13 | |         emit_lineage_event,
14 | |         emit_start,
15 | |         emit_complete,
16 | |         emit_fail,
17 | |     )
18 | |     from .lineage_decorator import lineage_job
   | |______________________________________________^
19 |   except ImportError:
20 |       # Fallback if lineage not available
   |
help: Organize imports

E731 Do not assign a `lambda` expression, use a `def`
  --> src/project_a/pyspark_interview_project/monitoring/__init__.py:21:5
   |
19 | except ImportError:
20 |     # Fallback if lineage not available
21 |     lineage_job = lambda *args, **kwargs: lambda f: f
   |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
22 |     def emit_lineage_event(*args, **kwargs): pass
23 |     def emit_start(*args, **kwargs): pass
   |
help: Rewrite `lineage_job` as a `def`

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/monitoring/__init__.py:28:5
   |
27 |   try:
28 | /     from .alerts import (
29 | |         send_slack_alert,
30 | |         send_email_alert,
31 | |         alert_on_dq_failure,
32 | |         alert_on_sla_breach,
33 | |     )
   | |_____^
34 |   except ImportError:
35 |       # Fallback if alerts not available
   |
help: Organize imports

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/monitoring/alerts.py:5:1
   |
 3 |   """
 4 |
 5 | / import logging
 6 | | import json
 7 | | from typing import Dict, Any, Optional, List
 8 | | from datetime import datetime
 9 | | import urllib.error
10 | | import urllib.request
   | |_____________________^
11 |
12 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
 --> src/project_a/pyspark_interview_project/monitoring/alerts.py:7:1
  |
5 | import logging
6 | import json
7 | from typing import Dict, Any, Optional, List
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
8 | from datetime import datetime
9 | import urllib.error
  |

UP035 `typing.List` is deprecated, use `list` instead
 --> src/project_a/pyspark_interview_project/monitoring/alerts.py:7:1
  |
5 | import logging
6 | import json
7 | from typing import Dict, Any, Optional, List
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
8 | from datetime import datetime
9 | import urllib.error
  |

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/monitoring/alerts.py:18:13
   |
16 |     message: str,
17 |     severity: str = "info",
18 |     config: Optional[Dict[str, Any]] = None,
   |             ^^^^^^^^^^^^^^^^^^^^^^^^
19 |     context: Optional[Dict[str, Any]] = None
20 | ) -> bool:
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/monitoring/alerts.py:18:22
   |
16 |     message: str,
17 |     severity: str = "info",
18 |     config: Optional[Dict[str, Any]] = None,
   |                      ^^^^
19 |     context: Optional[Dict[str, Any]] = None
20 | ) -> bool:
   |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/monitoring/alerts.py:19:14
   |
17 |     severity: str = "info",
18 |     config: Optional[Dict[str, Any]] = None,
19 |     context: Optional[Dict[str, Any]] = None
   |              ^^^^^^^^^^^^^^^^^^^^^^^^
20 | ) -> bool:
21 |     """
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/monitoring/alerts.py:19:23
   |
17 |     severity: str = "info",
18 |     config: Optional[Dict[str, Any]] = None,
19 |     context: Optional[Dict[str, Any]] = None
   |                       ^^^^
20 | ) -> bool:
21 |     """
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/alerts.py:23:1
   |
21 |     """
22 |     Send alert to Slack via webhook.
23 |     
   | ^^^^
24 |     Args:
25 |         message: Alert message
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/alerts.py:29:1
   |
27 |         config: Configuration dict with Slack webhook URL
28 |         context: Optional additional context
29 |         
   | ^^^^^^^^
30 |     Returns:
31 |         True if sent successfully, False otherwise
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/alerts.py:36:1
   |
34 |         logger.warning("No config provided for Slack alert")
35 |         return False
36 |     
   | ^^^^
37 |     webhook_url = config.get("monitoring", {}).get("notifications", {}).get("slack", {}).get("webhook_url")
38 |     if not webhook_url:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/alerts.py:41:1
   |
39 |         logger.debug("Slack webhook URL not configured")
40 |         return False
41 |     
   | ^^^^
42 |     color_map = {
43 |         "info": "#36a64f",
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/alerts.py:48:1
   |
46 |         "critical": "#cc0000"
47 |     }
48 |     
   | ^^^^
49 |     payload = {
50 |         "attachments": [
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/alerts.py:66:1
   |
64 |         ]
65 |     }
66 |     
   | ^^^^
67 |     if context:
68 |         for key, value in context.items():
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/alerts.py:74:1
   |
72 |                 "short": True
73 |             })
74 |     
   | ^^^^
75 |     try:
76 |         data = json.dumps(payload).encode("utf-8")
   |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/pyspark_interview_project/monitoring/alerts.py:91:17
   |
89 |     message: str,
90 |     subject: str,
91 |     recipients: List[str],
   |                 ^^^^
92 |     config: Optional[Dict[str, Any]] = None
93 | ) -> bool:
   |
help: Replace with `list`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/monitoring/alerts.py:92:13
   |
90 |     subject: str,
91 |     recipients: List[str],
92 |     config: Optional[Dict[str, Any]] = None
   |             ^^^^^^^^^^^^^^^^^^^^^^^^
93 | ) -> bool:
94 |     """
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/monitoring/alerts.py:92:22
   |
90 |     subject: str,
91 |     recipients: List[str],
92 |     config: Optional[Dict[str, Any]] = None
   |                      ^^^^
93 | ) -> bool:
94 |     """
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/alerts.py:96:1
   |
94 |     """
95 |     Send alert via email (requires AWS SES or similar).
96 |     
   | ^^^^
97 |     Args:
98 |         message: Alert message
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/alerts.py:102:1
    |
100 |         recipients: List of email addresses
101 |         config: Configuration dict
102 |         
    | ^^^^^^^^
103 |     Returns:
104 |         True if sent successfully, False otherwise
    |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/pyspark_interview_project/monitoring/alerts.py:114:20
    |
112 | def alert_on_dq_failure(
113 |     table_name: str,
114 |     failed_checks: List[str],
    |                    ^^^^
115 |     config: Dict[str, Any]
116 | ) -> bool:
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/monitoring/alerts.py:115:13
    |
113 |     table_name: str,
114 |     failed_checks: List[str],
115 |     config: Dict[str, Any]
    |             ^^^^
116 | ) -> bool:
117 |     """
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/alerts.py:119:1
    |
117 |     """
118 |     Send alert when data quality checks fail.
119 |     
    | ^^^^
120 |     Args:
121 |         table_name: Name of the table
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/alerts.py:124:1
    |
122 |         failed_checks: List of failed check names
123 |         config: Configuration dict
124 |         
    | ^^^^^^^^
125 |     Returns:
126 |         True if alert sent successfully
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/alerts.py:134:1
    |
132 |         "count": len(failed_checks)
133 |     }
134 |     
    | ^^^^
135 |     return send_slack_alert(message, "error", config, context)
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/monitoring/alerts.py:142:13
    |
140 |     expected_duration: float,
141 |     actual_duration: float,
142 |     config: Dict[str, Any]
    |             ^^^^
143 | ) -> bool:
144 |     """
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/alerts.py:146:1
    |
144 |     """
145 |     Send alert when SLA is breached.
146 |     
    | ^^^^
147 |     Args:
148 |         job_name: Name of the job
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/alerts.py:152:1
    |
150 |         actual_duration: Actual duration in seconds
151 |         config: Configuration dict
152 |         
    | ^^^^^^^^
153 |     Returns:
154 |         True if alert sent successfully
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/alerts.py:163:1
    |
161 |         "overage_seconds": actual_duration - expected_duration
162 |     }
163 |     
    | ^^^^
164 |     return send_slack_alert(message, "warning", config, context)
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:8:1
   |
 6 |   """
 7 |
 8 | / import os
 9 | | import functools
10 | | import logging
11 | | import yaml
12 | | from typing import List, Dict, Any, Optional, Callable
13 | | from datetime import datetime
14 | | from pathlib import Path
15 | | import json
16 | | import urllib.error
17 | | import urllib.request
18 | |
19 | | from pyspark.sql import DataFrame
   | |_________________________________^
20 |
21 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

F401 [*] `os` imported but unused
  --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:8:8
   |
 6 | """
 7 |
 8 | import os
   |        ^^
 9 | import functools
10 | import logging
   |
help: Remove unused import: `os`

UP035 [*] Import from `collections.abc` instead: `Callable`
  --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:12:1
   |
10 | import logging
11 | import yaml
12 | from typing import List, Dict, Any, Optional, Callable
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
13 | from datetime import datetime
14 | from pathlib import Path
   |
help: Import from `collections.abc`

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:12:1
   |
10 | import logging
11 | import yaml
12 | from typing import List, Dict, Any, Optional, Callable
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
13 | from datetime import datetime
14 | from pathlib import Path
   |

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:12:1
   |
10 | import logging
11 | import yaml
12 | from typing import List, Dict, Any, Optional, Callable
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
13 | from datetime import datetime
14 | from pathlib import Path
   |

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:24:34
   |
24 | def _load_lineage_config(config: Dict[str, Any]) -> Dict[str, Any]:
   |                                  ^^^^
25 |     """Load lineage configuration from config/lineage.yaml."""
26 |     try:
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:24:53
   |
24 | def _load_lineage_config(config: Dict[str, Any]) -> Dict[str, Any]:
   |                                                     ^^^^
25 |     """Load lineage configuration from config/lineage.yaml."""
26 |     try:
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:36:51
   |
36 | def _extract_df_metadata(spark, df: DataFrame) -> Dict[str, Any]:
   |                                                   ^^^^
37 |     """Extract schema and row count from DataFrame."""
38 |     try:
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:41:1
   |
39 |         # Get schema as dict of {name: dtype}
40 |         schema = {f.name: str(f.dataType) for f in df.schema.fields}
41 |         
   | ^^^^^^^^
42 |         # Get row count
43 |         row_count = df.count()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:44:1
   |
42 |         # Get row count
43 |         row_count = df.count()
44 |         
   | ^^^^^^^^
45 |         return {
46 |             "schema": schema,
   |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:57:13
   |
55 |     job_name: str,
56 |     event_type: str,
57 |     inputs: List[Dict[str, Any]],
   |             ^^^^
58 |     outputs: List[Dict[str, Any]],
59 |     config: Dict[str, Any],
   |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:57:18
   |
55 |     job_name: str,
56 |     event_type: str,
57 |     inputs: List[Dict[str, Any]],
   |                  ^^^^
58 |     outputs: List[Dict[str, Any]],
59 |     config: Dict[str, Any],
   |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:58:14
   |
56 |     event_type: str,
57 |     inputs: List[Dict[str, Any]],
58 |     outputs: List[Dict[str, Any]],
   |              ^^^^
59 |     config: Dict[str, Any],
60 |     run_id: str = None,
   |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:58:19
   |
56 |     event_type: str,
57 |     inputs: List[Dict[str, Any]],
58 |     outputs: List[Dict[str, Any]],
   |                   ^^^^
59 |     config: Dict[str, Any],
60 |     run_id: str = None,
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:59:13
   |
57 |     inputs: List[Dict[str, Any]],
58 |     outputs: List[Dict[str, Any]],
59 |     config: Dict[str, Any],
   |             ^^^^
60 |     run_id: str = None,
61 |     error: str = None
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:66:1
   |
64 |     # Load lineage config
65 |     lineage_config = _load_lineage_config(config)
66 |     
   | ^^^^
67 |     if not lineage_config.get('enabled', False):
68 |         return False
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:69:1
   |
67 |     if not lineage_config.get('enabled', False):
68 |         return False
69 |     
   | ^^^^
70 |     url = lineage_config.get('url', '').rstrip('/') + '/api/v1/lineage'
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:71:1
   |
70 |     url = lineage_config.get('url', '').rstrip('/') + '/api/v1/lineage'
71 |     
   | ^^^^
72 |     if not url or url == '/api/v1/lineage':
73 |         logger.warning("Lineage URL not configured")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:75:1
   |
73 |         logger.warning("Lineage URL not configured")
74 |         return False
75 |     
   | ^^^^
76 |     # Build OpenLineage event
77 |     namespace = lineage_config.get('namespace', 'data-platform')
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:78:1
   |
76 |     # Build OpenLineage event
77 |     namespace = lineage_config.get('namespace', 'data-platform')
78 |     
   | ^^^^
79 |     event = {
80 |         "eventType": event_type,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:136:1
    |
134 |         ]
135 |     }
136 |     
    | ^^^^
137 |     # Add error if present
138 |     if error:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:143:1
    |
141 |             "programmingLanguage": "python"
142 |         }
143 |     
    | ^^^^
144 |     try:
145 |         data = json.dumps(event).encode("utf-8")
    |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:159:13
    |
157 | def lineage_job(
158 |     name: str,
159 |     inputs: List[str],
    |             ^^^^
160 |     outputs: List[str],
161 |     namespace: Optional[str] = None
    |
help: Replace with `list`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:160:14
    |
158 |     name: str,
159 |     inputs: List[str],
160 |     outputs: List[str],
    |              ^^^^
161 |     namespace: Optional[str] = None
162 | ):
    |
help: Replace with `list`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:161:16
    |
159 |     inputs: List[str],
160 |     outputs: List[str],
161 |     namespace: Optional[str] = None
    |                ^^^^^^^^^^^^^
162 | ):
163 |     """
    |
help: Convert to `X | None`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:165:1
    |
163 |     """
164 |     Decorator to automatically emit lineage events for a job.
165 |     
    | ^^^^
166 |     Reads config/lineage.yaml, wraps spark job, captures schema/row counts,
167 |     and POSTs to HTTP endpoint.
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:168:1
    |
166 |     Reads config/lineage.yaml, wraps spark job, captures schema/row counts,
167 |     and POSTs to HTTP endpoint.
168 |     
    | ^^^^
169 |     Args:
170 |         name: Job name (e.g., 'extract_snowflake_orders')
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:174:1
    |
172 |         outputs: List of output dataset names (e.g., ['s3://bucket/bronze/orders'])
173 |         namespace: OpenLineage namespace (defaults to config or env var)
174 |     
    | ^^^^
175 |     Example:
176 |         @lineage_job(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:191:1
    |
189 |             config = None
190 |             spark = None
191 |             
    | ^^^^^^^^^^^^
192 |             # Try to find config in kwargs first
193 |             if 'config' in kwargs:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:197:1
    |
195 |             elif len(args) > 1 and isinstance(args[1], dict):
196 |                 config = args[1]
197 |             
    | ^^^^^^^^^^^^
198 |             # Try to find spark in kwargs or args
199 |             if 'spark' in kwargs:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:206:1
    |
204 |                 if hasattr(first_arg, 'read') and hasattr(first_arg, 'createDataFrame'):
205 |                     spark = first_arg
206 |             
    | ^^^^^^^^^^^^
207 |             # Generate run ID
208 |             run_id = f"{name}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S_%f')}"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:209:1
    |
207 |             # Generate run ID
208 |             run_id = f"{name}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S_%f')}"
209 |             
    | ^^^^^^^^^^^^
210 |             # Emit start event
211 |             try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:220:1
    |
218 |                     run_id=run_id
219 |                 )
220 |                 
    | ^^^^^^^^^^^^^^^^
221 |                 # Execute function
222 |                 result = func(*args, **kwargs)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:223:1
    |
221 |                 # Execute function
222 |                 result = func(*args, **kwargs)
223 |                 
    | ^^^^^^^^^^^^^^^^
224 |                 # Extract metadata from result if DataFrame
225 |                 input_metadata = {}
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:227:1
    |
225 |                 input_metadata = {}
226 |                 output_metadata = {}
227 |                 
    | ^^^^^^^^^^^^^^^^
228 |                 if isinstance(result, DataFrame) and spark:
229 |                     output_metadata = _extract_df_metadata(spark, result)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:230:1
    |
228 |                 if isinstance(result, DataFrame) and spark:
229 |                     output_metadata = _extract_df_metadata(spark, result)
230 |                 
    | ^^^^^^^^^^^^^^^^
231 |                 # Emit complete event
232 |                 _post_lineage_event(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:240:1
    |
238 |                     run_id=run_id
239 |                 )
240 |                 
    | ^^^^^^^^^^^^^^^^
241 |                 return result
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:242:1
    |
241 |                 return result
242 |                 
    | ^^^^^^^^^^^^^^^^
243 |             except Exception as e:
244 |                 # Emit failure event
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/lineage_decorator.py:256:1
    |
254 |                 )
255 |                 raise
256 |         
    | ^^^^^^^^
257 |         return wrapper
258 |     return decorator
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/monitoring/lineage_emitter.py:7:1
   |
 5 |   Supports both Marquez and generic OpenLineage backends.
 6 |   """
 7 | / import json
 8 | | import logging
 9 | | import urllib.error
10 | | import urllib.request
11 | | from datetime import datetime, timezone
12 | | from typing import List, Dict, Optional, Any
13 | | from pathlib import Path
   | |________________________^
14 |
15 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/pyspark_interview_project/monitoring/lineage_emitter.py:12:1
   |
10 | import urllib.request
11 | from datetime import datetime, timezone
12 | from typing import List, Dict, Optional, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
13 | from pathlib import Path
   |

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/monitoring/lineage_emitter.py:12:1
   |
10 | import urllib.request
11 | from datetime import datetime, timezone
12 | from typing import List, Dict, Optional, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
13 | from pathlib import Path
   |

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/lineage_emitter.py:22:1
   |
20 |     Emits lineage events to OpenLineage/Marquez backend.
21 |     """
22 |     
   | ^^^^
23 |     def __init__(self, config: Dict[str, Any]):
24 |         """
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/monitoring/lineage_emitter.py:23:32
   |
21 |     """
22 |     
23 |     def __init__(self, config: Dict[str, Any]):
   |                                ^^^^
24 |         """
25 |         Initialize lineage emitter from config.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/lineage_emitter.py:26:1
   |
24 |         """
25 |         Initialize lineage emitter from config.
26 |         
   | ^^^^^^^^
27 |         Args:
28 |             config: Lineage configuration dict with:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/lineage_emitter.py:40:1
   |
38 |         self.namespace = config.get("namespace", "project_a_dev")
39 |         self.api_key = config.get("api_key", "")
40 |         
   | ^^^^^^^^
41 |         if self.enabled and not self.url:
42 |             logger.warning("âš ï¸  Lineage enabled but no URL configured")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/lineage_emitter.py:44:1
   |
42 |             logger.warning("âš ï¸  Lineage enabled but no URL configured")
43 |             self.enabled = False
44 |     
   | ^^^^
45 |     def _get_api_path(self) -> str:
46 |         """Get API path based on backend type."""
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/lineage_emitter.py:51:1
   |
49 |         else:  # openlineage
50 |             return "/api/v1/lineage"
51 |     
   | ^^^^
52 |     def emit_job(
53 |         self,
   |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/pyspark_interview_project/monitoring/lineage_emitter.py:56:17
   |
54 |         job_name: str,
55 |         run_id: str,
56 |         inputs: List[str],
   |                 ^^^^
57 |         outputs: List[str],
58 |         status: str = "SUCCESS",
   |
help: Replace with `list`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/pyspark_interview_project/monitoring/lineage_emitter.py:57:18
   |
55 |         run_id: str,
56 |         inputs: List[str],
57 |         outputs: List[str],
   |                  ^^^^
58 |         status: str = "SUCCESS",
59 |         error_message: Optional[str] = None,
   |
help: Replace with `list`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/monitoring/lineage_emitter.py:59:24
   |
57 |         outputs: List[str],
58 |         status: str = "SUCCESS",
59 |         error_message: Optional[str] = None,
   |                        ^^^^^^^^^^^^^
60 |         metadata: Optional[Dict[str, Any]] = None
61 |     ) -> None:
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/monitoring/lineage_emitter.py:60:19
   |
58 |         status: str = "SUCCESS",
59 |         error_message: Optional[str] = None,
60 |         metadata: Optional[Dict[str, Any]] = None
   |                   ^^^^^^^^^^^^^^^^^^^^^^^^
61 |     ) -> None:
62 |         """
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/monitoring/lineage_emitter.py:60:28
   |
58 |         status: str = "SUCCESS",
59 |         error_message: Optional[str] = None,
60 |         metadata: Optional[Dict[str, Any]] = None
   |                            ^^^^
61 |     ) -> None:
62 |         """
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/lineage_emitter.py:64:1
   |
62 |         """
63 |         Emit a lineage event for a job run.
64 |         
   | ^^^^^^^^
65 |         Args:
66 |             job_name: Name of the job (e.g., "bronze_to_silver")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/lineage_emitter.py:77:1
   |
75 |             logger.debug("Lineage disabled, skipping emission")
76 |             return
77 |         
   | ^^^^^^^^
78 |         if not self.url:
79 |             logger.warning("Lineage URL not configured, skipping emission")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/lineage_emitter.py:81:1
   |
79 |             logger.warning("Lineage URL not configured, skipping emission")
80 |             return
81 |         
   | ^^^^^^^^
82 |         try:
83 |             event_time = datetime.now(timezone.utc).isoformat()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/lineage_emitter.py:84:1
   |
82 |         try:
83 |             event_time = datetime.now(timezone.utc).isoformat()
84 |             
   | ^^^^^^^^^^^^
85 |             # Determine event type
86 |             if status.upper() == "SUCCESS":
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/lineage_emitter.py:90:1
   |
88 |             else:
89 |                 event_type = "FAIL"
90 |             
   | ^^^^^^^^^^^^
91 |             # Build OpenLineage-compatible event
92 |             payload = {
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/lineage_emitter.py:145:1
    |
143 |                 "schemaURL": "https://openlineage.io/spec/1-0-5/OpenLineage.json"
144 |             }
145 |             
    | ^^^^^^^^^^^^
146 |             # Add error information if failed
147 |             if event_type == "FAIL" and error_message:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/lineage_emitter.py:151:1
    |
149 |                     "message": error_message
150 |                 }
151 |             
    | ^^^^^^^^^^^^
152 |             # Add custom metadata if provided
153 |             if metadata:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/lineage_emitter.py:155:1
    |
153 |             if metadata:
154 |                 payload["run"]["facets"]["custom"] = metadata
155 |             
    | ^^^^^^^^^^^^
156 |             # Send to backend (best-effort, no external 'requests' dependency)
157 |             api_path = self._get_api_path()
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/monitoring/lineage_emitter.py:178:46
    |
178 | def load_lineage_config(config_path: str) -> Dict[str, Any]:
    |                                              ^^^^
179 |     """
180 |     Load lineage configuration from YAML file (local or S3).
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/lineage_emitter.py:181:1
    |
179 |     """
180 |     Load lineage configuration from YAML file (local or S3).
181 |     
    | ^^^^
182 |     Args:
183 |         config_path: Path to config file (local or s3://...)
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/lineage_emitter.py:184:1
    |
182 |     Args:
183 |         config_path: Path to config file (local or s3://...)
184 |     
    | ^^^^
185 |     Returns:
186 |         Configuration dictionary
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/lineage_emitter.py:189:1
    |
187 |     """
188 |     import yaml
189 |     
    | ^^^^
190 |     if config_path.startswith("s3://"):
191 |         import boto3
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/lineage_emitter.py:192:1
    |
190 |     if config_path.startswith("s3://"):
191 |         import boto3
192 |         
    | ^^^^^^^^
193 |         s3 = boto3.client("s3")
194 |         path_parts = config_path.replace("s3://", "").split("/", 1)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/lineage_emitter.py:197:1
    |
195 |         bucket = path_parts[0]
196 |         key = path_parts[1] if len(path_parts) > 1 else ""
197 |         
    | ^^^^^^^^
198 |         try:
199 |             obj = s3.get_object(Bucket=bucket, Key=key)
    |
help: Remove whitespace from blank line

UP015 [*] Unnecessary mode argument
   --> src/project_a/pyspark_interview_project/monitoring/lineage_emitter.py:209:36
    |
207 |         config_file = Path(config_path)
208 |         if config_file.exists():
209 |             with open(config_file, "r") as f:
    |                                    ^^^
210 |                 return yaml.safe_load(f) or {}
211 |         else:
    |
help: Remove mode argument

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/monitoring/metrics.py:6:1
   |
 4 |   Sends custom metrics to CloudWatch for observability and alerting.
 5 |   """
 6 | / import os
 7 | | import logging
 8 | | from datetime import datetime, timezone
 9 | | from typing import Optional, List, Dict
   | |_______________________________________^
10 |
11 |   try:
   |
help: Organize imports

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/pyspark_interview_project/monitoring/metrics.py:9:1
   |
 7 | import logging
 8 | from datetime import datetime, timezone
 9 | from typing import Optional, List, Dict
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 |
11 | try:
   |

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/monitoring/metrics.py:9:1
   |
 7 | import logging
 8 | from datetime import datetime, timezone
 9 | from typing import Optional, List, Dict
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 |
11 | try:
   |

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/monitoring/metrics.py:27:11
   |
25 |     value: float,
26 |     unit: str = "Count",
27 |     dims: Optional[List[Dict[str, str]]] = None
   |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
28 | ):
29 |     """
   |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/pyspark_interview_project/monitoring/metrics.py:27:20
   |
25 |     value: float,
26 |     unit: str = "Count",
27 |     dims: Optional[List[Dict[str, str]]] = None
   |                    ^^^^
28 | ):
29 |     """
   |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/monitoring/metrics.py:27:25
   |
25 |     value: float,
26 |     unit: str = "Count",
27 |     dims: Optional[List[Dict[str, str]]] = None
   |                         ^^^^
28 | ):
29 |     """
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/metrics.py:31:1
   |
29 |     """
30 |     Put a custom metric to CloudWatch.
31 |     
   | ^^^^
32 |     Args:
33 |         name: Metric name
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/metrics.py:37:1
   |
35 |         unit: Unit type (Count, Bytes, Seconds, etc.)
36 |         dims: List of dimension dictionaries
37 |         
   | ^^^^^^^^
38 |     Example:
39 |         put_metric("customer_behavior_records", 10000, dims=[{"Name": "env", "Value": "dev"}])
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/metrics.py:44:1
   |
42 |         logger.warning("boto3 not available, skipping CloudWatch metric")
43 |         return
44 |     
   | ^^^^
45 |     try:
46 |         cw = boto3.client(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/metrics.py:50:1
   |
48 |             region_name=os.getenv("AWS_REGION", "us-east-1")
49 |         )
50 |         
   | ^^^^^^^^
51 |         metric = {
52 |             "MetricName": name,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/metrics.py:57:1
   |
55 |             "Unit": unit
56 |         }
57 |         
   | ^^^^^^^^
58 |         if dims:
59 |             metric["Dimensions"] = dims
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/metrics.py:60:1
   |
58 |         if dims:
59 |             metric["Dimensions"] = dims
60 |         
   | ^^^^^^^^
61 |         cw.put_metric_data(
62 |             Namespace=NAMESPACE,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/metrics.py:65:1
   |
63 |             MetricData=[metric]
64 |         )
65 |         
   | ^^^^^^^^
66 |         logger.debug(f"Metric emitted: {name}={value}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/metrics.py:67:1
   |
66 |         logger.debug(f"Metric emitted: {name}={value}")
67 |         
   | ^^^^^^^^
68 |     except Exception as e:
69 |         logger.warning(f"Failed to emit CloudWatch metric: {e}")
   |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/monitoring/metrics.py:72:61
   |
72 | def emit_duration(name: str, duration_seconds: float, dims: Optional[List[Dict[str, str]]] = None):
   |                                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
73 |     """Emit duration metric to CloudWatch."""
74 |     put_metric(name, duration_seconds, unit="Seconds", dims=dims)
   |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/pyspark_interview_project/monitoring/metrics.py:72:70
   |
72 | def emit_duration(name: str, duration_seconds: float, dims: Optional[List[Dict[str, str]]] = None):
   |                                                                      ^^^^
73 |     """Emit duration metric to CloudWatch."""
74 |     put_metric(name, duration_seconds, unit="Seconds", dims=dims)
   |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/monitoring/metrics.py:72:75
   |
72 | def emit_duration(name: str, duration_seconds: float, dims: Optional[List[Dict[str, str]]] = None):
   |                                                                           ^^^^
73 |     """Emit duration metric to CloudWatch."""
74 |     put_metric(name, duration_seconds, unit="Seconds", dims=dims)
   |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/monitoring/metrics.py:77:45
   |
77 | def emit_count(name: str, count: int, dims: Optional[List[Dict[str, str]]] = None):
   |                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
78 |     """Emit count metric to CloudWatch."""
79 |     put_metric(name, float(count), unit="Count", dims=dims)
   |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/pyspark_interview_project/monitoring/metrics.py:77:54
   |
77 | def emit_count(name: str, count: int, dims: Optional[List[Dict[str, str]]] = None):
   |                                                      ^^^^
78 |     """Emit count metric to CloudWatch."""
79 |     put_metric(name, float(count), unit="Count", dims=dims)
   |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/monitoring/metrics.py:77:59
   |
77 | def emit_count(name: str, count: int, dims: Optional[List[Dict[str, str]]] = None):
   |                                                           ^^^^
78 |     """Emit count metric to CloudWatch."""
79 |     put_metric(name, float(count), unit="Count", dims=dims)
   |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/monitoring/metrics.py:82:49
   |
82 | def emit_size(name: str, size_bytes: int, dims: Optional[List[Dict[str, str]]] = None):
   |                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
83 |     """Emit size metric to CloudWatch."""
84 |     put_metric(name, float(size_bytes), unit="Bytes", dims=dims)
   |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/pyspark_interview_project/monitoring/metrics.py:82:58
   |
82 | def emit_size(name: str, size_bytes: int, dims: Optional[List[Dict[str, str]]] = None):
   |                                                          ^^^^
83 |     """Emit size metric to CloudWatch."""
84 |     put_metric(name, float(size_bytes), unit="Bytes", dims=dims)
   |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/monitoring/metrics.py:82:63
   |
82 | def emit_size(name: str, size_bytes: int, dims: Optional[List[Dict[str, str]]] = None):
   |                                                               ^^^^
83 |     """Emit size metric to CloudWatch."""
84 |     put_metric(name, float(size_bytes), unit="Bytes", dims=dims)
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/metrics.py:90:1
   |
88 |     """
89 |     Emit DQ validation metrics to CloudWatch.
90 |     
   | ^^^^
91 |     Args:
92 |         results: DQ validation results dict
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/metrics.py:97:1
   |
95 |     """
96 |     dims = [{"Name": "suite", "Value": suite_name}, {"Name": "layer", "Value": layer}]
97 |     
   | ^^^^
98 |     emit_count(
99 |         "dq_validations_total",
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/metrics.py:103:1
    |
101 |         dims=dims
102 |     )
103 |     
    | ^^^^
104 |     emit_count(
105 |         "dq_critical_failures",
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/metrics.py:109:1
    |
107 |         dims=dims
108 |     )
109 |     
    | ^^^^
110 |     emit_count(
111 |         "dq_warning_failures",
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/metrics.py:115:1
    |
113 |         dims=dims
114 |     )
115 |     
    | ^^^^
116 |     # Pass rate percentage
117 |     if results.get("expectations"):
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:7:1
   |
 5 |   """
 6 |
 7 | / import os
 8 | | import json
 9 | | import logging
10 | | from typing import Dict, Any, Optional
11 | | from datetime import datetime
12 | | from pathlib import Path
   | |________________________^
13 |
14 |   try:
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:10:1
   |
 8 | import json
 9 | import logging
10 | from typing import Dict, Any, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
11 | from datetime import datetime
12 | from pathlib import Path
   |

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:26:1
   |
24 | class MetricsCollector:
25 |     """Collects and emits metrics for pipeline observability."""
26 |     
   | ^^^^
27 |     def __init__(self, config: Optional[Dict[str, Any]] = None):
28 |         """
   |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:27:32
   |
25 |     """Collects and emits metrics for pipeline observability."""
26 |     
27 |     def __init__(self, config: Optional[Dict[str, Any]] = None):
   |                                ^^^^^^^^^^^^^^^^^^^^^^^^
28 |         """
29 |         Initialize metrics collector.
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:27:41
   |
25 |     """Collects and emits metrics for pipeline observability."""
26 |     
27 |     def __init__(self, config: Optional[Dict[str, Any]] = None):
   |                                         ^^^^
28 |         """
29 |         Initialize metrics collector.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:30:1
   |
28 |         """
29 |         Initialize metrics collector.
30 |         
   | ^^^^^^^^
31 |         Args:
32 |             config: Configuration dict with AWS settings
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:37:1
   |
35 |         self.environment = self.config.get('environment', os.getenv('ENV', 'local'))
36 |         self.cloudwatch_enabled = self.environment != 'local' and CLOUDWATCH_AVAILABLE
37 |         
   | ^^^^^^^^
38 |         # CloudWatch client
39 |         self.cloudwatch = None
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:46:1
   |
44 |                 logger.warning(f"Could not initialize CloudWatch: {e}")
45 |                 self.cloudwatch_enabled = False
46 |         
   | ^^^^^^^^
47 |         # Local metrics file
48 |         self.local_metrics_file = Path("data/metrics/pipeline_metrics.log")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:50:1
   |
48 |         self.local_metrics_file = Path("data/metrics/pipeline_metrics.log")
49 |         self.local_metrics_file.parent.mkdir(parents=True, exist_ok=True)
50 |     
   | ^^^^
51 |     def emit_rowcount(self, metric_name: str, value: int, labels: Optional[Dict[str, str]] = None):
52 |         """
   |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:51:67
   |
49 |         self.local_metrics_file.parent.mkdir(parents=True, exist_ok=True)
50 |     
51 |     def emit_rowcount(self, metric_name: str, value: int, labels: Optional[Dict[str, str]] = None):
   |                                                                   ^^^^^^^^^^^^^^^^^^^^^^^^
52 |         """
53 |         Emit row count metric.
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:51:76
   |
49 |         self.local_metrics_file.parent.mkdir(parents=True, exist_ok=True)
50 |     
51 |     def emit_rowcount(self, metric_name: str, value: int, labels: Optional[Dict[str, str]] = None):
   |                                                                            ^^^^
52 |         """
53 |         Emit row count metric.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:54:1
   |
52 |         """
53 |         Emit row count metric.
54 |         
   | ^^^^^^^^
55 |         Args:
56 |             metric_name: Name of metric (e.g., 'records_extracted', 'records_bronze')
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:62:1
   |
60 |         labels = labels or {}
61 |         labels['environment'] = self.environment
62 |         
   | ^^^^^^^^
63 |         if self.cloudwatch_enabled and self.cloudwatch:
64 |             try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:66:1
   |
64 |             try:
65 |                 namespace = self.config.get('monitoring', {}).get('cloudwatch_namespace', 'ETLPipelineMetrics')
66 |                 
   | ^^^^^^^^^^^^^^^^
67 |                 metric_data = {
68 |                     'MetricName': metric_name,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:73:1
   |
71 |                     'Timestamp': datetime.utcnow()
72 |                 }
73 |                 
   | ^^^^^^^^^^^^^^^^
74 |                 # Add dimensions
75 |                 dimensions = [{'Name': k, 'Value': str(v)} for k, v in labels.items()]
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:78:1
   |
76 |                 if dimensions:
77 |                     metric_data['Dimensions'] = dimensions[:10]  # CloudWatch limit
78 |                 
   | ^^^^^^^^^^^^^^^^
79 |                 self.cloudwatch.put_metric_data(
80 |                     Namespace=namespace,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:83:1
   |
81 |                     MetricData=[metric_data]
82 |                 )
83 |                 
   | ^^^^^^^^^^^^^^^^
84 |                 logger.debug(f"Emitted CloudWatch metric: {metric_name}={value}")
85 |             except Exception as e:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:87:1
   |
85 |             except Exception as e:
86 |                 logger.warning(f"Failed to emit CloudWatch metric: {e}")
87 |         
   | ^^^^^^^^
88 |         # Also log locally
89 |         self._log_local_metric(metric_name, value, labels, "rowcount")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:90:1
   |
88 |         # Also log locally
89 |         self._log_local_metric(metric_name, value, labels, "rowcount")
90 |     
   | ^^^^
91 |     def emit_duration(self, metric_name: str, milliseconds: float, labels: Optional[Dict[str, str]] = None):
92 |         """
   |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:91:76
   |
89 |         self._log_local_metric(metric_name, value, labels, "rowcount")
90 |     
91 |     def emit_duration(self, metric_name: str, milliseconds: float, labels: Optional[Dict[str, str]] = None):
   |                                                                            ^^^^^^^^^^^^^^^^^^^^^^^^
92 |         """
93 |         Emit duration metric.
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:91:85
   |
89 |         self._log_local_metric(metric_name, value, labels, "rowcount")
90 |     
91 |     def emit_duration(self, metric_name: str, milliseconds: float, labels: Optional[Dict[str, str]] = None):
   |                                                                                     ^^^^
92 |         """
93 |         Emit duration metric.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:94:1
   |
92 |         """
93 |         Emit duration metric.
94 |         
   | ^^^^^^^^
95 |         Args:
96 |             metric_name: Name of metric (e.g., 'extraction_duration', 'transformation_duration')
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:102:1
    |
100 |         labels = labels or {}
101 |         labels['environment'] = self.environment
102 |         
    | ^^^^^^^^
103 |         if self.cloudwatch_enabled and self.cloudwatch:
104 |             try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:106:1
    |
104 |             try:
105 |                 namespace = self.config.get('monitoring', {}).get('cloudwatch_namespace', 'ETLPipelineMetrics')
106 |                 
    | ^^^^^^^^^^^^^^^^
107 |                 metric_data = {
108 |                     'MetricName': metric_name,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:113:1
    |
111 |                     'Timestamp': datetime.utcnow()
112 |                 }
113 |                 
    | ^^^^^^^^^^^^^^^^
114 |                 dimensions = [{'Name': k, 'Value': str(v)} for k, v in labels.items()]
115 |                 if dimensions:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:117:1
    |
115 |                 if dimensions:
116 |                     metric_data['Dimensions'] = dimensions[:10]
117 |                 
    | ^^^^^^^^^^^^^^^^
118 |                 self.cloudwatch.put_metric_data(
119 |                     Namespace=namespace,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:122:1
    |
120 |                     MetricData=[metric_data]
121 |                 )
122 |                 
    | ^^^^^^^^^^^^^^^^
123 |                 logger.debug(f"Emitted CloudWatch metric: {metric_name}={milliseconds}ms")
124 |             except Exception as e:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:126:1
    |
124 |             except Exception as e:
125 |                 logger.warning(f"Failed to emit CloudWatch metric: {e}")
126 |         
    | ^^^^^^^^
127 |         # Also log locally
128 |         self._log_local_metric(metric_name, milliseconds, labels, "duration")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:129:1
    |
127 |         # Also log locally
128 |         self._log_local_metric(metric_name, milliseconds, labels, "duration")
129 |     
    | ^^^^
130 |     def _log_local_metric(self, metric_name: str, value: Any, labels: Dict[str, str], metric_type: str):
131 |         """Log metric to local JSON file."""
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:130:71
    |
128 |         self._log_local_metric(metric_name, milliseconds, labels, "duration")
129 |     
130 |     def _log_local_metric(self, metric_name: str, value: Any, labels: Dict[str, str], metric_type: str):
    |                                                                       ^^^^
131 |         """Log metric to local JSON file."""
132 |         metric_entry = {
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:139:1
    |
137 |             "labels": labels
138 |         }
139 |         
    | ^^^^^^^^
140 |         try:
141 |             with open(self.local_metrics_file, "a") as f:
    |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:151:35
    |
151 | def get_metrics_collector(config: Optional[Dict[str, Any]] = None) -> MetricsCollector:
    |                                   ^^^^^^^^^^^^^^^^^^^^^^^^
152 |     """Get or create global metrics collector."""
153 |     global _metrics_collector
    |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:151:44
    |
151 | def get_metrics_collector(config: Optional[Dict[str, Any]] = None) -> MetricsCollector:
    |                                            ^^^^
152 |     """Get or create global metrics collector."""
153 |     global _metrics_collector
    |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:159:57
    |
159 | def emit_rowcount(metric_name: str, value: int, labels: Optional[Dict[str, str]] = None, config: Optional[Dict[str, Any]] = None):
    |                                                         ^^^^^^^^^^^^^^^^^^^^^^^^
160 |     """Convenience function to emit row count."""
161 |     collector = get_metrics_collector(config)
    |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:159:66
    |
159 | def emit_rowcount(metric_name: str, value: int, labels: Optional[Dict[str, str]] = None, config: Optional[Dict[str, Any]] = None):
    |                                                                  ^^^^
160 |     """Convenience function to emit row count."""
161 |     collector = get_metrics_collector(config)
    |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:159:98
    |
159 | def emit_rowcount(metric_name: str, value: int, labels: Optional[Dict[str, str]] = None, config: Optional[Dict[str, Any]] = None):
    |                                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^
160 |     """Convenience function to emit row count."""
161 |     collector = get_metrics_collector(config)
    |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:159:107
    |
159 | def emit_rowcount(metric_name: str, value: int, labels: Optional[Dict[str, str]] = None, config: Optional[Dict[str, Any]] = None):
    |                                                                                                           ^^^^
160 |     """Convenience function to emit row count."""
161 |     collector = get_metrics_collector(config)
    |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:165:66
    |
165 | def emit_duration(metric_name: str, milliseconds: float, labels: Optional[Dict[str, str]] = None, config: Optional[Dict[str, Any]] = â€¦
    |                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^
166 |     """Convenience function to emit duration."""
167 |     collector = get_metrics_collector(config)
    |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:165:75
    |
165 | def emit_duration(metric_name: str, milliseconds: float, labels: Optional[Dict[str, str]] = None, config: Optional[Dict[str, Any]] = â€¦
    |                                                                           ^^^^
166 |     """Convenience function to emit duration."""
167 |     collector = get_metrics_collector(config)
    |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:165:107
    |
165 | â€¦oat, labels: Optional[Dict[str, str]] = None, config: Optional[Dict[str, Any]] = None):
    |                                                        ^^^^^^^^^^^^^^^^^^^^^^^^
166 | â€¦
167 | â€¦
    |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:165:116
    |
165 | def emit_duration(metric_name: str, milliseconds: float, labels: Optional[Dict[str, str]] = None, config: Optional[Dict[str, Any]] = â€¦
    |                                                                                                                    ^^^^
166 |     """Convenience function to emit duration."""
167 |     collector = get_metrics_collector(config)
    |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:177:13
    |
175 |     duration_seconds: float,
176 |     dq_status: str,
177 |     config: Optional[Dict[str, Any]] = None
    |             ^^^^^^^^^^^^^^^^^^^^^^^^
178 | ) -> None:
179 |     """Emit standard metrics for a job run.
    |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/monitoring/metrics_collector.py:177:22
    |
175 |     duration_seconds: float,
176 |     dq_status: str,
177 |     config: Optional[Dict[str, Any]] = None
    |                      ^^^^
178 | ) -> None:
179 |     """Emit standard metrics for a job run.
    |
help: Replace with `dict`

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/performance_optimizer.py:14:1
   |
12 |   """
13 |
14 | / import logging
15 | | import json
16 | | import time
17 | | from datetime import datetime
18 | | from typing import Dict, List, Any, Optional
19 | | from dataclasses import dataclass, asdict
20 | |
21 | | from pyspark.sql import SparkSession, DataFrame
22 | | from pyspark.sql.functions import col, broadcast
23 | | from pyspark.storagelevel import StorageLevel
   | |_____________________________________________^
24 |
25 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/performance_optimizer.py:18:1
   |
16 | import time
17 | from datetime import datetime
18 | from typing import Dict, List, Any, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
19 | from dataclasses import dataclass, asdict
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/pyspark_interview_project/performance_optimizer.py:18:1
   |
16 | import time
17 | from datetime import datetime
18 | from typing import Dict, List, Any, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
19 | from dataclasses import dataclass, asdict
   |

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/pyspark_interview_project/performance_optimizer.py:41:27
   |
39 |     shuffle_write_mb: float
40 |     storage_level: str
41 |     optimization_applied: List[str]
   |                           ^^^^
42 |     timestamp: str = None
   |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/performance_optimizer.py:54:31
   |
52 |     def __init__(self, spark: SparkSession):
53 |         self.spark = spark
54 |         self.cached_datasets: Dict[str, DataFrame] = {}
   |                               ^^^^
55 |         self.cache_stats: Dict[str, Dict[str, Any]] = {}
56 |         self.cache_hits: Dict[str, int] = {}
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/performance_optimizer.py:55:27
   |
53 |         self.spark = spark
54 |         self.cached_datasets: Dict[str, DataFrame] = {}
55 |         self.cache_stats: Dict[str, Dict[str, Any]] = {}
   |                           ^^^^
56 |         self.cache_hits: Dict[str, int] = {}
57 |         self.cache_misses: Dict[str, int] = {}
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/performance_optimizer.py:55:37
   |
53 |         self.spark = spark
54 |         self.cached_datasets: Dict[str, DataFrame] = {}
55 |         self.cache_stats: Dict[str, Dict[str, Any]] = {}
   |                                     ^^^^
56 |         self.cache_hits: Dict[str, int] = {}
57 |         self.cache_misses: Dict[str, int] = {}
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/performance_optimizer.py:56:26
   |
54 |         self.cached_datasets: Dict[str, DataFrame] = {}
55 |         self.cache_stats: Dict[str, Dict[str, Any]] = {}
56 |         self.cache_hits: Dict[str, int] = {}
   |                          ^^^^
57 |         self.cache_misses: Dict[str, int] = {}
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/performance_optimizer.py:57:28
   |
55 |         self.cache_stats: Dict[str, Dict[str, Any]] = {}
56 |         self.cache_hits: Dict[str, int] = {}
57 |         self.cache_misses: Dict[str, int] = {}
   |                            ^^^^
58 |
59 |         # Storage level thresholds (in MB)
   |
help: Replace with `dict`

E722 Do not use bare `except`
   --> src/project_a/pyspark_interview_project/performance_optimizer.py:120:9
    |
118 |             estimated_bytes = row_count * col_count * 8 * 1.2  # 20% overhead
119 |             return estimated_bytes / (1024 * 1024)
120 |         except:
    |         ^^^^^^
121 |             return 100.0  # Default estimate
    |

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/performance_optimizer.py:147:56
    |
145 |             return str(storage_level)
146 |
147 |     def get_cached_dataset(self, dataset_name: str) -> Optional[DataFrame]:
    |                                                        ^^^^^^^^^^^^^^^^^^^
148 |         """Get cached dataset and update access statistics."""
149 |         if dataset_name in self.cached_datasets:
    |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/performance_optimizer.py:184:36
    |
182 |         logger.info("Cleared all caches")
183 |
184 |     def get_cache_summary(self) -> Dict[str, Any]:
    |                                    ^^^^
185 |         """Get summary of cache usage."""
186 |         return {
    |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/pyspark_interview_project/performance_optimizer.py:199:33
    |
197 |     def __init__(self, spark: SparkSession):
198 |         self.spark = spark
199 |         self.benchmark_results: List[PerformanceMetrics] = []
    |                                 ^^^^
200 |
201 |     def benchmark_dataset(self, df: DataFrame, dataset_name: str,
    |
help: Replace with `list`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/pyspark_interview_project/performance_optimizer.py:202:38
    |
201 |     def benchmark_dataset(self, df: DataFrame, dataset_name: str,
202 |                          operations: List[str] = None) -> PerformanceMetrics:
    |                                      ^^^^
203 |         """Benchmark dataset performance with specified operations."""
204 |         if operations is None:
    |
help: Replace with `list`

E722 Do not use bare `except`
   --> src/project_a/pyspark_interview_project/performance_optimizer.py:290:9
    |
288 |             estimated_bytes = row_count * col_count * 8 * 1.2
289 |             return estimated_bytes / (1024 * 1024)
290 |         except:
    |         ^^^^^^
291 |             return 100.0
    |

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/performance_optimizer.py:310:53
    |
308 |     """Main performance optimization orchestrator."""
309 |
310 |     def __init__(self, spark: SparkSession, config: Dict[str, Any]):
    |                                                     ^^^^
311 |         self.spark = spark
312 |         self.config = config
    |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/pyspark_interview_project/performance_optimizer.py:444:47
    |
443 |     def optimize_broadcast_joins(self, df: DataFrame, join_df: DataFrame,
444 |                                 join_columns: List[str]) -> DataFrame:
    |                                               ^^^^
445 |         """Optimize joins using broadcast joins when appropriate."""
446 |         try:
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/performance_optimizer.py:464:56
    |
462 |             return df.join(join_df, join_columns)
463 |
464 |     def run_full_optimization_pipeline(self, datasets: Dict[str, DataFrame]) -> Dict[str, DataFrame]:
    |                                                        ^^^^
465 |         """Run full optimization pipeline on multiple datasets."""
466 |         logger.info("Starting full optimization pipeline...")
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/performance_optimizer.py:464:81
    |
462 |             return df.join(join_df, join_columns)
463 |
464 |     def run_full_optimization_pipeline(self, datasets: Dict[str, DataFrame]) -> Dict[str, DataFrame]:
    |                                                                                 ^^^^
465 |         """Run full optimization pipeline on multiple datasets."""
466 |         logger.info("Starting full optimization pipeline...")
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/performance_optimizer.py:506:54
    |
504 |         return optimized_df
505 |
506 |     def _run_comprehensive_benchmark(self, datasets: Dict[str, DataFrame]):
    |                                                      ^^^^
507 |         """Run comprehensive benchmark on all datasets."""
508 |         logger.info("Running comprehensive benchmark...")
    |
help: Replace with `dict`

E722 Do not use bare `except`
   --> src/project_a/pyspark_interview_project/performance_optimizer.py:523:9
    |
521 |             estimated_bytes = row_count * col_count * 8 * 1.2
522 |             return estimated_bytes / (1024 * 1024)
523 |         except:
    |         ^^^^^^
524 |             return 100.0
    |

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/performance_optimizer.py:526:42
    |
524 |             return 100.0
525 |
526 |     def get_performance_summary(self) -> Dict[str, Any]:
    |                                          ^^^^
527 |         """Get comprehensive performance summary."""
528 |         cache_summary = self.cache_manager.get_cache_summary()
    |
help: Replace with `dict`

C403 Unnecessary list comprehension (rewrite as a set comprehension)
   --> src/project_a/pyspark_interview_project/performance_optimizer.py:535:42
    |
533 |               "cache_hit_ratios": cache_summary["hit_ratios"],
534 |               "benchmark_results": len(self.benchmark.benchmark_results),
535 |               "optimization_applied": list(set([
    |  __________________________________________^
536 | |                 opt for metrics in self.benchmark.benchmark_results
537 | |                 for opt in metrics.optimization_applied
538 | |             ])),
    | |______________^
539 |               "spark_config": {
540 |                   "shuffle_partitions": self.shuffle_partitions,
    |
help: Rewrite as a set comprehension

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/performance_optimizer.py:546:63
    |
546 | def create_performance_optimizer(spark: SparkSession, config: Dict[str, Any]) -> PerformanceOptimizer:
    |                                                               ^^^^
547 |     """Factory function to create PerformanceOptimizer."""
548 |     return PerformanceOptimizer(spark, config)
    |
help: Replace with `dict`

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/production_pipeline.py:5:1
   |
 3 |   """
 4 |
 5 | / import os
 6 | | import sys
 7 | | import pandas as pd
 8 | | import numpy as np
 9 | | import json
10 | | from datetime import datetime
11 | | from typing import Dict, Any
12 | | import logging
   | |______________^
13 |
14 |   # Setup logging
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/production_pipeline.py:11:1
   |
 9 | import json
10 | from datetime import datetime
11 | from typing import Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
12 | import logging
   |

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/production_pipeline.py:22:1
   |
20 |     Production ETL Pipeline that creates Delta Lake tables from existing data
21 |     """
22 |     
   | ^^^^
23 |     def __init__(self, config: Dict[str, Any]):
24 |         self.config = config
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/production_pipeline.py:23:32
   |
21 |     """
22 |     
23 |     def __init__(self, config: Dict[str, Any]):
   |                                ^^^^
24 |         self.config = config
25 |         self.delta_path = config.get("delta_path", "data/lakehouse_delta")
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/production_pipeline.py:26:1
   |
24 |         self.config = config
25 |         self.delta_path = config.get("delta_path", "data/lakehouse_delta")
26 |         
   | ^^^^^^^^
27 |     def create_delta_lake_structure(self, table_name: str, data: pd.DataFrame, layer: str):
28 |         """Create Delta Lake table structure with transaction logs."""
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/production_pipeline.py:30:1
   |
28 |         """Create Delta Lake table structure with transaction logs."""
29 |         logger.info(f"ðŸ—ï¸ Creating Delta Lake structure for {table_name} in {layer} layer...")
30 |         
   | ^^^^^^^^
31 |         # Create directory structure
32 |         table_dir = f"{self.delta_path}/{layer}/{table_name}"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/production_pipeline.py:34:1
   |
32 |         table_dir = f"{self.delta_path}/{layer}/{table_name}"
33 |         delta_log_dir = f"{table_dir}/_delta_log"
34 |         
   | ^^^^^^^^
35 |         os.makedirs(delta_log_dir, exist_ok=True)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/production_pipeline.py:36:1
   |
35 |         os.makedirs(delta_log_dir, exist_ok=True)
36 |         
   | ^^^^^^^^
37 |         # Write data as Parquet
38 |         parquet_file = f"{table_dir}/part-00000-{table_name}.parquet"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/production_pipeline.py:40:1
   |
38 |         parquet_file = f"{table_dir}/part-00000-{table_name}.parquet"
39 |         data.to_parquet(parquet_file, index=False)
40 |         
   | ^^^^^^^^
41 |         # Create transaction log
42 |         transaction_log = {
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/production_pipeline.py:67:1
   |
65 |             }
66 |         }
67 |         
   | ^^^^^^^^
68 |         # Write transaction log
69 |         log_file = f"{delta_log_dir}/00000000000000000000.json"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/production_pipeline.py:72:1
   |
70 |         with open(log_file, 'w') as f:
71 |             json.dump(transaction_log, f, indent=2)
72 |         
   | ^^^^^^^^
73 |         # Create checksum
74 |         checksum_file = f"{delta_log_dir}/00000000000000000000.crc"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/production_pipeline.py:77:1
   |
75 |         with open(checksum_file, 'w') as f:
76 |             f.write("7a8b9c0d1e2f3a4b5c6d7e8f9a0b1c2d")  # Sample checksum
77 |         
   | ^^^^^^^^
78 |         logger.info(f"âœ… Created Delta Lake table: {table_dir}")
79 |         logger.info(f"   - Data: {parquet_file}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/production_pipeline.py:82:1
   |
80 |         logger.info(f"   - Transaction log: {log_file}")
81 |         logger.info(f"   - Checksum: {checksum_file}")
82 |         
   | ^^^^^^^^
83 |         return table_dir
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/production_pipeline.py:84:1
   |
83 |         return table_dir
84 |     
   | ^^^^
85 |     def _generate_schema_string(self, data: pd.DataFrame) -> str:
86 |         """Generate Delta Lake schema string from DataFrame."""
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/production_pipeline.py:88:1
   |
86 |         """Generate Delta Lake schema string from DataFrame."""
87 |         schema = {"type": "struct", "fields": []}
88 |         
   | ^^^^^^^^
89 |         for column, dtype in data.dtypes.items():
90 |             field = {
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/production_pipeline.py:96:1
   |
94 |             }
95 |             schema["fields"].append(field)
96 |         
   | ^^^^^^^^
97 |         return json.dumps(schema)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:98:1
    |
 97 |         return json.dumps(schema)
 98 |     
    | ^^^^
 99 |     def _pandas_to_spark_type(self, pandas_type: str) -> str:
100 |         """Convert pandas dtype to Spark type."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:111:1
    |
109 |         }
110 |         return type_mapping.get(pandas_type, 'string')
111 |     
    | ^^^^
112 |     def _generate_stats(self, data: pd.DataFrame) -> str:
113 |         """Generate statistics for Delta Lake."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:120:1
    |
118 |             "nullCount": {}
119 |         }
120 |         
    | ^^^^^^^^
121 |         for column in data.columns:
122 |             if data[column].dtype in ['int64', 'float64']:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:126:1
    |
124 |                 stats["maxValues"][column] = float(data[column].max())
125 |             stats["nullCount"][column] = int(data[column].isnull().sum())
126 |         
    | ^^^^^^^^
127 |         return json.dumps(stats)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:128:1
    |
127 |         return json.dumps(stats)
128 |     
    | ^^^^
129 |     def process_existing_data(self):
130 |         """Process existing Parquet data and convert to Delta Lake format."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:132:1
    |
130 |         """Process existing Parquet data and convert to Delta Lake format."""
131 |         logger.info("ðŸ”„ Processing existing data to Delta Lake format...")
132 |         
    | ^^^^^^^^
133 |         # Check if layers exist, if not create them
134 |         if not os.path.exists(self.delta_path):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:137:1
    |
135 |             logger.info("ðŸ“Š Creating initial Delta Lake structure...")
136 |             self._create_initial_layers()
137 |         
    | ^^^^^^^^
138 |         # Process Bronze layer
139 |         self._process_layer("bronze")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:140:1
    |
138 |         # Process Bronze layer
139 |         self._process_layer("bronze")
140 |         
    | ^^^^^^^^
141 |         # Process Silver layer
142 |         self._process_layer("silver")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:143:1
    |
141 |         # Process Silver layer
142 |         self._process_layer("silver")
143 |         
    | ^^^^^^^^
144 |         # Process Gold layer
145 |         self._process_layer("gold")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:146:1
    |
144 |         # Process Gold layer
145 |         self._process_layer("gold")
146 |         
    | ^^^^^^^^
147 |         logger.info("âœ… All layers processed successfully!")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:148:1
    |
147 |         logger.info("âœ… All layers processed successfully!")
148 |         
    | ^^^^^^^^
149 |         # Log final counts for verification
150 |         self._log_final_counts()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:151:1
    |
149 |         # Log final counts for verification
150 |         self._log_final_counts()
151 |     
    | ^^^^
152 |     def _create_initial_layers(self):
153 |         """Create initial Delta Lake layers with sample data"""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:155:1
    |
153 |         """Create initial Delta Lake layers with sample data"""
154 |         logger.info("ðŸ—ï¸ Creating initial Delta Lake layers...")
155 |         
    | ^^^^^^^^
156 |         # Create directory structure
157 |         os.makedirs(self.delta_path, exist_ok=True)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:158:1
    |
156 |         # Create directory structure
157 |         os.makedirs(self.delta_path, exist_ok=True)
158 |         
    | ^^^^^^^^
159 |         # Create Bronze layer data
160 |         bronze_path = os.path.join(self.delta_path, "bronze")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:162:1
    |
160 |         bronze_path = os.path.join(self.delta_path, "bronze")
161 |         os.makedirs(bronze_path, exist_ok=True)
162 |         
    | ^^^^^^^^
163 |         # Create Silver layer data
164 |         silver_path = os.path.join(self.delta_path, "silver")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:166:1
    |
164 |         silver_path = os.path.join(self.delta_path, "silver")
165 |         os.makedirs(silver_path, exist_ok=True)
166 |         
    | ^^^^^^^^
167 |         # Create Gold layer data
168 |         gold_path = os.path.join(self.delta_path, "gold")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:170:1
    |
168 |         gold_path = os.path.join(self.delta_path, "gold")
169 |         os.makedirs(gold_path, exist_ok=True)
170 |         
    | ^^^^^^^^
171 |         # Create sample data for each layer
172 |         self._create_sample_bronze_data()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:175:1
    |
173 |         self._create_sample_silver_data()
174 |         self._create_sample_gold_data()
175 |         
    | ^^^^^^^^
176 |         logger.info("âœ… Initial Delta Lake structure created")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:177:1
    |
176 |         logger.info("âœ… Initial Delta Lake structure created")
177 |     
    | ^^^^
178 |     def _create_sample_bronze_data(self):
179 |         """Create sample bronze layer data"""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:189:1
    |
187 |             'country': ['USA'] * 600 + ['Canada'] * 200 + ['UK'] * 200
188 |         })
189 |         
    | ^^^^^^^^
190 |         # Create sample orders data
191 |         orders_data = pd.DataFrame({
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:200:1
    |
198 |             'currency': ['USD'] * 5000
199 |         })
200 |         
    | ^^^^^^^^
201 |         # Save as parquet files
202 |         customers_data.to_parquet(f"{self.delta_path}/bronze/customers.parquet", index=False)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:204:1
    |
202 |         customers_data.to_parquet(f"{self.delta_path}/bronze/customers.parquet", index=False)
203 |         orders_data.to_parquet(f"{self.delta_path}/bronze/orders.parquet", index=False)
204 |         
    | ^^^^^^^^
205 |         logger.info(f"âœ… Created bronze layer with {len(customers_data)} customers and {len(orders_data)} orders")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:206:1
    |
205 |         logger.info(f"âœ… Created bronze layer with {len(customers_data)} customers and {len(orders_data)} orders")
206 |     
    | ^^^^
207 |     def _create_sample_silver_data(self):
208 |         """Create sample silver layer data"""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:212:1
    |
210 |         customers_df = pd.read_parquet(f"{self.delta_path}/bronze/customers.parquet")
211 |         orders_df = pd.read_parquet(f"{self.delta_path}/bronze/orders.parquet")
212 |         
    | ^^^^^^^^
213 |         # Add computed columns for silver layer
214 |         customers_df['customer_lifetime_days'] = (datetime.now() - customers_df['created_date']).dt.days
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:216:1
    |
214 |         customers_df['customer_lifetime_days'] = (datetime.now() - customers_df['created_date']).dt.days
215 |         customers_df['is_premium'] = customers_df['segment'] == 'Premium'
216 |         
    | ^^^^^^^^
217 |         orders_df['order_month'] = orders_df['order_date'].dt.month
218 |         orders_df['amount_category'] = pd.cut(orders_df['amount'], 
    |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:218:67
    |
217 |         orders_df['order_month'] = orders_df['order_date'].dt.month
218 |         orders_df['amount_category'] = pd.cut(orders_df['amount'], 
    |                                                                   ^
219 |                                             bins=[0, 100, 500, float('inf')], 
220 |                                             labels=['Small', 'Medium', 'Large'])
    |
help: Remove trailing whitespace

W291 [*] Trailing whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:219:78
    |
217 |         orders_df['order_month'] = orders_df['order_date'].dt.month
218 |         orders_df['amount_category'] = pd.cut(orders_df['amount'], 
219 |                                             bins=[0, 100, 500, float('inf')], 
    |                                                                              ^
220 |                                             labels=['Small', 'Medium', 'Large'])
    |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:221:1
    |
219 |                                             bins=[0, 100, 500, float('inf')], 
220 |                                             labels=['Small', 'Medium', 'Large'])
221 |         
    | ^^^^^^^^
222 |         # Save as parquet files
223 |         customers_df.to_parquet(f"{self.delta_path}/silver/customers.parquet", index=False)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:225:1
    |
223 |         customers_df.to_parquet(f"{self.delta_path}/silver/customers.parquet", index=False)
224 |         orders_df.to_parquet(f"{self.delta_path}/silver/orders.parquet", index=False)
225 |         
    | ^^^^^^^^
226 |         logger.info(f"âœ… Created silver layer with transformed data")
    |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
   --> src/project_a/pyspark_interview_project/production_pipeline.py:226:21
    |
224 |         orders_df.to_parquet(f"{self.delta_path}/silver/orders.parquet", index=False)
225 |         
226 |         logger.info(f"âœ… Created silver layer with transformed data")
    |                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
227 |     
228 |     def _create_sample_gold_data(self):
    |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:227:1
    |
226 |         logger.info(f"âœ… Created silver layer with transformed data")
227 |     
    | ^^^^
228 |     def _create_sample_gold_data(self):
229 |         """Create sample gold layer data"""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:233:1
    |
231 |         customers_df = pd.read_parquet(f"{self.delta_path}/silver/customers.parquet")
232 |         orders_df = pd.read_parquet(f"{self.delta_path}/silver/orders.parquet")
233 |         
    | ^^^^^^^^
234 |         # Customer analytics
235 |         customer_analytics = customers_df.groupby('segment').agg({
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:240:1
    |
238 |         }).reset_index()
239 |         customer_analytics.columns = ['segment', 'customer_count', 'avg_lifetime_days']
240 |         
    | ^^^^^^^^
241 |         # Order analytics
242 |         order_analytics = orders_df.groupby('amount_category').agg({
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:247:1
    |
245 |         }).reset_index()
246 |         order_analytics.columns = ['amount_category', 'order_count', 'total_amount', 'avg_amount']
247 |         
    | ^^^^^^^^
248 |         # Monthly revenue
249 |         monthly_revenue = orders_df.groupby(orders_df['order_date'].dt.month).agg({
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:253:1
    |
251 |         }).reset_index()
252 |         monthly_revenue.columns = ['month', 'total_revenue']
253 |         
    | ^^^^^^^^
254 |         # Save as parquet files
255 |         customer_analytics.to_parquet(f"{self.delta_path}/gold/customer_analytics.parquet", index=False)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:258:1
    |
256 |         order_analytics.to_parquet(f"{self.delta_path}/gold/order_analytics.parquet", index=False)
257 |         monthly_revenue.to_parquet(f"{self.delta_path}/gold/monthly_revenue.parquet", index=False)
258 |         
    | ^^^^^^^^
259 |         logger.info(f"âœ… Created gold layer with analytics data")
    |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
   --> src/project_a/pyspark_interview_project/production_pipeline.py:259:21
    |
257 |         monthly_revenue.to_parquet(f"{self.delta_path}/gold/monthly_revenue.parquet", index=False)
258 |         
259 |         logger.info(f"âœ… Created gold layer with analytics data")
    |                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
260 |     
261 |     def _process_layer(self, layer: str):
    |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:260:1
    |
259 |         logger.info(f"âœ… Created gold layer with analytics data")
260 |     
    | ^^^^
261 |     def _process_layer(self, layer: str):
262 |         """Process a specific layer."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:264:1
    |
262 |         """Process a specific layer."""
263 |         logger.info(f"ðŸ“Š Processing {layer} layer...")
264 |         
    | ^^^^^^^^
265 |         layer_path = f"data/lakehouse_delta/{layer}"
266 |         if not os.path.exists(layer_path):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:269:1
    |
267 |             logger.warning(f"âš ï¸ Layer path not found: {layer_path}")
268 |             return
269 |         
    | ^^^^^^^^
270 |         # Find Parquet files in layer
271 |         parquet_files = [f for f in os.listdir(layer_path) if f.endswith('.parquet')]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:272:1
    |
270 |         # Find Parquet files in layer
271 |         parquet_files = [f for f in os.listdir(layer_path) if f.endswith('.parquet')]
272 |         
    | ^^^^^^^^
273 |         for parquet_file in parquet_files:
274 |             try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:278:1
    |
276 |                 file_path = f"{layer_path}/{parquet_file}"
277 |                 data = pd.read_parquet(file_path)
278 |                 
    | ^^^^^^^^^^^^^^^^
279 |                 # Extract table name
280 |                 table_name = parquet_file.replace('.parquet', '').replace(f'_{layer}', '')
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:281:1
    |
279 |                 # Extract table name
280 |                 table_name = parquet_file.replace('.parquet', '').replace(f'_{layer}', '')
281 |                 
    | ^^^^^^^^^^^^^^^^
282 |                 # Create Delta Lake structure
283 |                 self.create_delta_lake_structure(table_name, data, layer)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:284:1
    |
282 |                 # Create Delta Lake structure
283 |                 self.create_delta_lake_structure(table_name, data, layer)
284 |                 
    | ^^^^^^^^^^^^^^^^
285 |                 logger.info(f"âœ… Processed {table_name} ({len(data)} records)")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:286:1
    |
285 |                 logger.info(f"âœ… Processed {table_name} ({len(data)} records)")
286 |                 
    | ^^^^^^^^^^^^^^^^
287 |             except Exception as e:
288 |                 logger.error(f"âŒ Failed to process {parquet_file}: {e}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:289:1
    |
287 |             except Exception as e:
288 |                 logger.error(f"âŒ Failed to process {parquet_file}: {e}")
289 |     
    | ^^^^
290 |     def demonstrate_time_travel(self):
291 |         """Demonstrate Delta Lake time travel capabilities."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:293:1
    |
291 |         """Demonstrate Delta Lake time travel capabilities."""
292 |         logger.info("ðŸ• Demonstrating Delta Lake time travel...")
293 |         
    | ^^^^^^^^
294 |         # Create multiple versions for bronze customers table
295 |         customers_path = f"{self.delta_path}/bronze/customers"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:298:1
    |
296 |         if os.path.exists(customers_path):
297 |             self._create_table_version(customers_path, "customers", "bronze")
298 |             
    | ^^^^^^^^^^^^
299 |             # Show version history
300 |             self._show_version_history(customers_path)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:301:1
    |
299 |             # Show version history
300 |             self._show_version_history(customers_path)
301 |         
    | ^^^^^^^^
302 |         # Create multiple versions for silver layer tables
303 |         silver_path = f"{self.delta_path}/silver"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:311:1
    |
309 |                     self._create_silver_table_version(table_path, table_dir)
310 |                     self._show_version_history(table_path)
311 |         
    | ^^^^^^^^
312 |         # Create multiple versions for gold layer tables
313 |         gold_path = f"{self.delta_path}/gold"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:321:1
    |
319 |                     self._create_gold_table_version(table_path, table_dir)
320 |                     self._show_version_history(table_path)
321 |     
    | ^^^^
322 |     def _create_table_version(self, table_path: str, table_name: str, layer: str):
323 |         """Create multiple versions of a table."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:325:1
    |
323 |         """Create multiple versions of a table."""
324 |         logger.info(f"ðŸ“ Creating multiple versions of {table_name}...")
325 |         
    | ^^^^^^^^
326 |         # Read existing data
327 |         data_file = f"{table_path}/part-00000-{table_name}.parquet"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:330:1
    |
328 |         if os.path.exists(data_file):
329 |             data = pd.read_parquet(data_file)
330 |             
    | ^^^^^^^^^^^^
331 |             # Create 3 versions
332 |             for version in range(1, 4):
    |
help: Remove whitespace from blank line

F841 Local variable `new_data` is assigned to but never used
   --> src/project_a/pyspark_interview_project/production_pipeline.py:334:17
    |
332 |             for version in range(1, 4):
333 |                 # Modify data (add new records)
334 |                 new_data = data.copy()
    |                 ^^^^^^^^
335 |                 new_records = data.head(25 * version).copy()  # Add more records each version
336 |                 new_records = new_records.reset_index(drop=True)  # Reset index to avoid length mismatch
    |
help: Remove assignment to unused variable `new_data`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:337:1
    |
335 |                 new_records = data.head(25 * version).copy()  # Add more records each version
336 |                 new_records = new_records.reset_index(drop=True)  # Reset index to avoid length mismatch
337 |                 
    | ^^^^^^^^^^^^^^^^
338 |                 # Create new customer IDs as a list with correct length
339 |                 num_new_records = len(new_records)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:342:1
    |
340 |                 new_customer_ids = list(range(len(data) + (version-1)*25 + 1, len(data) + (version-1)*25 + 1 + num_new_records))
341 |                 new_names = [f'Customer_v{version}_{i}' for i in new_customer_ids]
342 |                 
    | ^^^^^^^^^^^^^^^^
343 |                 # Assign new values with correct length
344 |                 new_records = new_records.copy()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:347:1
    |
345 |                 new_records['customer_id'] = new_customer_ids
346 |                 new_records['name'] = new_names
347 |                 
    | ^^^^^^^^^^^^^^^^
348 |                 updated_data = pd.concat([data, new_records], ignore_index=True)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:349:1
    |
348 |                 updated_data = pd.concat([data, new_records], ignore_index=True)
349 |                 
    | ^^^^^^^^^^^^^^^^
350 |                 # Write new version
351 |                 new_parquet_file = f"{table_path}/part-0000{version}-{table_name}-v{version+1}.parquet"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:353:1
    |
351 |                 new_parquet_file = f"{table_path}/part-0000{version}-{table_name}-v{version+1}.parquet"
352 |                 updated_data.to_parquet(new_parquet_file, index=False)
353 |                 
    | ^^^^^^^^^^^^^^^^
354 |                 # Create transaction log
355 |                 delta_log_dir = f"{table_path}/_delta_log"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:370:1
    |
368 |                     }
369 |                 }
370 |                 
    | ^^^^^^^^^^^^^^^^
371 |                 # Write transaction log
372 |                 log_file = f"{delta_log_dir}/0000000000000000000{version}.json"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:375:1
    |
373 |                 with open(log_file, 'w') as f:
374 |                     json.dump(transaction_log, f, indent=2)
375 |                 
    | ^^^^^^^^^^^^^^^^
376 |                 # Create checksum
377 |                 checksum_file = f"{delta_log_dir}/0000000000000000000{version}.crc"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:380:1
    |
378 |                 with open(checksum_file, 'w') as f:
379 |                     f.write(f"8c9d0e1f2a3b4c5d6e7f8a9b0c1d2e3{version}")
380 |                 
    | ^^^^^^^^^^^^^^^^
381 |                 logger.info(f"âœ… Created version {version} of {table_name} ({len(updated_data)} records)")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:382:1
    |
381 |                 logger.info(f"âœ… Created version {version} of {table_name} ({len(updated_data)} records)")
382 |     
    | ^^^^
383 |     def _create_silver_table_version(self, table_path: str, table_name: str):
384 |         """Create multiple versions for silver layer tables."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:386:1
    |
384 |         """Create multiple versions for silver layer tables."""
385 |         logger.info(f"ðŸ“ Creating multiple versions for silver table: {table_name}...")
386 |         
    | ^^^^^^^^
387 |         # Read existing data
388 |         data_file = f"{table_path}/part-00000-{table_name}.parquet"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:391:1
    |
389 |         if os.path.exists(data_file):
390 |             data = pd.read_parquet(data_file)
391 |             
    | ^^^^^^^^^^^^
392 |             # Create 2 versions
393 |             for version in range(1, 3):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:396:1
    |
394 |                 # Modify data based on table type and actual columns
395 |                 new_data = data.copy()
396 |                 
    | ^^^^^^^^^^^^^^^^
397 |                 if "customers" in table_name.lower():
398 |                     # For customers, update segment and add new customers
    |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:400:44
    |
398 |                     # For customers, update segment and add new customers
399 |                     segment_map = {
400 |                         'Basic': 'Premium', 
    |                                            ^
401 |                         'Premium': 'Enterprise', 
402 |                         'Enterprise': 'Basic'
    |
help: Remove trailing whitespace

W291 [*] Trailing whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:401:49
    |
399 |                     segment_map = {
400 |                         'Basic': 'Premium', 
401 |                         'Premium': 'Enterprise', 
    |                                                 ^
402 |                         'Enterprise': 'Basic'
403 |                     }
    |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:408:1
    |
406 |                     new_customers = data.head(15 * version).copy()
407 |                     new_customers = new_customers.reset_index(drop=True)  # Reset index to avoid length mismatch
408 |                     
    | ^^^^^^^^^^^^^^^^^^^^
409 |                     # Create new customer IDs as a list with correct length
410 |                     num_new_customers = len(new_customers)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:413:1
    |
411 |                     new_customer_ids = list(range(len(data) + (version-1)*15 + 1, len(data) + (version-1)*15 + 1 + num_new_customers))
412 |                     new_names = [f'Silver_v{version}_{i}' for i in new_customer_ids]
413 |                     
    | ^^^^^^^^^^^^^^^^^^^^
414 |                     # Assign new values with correct length
415 |                     new_customers = new_customers.copy()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:433:1
    |
431 |                         'PENDING': 'COMPLETED'
432 |                     }).fillna('Completed')
433 |                 
    | ^^^^^^^^^^^^^^^^
434 |                 # Write new version
435 |                 new_parquet_file = f"{table_path}/part-0000{version}-{table_name}-v{version+1}.parquet"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:437:1
    |
435 |                 new_parquet_file = f"{table_path}/part-0000{version}-{table_name}-v{version+1}.parquet"
436 |                 new_data.to_parquet(new_parquet_file, index=False)
437 |                 
    | ^^^^^^^^^^^^^^^^
438 |                 # Create transaction log
439 |                 delta_log_dir = f"{table_path}/_delta_log"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:454:1
    |
452 |                     }
453 |                 }
454 |                 
    | ^^^^^^^^^^^^^^^^
455 |                 # Write transaction log
456 |                 log_file = f"{delta_log_dir}/0000000000000000000{version}.json"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:459:1
    |
457 |                 with open(log_file, 'w') as f:
458 |                     json.dump(transaction_log, f, indent=2)
459 |                 
    | ^^^^^^^^^^^^^^^^
460 |                 # Create checksum
461 |                 checksum_file = f"{delta_log_dir}/0000000000000000000{version}.crc"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:464:1
    |
462 |                 with open(checksum_file, 'w') as f:
463 |                     f.write(f"8c9d0e1f2a3b4c5d6e7f8a9b0c1d2e3{version}")
464 |                 
    | ^^^^^^^^^^^^^^^^
465 |                 logger.info(f"âœ… Created version {version} of silver {table_name} ({len(new_data)} records)")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:466:1
    |
465 |                 logger.info(f"âœ… Created version {version} of silver {table_name} ({len(new_data)} records)")
466 |     
    | ^^^^
467 |     def _create_gold_table_version(self, table_path: str, table_name: str):
468 |         """Create multiple versions for gold layer tables."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:470:1
    |
468 |         """Create multiple versions for gold layer tables."""
469 |         logger.info(f"ðŸ“ Creating multiple versions for gold table: {table_name}...")
470 |         
    | ^^^^^^^^
471 |         # Read existing data
472 |         data_file = f"{table_path}/part-00000-{table_name}.parquet"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:475:1
    |
473 |         if os.path.exists(data_file):
474 |             data = pd.read_parquet(data_file)
475 |             
    | ^^^^^^^^^^^^
476 |             # Create 2 versions
477 |             for version in range(1, 3):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:480:1
    |
478 |                 # Modify data based on table type and actual columns
479 |                 new_data = data.copy()
480 |                 
    | ^^^^^^^^^^^^^^^^
481 |                 if "customer_analytics" in table_name.lower():
482 |                     # For customer analytics, increase customer counts
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:498:1
    |
496 |                     if 'id' in new_data.columns:
497 |                         new_data['id'] = new_data['id'] + (1000 * version)
498 |                 
    | ^^^^^^^^^^^^^^^^
499 |                 # Write new version
500 |                 new_parquet_file = f"{table_path}/part-0000{version}-{table_name}-v{version+1}.parquet"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:502:1
    |
500 |                 new_parquet_file = f"{table_path}/part-0000{version}-{table_name}-v{version+1}.parquet"
501 |                 new_data.to_parquet(new_parquet_file, index=False)
502 |                 
    | ^^^^^^^^^^^^^^^^
503 |                 # Create transaction log
504 |                 delta_log_dir = f"{table_path}/_delta_log"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:519:1
    |
517 |                     }
518 |                 }
519 |                 
    | ^^^^^^^^^^^^^^^^
520 |                 # Write transaction log
521 |                 log_file = f"{delta_log_dir}/0000000000000000000{version}.json"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:524:1
    |
522 |                 with open(log_file, 'w') as f:
523 |                     json.dump(transaction_log, f, indent=2)
524 |                 
    | ^^^^^^^^^^^^^^^^
525 |                 # Create checksum
526 |                 checksum_file = f"{delta_log_dir}/0000000000000000000{version}.crc"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:529:1
    |
527 |                 with open(checksum_file, 'w') as f:
528 |                     f.write(f"8c9d0e1f2a3b4c5d6e7f8a9b0c1d2e3{version}")
529 |                 
    | ^^^^^^^^^^^^^^^^
530 |                 logger.info(f"âœ… Created version {version} of gold {table_name} ({len(new_data)} records)")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:531:1
    |
530 |                 logger.info(f"âœ… Created version {version} of gold {table_name} ({len(new_data)} records)")
531 |     
    | ^^^^
532 |     def _show_version_history(self, table_path: str):
533 |         """Show version history of a Delta Lake table."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:535:1
    |
533 |         """Show version history of a Delta Lake table."""
534 |         logger.info(f"ðŸ“Š Version history for {table_path}:")
535 |         
    | ^^^^^^^^
536 |         delta_log_dir = f"{table_path}/_delta_log"
537 |         if os.path.exists(delta_log_dir):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:539:1
    |
537 |         if os.path.exists(delta_log_dir):
538 |             log_files = [f for f in os.listdir(delta_log_dir) if f.endswith('.json')]
539 |             
    | ^^^^^^^^^^^^
540 |             for i, log_file in enumerate(sorted(log_files)):
541 |                 log_path = f"{delta_log_dir}/{log_file}"
    |
help: Remove whitespace from blank line

UP015 [*] Unnecessary mode argument
   --> src/project_a/pyspark_interview_project/production_pipeline.py:542:37
    |
540 |             for i, log_file in enumerate(sorted(log_files)):
541 |                 log_path = f"{delta_log_dir}/{log_file}"
542 |                 with open(log_path, 'r') as f:
    |                                     ^^^
543 |                     log_data = json.load(f)
    |
help: Remove mode argument

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:544:1
    |
542 |                 with open(log_path, 'r') as f:
543 |                     log_data = json.load(f)
544 |                 
    | ^^^^^^^^^^^^^^^^
545 |                 if 'metaData' in log_data:
546 |                     # Initial version
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:553:1
    |
551 |                     logger.info(f"   Version {i}: Update")
552 |                     logger.info(f"      Records: {json.loads(log_data['add']['stats'])['numRecords']}")
553 |     
    | ^^^^
554 |     def run_pipeline(self):
555 |         """Run the complete production pipeline."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:558:1
    |
556 |         logger.info("ðŸš€ Starting Production ETL Pipeline...")
557 |         start_time = datetime.now()
558 |         
    | ^^^^^^^^
559 |         try:
560 |             # Process existing data
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:562:1
    |
560 |             # Process existing data
561 |             self.process_existing_data()
562 |             
    | ^^^^^^^^^^^^
563 |             # Demonstrate time travel
564 |             self.demonstrate_time_travel()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:565:1
    |
563 |             # Demonstrate time travel
564 |             self.demonstrate_time_travel()
565 |             
    | ^^^^^^^^^^^^
566 |             duration = datetime.now() - start_time
567 |             logger.info(f"ðŸŽ‰ Pipeline completed successfully in {duration}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:568:1
    |
566 |             duration = datetime.now() - start_time
567 |             logger.info(f"ðŸŽ‰ Pipeline completed successfully in {duration}")
568 |             
    | ^^^^^^^^^^^^
569 |             return True
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:570:1
    |
569 |             return True
570 |             
    | ^^^^^^^^^^^^
571 |         except Exception as e:
572 |             logger.error(f"âŒ Pipeline failed: {e}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:574:1
    |
572 |             logger.error(f"âŒ Pipeline failed: {e}")
573 |             raise
574 |     
    | ^^^^
575 |     def _log_final_counts(self):
576 |         """Log final row counts for verification"""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:578:1
    |
576 |         """Log final row counts for verification"""
577 |         logger.info("ðŸ“Š Final Delta Lake table counts:")
578 |         
    | ^^^^^^^^
579 |         layers = ["bronze", "silver", "gold"]
580 |         for layer in layers:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:595:1
    |
593 |                             except Exception as e:
594 |                                 logger.warning(f"  âš ï¸ {layer}.{table_dir}: Could not read - {e}")
595 |         
    | ^^^^^^^^
596 |         logger.info("âœ… Final counts logged")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:604:1
    |
602 |         "delta_path": "data/lakehouse_delta"
603 |     }
604 |     
    | ^^^^
605 |     pipeline = ProductionETLPipeline(config)
606 |     success = pipeline.run_pipeline()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/production_pipeline.py:607:1
    |
605 |     pipeline = ProductionETLPipeline(config)
606 |     success = pipeline.run_pipeline()
607 |     
    | ^^^^
608 |     if success:
609 |         print("\nðŸŽ‰ Production ETL Pipeline completed successfully!")
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/schema_validator.py:10:1
   |
 8 |   """
 9 |
10 | / import json
11 | | import logging
12 | | from pathlib import Path
13 | | from typing import Dict, Any, Optional
14 | |
15 | | from pyspark.sql import DataFrame, SparkSession
16 | | from pyspark.sql.types import StructType
   | |________________________________________^
17 |
18 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/schema_validator.py:13:1
   |
11 | import logging
12 | from pathlib import Path
13 | from typing import Dict, Any, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
14 |
15 | from pyspark.sql import DataFrame, SparkSession
   |

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/schema_validator.py:23:1
   |
21 | class SchemaValidator:
22 |     """Validates schemas against evolution policies."""
23 |     
   | ^^^^
24 |     def __init__(self, spark: SparkSession, contracts_dir: str = "config/contracts"):
25 |         self.spark = spark
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/schema_validator.py:27:1
   |
25 |         self.spark = spark
26 |         self.contracts_dir = Path(contracts_dir)
27 |         
   | ^^^^^^^^
28 |     def validate_bronze_schema(self, df: DataFrame, table_name: str) -> Dict[str, Any]:
29 |         """
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/schema_validator.py:28:73
   |
26 |         self.contracts_dir = Path(contracts_dir)
27 |         
28 |     def validate_bronze_schema(self, df: DataFrame, table_name: str) -> Dict[str, Any]:
   |                                                                         ^^^^
29 |         """
30 |         Bronze layer validation - permissive.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/schema_validator.py:31:1
   |
29 |         """
30 |         Bronze layer validation - permissive.
31 |         
   | ^^^^^^^^
32 |         Args:
33 |             df: DataFrame to validate
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/schema_validator.py:35:1
   |
33 |             df: DataFrame to validate
34 |             table_name: Name of the table
35 |             
   | ^^^^^^^^^^^^
36 |         Returns:
37 |             Validation results
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/schema_validator.py:40:1
   |
38 |         """
39 |         logger.info(f"Bronze validation for {table_name}: permissive mode")
40 |         
   | ^^^^^^^^
41 |         return {
42 |             "layer": "bronze",
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/schema_validator.py:51:1
   |
49 |             }
50 |         }
51 |     
   | ^^^^
52 |     def validate_silver_schema(self, df: DataFrame, table_name: str, 
53 |                              expected_schema: Optional[StructType] = None) -> Dict[str, Any]:
   |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
  --> src/project_a/pyspark_interview_project/schema_validator.py:52:69
   |
50 |         }
51 |     
52 |     def validate_silver_schema(self, df: DataFrame, table_name: str, 
   |                                                                     ^
53 |                              expected_schema: Optional[StructType] = None) -> Dict[str, Any]:
54 |         """
   |
help: Remove trailing whitespace

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/schema_validator.py:53:47
   |
52 |     def validate_silver_schema(self, df: DataFrame, table_name: str, 
53 |                              expected_schema: Optional[StructType] = None) -> Dict[str, Any]:
   |                                               ^^^^^^^^^^^^^^^^^^^^
54 |         """
55 |         Silver layer validation - fixed schema.
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/schema_validator.py:53:79
   |
52 |     def validate_silver_schema(self, df: DataFrame, table_name: str, 
53 |                              expected_schema: Optional[StructType] = None) -> Dict[str, Any]:
   |                                                                               ^^^^
54 |         """
55 |         Silver layer validation - fixed schema.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/schema_validator.py:56:1
   |
54 |         """
55 |         Silver layer validation - fixed schema.
56 |         
   | ^^^^^^^^
57 |         Args:
58 |             df: DataFrame to validate
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/schema_validator.py:61:1
   |
59 |             table_name: Name of the table
60 |             expected_schema: Expected schema (if None, will be inferred from existing table)
61 |             
   | ^^^^^^^^^^^^
62 |         Returns:
63 |             Validation results
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/schema_validator.py:66:1
   |
64 |         """
65 |         logger.info(f"Silver validation for {table_name}: fixed schema mode")
66 |         
   | ^^^^^^^^
67 |         if expected_schema is None:
68 |             # Try to infer from existing table
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/schema_validator.py:84:1
   |
82 |                     }
83 |                 }
84 |         
   | ^^^^^^^^
85 |         # Compare schemas
86 |         actual_schema = df.schema
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/schema_validator.py:88:1
   |
86 |         actual_schema = df.schema
87 |         differences = self._compare_schemas(expected_schema, actual_schema)
88 |         
   | ^^^^^^^^
89 |         if differences["breaking_changes"]:
90 |             return {
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/schema_validator.py:97:1
   |
95 |                 "differences": differences
96 |             }
97 |         
   | ^^^^^^^^
98 |         return {
99 |             "layer": "silver",
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schema_validator.py:105:1
    |
103 |             "differences": differences
104 |         }
105 |     
    | ^^^^
106 |     def validate_gold_schema(self, df: DataFrame, table_name: str) -> Dict[str, Any]:
107 |         """
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/schema_validator.py:106:71
    |
104 |         }
105 |     
106 |     def validate_gold_schema(self, df: DataFrame, table_name: str) -> Dict[str, Any]:
    |                                                                       ^^^^
107 |         """
108 |         Gold layer validation - contract schema.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schema_validator.py:109:1
    |
107 |         """
108 |         Gold layer validation - contract schema.
109 |         
    | ^^^^^^^^
110 |         Args:
111 |             df: DataFrame to validate
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schema_validator.py:113:1
    |
111 |             df: DataFrame to validate
112 |             table_name: Name of the table
113 |             
    | ^^^^^^^^^^^^
114 |         Returns:
115 |             Validation results
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schema_validator.py:118:1
    |
116 |         """
117 |         logger.info(f"Gold validation for {table_name}: contract schema mode")
118 |         
    | ^^^^^^^^
119 |         # Load contract
120 |         contract_path = self.contracts_dir / f"{table_name}.json"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schema_validator.py:129:1
    |
127 |                 "contract_path": str(contract_path)
128 |             }
129 |         
    | ^^^^^^^^
130 |         try:
131 |             with open(contract_path, 'r') as f:
    |
help: Remove whitespace from blank line

UP015 [*] Unnecessary mode argument
   --> src/project_a/pyspark_interview_project/schema_validator.py:131:38
    |
130 |         try:
131 |             with open(contract_path, 'r') as f:
    |                                      ^^^
132 |                 contract = json.load(f)
133 |         except Exception as e:
    |
help: Remove mode argument

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schema_validator.py:141:1
    |
139 |                 "contract_path": str(contract_path)
140 |             }
141 |         
    | ^^^^^^^^
142 |         # Validate against contract
143 |         validation_result = self._validate_against_contract(df, contract)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schema_validator.py:144:1
    |
142 |         # Validate against contract
143 |         validation_result = self._validate_against_contract(df, contract)
144 |         
    | ^^^^^^^^
145 |         return {
146 |             "layer": "gold",
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schema_validator.py:153:1
    |
151 |             "validation_details": validation_result
152 |         }
153 |     
    | ^^^^
154 |     def _compare_schemas(self, expected: StructType, actual: StructType) -> Dict[str, Any]:
155 |         """Compare two schemas and identify differences."""
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/schema_validator.py:154:77
    |
152 |         }
153 |     
154 |     def _compare_schemas(self, expected: StructType, actual: StructType) -> Dict[str, Any]:
    |                                                                             ^^^^
155 |         """Compare two schemas and identify differences."""
156 |         expected_fields = {f.name: f for f in expected.fields}
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schema_validator.py:158:1
    |
156 |         expected_fields = {f.name: f for f in expected.fields}
157 |         actual_fields = {f.name: f for f in actual.fields}
158 |         
    | ^^^^^^^^
159 |         missing_columns = set(expected_fields.keys()) - set(actual_fields.keys())
160 |         new_columns = set(actual_fields.keys()) - set(expected_fields.keys())
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schema_validator.py:161:1
    |
159 |         missing_columns = set(expected_fields.keys()) - set(actual_fields.keys())
160 |         new_columns = set(actual_fields.keys()) - set(expected_fields.keys())
161 |         
    | ^^^^^^^^
162 |         type_changes = []
163 |         for col in set(expected_fields.keys()) & set(actual_fields.keys()):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schema_validator.py:170:1
    |
168 |                     "actual": str(actual_fields[col].dataType)
169 |                 })
170 |         
    | ^^^^^^^^
171 |         nullable_changes = []
172 |         for col in set(expected_fields.keys()) & set(actual_fields.keys()):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schema_validator.py:179:1
    |
177 |                     "actual_nullable": actual_fields[col].nullable
178 |                 })
179 |         
    | ^^^^^^^^
180 |         # Breaking changes: missing required columns, type changes, nullable changes
181 |         breaking_changes = []
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schema_validator.py:188:1
    |
186 |         if nullable_changes:
187 |             breaking_changes.append(f"Nullable changes: {nullable_changes}")
188 |         
    | ^^^^^^^^
189 |         return {
190 |             "missing_columns": list(missing_columns),
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schema_validator.py:197:1
    |
195 |             "non_breaking_changes": list(new_columns)
196 |         }
197 |     
    | ^^^^
198 |     def _validate_against_contract(self, df: DataFrame, contract: Dict[str, Any]) -> Dict[str, Any]:
199 |         """Validate DataFrame against JSON schema contract."""
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/schema_validator.py:198:67
    |
196 |         }
197 |     
198 |     def _validate_against_contract(self, df: DataFrame, contract: Dict[str, Any]) -> Dict[str, Any]:
    |                                                                   ^^^^
199 |         """Validate DataFrame against JSON schema contract."""
200 |         schema = contract.get("schema", {})
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/schema_validator.py:198:86
    |
196 |         }
197 |     
198 |     def _validate_against_contract(self, df: DataFrame, contract: Dict[str, Any]) -> Dict[str, Any]:
    |                                                                                      ^^^^
199 |         """Validate DataFrame against JSON schema contract."""
200 |         schema = contract.get("schema", {})
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schema_validator.py:203:1
    |
201 |         properties = schema.get("properties", {})
202 |         required = schema.get("required", [])
203 |         
    | ^^^^^^^^
204 |         # Check required columns
205 |         missing_required = [col for col in required if col not in df.columns]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schema_validator.py:211:1
    |
209 |                 "message": f"Missing required columns: {missing_required}"
210 |             }
211 |         
    | ^^^^^^^^
212 |         # Check for additional properties
213 |         if not schema.get("additionalProperties", True):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schema_validator.py:220:1
    |
218 |                     "message": f"Extra columns not allowed: {extra_columns}"
219 |                 }
220 |         
    | ^^^^^^^^
221 |         # Basic type validation (simplified)
222 |         type_violations = []
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schema_validator.py:230:1
    |
228 |                     if null_count > 0:
229 |                         type_violations.append(f"Column {col} has {null_count} null values but is not nullable")
230 |         
    | ^^^^^^^^
231 |         if type_violations:
232 |             return {
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schema_validator.py:236:1
    |
234 |                 "message": f"Type violations: {type_violations}"
235 |             }
236 |         
    | ^^^^^^^^
237 |         return {
238 |             "valid": True,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schema_validator.py:241:1
    |
239 |             "message": "Contract validation passed"
240 |         }
241 |     
    | ^^^^
242 |     def validate_table(self, df: DataFrame, table_name: str, layer: str) -> Dict[str, Any]:
243 |         """
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/schema_validator.py:242:77
    |
240 |         }
241 |     
242 |     def validate_table(self, df: DataFrame, table_name: str, layer: str) -> Dict[str, Any]:
    |                                                                             ^^^^
243 |         """
244 |         Validate table schema based on layer policy.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schema_validator.py:245:1
    |
243 |         """
244 |         Validate table schema based on layer policy.
245 |         
    | ^^^^^^^^
246 |         Args:
247 |             df: DataFrame to validate
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schema_validator.py:250:1
    |
248 |             table_name: Name of the table
249 |             layer: Data layer (bronze, silver, gold)
250 |             
    | ^^^^^^^^^^^^
251 |         Returns:
252 |             Validation results
    |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
   --> src/project_a/pyspark_interview_project/schema_validator.py:264:74
    |
264 | def validate_schema_evolution(df: DataFrame, table_name: str, layer: str, 
    |                                                                          ^
265 |                             spark: SparkSession) -> Dict[str, Any]:
266 |     """
    |
help: Remove trailing whitespace

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/schema_validator.py:265:53
    |
264 | def validate_schema_evolution(df: DataFrame, table_name: str, layer: str, 
265 |                             spark: SparkSession) -> Dict[str, Any]:
    |                                                     ^^^^
266 |     """
267 |     Convenience function for schema validation.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schema_validator.py:268:1
    |
266 |     """
267 |     Convenience function for schema validation.
268 |     
    | ^^^^
269 |     Args:
270 |         df: DataFrame to validate
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schema_validator.py:274:1
    |
272 |         layer: Data layer (bronze, silver, gold)
273 |         spark: SparkSession instance
274 |         
    | ^^^^^^^^
275 |     Returns:
276 |         Validation results
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/schemas/bronze_schemas.py:5:1
   |
 3 |   No schema inference in production.
 4 |   """
 5 | / from pyspark.sql.types import (
 6 | |     StructType, StructField, StringType, IntegerType, DoubleType,
 7 | |     TimestampType, DateType, BooleanType, LongType
 8 | | )
   | |_^
 9 |
10 |   # CRM Schemas
   |
help: Organize imports

F401 [*] `pyspark.sql.types.TimestampType` imported but unused
 --> src/project_a/pyspark_interview_project/schemas/bronze_schemas.py:7:5
  |
5 | from pyspark.sql.types import (
6 |     StructType, StructField, StringType, IntegerType, DoubleType,
7 |     TimestampType, DateType, BooleanType, LongType
  |     ^^^^^^^^^^^^^
8 | )
  |
help: Remove unused import

F401 [*] `pyspark.sql.types.DateType` imported but unused
 --> src/project_a/pyspark_interview_project/schemas/bronze_schemas.py:7:20
  |
5 | from pyspark.sql.types import (
6 |     StructType, StructField, StringType, IntegerType, DoubleType,
7 |     TimestampType, DateType, BooleanType, LongType
  |                    ^^^^^^^^
8 | )
  |
help: Remove unused import

F401 [*] `pyspark.sql.types.BooleanType` imported but unused
 --> src/project_a/pyspark_interview_project/schemas/bronze_schemas.py:7:30
  |
5 | from pyspark.sql.types import (
6 |     StructType, StructField, StringType, IntegerType, DoubleType,
7 |     TimestampType, DateType, BooleanType, LongType
  |                              ^^^^^^^^^^^
8 | )
  |
help: Remove unused import

F401 [*] `pyspark.sql.types.LongType` imported but unused
 --> src/project_a/pyspark_interview_project/schemas/bronze_schemas.py:7:43
  |
5 | from pyspark.sql.types import (
6 |     StructType, StructField, StringType, IntegerType, DoubleType,
7 |     TimestampType, DateType, BooleanType, LongType
  |                                           ^^^^^^^^
8 | )
  |
help: Remove unused import

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:11:1
   |
 9 |   """
10 |
11 | / import logging
12 | | from typing import Dict, Any
13 | | from pyspark.sql.types import (
14 | |     StructType,
15 | |     StructField,
16 | |     StringType,
17 | |     IntegerType,
18 | |     DoubleType,
19 | |     TimestampType,
20 | |     DateType,
21 | |     BooleanType,
22 | |     LongType,
23 | |     DecimalType
24 | | )
25 | | import json
26 | | from pathlib import Path
   | |________________________^
27 |
28 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:12:1
   |
11 | import logging
12 | from typing import Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
13 | from pyspark.sql.types import (
14 |     StructType,
   |

F401 [*] `pyspark.sql.types.DecimalType` imported but unused
  --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:23:5
   |
21 |     BooleanType,
22 |     LongType,
23 |     DecimalType
   |     ^^^^^^^^^^^
24 | )
25 | import json
   |
help: Remove unused import: `pyspark.sql.types.DecimalType`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:148:1
    |
146 |     """
147 |     Central schema registry for all data sources.
148 |     
    | ^^^^
149 |     Features:
150 |     - Version-controlled schemas
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:155:1
    |
153 |     - Schema evolution tracking
154 |     """
155 |     
    | ^^^^
156 |     def __init__(self, schema_dir: Path = None):
157 |         """
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:159:1
    |
157 |         """
158 |         Initialize schema registry.
159 |         
    | ^^^^^^^^
160 |         Args:
161 |             schema_dir: Directory to store schema versions (default: config/schemas)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:165:1
    |
163 |         self.schema_dir = schema_dir or Path("config/schemas")
164 |         self.schemas = self._load_all_schemas()
165 |         
    | ^^^^^^^^
166 |     def _load_all_schemas(self) -> Dict[str, StructType]:
167 |         """Load all defined schemas."""
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:166:36
    |
164 |         self.schemas = self._load_all_schemas()
165 |         
166 |     def _load_all_schemas(self) -> Dict[str, StructType]:
    |                                    ^^^^
167 |         """Load all defined schemas."""
168 |         return {
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:173:1
    |
171 |             "bronze.orders": BRONZE_ORDERS_SCHEMA,
172 |             "bronze.products": BRONZE_PRODUCTS_SCHEMA,
173 |             
    | ^^^^^^^^^^^^
174 |             # Silver
175 |             "silver.customers": SILVER_CUSTOMERS_SCHEMA,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:177:1
    |
175 |             "silver.customers": SILVER_CUSTOMERS_SCHEMA,
176 |             "silver.orders": SILVER_ORDERS_SCHEMA,
177 |             
    | ^^^^^^^^^^^^
178 |             # Gold
179 |             "gold.customer_analytics": GOLD_CUSTOMER_ANALYTICS_SCHEMA,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:182:1
    |
180 |             "gold.monthly_revenue": GOLD_MONTHLY_REVENUE_SCHEMA,
181 |         }
182 |     
    | ^^^^
183 |     def get_schema(self, table_name: str) -> StructType:
184 |         """
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:186:1
    |
184 |         """
185 |         Get schema for a table.
186 |         
    | ^^^^^^^^
187 |         Args:
188 |             table_name: Fully qualified table name (e.g., 'bronze.customers')
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:189:1
    |
187 |         Args:
188 |             table_name: Fully qualified table name (e.g., 'bronze.customers')
189 |             
    | ^^^^^^^^^^^^
190 |         Returns:
191 |             StructType schema
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:192:1
    |
190 |         Returns:
191 |             StructType schema
192 |             
    | ^^^^^^^^^^^^
193 |         Raises:
194 |             ValueError: If schema not found
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:200:1
    |
198 |             raise ValueError(f"Schema not found for table: {table_name}")
199 |         return schema
200 |     
    | ^^^^
201 |     def validate_schema(
202 |         self,
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:206:10
    |
204 |         expected_table: str,
205 |         allow_additional_columns: bool = False
206 |     ) -> Dict[str, Any]:
    |          ^^^^
207 |         """
208 |         Validate actual schema against expected schema.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:209:1
    |
207 |         """
208 |         Validate actual schema against expected schema.
209 |         
    | ^^^^^^^^
210 |         Args:
211 |             actual_schema: Actual DataFrame schema
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:214:1
    |
212 |             expected_table: Expected table name
213 |             allow_additional_columns: Whether to allow extra columns
214 |             
    | ^^^^^^^^^^^^
215 |         Returns:
216 |             Dictionary with validation results
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:219:1
    |
217 |         """
218 |         expected_schema = self.get_schema(expected_table)
219 |         
    | ^^^^^^^^
220 |         issues = []
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:221:1
    |
220 |         issues = []
221 |         
    | ^^^^^^^^
222 |         # Check for missing columns
223 |         expected_fields = {f.name: f for f in expected_schema.fields}
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:225:1
    |
223 |         expected_fields = {f.name: f for f in expected_schema.fields}
224 |         actual_fields = {f.name: f for f in actual_schema.fields}
225 |         
    | ^^^^^^^^
226 |         for col_name, expected_field in expected_fields.items():
227 |             if col_name not in actual_fields:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:235:1
    |
233 |             else:
234 |                 actual_field = actual_fields[col_name]
235 |                 
    | ^^^^^^^^^^^^^^^^
236 |                 # Check data type
237 |                 if actual_field.dataType != expected_field.dataType:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:249:1
    |
247 |                         )
248 |                     })
249 |                 
    | ^^^^^^^^^^^^^^^^
250 |                 # Check nullability (warn only if stricter than expected)
251 |                 if not expected_field.nullable and actual_field.nullable:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:261:1
    |
259 |                         )
260 |                     })
261 |         
    | ^^^^^^^^
262 |         # Check for additional columns
263 |         if not allow_additional_columns:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:271:1
    |
269 |                         "message": f"Unexpected column '{col_name}' found"
270 |                     })
271 |         
    | ^^^^^^^^
272 |         is_valid = len([i for i in issues if i.get("severity") != "WARNING"]) == 0
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:273:1
    |
272 |         is_valid = len([i for i in issues if i.get("severity") != "WARNING"]) == 0
273 |         
    | ^^^^^^^^
274 |         result = {
275 |             "valid": is_valid,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:280:1
    |
278 |             "issue_count": len(issues)
279 |         }
280 |         
    | ^^^^^^^^
281 |         if not is_valid:
282 |             logger.error(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:293:1
    |
291 |         else:
292 |             logger.info(f"Schema validation passed for {expected_table}")
293 |         
    | ^^^^^^^^
294 |         return result
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:295:1
    |
294 |         return result
295 |     
    | ^^^^
296 |     def save_schema_version(
297 |         self,
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:304:1
    |
302 |         """
303 |         Save a schema version to disk.
304 |         
    | ^^^^^^^^
305 |         Args:
306 |             table_name: Table name
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:311:1
    |
309 |         """
310 |         self.schema_dir.mkdir(parents=True, exist_ok=True)
311 |         
    | ^^^^^^^^
312 |         schema_file = self.schema_dir / f"{table_name}.{version}.json"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:313:1
    |
312 |         schema_file = self.schema_dir / f"{table_name}.{version}.json"
313 |         
    | ^^^^^^^^
314 |         schema_dict = {
315 |             "table": table_name,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:326:1
    |
324 |             ]
325 |         }
326 |         
    | ^^^^^^^^
327 |         with open(schema_file, 'w') as f:
328 |             json.dump(schema_dict, f, indent=2)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:329:1
    |
327 |         with open(schema_file, 'w') as f:
328 |             json.dump(schema_dict, f, indent=2)
329 |         
    | ^^^^^^^^
330 |         logger.info(f"Saved schema version {version} for {table_name}")
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:340:1
    |
338 |     """
339 |     Convenience function to get schema.
340 |     
    | ^^^^
341 |     Args:
342 |         table_name: Fully qualified table name
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:343:1
    |
341 |     Args:
342 |         table_name: Fully qualified table name
343 |         
    | ^^^^^^^^
344 |     Returns:
345 |         StructType schema
    |
help: Remove whitespace from blank line

F821 Undefined name `pyspark`
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:351:10
    |
350 | def validate_dataframe_schema(
351 |     df: "pyspark.sql.DataFrame",
    |          ^^^^^^^
352 |     expected_table: str,
353 |     fail_on_mismatch: bool = True
    |

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:354:6
    |
352 |     expected_table: str,
353 |     fail_on_mismatch: bool = True
354 | ) -> Dict[str, Any]:
    |      ^^^^
355 |     """
356 |     Validate DataFrame schema against expected schema.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:357:1
    |
355 |     """
356 |     Validate DataFrame schema against expected schema.
357 |     
    | ^^^^
358 |     Args:
359 |         df: DataFrame to validate
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:362:1
    |
360 |         expected_table: Expected table name
361 |         fail_on_mismatch: Whether to raise exception on mismatch
362 |         
    | ^^^^^^^^
363 |     Returns:
364 |         Validation results
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:365:1
    |
363 |     Returns:
364 |         Validation results
365 |         
    | ^^^^^^^^
366 |     Raises:
367 |         RuntimeError: If validation fails and fail_on_mismatch is True
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:370:1
    |
368 |     """
369 |     result = schema_registry.validate_schema(df.schema, expected_table)
370 |     
    | ^^^^
371 |     if not result["valid"] and fail_on_mismatch:
372 |         error_msg = (
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:377:1
    |
375 |         )
376 |         raise RuntimeError(error_msg)
377 |     
    | ^^^^
378 |     return result
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:381:97
    |
381 | def validate_schema_drift(df: Any, expected_schema: StructType, table_name: str = "unknown") -> Dict[str, Any]:
    |                                                                                                 ^^^^
382 |     """
383 |     Validate schema drift for a DataFrame against expected schema.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:384:1
    |
382 |     """
383 |     Validate schema drift for a DataFrame against expected schema.
384 |     
    | ^^^^
385 |     Args:
386 |         df: DataFrame to validate
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:389:1
    |
387 |         expected_schema: Expected schema structure
388 |         table_name: Name of the table for logging
389 |         
    | ^^^^^^^^
390 |     Returns:
391 |         Dictionary with validation results
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:396:1
    |
394 |         actual_schema = df.schema
395 |         issues = []
396 |         
    | ^^^^^^^^
397 |         # Check for missing columns
398 |         expected_fields = {field.name for field in expected_schema.fields}
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:400:1
    |
398 |         expected_fields = {field.name for field in expected_schema.fields}
399 |         actual_fields = {field.name for field in actual_schema.fields}
400 |         
    | ^^^^^^^^
401 |         missing_fields = expected_fields - actual_fields
402 |         extra_fields = actual_fields - expected_fields
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:403:1
    |
401 |         missing_fields = expected_fields - actual_fields
402 |         extra_fields = actual_fields - expected_fields
403 |         
    | ^^^^^^^^
404 |         if missing_fields:
405 |             issues.append(f"Missing columns: {missing_fields}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:408:1
    |
406 |         if extra_fields:
407 |             issues.append(f"Extra columns: {extra_fields}")
408 |             
    | ^^^^^^^^^^^^
409 |         # Check field types for common fields
410 |         common_fields = expected_fields & actual_fields
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:414:1
    |
412 |             expected_field = next(f for f in expected_schema.fields if f.name == field_name)
413 |             actual_field = next(f for f in actual_schema.fields if f.name == field_name)
414 |             
    | ^^^^^^^^^^^^
415 |             if expected_field.dataType != actual_field.dataType:
416 |                 issues.append(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:420:1
    |
418 |                     f"expected {expected_field.dataType}, got {actual_field.dataType}"
419 |                 )
420 |         
    | ^^^^^^^^
421 |         return {
422 |             "valid": len(issues) == 0,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/schemas/production_schemas.py:427:1
    |
425 |             "table_name": table_name
426 |         }
427 |         
    | ^^^^^^^^
428 |     except Exception as e:
429 |         logger.error(f"Schema drift validation failed for {table_name}: {e}")
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:4:1
   |
 2 |   Standard ETL Pipeline with Proper Delta Lake Implementation
 3 |   """
 4 | / import os
 5 | | import sys
 6 | | import pandas as pd
 7 | | import yaml
 8 | | from datetime import datetime
 9 | | from typing import Dict, Any
10 | | import logging
11 | | from logging.config import fileConfig
   | |_____________________________________^
12 |
13 |   # Setup logging
   |
help: Organize imports

F401 [*] `pandas` imported but unused
 --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:6:18
  |
4 | import os
5 | import sys
6 | import pandas as pd
  |                  ^^
7 | import yaml
8 | from datetime import datetime
  |
help: Remove unused import: `pandas`

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:9:1
   |
 7 | import yaml
 8 | from datetime import datetime
 9 | from typing import Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 | import logging
11 | from logging.config import fileConfig
   |

F401 [*] `logging.config.fileConfig` imported but unused
  --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:11:28
   |
 9 | from typing import Dict, Any
10 | import logging
11 | from logging.config import fileConfig
   |                            ^^^^^^^^^^
12 |
13 | # Setup logging
   |
help: Remove unused import: `logging.config.fileConfig`

E402 Module level import not at top of file
  --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:17:1
   |
15 | logger = logging.getLogger(__name__)
16 |
17 | from .delta_lake_standard import StandardDeltaLake, create_standard_delta_tables
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
18 | from .gold_writer import GoldWriter
   |

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:17:1
   |
15 |   logger = logging.getLogger(__name__)
16 |
17 | / from .delta_lake_standard import StandardDeltaLake, create_standard_delta_tables
18 | | from .gold_writer import GoldWriter
   | |___________________________________^
19 |
20 |   class StandardETLPipeline:
   |
help: Organize imports

E402 Module level import not at top of file
  --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:18:1
   |
17 | from .delta_lake_standard import StandardDeltaLake, create_standard_delta_tables
18 | from .gold_writer import GoldWriter
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
19 |
20 | class StandardETLPipeline:
   |

F401 [*] `.gold_writer.GoldWriter` imported but unused
  --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:18:26
   |
17 | from .delta_lake_standard import StandardDeltaLake, create_standard_delta_tables
18 | from .gold_writer import GoldWriter
   |                          ^^^^^^^^^^
19 |
20 | class StandardETLPipeline:
   |
help: Remove unused import: `.gold_writer.GoldWriter`

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:24:1
   |
22 |     Standard ETL Pipeline with proper Delta Lake implementation
23 |     """
24 |     
   | ^^^^
25 |     def __init__(self, config: Dict[str, Any]):
26 |         self.config = config
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:25:32
   |
23 |     """
24 |     
25 |     def __init__(self, config: Dict[str, Any]):
   |                                ^^^^
26 |         self.config = config
27 |         self.delta_path = config.get("delta_path", "data/lakehouse_delta_standard")
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:28:1
   |
26 |         self.config = config
27 |         self.delta_path = config.get("delta_path", "data/lakehouse_delta_standard")
28 |         
   | ^^^^^^^^
29 |     def run_pipeline(self):
30 |         """Run the complete standard ETL pipeline"""
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:33:1
   |
31 |         logger.info("ðŸš€ Starting Standard ETL Pipeline...")
32 |         start_time = datetime.now()
33 |         
   | ^^^^^^^^
34 |         try:
35 |             # Create standard Delta Lake tables
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:37:1
   |
35 |             # Create standard Delta Lake tables
36 |             self.create_standard_tables()
37 |             
   | ^^^^^^^^^^^^
38 |             # Process data layers
39 |             self.process_bronze_layer()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:42:1
   |
40 |             self.process_silver_layer()
41 |             self.process_gold_layer()
42 |             
   | ^^^^^^^^^^^^
43 |             # Demonstrate time travel
44 |             self.demonstrate_time_travel()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:45:1
   |
43 |             # Demonstrate time travel
44 |             self.demonstrate_time_travel()
45 |             
   | ^^^^^^^^^^^^
46 |             duration = datetime.now() - start_time
47 |             logger.info(f"ðŸŽ‰ Pipeline completed successfully in {duration}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:48:1
   |
46 |             duration = datetime.now() - start_time
47 |             logger.info(f"ðŸŽ‰ Pipeline completed successfully in {duration}")
48 |             
   | ^^^^^^^^^^^^
49 |             return True
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:50:1
   |
49 |             return True
50 |             
   | ^^^^^^^^^^^^
51 |         except Exception as e:
52 |             logger.error(f"âŒ Pipeline failed: {e}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:54:1
   |
52 |             logger.error(f"âŒ Pipeline failed: {e}")
53 |             raise
54 |     
   | ^^^^
55 |     def create_standard_tables(self):
56 |         """Create or update standard Delta Lake tables"""
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:58:1
   |
56 |         """Create or update standard Delta Lake tables"""
57 |         logger.info("ðŸ—ï¸ Creating/updating standard Delta Lake tables...")
58 |         
   | ^^^^^^^^
59 |         # Check if tables already exist
60 |         if os.path.exists(self.delta_path):
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:67:1
   |
65 |             # Create standard tables
66 |             create_standard_delta_tables()
67 |         
   | ^^^^^^^^
68 |         logger.info("âœ… Standard Delta Lake tables created/updated")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:69:1
   |
68 |         logger.info("âœ… Standard Delta Lake tables created/updated")
69 |     
   | ^^^^
70 |     def _add_new_versions(self):
71 |         """Add new versions to existing Delta Lake tables"""
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:73:1
   |
71 |         """Add new versions to existing Delta Lake tables"""
72 |         logger.info("ðŸ”„ Adding new versions to existing tables...")
73 |         
   | ^^^^^^^^
74 |         # Get the latest version for each table
75 |         layers = ["bronze", "silver", "gold"]
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:76:1
   |
74 |         # Get the latest version for each table
75 |         layers = ["bronze", "silver", "gold"]
76 |         
   | ^^^^^^^^
77 |         for layer in layers:
78 |             layer_path = os.path.join(self.delta_path, layer)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:84:1
   |
82 |                     if os.path.isdir(table_path):
83 |                         delta_lake = StandardDeltaLake(table_path)
84 |                         
   | ^^^^^^^^^^^^^^^^^^^^^^^^
85 |                         # Get current version
86 |                         table_info = delta_lake.get_table_info()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:89:1
   |
87 |                         current_version = table_info['versions'] - 1
88 |                         new_version = current_version + 1
89 |                         
   | ^^^^^^^^^^^^^^^^^^^^^^^^
90 |                         logger.info(f"ðŸ“Š Adding version {new_version} to {layer}.{table_dir}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:91:1
   |
90 |                         logger.info(f"ðŸ“Š Adding version {new_version} to {layer}.{table_dir}")
91 |                         
   | ^^^^^^^^^^^^^^^^^^^^^^^^
92 |                         # Generate new data for the version
93 |                         if "customers" in table_dir:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:103:1
    |
101 |                         else:
102 |                             continue
103 |                         
    | ^^^^^^^^^^^^^^^^^^^^^^^^
104 |                         # Append new version
105 |                         delta_lake.append_data(new_data, table_dir, new_version)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:106:1
    |
104 |                         # Append new version
105 |                         delta_lake.append_data(new_data, table_dir, new_version)
106 |         
    | ^^^^^^^^
107 |         logger.info("âœ… New versions added successfully")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:108:1
    |
107 |         logger.info("âœ… New versions added successfully")
108 |     
    | ^^^^
109 |     def _generate_customer_data(self):
110 |         """Generate new customer data for versioning"""
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:111:9
    |
109 |       def _generate_customer_data(self):
110 |           """Generate new customer data for versioning"""
111 | /         import pandas as pd
112 | |         import numpy as np
113 | |         from datetime import datetime, timedelta
    | |________________________________________________^
114 |           
115 |           # Generate 1000 new customers
    |
help: Organize imports

F811 [*] Redefinition of unused `pd` from line 6
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:111:26
    |
109 |     def _generate_customer_data(self):
110 |         """Generate new customer data for versioning"""
111 |         import pandas as pd
    |                          ^^
112 |         import numpy as np
113 |         from datetime import datetime, timedelta
    |
help: Remove definition: `pd`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:114:1
    |
112 |         import numpy as np
113 |         from datetime import datetime, timedelta
114 |         
    | ^^^^^^^^
115 |         # Generate 1000 new customers
116 |         n_customers = 1000
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:123:1
    |
121 |         segments = np.random.choice(['Basic', 'Standard', 'Premium'], n_customers, p=[0.4, 0.4, 0.2])
122 |         countries = np.random.choice(['USA', 'Canada', 'UK', 'Germany', 'France'], n_customers)
123 |         
    | ^^^^^^^^
124 |         return pd.DataFrame({
125 |             'customer_id': customer_ids,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:132:1
    |
130 |             'country': countries
131 |         })
132 |     
    | ^^^^
133 |     def _generate_order_data(self):
134 |         """Generate new order data for versioning"""
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:135:9
    |
133 |       def _generate_order_data(self):
134 |           """Generate new order data for versioning"""
135 | /         import pandas as pd
136 | |         import numpy as np
137 | |         from datetime import datetime, timedelta
    | |________________________________________________^
138 |           
139 |           # Generate 5000 new orders
    |
help: Organize imports

F811 [*] Redefinition of unused `pd` from line 6
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:135:26
    |
133 |     def _generate_order_data(self):
134 |         """Generate new order data for versioning"""
135 |         import pandas as pd
    |                          ^^
136 |         import numpy as np
137 |         from datetime import datetime, timedelta
    |
help: Remove definition: `pd`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:138:1
    |
136 |         import numpy as np
137 |         from datetime import datetime, timedelta
138 |         
    | ^^^^^^^^
139 |         # Generate 5000 new orders
140 |         n_orders = 5000
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:148:1
    |
146 |         statuses = np.random.choice(['completed', 'pending', 'cancelled'], n_orders, p=[0.8, 0.15, 0.05])
147 |         currencies = ['USD'] * n_orders
148 |         
    | ^^^^^^^^
149 |         return pd.DataFrame({
150 |             'order_id': order_ids,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:158:1
    |
156 |             'currency': currencies
157 |         })
158 |     
    | ^^^^
159 |     def _generate_customer_analytics(self):
160 |         """Generate new customer analytics data for versioning"""
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:161:9
    |
159 |       def _generate_customer_analytics(self):
160 |           """Generate new customer analytics data for versioning"""
161 | /         import pandas as pd
162 | |         import numpy as np
    | |__________________________^
163 |           
164 |           segments = ['Basic', 'Standard', 'Premium']
    |
help: Organize imports

F811 [*] Redefinition of unused `pd` from line 6
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:161:26
    |
159 |     def _generate_customer_analytics(self):
160 |         """Generate new customer analytics data for versioning"""
161 |         import pandas as pd
    |                          ^^
162 |         import numpy as np
    |
help: Remove definition: `pd`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:163:1
    |
161 |         import pandas as pd
162 |         import numpy as np
163 |         
    | ^^^^^^^^
164 |         segments = ['Basic', 'Standard', 'Premium']
165 |         customer_counts = [np.random.randint(300, 400) for _ in segments]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:167:1
    |
165 |         customer_counts = [np.random.randint(300, 400) for _ in segments]
166 |         avg_lifetime_days = [np.random.uniform(150, 250) for _ in segments]
167 |         
    | ^^^^^^^^
168 |         return pd.DataFrame({
169 |             'segment': segments,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:173:1
    |
171 |             'avg_lifetime_days': avg_lifetime_days
172 |         })
173 |     
    | ^^^^
174 |     def _generate_monthly_revenue(self):
175 |         """Generate new monthly revenue data for versioning"""
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:176:9
    |
174 |       def _generate_monthly_revenue(self):
175 |           """Generate new monthly revenue data for versioning"""
176 | /         import pandas as pd
177 | |         import numpy as np
    | |__________________________^
178 |           
179 |           months = [9, 10, 11, 12]  # Next 4 months
    |
help: Organize imports

F811 [*] Redefinition of unused `pd` from line 6
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:176:26
    |
174 |     def _generate_monthly_revenue(self):
175 |         """Generate new monthly revenue data for versioning"""
176 |         import pandas as pd
    |                          ^^
177 |         import numpy as np
    |
help: Remove definition: `pd`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:178:1
    |
176 |         import pandas as pd
177 |         import numpy as np
178 |         
    | ^^^^^^^^
179 |         months = [9, 10, 11, 12]  # Next 4 months
180 |         total_revenues = [np.random.uniform(400000, 500000) for _ in months]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:181:1
    |
179 |         months = [9, 10, 11, 12]  # Next 4 months
180 |         total_revenues = [np.random.uniform(400000, 500000) for _ in months]
181 |         
    | ^^^^^^^^
182 |         return pd.DataFrame({
183 |             'month': months,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:186:1
    |
184 |             'total_revenue': total_revenues
185 |         })
186 |     
    | ^^^^
187 |     def process_bronze_layer(self):
188 |         """Process bronze layer data"""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:190:1
    |
188 |         """Process bronze layer data"""
189 |         logger.info("ðŸ“Š Processing bronze layer...")
190 |         
    | ^^^^^^^^
191 |         bronze_path = os.path.join(self.delta_path, "bronze")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:192:1
    |
191 |         bronze_path = os.path.join(self.delta_path, "bronze")
192 |         
    | ^^^^^^^^
193 |         # Process each table in bronze layer
194 |         for table_dir in os.listdir(bronze_path):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:198:1
    |
196 |             if os.path.isdir(table_path):
197 |                 logger.info(f"  Processing bronze table: {table_dir}")
198 |                 
    | ^^^^^^^^^^^^^^^^
199 |                 # Get table info
200 |                 delta_lake = StandardDeltaLake(table_path)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:203:1
    |
201 |                 table_info = delta_lake.get_table_info()
202 |                 logger.info(f"    Versions: {table_info['versions']}, Records: {table_info['parquet_files']}")
203 |         
    | ^^^^^^^^
204 |         logger.info("âœ… Bronze layer processed")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:205:1
    |
204 |         logger.info("âœ… Bronze layer processed")
205 |     
    | ^^^^
206 |     def process_silver_layer(self):
207 |         """Process silver layer data"""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:209:1
    |
207 |         """Process silver layer data"""
208 |         logger.info("ðŸ“Š Processing silver layer...")
209 |         
    | ^^^^^^^^
210 |         silver_path = os.path.join(self.delta_path, "silver")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:211:1
    |
210 |         silver_path = os.path.join(self.delta_path, "silver")
211 |         
    | ^^^^^^^^
212 |         # Process each table in silver layer
213 |         for table_dir in os.listdir(silver_path):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:217:1
    |
215 |             if os.path.isdir(table_path):
216 |                 logger.info(f"  Processing silver table: {table_dir}")
217 |                 
    | ^^^^^^^^^^^^^^^^
218 |                 # Get table info
219 |                 delta_lake = StandardDeltaLake(table_path)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:222:1
    |
220 |                 table_info = delta_lake.get_table_info()
221 |                 logger.info(f"    Versions: {table_info['versions']}, Records: {table_info['parquet_files']}")
222 |         
    | ^^^^^^^^
223 |         logger.info("âœ… Silver layer processed")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:224:1
    |
223 |         logger.info("âœ… Silver layer processed")
224 |     
    | ^^^^
225 |     def process_gold_layer(self):
226 |         """Process gold layer data"""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:228:1
    |
226 |         """Process gold layer data"""
227 |         logger.info("ðŸ“Š Processing gold layer...")
228 |         
    | ^^^^^^^^
229 |         gold_path = os.path.join(self.delta_path, "gold")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:230:1
    |
229 |         gold_path = os.path.join(self.delta_path, "gold")
230 |         
    | ^^^^^^^^
231 |         # Process each table in gold layer
232 |         for table_dir in os.listdir(gold_path):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:236:1
    |
234 |             if os.path.isdir(table_path):
235 |                 logger.info(f"  Processing gold table: {table_dir}")
236 |                 
    | ^^^^^^^^^^^^^^^^
237 |                 # Get table info
238 |                 delta_lake = StandardDeltaLake(table_path)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:241:1
    |
239 |                 table_info = delta_lake.get_table_info()
240 |                 logger.info(f"    Versions: {table_info['versions']}, Records: {table_info['parquet_files']}")
241 |         
    | ^^^^^^^^
242 |         logger.info("âœ… Gold layer processed")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:243:1
    |
242 |         logger.info("âœ… Gold layer processed")
243 |     
    | ^^^^
244 |     def demonstrate_time_travel(self):
245 |         """Demonstrate Delta Lake time travel capabilities"""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:247:1
    |
245 |         """Demonstrate Delta Lake time travel capabilities"""
246 |         logger.info("ðŸ• Demonstrating Delta Lake time travel...")
247 |         
    | ^^^^^^^^
248 |         # Show version history for each table
249 |         layers = ["bronze", "silver", "gold"]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:250:1
    |
248 |         # Show version history for each table
249 |         layers = ["bronze", "silver", "gold"]
250 |         
    | ^^^^^^^^
251 |         for layer in layers:
252 |             layer_path = os.path.join(self.delta_path, layer)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:255:1
    |
253 |             if os.path.exists(layer_path):
254 |                 logger.info(f"ðŸ“Š {layer.upper()} LAYER:")
255 |                 
    | ^^^^^^^^^^^^^^^^
256 |                 for table_dir in os.listdir(layer_path):
257 |                     table_path = os.path.join(layer_path, table_dir)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:262:1
    |
260 |                         delta_lake.show_version_history()
261 |                         print()
262 |         
    | ^^^^^^^^
263 |         logger.info("âœ… Time travel demonstration completed")
    |
help: Remove whitespace from blank line

UP015 [*] Unnecessary mode argument
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:272:36
    |
270 |         config_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "..", "config", "local.yaml")
271 |         if os.path.exists(config_path):
272 |             with open(config_path, 'r') as f:
    |                                    ^^^
273 |                 config_data = yaml.safe_load(f)
274 |         else:
    |
help: Remove mode argument

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:280:1
    |
278 |                 "app_name": "standard_etl_pipeline"
279 |             }
280 |         
    | ^^^^^^^^
281 |         # Create and run pipeline
282 |         pipeline = StandardETLPipeline(config_data)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:284:1
    |
282 |         pipeline = StandardETLPipeline(config_data)
283 |         success = pipeline.run_pipeline()
284 |         
    | ^^^^^^^^
285 |         if success:
286 |             print("\nðŸŽ‰ Standard ETL Pipeline completed successfully!")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/standard_etl_pipeline.py:292:1
    |
290 |             print("âŒ Standard ETL Pipeline failed!")
291 |             sys.exit(1)
292 |             
    | ^^^^^^^^^^^^
293 |     except Exception as e:
294 |         print(f"âŒ Pipeline execution failed: {e}")
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/transform/__init__.py:12:5
   |
10 |   # Import base transformer (with optional DQ utils)
11 |   try:
12 | /     from .base_transformer import (
13 | |         BaseTransformer,
14 | |         BronzeToSilverTransformer,
15 | |         SilverToGoldTransformer
16 | |     )
   | |_____^
17 |   except ImportError:
18 |       # If base_transformer has issues, create stubs
   |
help: Organize imports

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/transform/__init__.py:85:5
   |
83 |   def window_function_demo(spark, df, partition_col, order_col):
84 |       """Demo function for window operations."""
85 | /     from pyspark.sql.window import Window
86 | |     from pyspark.sql.functions import row_number
   | |________________________________________________^
87 |       
88 |       window = Window.partitionBy(partition_col).orderBy(order_col)
   |
help: Organize imports

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/__init__.py:87:1
   |
85 |     from pyspark.sql.window import Window
86 |     from pyspark.sql.functions import row_number
87 |     
   | ^^^^
88 |     window = Window.partitionBy(partition_col).orderBy(order_col)
89 |     return df.withColumn("row_number", row_number().over(window))
   |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
   --> src/project_a/pyspark_interview_project/transform/__init__.py:98:24
    |
 96 |     "transform_silver_to_gold",
 97 |     "cleanse_customers",
 98 |     "cleanse_products", 
    |                        ^
 99 |     "cleanse_orders",
100 |     "cleanse_returns",
    |
help: Remove trailing whitespace

W292 [*] No newline at end of file
   --> src/project_a/pyspark_interview_project/transform/__init__.py:110:2
    |
108 |     "broadcast_join_demo",
109 |     "window_function_demo"
110 | ]
    |  ^
    |
help: Add trailing newline

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/transform/base_transformer.py:12:1
   |
10 |   """
11 |
12 | / import logging
13 | | from abc import ABC, abstractmethod
14 | | from typing import Dict, Any, List, Optional
15 | | from pyspark.sql import SparkSession, DataFrame
16 | | from pyspark.sql.functions import trim, col, current_timestamp
   | |______________________________________________________________^
17 |
18 |   # DQ utilities - optional import
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/transform/base_transformer.py:14:1
   |
12 | import logging
13 | from abc import ABC, abstractmethod
14 | from typing import Dict, Any, List, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
15 | from pyspark.sql import SparkSession, DataFrame
16 | from pyspark.sql.functions import trim, col, current_timestamp
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/pyspark_interview_project/transform/base_transformer.py:14:1
   |
12 | import logging
13 | from abc import ABC, abstractmethod
14 | from typing import Dict, Any, List, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
15 | from pyspark.sql import SparkSession, DataFrame
16 | from pyspark.sql.functions import trim, col, current_timestamp
   |

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/base_transformer.py:33:1
   |
31 | class BaseTransformer(ABC):
32 |     """Base class for all transformers."""
33 |     
   | ^^^^
34 |     def __init__(self, source_name: str, target_layer: str, config: Dict[str, Any]):
35 |         """
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/transform/base_transformer.py:34:69
   |
32 |     """Base class for all transformers."""
33 |     
34 |     def __init__(self, source_name: str, target_layer: str, config: Dict[str, Any]):
   |                                                                     ^^^^
35 |         """
36 |         Initialize transformer.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/base_transformer.py:37:1
   |
35 |         """
36 |         Initialize transformer.
37 |         
   | ^^^^^^^^
38 |         Args:
39 |             source_name: Source identifier (e.g., 'behavior', 'orders')
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/base_transformer.py:46:1
   |
44 |         self.target_layer = target_layer
45 |         self.config = config
46 |     
   | ^^^^
47 |     def _trim_string_columns(self, df: DataFrame) -> DataFrame:
48 |         """Trim whitespace from all string columns."""
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/base_transformer.py:53:1
   |
51 |             df = df.withColumn(col_name, trim(col(col_name)))
52 |         return df
53 |     
   | ^^^^
54 |     def _handle_nulls(self, df: DataFrame, fill_value: str = "") -> DataFrame:
55 |         """Fill nulls in string columns."""
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/base_transformer.py:58:1
   |
56 |         string_cols = [f.name for f in df.schema.fields if f.dataType.typeName() == 'string']
57 |         return df.na.fill(fill_value, subset=string_cols)
58 |     
   | ^^^^
59 |     def _add_metadata(self, df: DataFrame, run_id: Optional[str] = None) -> DataFrame:
60 |         """Add processing metadata columns."""
   |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/transform/base_transformer.py:59:52
   |
57 |         return df.na.fill(fill_value, subset=string_cols)
58 |     
59 |     def _add_metadata(self, df: DataFrame, run_id: Optional[str] = None) -> DataFrame:
   |                                                    ^^^^^^^^^^^^^
60 |         """Add processing metadata columns."""
61 |         from pyspark.sql.functions import lit
   |
help: Convert to `X | None`

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/base_transformer.py:66:1
   |
64 |             df = df.withColumn("_run_id", col("_run_id") if "_run_id" in df.columns else lit(run_id))
65 |         return df
66 |     
   | ^^^^
67 |     def _validate_dq(self, df: DataFrame, key_columns: List[str]) -> DataFrame:
68 |         """Apply DQ validations."""
   |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/pyspark_interview_project/transform/base_transformer.py:67:56
   |
65 |         return df
66 |     
67 |     def _validate_dq(self, df: DataFrame, key_columns: List[str]) -> DataFrame:
   |                                                        ^^^^
68 |         """Apply DQ validations."""
69 |         for key_col in key_columns:
   |
help: Replace with `list`

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/base_transformer.py:74:1
   |
72 |                 df = validate_unique(df, key_col)
73 |         return df
74 |     
   | ^^^^
75 |     @abstractmethod
76 |     def transform(self, spark: SparkSession, df: DataFrame, **kwargs) -> DataFrame:
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/base_transformer.py:79:1
   |
77 |         """
78 |         Transform data. Must be implemented by subclasses.
79 |         
   | ^^^^^^^^
80 |         Args:
81 |             spark: SparkSession
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/base_transformer.py:84:1
   |
82 |             df: Input DataFrame
83 |             **kwargs: Additional arguments
84 |             
   | ^^^^^^^^^^^^
85 |         Returns:
86 |             Transformed DataFrame
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/base_transformer.py:89:1
   |
87 |         """
88 |         pass
89 |     
   | ^^^^
90 |     def transform_with_cleanup(self, spark: SparkSession, df: DataFrame, **kwargs) -> DataFrame:
91 |         """
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/base_transformer.py:93:1
   |
91 |         """
92 |         Transform with standard cleanup steps.
93 |         
   | ^^^^^^^^
94 |         Args:
95 |             spark: SparkSession
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/base_transformer.py:98:1
    |
 96 |             df: Input DataFrame
 97 |             **kwargs: Additional arguments
 98 |             
    | ^^^^^^^^^^^^
 99 |         Returns:
100 |             Transformed DataFrame
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/base_transformer.py:105:1
    |
103 |         df = self._trim_string_columns(df)
104 |         df = self._handle_nulls(df)
105 |         
    | ^^^^^^^^
106 |         # Custom transformation
107 |         df = self.transform(spark, df, **kwargs)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/base_transformer.py:108:1
    |
106 |         # Custom transformation
107 |         df = self.transform(spark, df, **kwargs)
108 |         
    | ^^^^^^^^
109 |         # Add metadata
110 |         run_id = kwargs.get('run_id')
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/base_transformer.py:112:1
    |
110 |         run_id = kwargs.get('run_id')
111 |         df = self._add_metadata(df, run_id)
112 |         
    | ^^^^^^^^
113 |         return df
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/base_transformer.py:118:1
    |
116 | class BronzeToSilverTransformer(BaseTransformer):
117 |     """Base transformer for Bronze â†’ Silver transformations."""
118 |     
    | ^^^^
119 |     def __init__(self, source_name: str, config: Dict[str, Any]):
120 |         super().__init__(source_name, "silver", config)
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/transform/base_transformer.py:119:50
    |
117 |     """Base transformer for Bronze â†’ Silver transformations."""
118 |     
119 |     def __init__(self, source_name: str, config: Dict[str, Any]):
    |                                                  ^^^^
120 |         super().__init__(source_name, "silver", config)
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/base_transformer.py:121:1
    |
119 |     def __init__(self, source_name: str, config: Dict[str, Any]):
120 |         super().__init__(source_name, "silver", config)
121 |     
    | ^^^^
122 |     def transform(self, spark: SparkSession, df: DataFrame, **kwargs) -> DataFrame:
123 |         """Transform Bronze to Silver - implement in subclasses."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/base_transformer.py:129:1
    |
127 | class SilverToGoldTransformer(BaseTransformer):
128 |     """Base transformer for Silver â†’ Gold transformations."""
129 |     
    | ^^^^
130 |     def __init__(self, source_name: str, config: Dict[str, Any]):
131 |         super().__init__(source_name, "gold", config)
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/transform/base_transformer.py:130:50
    |
128 |     """Base transformer for Silver â†’ Gold transformations."""
129 |     
130 |     def __init__(self, source_name: str, config: Dict[str, Any]):
    |                                                  ^^^^
131 |         super().__init__(source_name, "gold", config)
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/base_transformer.py:132:1
    |
130 |     def __init__(self, source_name: str, config: Dict[str, Any]):
131 |         super().__init__(source_name, "gold", config)
132 |     
    | ^^^^
133 |     def transform(self, spark: SparkSession, df: DataFrame, **kwargs) -> DataFrame:
134 |         """Transform Silver to Gold - implement in subclasses."""
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:8:1
   |
 6 |   """
 7 |
 8 | / import logging
 9 | | from typing import Dict, Any, Tuple, Optional
10 | | from pyspark.sql import SparkSession, DataFrame
11 | | from pyspark.sql.types import StructType
   | |________________________________________^
12 |
13 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:9:1
   |
 8 | import logging
 9 | from typing import Dict, Any, Tuple, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 | from pyspark.sql import SparkSession, DataFrame
11 | from pyspark.sql.types import StructType
   |

UP035 `typing.Tuple` is deprecated, use `tuple` instead
  --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:9:1
   |
 8 | import logging
 9 | from typing import Dict, Any, Tuple, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 | from pyspark.sql import SparkSession, DataFrame
11 | from pyspark.sql.types import StructType
   |

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:19:13
   |
17 |     spark: SparkSession,
18 |     path: str,
19 |     schema: Optional[StructType] = None,
   |             ^^^^^^^^^^^^^^^^^^^^
20 |     format_hint: Optional[str] = None
21 | ) -> DataFrame:
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:20:18
   |
18 |     path: str,
19 |     schema: Optional[StructType] = None,
20 |     format_hint: Optional[str] = None
   |                  ^^^^^^^^^^^^^
21 | ) -> DataFrame:
22 |     """
   |
help: Convert to `X | None`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:24:1
   |
22 |     """
23 |     Load data with fallback: Delta â†’ Parquet â†’ CSV.
24 |     
   | ^^^^
25 |     Args:
26 |         spark: SparkSession
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:30:1
   |
28 |         schema: Optional schema for CSV reads
29 |         format_hint: Hint for format (e.g., "csv", "json")
30 |         
   | ^^^^^^^^
31 |     Returns:
32 |         DataFrame with data, or empty DataFrame with schema if no data found
   |
help: Remove whitespace from blank line

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
  --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:44:13
   |
42 |             return df
43 |         except Exception:
44 |             raise Exception("Delta table is empty")
   |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
45 |     except Exception as e:
46 |         logger.debug(f"Delta load failed: {e}, trying Parquet...")
   |

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:47:1
   |
45 |     except Exception as e:
46 |         logger.debug(f"Delta load failed: {e}, trying Parquet...")
47 |     
   | ^^^^
48 |     # Try Parquet
49 |     try:
   |
help: Remove whitespace from blank line

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
  --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:57:13
   |
55 |             return df
56 |         except Exception:
57 |             raise Exception("Parquet table is empty")
   |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
58 |     except Exception as e:
59 |         logger.debug(f"Parquet load failed: {e}, trying CSV/JSON...")
   |

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:60:1
   |
58 |     except Exception as e:
59 |         logger.debug(f"Parquet load failed: {e}, trying CSV/JSON...")
60 |     
   | ^^^^
61 |     # Try CSV or JSON
62 |     try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:72:1
   |
70 |             # Schema will be applied later via explicit column selection
71 |             df = spark.read.option("header", "true").option("inferSchema", "true").csv(path)
72 |         
   | ^^^^^^^^
73 |         logger.info(f"Loaded from {format_hint or 'CSV'}: {path}")
74 |         return df
   |
help: Remove whitespace from blank line

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
  --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:81:13
   |
79 |             return spark.createDataFrame([], schema)
80 |         else:
81 |             raise ValueError(f"Could not load data from {path} and no schema provided for empty DataFrame")
   |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   |

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:86:13
   |
84 | def load_crm_bronze_data(
85 |     spark: SparkSession,
86 |     config: Dict[str, Any]
   |             ^^^^
87 | ) -> Tuple[DataFrame, DataFrame, DataFrame]:
88 |     """
   |
help: Replace with `dict`

UP006 [*] Use `tuple` instead of `Tuple` for type annotation
  --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:87:6
   |
85 |     spark: SparkSession,
86 |     config: Dict[str, Any]
87 | ) -> Tuple[DataFrame, DataFrame, DataFrame]:
   |      ^^^^^
88 |     """
89 |     Load CRM bronze data (accounts, contacts, opportunities).
   |
help: Replace with `tuple`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:90:1
   |
88 |     """
89 |     Load CRM bronze data (accounts, contacts, opportunities).
90 |     
   | ^^^^
91 |     Args:
92 |         spark: SparkSession
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:94:1
   |
92 |         spark: SparkSession
93 |         config: Configuration dictionary
94 |         
   | ^^^^^^^^
95 |     Returns:
96 |         Tuple of (accounts_df, contacts_df, opportunities_df)
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:98:5
    |
 96 |           Tuple of (accounts_df, contacts_df, opportunities_df)
 97 |       """
 98 | /     from project_a.utils.path_resolver import resolve_source_file_path
 99 | |     from project_a.schemas.bronze_schemas import (
100 | |         CRM_ACCOUNTS_SCHEMA,
101 | |         CRM_CONTACTS_SCHEMA,
102 | |         CRM_OPPORTUNITIES_SCHEMA
103 | |     )
    | |_____^
104 |       
105 |       logger.info("ðŸ“¥ Loading CRM bronze data...")
    |
help: Organize imports

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:104:1
    |
102 |         CRM_OPPORTUNITIES_SCHEMA
103 |     )
104 |     
    | ^^^^
105 |     logger.info("ðŸ“¥ Loading CRM bronze data...")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:106:1
    |
105 |     logger.info("ðŸ“¥ Loading CRM bronze data...")
106 |     
    | ^^^^
107 |     try:
108 |         accounts_path = resolve_source_file_path(config, "crm", "accounts")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:119:1
    |
117 |         contacts_path = f"{base_path}/{files.get('contacts', 'contacts.csv')}"
118 |         opps_path = f"{base_path}/{files.get('opportunities', 'opportunities.csv')}"
119 |     
    | ^^^^
120 |     df_accounts = load_with_fallback(spark, accounts_path, CRM_ACCOUNTS_SCHEMA)
121 |     df_contacts = load_with_fallback(spark, contacts_path, CRM_CONTACTS_SCHEMA)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:123:1
    |
121 |     df_contacts = load_with_fallback(spark, contacts_path, CRM_CONTACTS_SCHEMA)
122 |     df_opps = load_with_fallback(spark, opps_path, CRM_OPPORTUNITIES_SCHEMA)
123 |     
    | ^^^^
124 |     # Log row counts safely
125 |     try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:131:1
    |
129 |     except Exception as e:
130 |         logger.warning(f"Could not count rows: {e}")
131 |     
    | ^^^^
132 |     return df_accounts, df_contacts, df_opps
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:137:13
    |
135 | def load_snowflake_bronze_data(
136 |     spark: SparkSession,
137 |     config: Dict[str, Any]
    |             ^^^^
138 | ) -> Tuple[DataFrame, DataFrame, DataFrame]:
139 |     """
    |
help: Replace with `dict`

UP006 [*] Use `tuple` instead of `Tuple` for type annotation
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:138:6
    |
136 |     spark: SparkSession,
137 |     config: Dict[str, Any]
138 | ) -> Tuple[DataFrame, DataFrame, DataFrame]:
    |      ^^^^^
139 |     """
140 |     Load Snowflake bronze data (customers, orders, products).
    |
help: Replace with `tuple`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:141:1
    |
139 |     """
140 |     Load Snowflake bronze data (customers, orders, products).
141 |     
    | ^^^^
142 |     Args:
143 |         spark: SparkSession
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:145:1
    |
143 |         spark: SparkSession
144 |         config: Configuration dictionary
145 |         
    | ^^^^^^^^
146 |     Returns:
147 |         Tuple of (customers_df, orders_df, products_df)
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:149:5
    |
147 |           Tuple of (customers_df, orders_df, products_df)
148 |       """
149 | /     from project_a.utils.path_resolver import resolve_source_file_path
150 | |     from project_a.schemas.bronze_schemas import (
151 | |         SNOWFLAKE_CUSTOMERS_SCHEMA,
152 | |         SNOWFLAKE_ORDERS_SCHEMA,
153 | |         SNOWFLAKE_PRODUCTS_SCHEMA
154 | |     )
    | |_____^
155 |       
156 |       logger.info("ðŸ“¥ Loading Snowflake bronze data...")
    |
help: Organize imports

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:155:1
    |
153 |         SNOWFLAKE_PRODUCTS_SCHEMA
154 |     )
155 |     
    | ^^^^
156 |     logger.info("ðŸ“¥ Loading Snowflake bronze data...")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:157:1
    |
156 |     logger.info("ðŸ“¥ Loading Snowflake bronze data...")
157 |     
    | ^^^^
158 |     try:
159 |         customers_path = resolve_source_file_path(config, "snowflake", "customers")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:170:1
    |
168 |         orders_path = f"{base_path}/{files.get('orders', 'snowflake_orders_100000.csv')}"
169 |         products_path = f"{base_path}/{files.get('products', 'snowflake_products_10000.csv')}"
170 |     
    | ^^^^
171 |     df_customers = load_with_fallback(spark, customers_path, SNOWFLAKE_CUSTOMERS_SCHEMA)
172 |     df_orders = load_with_fallback(spark, orders_path, SNOWFLAKE_ORDERS_SCHEMA)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:174:1
    |
172 |     df_orders = load_with_fallback(spark, orders_path, SNOWFLAKE_ORDERS_SCHEMA)
173 |     df_products = load_with_fallback(spark, products_path, SNOWFLAKE_PRODUCTS_SCHEMA)
174 |     
    | ^^^^
175 |     try:
176 |         logger.info(f"  Customers: {df_customers.count():,} rows")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:181:1
    |
179 |     except Exception as e:
180 |         logger.warning(f"Could not count rows: {e}")
181 |     
    | ^^^^
182 |     return df_customers, df_orders, df_products
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:187:13
    |
185 | def load_redshift_behavior_bronze_data(
186 |     spark: SparkSession,
187 |     config: Dict[str, Any]
    |             ^^^^
188 | ) -> DataFrame:
189 |     """
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:191:1
    |
189 |     """
190 |     Load Redshift behavior bronze data.
191 |     
    | ^^^^
192 |     Args:
193 |         spark: SparkSession
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:195:1
    |
193 |         spark: SparkSession
194 |         config: Configuration dictionary
195 |         
    | ^^^^^^^^
196 |     Returns:
197 |         Behavior DataFrame
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:199:5
    |
197 |           Behavior DataFrame
198 |       """
199 | /     from project_a.utils.path_resolver import resolve_source_file_path
200 | |     from project_a.schemas.bronze_schemas import REDSHIFT_BEHAVIOR_SCHEMA
    | |_________________________________________________________________________^
201 |       
202 |       logger.info("ðŸ“¥ Loading Redshift behavior bronze data...")
    |
help: Organize imports

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:201:1
    |
199 |     from project_a.utils.path_resolver import resolve_source_file_path
200 |     from project_a.schemas.bronze_schemas import REDSHIFT_BEHAVIOR_SCHEMA
201 |     
    | ^^^^
202 |     logger.info("ðŸ“¥ Loading Redshift behavior bronze data...")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:203:1
    |
202 |     logger.info("ðŸ“¥ Loading Redshift behavior bronze data...")
203 |     
    | ^^^^
204 |     try:
205 |         behavior_path = resolve_source_file_path(config, "redshift", "behavior")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:212:1
    |
210 |         files = redshift_cfg.get("files", {})
211 |         behavior_path = f"{base_path}/{files.get('behavior', 'redshift_customer_behavior_50000.csv')}"
212 |     
    | ^^^^
213 |     df_behavior = load_with_fallback(spark, behavior_path, REDSHIFT_BEHAVIOR_SCHEMA)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:214:1
    |
213 |     df_behavior = load_with_fallback(spark, behavior_path, REDSHIFT_BEHAVIOR_SCHEMA)
214 |     
    | ^^^^
215 |     try:
216 |         logger.info(f"  Behavior: {df_behavior.count():,} rows")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:219:1
    |
217 |     except Exception as e:
218 |         logger.warning(f"Could not count rows: {e}")
219 |     
    | ^^^^
220 |     return df_behavior
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:225:13
    |
223 | def load_kafka_bronze_data(
224 |     spark: SparkSession,
225 |     config: Dict[str, Any]
    |             ^^^^
226 | ) -> DataFrame:
227 |     """
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:229:1
    |
227 |     """
228 |     Load Kafka bronze data (from CSV seed file or actual Kafka stream).
229 |     
    | ^^^^
230 |     Args:
231 |         spark: SparkSession
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:233:1
    |
231 |         spark: SparkSession
232 |         config: Configuration dictionary
233 |         
    | ^^^^^^^^
234 |     Returns:
235 |         Kafka events DataFrame
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:237:5
    |
235 |           Kafka events DataFrame
236 |       """
237 | /     from project_a.utils.path_resolver import resolve_source_file_path
238 | |     from project_a.schemas.bronze_schemas import KAFKA_EVENTS_SCHEMA
    | |____________________________________________________________________^
239 |       
240 |       logger.info("ðŸ“¥ Loading Kafka bronze data...")
    |
help: Organize imports

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:239:1
    |
237 |     from project_a.utils.path_resolver import resolve_source_file_path
238 |     from project_a.schemas.bronze_schemas import KAFKA_EVENTS_SCHEMA
239 |     
    | ^^^^
240 |     logger.info("ðŸ“¥ Loading Kafka bronze data...")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:241:1
    |
240 |     logger.info("ðŸ“¥ Loading Kafka bronze data...")
241 |     
    | ^^^^
242 |     try:
243 |         kafka_path = resolve_source_file_path(config, "kafka_sim", "orders_seed")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:250:1
    |
248 |         files = kafka_cfg.get("files", {})
249 |         kafka_path = f"{base_path}/{files.get('orders_seed', 'stream_kafka_events_100000.csv')}"
250 |     
    | ^^^^
251 |     df_kafka = load_with_fallback(spark, kafka_path, KAFKA_EVENTS_SCHEMA)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:252:1
    |
251 |     df_kafka = load_with_fallback(spark, kafka_path, KAFKA_EVENTS_SCHEMA)
252 |     
    | ^^^^
253 |     try:
254 |         logger.info(f"  Kafka events: {df_kafka.count():,} rows")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/bronze_loaders.py:257:1
    |
255 |     except Exception as e:
256 |         logger.warning(f"Could not count rows: {e}")
257 |     
    | ^^^^
258 |     return df_kafka
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/transform/gold_builders.py:8:1
   |
 6 |   """
 7 |
 8 | / import logging
 9 | | from typing import Optional
10 | | from pyspark.sql import SparkSession, DataFrame
11 | | from pyspark.sql import functions as F
   | |______________________________________^
12 |
13 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/gold_builders.py:19:1
   |
17 |     """
18 |     Build date dimension table with full calendar range.
19 |     
   | ^^^^
20 |     Generates a date dimension covering the range from min(order_date) to max(order_date)
21 |     in orders_df, with a minimum of 2 years (730 days) if orders_df is empty or has limited dates.
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/gold_builders.py:22:1
   |
20 |     Generates a date dimension covering the range from min(order_date) to max(order_date)
21 |     in orders_df, with a minimum of 2 years (730 days) if orders_df is empty or has limited dates.
22 |     
   | ^^^^
23 |     Args:
24 |         spark: SparkSession
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/gold_builders.py:26:1
   |
24 |         spark: SparkSession
25 |         orders_df: Orders DataFrame with order_date column (used to determine date range)
26 |         
   | ^^^^^^^^
27 |     Returns:
28 |         Date dimension DataFrame with columns: date_sk, date, year, quarter, month, day_of_week, is_weekend
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/gold_builders.py:31:1
   |
29 |     """
30 |     logger.info("ðŸ”§ Building dim_date...")
31 |     
   | ^^^^
32 |     # Determine date range from orders or use default range
33 |     try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/gold_builders.py:40:1
   |
38 |                 date_col = col
39 |                 break
40 |         
   | ^^^^^^^^
41 |         if date_col and orders_df.count() > 0:
42 |             min_max = orders_df.agg(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/gold_builders.py:46:1
   |
44 |                 F.max(F.col(date_col)).alias("max_date")
45 |             ).collect()[0]
46 |             
   | ^^^^^^^^^^^^
47 |             start_date = min_max["min_date"]
48 |             end_date = min_max["max_date"]
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/gold_builders.py:49:1
   |
47 |             start_date = min_max["min_date"]
48 |             end_date = min_max["max_date"]
49 |             
   | ^^^^^^^^^^^^
50 |             # Ensure we have at least 2 years of dates
51 |             from datetime import date, timedelta
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/gold_builders.py:61:1
   |
59 |                 if hasattr(end_date, 'date'):
60 |                     end_date = end_date.date()
61 |                 
   | ^^^^^^^^^^^^^^^^
62 |                 # Extend range to at least 730 days
63 |                 days_diff = (end_date - start_date).days
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/gold_builders.py:79:1
   |
77 |         end_date = date.today()
78 |         start_date = end_date - timedelta(days=730)
79 |     
   | ^^^^
80 |     # Generate date range using Spark SQL
81 |     # Calculate number of days
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/gold_builders.py:90:1
   |
88 |         num_days = 731
89 |         start_date_str = "2023-01-01"
90 |     
   | ^^^^
91 |     # Create date range using Spark range and date_add
92 |     # Cast id to INT because date_add requires INT, not BIGINT
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:107:1
    |
105 |         .orderBy("date")
106 |     )
107 |     
    | ^^^^
108 |     try:
109 |         count = dim_date.count()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:113:1
    |
111 |     except Exception as e:
112 |         logger.warning(f"Could not count dim_date: {e}")
113 |     
    | ^^^^
114 |     return dim_date
    |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:117:60
    |
117 | def build_dim_customer(customers_silver: DataFrame, spark: Optional[SparkSession] = None) -> DataFrame:
    |                                                            ^^^^^^^^^^^^^^^^^^^^^^
118 |     """
119 |     Build customer dimension with SCD-lite support.
    |
help: Convert to `X | None`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:120:1
    |
118 |     """
119 |     Build customer dimension with SCD-lite support.
120 |     
    | ^^^^
121 |     Args:
122 |         customers_silver: Silver customers DataFrame
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:124:1
    |
122 |         customers_silver: Silver customers DataFrame
123 |         spark: Optional SparkSession (extracted from DataFrame if not provided)
124 |         
    | ^^^^^^^^
125 |     Returns:
126 |         Customer dimension DataFrame
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:129:1
    |
127 |     """
128 |     logger.info("ðŸ”§ Building dim_customer...")
129 |     
    | ^^^^
130 |     # Get SparkSession if not provided
131 |     if spark is None:
    |
help: Remove whitespace from blank line

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:141:21
    |
139 |                 spark = SparkSession.getActiveSession()
140 |                 if spark is None:
141 |                     raise ValueError("Could not get SparkSession")
    |                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
142 |     
143 |     # Handle empty DataFrame
    |

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:142:1
    |
140 |                 if spark is None:
141 |                     raise ValueError("Could not get SparkSession")
142 |     
    | ^^^^
143 |     # Handle empty DataFrame
144 |     try:
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:147:13
    |
145 |         if customers_silver.count() == 0:
146 |             logger.warning("customers_silver is empty, creating empty dim_customer")
147 |             from pyspark.sql.types import StructType, StructField, StringType, DateType, DoubleType, BooleanType
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
148 |             schema = StructType([
149 |                 StructField("customer_id", StringType(), True),
    |
help: Organize imports

F401 [*] `pyspark.sql.types.DoubleType` imported but unused
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:147:90
    |
145 |         if customers_silver.count() == 0:
146 |             logger.warning("customers_silver is empty, creating empty dim_customer")
147 |             from pyspark.sql.types import StructType, StructField, StringType, DateType, DoubleType, BooleanType
    |                                                                                          ^^^^^^^^^^
148 |             schema = StructType([
149 |                 StructField("customer_id", StringType(), True),
    |
help: Remove unused import: `pyspark.sql.types.DoubleType`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:162:1
    |
160 |     except Exception:
161 |         pass  # Continue if count fails
162 |     
    | ^^^^
163 |     # Build dimension with required columns
164 |     dim_customer = customers_silver.select(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:175:1
    |
173 |         F.lit(True).alias("is_active")  # Assume all customers are active
174 |     )
175 |     
    | ^^^^
176 |     try:
177 |         logger.info(f"âœ… dim_customer: {dim_customer.count():,} rows")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:180:1
    |
178 |     except Exception as e:
179 |         logger.warning(f"Could not count dim_customer: {e}")
180 |     
    | ^^^^
181 |     return dim_customer
    |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:184:58
    |
184 | def build_dim_product(products_silver: DataFrame, spark: Optional[SparkSession] = None) -> DataFrame:
    |                                                          ^^^^^^^^^^^^^^^^^^^^^^
185 |     """
186 |     Build product dimension.
    |
help: Convert to `X | None`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:187:1
    |
185 |     """
186 |     Build product dimension.
187 |     
    | ^^^^
188 |     Args:
189 |         products_silver: Silver products DataFrame
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:191:1
    |
189 |         products_silver: Silver products DataFrame
190 |         spark: Optional SparkSession
191 |         
    | ^^^^^^^^
192 |     Returns:
193 |         Product dimension DataFrame
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:196:1
    |
194 |     """
195 |     logger.info("ðŸ”§ Building dim_product...")
196 |     
    | ^^^^
197 |     if spark is None:
198 |         try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:202:1
    |
200 |         except Exception:
201 |             spark = products_silver.sparkSession
202 |     
    | ^^^^
203 |     # Handle empty DataFrame
204 |     try:
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:207:13
    |
205 |         if products_silver.count() == 0:
206 |             logger.warning("products_silver is empty, creating empty dim_product")
207 |             from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType, BooleanType
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
208 |             schema = StructType([
209 |                 StructField("product_id", StringType(), True),
    |
help: Organize imports

F401 [*] `pyspark.sql.types.DateType` imported but unused
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:207:92
    |
205 |         if products_silver.count() == 0:
206 |             logger.warning("products_silver is empty, creating empty dim_product")
207 |             from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType, BooleanType
    |                                                                                            ^^^^^^^^
208 |             schema = StructType([
209 |                 StructField("product_id", StringType(), True),
    |
help: Remove unused import: `pyspark.sql.types.DateType`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:219:1
    |
217 |     except Exception:
218 |         pass
219 |     
    | ^^^^
220 |     dim_product = products_silver.select(
221 |         F.col("product_id"),
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:228:1
    |
226 |         F.lit(True).alias("is_active")
227 |     )
228 |     
    | ^^^^
229 |     try:
230 |         logger.info(f"âœ… dim_product: {dim_product.count():,} rows")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:233:1
    |
231 |     except Exception as e:
232 |         logger.warning(f"Could not count dim_product: {e}")
233 |     
    | ^^^^
234 |     return dim_product
    |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:242:12
    |
240 |     dim_product: DataFrame,
241 |     dim_date: DataFrame,
242 |     spark: Optional[SparkSession] = None
    |            ^^^^^^^^^^^^^^^^^^^^^^
243 | ) -> DataFrame:
244 |     """
    |
help: Convert to `X | None`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:246:1
    |
244 |     """
245 |     Build fact_orders table with dimension surrogate keys.
246 |     
    | ^^^^
247 |     Args:
248 |         orders_silver: Silver orders DataFrame
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:253:1
    |
251 |         dim_date: Date dimension
252 |         spark: Optional SparkSession
253 |         
    | ^^^^^^^^
254 |     Returns:
255 |         Fact orders DataFrame
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:258:1
    |
256 |     """
257 |     logger.info("ðŸ”§ Building fact_orders...")
258 |     
    | ^^^^
259 |     if spark is None:
260 |         try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:264:1
    |
262 |         except Exception:
263 |             spark = orders_silver.sparkSession
264 |     
    | ^^^^
265 |     # Handle empty orders
266 |     try:
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:269:13
    |
267 |         if orders_silver.count() == 0:
268 |             logger.warning("orders_silver is empty, creating empty fact_orders")
269 |             from pyspark.sql.types import StructType, StructField, StringType, DateType, DoubleType, IntegerType
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
270 |             schema = StructType([
271 |                 StructField("order_id", StringType(), True),
    |
help: Organize imports

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:282:1
    |
280 |     except Exception:
281 |         pass
282 |     
    | ^^^^
283 |     # Get sales amount (prefer amount_usd, fallback to amount_orig)
284 |     orders = orders_silver.withColumn(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:292:1
    |
290 |         )
291 |     )
292 |     
    | ^^^^
293 |     # Join with dimensions to get surrogate keys
294 |     # Add row numbers as surrogate keys (in production, use proper SCD2 logic)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:301:1
    |
299 |         F.monotonically_increasing_id().cast("long")
300 |     )
301 |     
    | ^^^^
302 |     dim_product_with_sk = dim_product.withColumn(
303 |         "product_sk",
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:306:1
    |
304 |         F.monotonically_increasing_id().cast("long")
305 |     )
306 |     
    | ^^^^
307 |     # Join orders with dimensions
308 |     # NOTE: These are left joins, so missing dimensions will result in -1 surrogate keys.
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:340:1
    |
338 |         "order_date"
339 |     )
340 |     
    | ^^^^
341 |     try:
342 |         logger.info(f"âœ… fact_orders: {fact.count():,} rows")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:345:1
    |
343 |     except Exception as e:
344 |         logger.warning(f"Could not count fact_orders: {e}")
345 |     
    | ^^^^
346 |     return fact
    |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:353:12
    |
351 |     orders_silver: DataFrame,
352 |     behavior_silver: DataFrame,
353 |     spark: Optional[SparkSession] = None
    |            ^^^^^^^^^^^^^^^^^^^^^^
354 | ) -> DataFrame:
355 |     """
    |
help: Convert to `X | None`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:357:1
    |
355 |     """
356 |     Build customer 360 view with aggregated metrics.
357 |     
    | ^^^^
358 |     Args:
359 |         dim_customer: Customer dimension
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:363:1
    |
361 |         behavior_silver: Silver behavior DataFrame
362 |         spark: Optional SparkSession
363 |         
    | ^^^^^^^^
364 |     Returns:
365 |         Customer 360 DataFrame with lifetime metrics
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:368:1
    |
366 |     """
367 |     logger.info("ðŸ”§ Building customer_360...")
368 |     
    | ^^^^
369 |     if spark is None:
370 |         try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:374:1
    |
372 |         except Exception:
373 |             spark = dim_customer.sparkSession
374 |     
    | ^^^^
375 |     # Aggregate orders by customer
376 |     order_agg = orders_silver.groupBy("customer_id").agg(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:382:1
    |
380 |         F.max("order_date").alias("last_order_date")
381 |     )
382 |     
    | ^^^^
383 |     # Aggregate behavior by customer
384 |     behavior_agg = behavior_silver.groupBy("customer_id").agg(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:390:1
    |
388 |         F.sum("revenue").alias("total_behavior_revenue")
389 |     )
390 |     
    | ^^^^
391 |     # Join all sources
392 |     customer_360 = dim_customer.join(order_agg, "customer_id", "left") \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:394:1
    |
392 |     customer_360 = dim_customer.join(order_agg, "customer_id", "left") \
393 |                                .join(behavior_agg, "customer_id", "left")
394 |     
    | ^^^^
395 |     # Fill nulls
396 |     customer_360 = customer_360.fillna({
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:405:1
    |
403 |         "total_behavior_revenue": 0.0
404 |     })
405 |     
    | ^^^^
406 |     try:
407 |         logger.info(f"âœ… customer_360: {customer_360.count():,} rows")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:410:1
    |
408 |     except Exception as e:
409 |         logger.warning(f"Could not count customer_360: {e}")
410 |     
    | ^^^^
411 |     return customer_360
    |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:417:12
    |
415 |     dim_product: DataFrame,
416 |     orders_silver: DataFrame,
417 |     spark: Optional[SparkSession] = None
    |            ^^^^^^^^^^^^^^^^^^^^^^
418 | ) -> DataFrame:
419 |     """
    |
help: Convert to `X | None`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:421:1
    |
419 |     """
420 |     Build product performance analytics table.
421 |     
    | ^^^^
422 |     Args:
423 |         dim_product: Product dimension
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:426:1
    |
424 |         orders_silver: Silver orders DataFrame
425 |         spark: Optional SparkSession
426 |         
    | ^^^^^^^^
427 |     Returns:
428 |         Product performance DataFrame
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:431:1
    |
429 |     """
430 |     logger.info("ðŸ”§ Building product_performance...")
431 |     
    | ^^^^
432 |     if spark is None:
433 |         try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:437:1
    |
435 |         except Exception:
436 |             spark = dim_product.sparkSession
437 |     
    | ^^^^
438 |     # Aggregate orders by product
439 |     product_agg = orders_silver.groupBy("product_id").agg(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:446:1
    |
444 |         F.max("order_date").alias("last_sale_date")
445 |     )
446 |     
    | ^^^^
447 |     # Join with dimension
448 |     product_performance = dim_product.join(product_agg, "product_id", "left")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:449:1
    |
447 |     # Join with dimension
448 |     product_performance = dim_product.join(product_agg, "product_id", "left")
449 |     
    | ^^^^
450 |     # Fill nulls
451 |     product_performance = product_performance.fillna({
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:457:1
    |
455 |         "total_quantity_sold": 0,
456 |     })
457 |     
    | ^^^^
458 |     try:
459 |         logger.info(f"âœ… product_performance: {product_performance.count():,} rows")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/gold_builders.py:462:1
    |
460 |     except Exception as e:
461 |         logger.warning(f"Could not count product_performance: {e}")
462 |     
    | ^^^^
463 |     return product_performance
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/transform/silver_builders.py:8:1
   |
 6 |   """
 7 |
 8 | / import logging
 9 | | from typing import Dict, Any
10 | | from pyspark.sql import SparkSession, DataFrame, Window
11 | | from pyspark.sql import functions as F
   | |______________________________________^
12 |
13 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/transform/silver_builders.py:9:1
   |
 8 | import logging
 9 | from typing import Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 | from pyspark.sql import SparkSession, DataFrame, Window
11 | from pyspark.sql import functions as F
   |

F401 [*] `typing.Dict` imported but unused
  --> src/project_a/pyspark_interview_project/transform/silver_builders.py:9:20
   |
 8 | import logging
 9 | from typing import Dict, Any
   |                    ^^^^
10 | from pyspark.sql import SparkSession, DataFrame, Window
11 | from pyspark.sql import functions as F
   |
help: Remove unused import

F401 [*] `typing.Any` imported but unused
  --> src/project_a/pyspark_interview_project/transform/silver_builders.py:9:26
   |
 8 | import logging
 9 | from typing import Dict, Any
   |                          ^^^
10 | from pyspark.sql import SparkSession, DataFrame, Window
11 | from pyspark.sql import functions as F
   |
help: Remove unused import

F401 [*] `pyspark.sql.SparkSession` imported but unused
  --> src/project_a/pyspark_interview_project/transform/silver_builders.py:10:25
   |
 8 | import logging
 9 | from typing import Dict, Any
10 | from pyspark.sql import SparkSession, DataFrame, Window
   |                         ^^^^^^^^^^^^
11 | from pyspark.sql import functions as F
   |
help: Remove unused import: `pyspark.sql.SparkSession`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/silver_builders.py:25:1
   |
23 |     """
24 |     Build silver customers table with joins and aggregations.
25 |     
   | ^^^^
26 |     Uses Snowflake customers as primary source, enriched with:
27 |     - CRM account information (if available)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/silver_builders.py:32:1
   |
30 |     """
31 |     logger.info("ðŸ”§ Building silver.customers...")
32 |     
   | ^^^^
33 |     # Use Snowflake customers as primary source (has proper CUST-XXXXX IDs)
34 |     # Map Snowflake columns: first_name+last_name -> customer_name, customer_segment -> segment, registration_date -> customer_since
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/silver_builders.py:48:1
   |
46 |         F.coalesce(F.col("lifetime_value"), F.lit(0.0)).alias("annual_revenue")  # Use lifetime_value as proxy
47 |     ).filter(F.col("customer_id").isNotNull())
48 |     
   | ^^^^
49 |     # Optionally enrich with CRM account data (if customer_id matches)
50 |     # Clean accounts - map actual CSV columns to expected schema
   |
help: Remove whitespace from blank line

F841 Local variable `accounts` is assigned to but never used
  --> src/project_a/pyspark_interview_project/transform/silver_builders.py:51:5
   |
49 |     # Optionally enrich with CRM account data (if customer_id matches)
50 |     # Clean accounts - map actual CSV columns to expected schema
51 |     accounts = df_accounts.select(
   |     ^^^^^^^^
52 |         F.col("Id").alias("account_id"),  # Keep as account_id to avoid confusion
53 |         F.trim(F.col("Name")).alias("account_name"),
   |
help: Remove assignment to unused variable `accounts`

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/silver_builders.py:64:1
   |
62 |         F.coalesce(F.col("AnnualRevenue"), F.lit(0.0)).alias("crm_annual_revenue")
63 |     ).filter(F.col("account_id").isNotNull())
64 |     
   | ^^^^
65 |     # Get latest contact per customer
66 |     # Use date column for sorting (safer than timestamp for date-only strings)
   |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
  --> src/project_a/pyspark_interview_project/transform/silver_builders.py:78:44
   |
76 |         )
77 |     ).filter(
78 |         F.col("contact_date").isNotNull() & 
   |                                            ^
79 |         F.col("AccountId").isNotNull()
80 |     ).withColumn(
   |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/transform/silver_builders.py:90:1
   |
88 |         F.col("Phone").alias("primary_phone")
89 |     )
90 |     
   | ^^^^
91 |     # Aggregate behavior metrics - derive from event_name and conversion_value
92 |     behavior_agg = df_behavior.groupBy("customer_id").agg(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/silver_builders.py:99:1
    |
 97 |         F.sum(F.coalesce(F.col("conversion_value"), F.lit(0.0))).alias("total_revenue_24m")
 98 |     )
 99 |     
    | ^^^^
100 |     # Join all sources - start with Snowflake customers as base
101 |     customers = customers_base.join(last_contact, "customer_id", "left") \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/silver_builders.py:103:1
    |
101 |     customers = customers_base.join(last_contact, "customer_id", "left") \
102 |                                .join(behavior_agg, "customer_id", "left")
103 |     
    | ^^^^
104 |     # Null handling
105 |     customers = customers.fillna({
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/silver_builders.py:114:1
    |
112 |         "total_revenue_24m": 0.0
113 |     })
114 |     
    | ^^^^
115 |     try:
116 |         logger.info(f"âœ… silver.customers: {customers.count():,} rows")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/silver_builders.py:119:1
    |
117 |     except Exception as e:
118 |         logger.warning(f"Could not count customers: {e}")
119 |     
    | ^^^^
120 |     return customers
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/silver_builders.py:131:1
    |
129 |     """
130 |     Build silver orders table with FX normalization.
131 |     
    | ^^^^
132 |     Joins orders with customers, products, and FX rates to:
133 |     - Enrich with customer/product information
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/silver_builders.py:137:1
    |
135 |     """
136 |     logger.info("ðŸ”§ Building silver.orders...")
137 |     
    | ^^^^
138 |     # Handle empty DataFrame case
139 |     try:
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
   --> src/project_a/pyspark_interview_project/transform/silver_builders.py:142:13
    |
140 |         if df_orders.count() == 0:
141 |             logger.warning("âš ï¸  df_orders is empty, creating empty DataFrame with expected schema")
142 |             from pyspark.sql.types import StructType, StructField, StringType, DateType, DoubleType, IntegerType
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
143 |             empty_schema = StructType([
144 |                 StructField("order_id", StringType(), True),
    |
help: Organize imports

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/silver_builders.py:158:1
    |
156 |     except Exception:
157 |         pass  # Continue if count fails
158 |     
    | ^^^^
159 |     # Map order columns correctly from CSV
160 |     # CSV structure: order_id, customer_id, product_id, currency, total_amount, quantity, order_date, order_timestamp, status, ...
    |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
   --> src/project_a/pyspark_interview_project/transform/silver_builders.py:188:42
    |
186 |         F.coalesce(F.col("quantity").cast("int"), F.lit(1)).alias("quantity")
187 |     ).filter(
188 |         F.col("order_date").isNotNull() & 
    |                                          ^
189 |         F.col("order_id").isNotNull() &
190 |         F.col("customer_id").isNotNull() &
    |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/silver_builders.py:193:1
    |
191 |         (F.col("amount_orig") > 0.0)  # Filter out zero-amount orders
192 |     )  # Filter out rows with invalid dates, missing keys, or zero amounts
193 |     
    | ^^^^
194 |     # Join with products to get product_name and category
195 |     products = df_products.select(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/silver_builders.py:201:1
    |
199 |         F.col("price_usd").alias("price")
200 |     )
201 |     
    | ^^^^
202 |     orders = orders.join(products, "product_id", "left")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/silver_builders.py:203:1
    |
202 |     orders = orders.join(products, "product_id", "left")
203 |     
    | ^^^^
204 |     # Join with FX rates for currency conversion (if FX data is available)
205 |     # fx_rates_silver has: trade_date, base_ccy, counter_ccy, rate
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/silver_builders.py:211:1
    |
209 |     except Exception:
210 |         fx_count = 0
211 |     
    | ^^^^
212 |     if fx_count > 0:
213 |         # Prepare FX rates: filter for USD conversions and rename for join
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/silver_builders.py:223:1
    |
221 |             F.col("fx_counter_currency") == "USD"  # Only USD conversion rates
222 |         )
223 |         
    | ^^^^^^^^
224 |         # Join orders with FX rates
225 |         # Match: orders.currency = fx_rates.base_ccy AND orders.order_date = fx_rates.trade_date
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/silver_builders.py:271:1
    |
269 |             "quantity"
270 |         )
271 |     
    | ^^^^
272 |     try:
273 |         logger.info(f"âœ… silver.orders: {orders.count():,} rows")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/silver_builders.py:276:1
    |
274 |     except Exception as e:
275 |         logger.warning(f"Could not count orders: {e}")
276 |     
    | ^^^^
277 |     return orders
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/silver_builders.py:283:1
    |
281 |     """Build silver products table."""
282 |     logger.info("ðŸ”§ Building silver.products...")
283 |     
    | ^^^^
284 |     # CSV structure: product_id, product_name, category, price_usd, currency, description, brand, sku, ...
285 |     # Ensure we select by column name to avoid misalignment
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/silver_builders.py:305:1
    |
303 |         (F.col("price_usd") > 0.0)  # Filter out zero-price products
304 |     )
305 |     
    | ^^^^
306 |     try:
307 |         logger.info(f"âœ… silver.products: {products.count():,} rows")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/silver_builders.py:310:1
    |
308 |     except Exception as e:
309 |         logger.warning(f"Could not count products: {e}")
310 |     
    | ^^^^
311 |     return products
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/silver_builders.py:317:1
    |
315 |     """Build silver behavior table with aggregated metrics."""
316 |     logger.info("ðŸ”§ Building silver.customer_behavior...")
317 |     
    | ^^^^
318 |     # Derive metrics from event_name and conversion_value
319 |     behavior = df_behavior.withColumn(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/silver_builders.py:344:1
    |
342 |         F.col("session_duration_minutes") / 60.0
343 |     )
344 |     
    | ^^^^
345 |     try:
346 |         logger.info(f"âœ… silver.customer_behavior: {behavior.count():,} rows")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/transform/silver_builders.py:349:1
    |
347 |     except Exception as e:
348 |         logger.warning(f"Could not count behavior: {e}")
349 |     
    | ^^^^
350 |     return behavior
    |
help: Remove whitespace from blank line

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/unity_catalog.py:15:1
   |
13 | import os
14 | from datetime import datetime
15 | from typing import Any, Dict, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
16 |
17 | from pyspark.sql import DataFrame, SparkSession
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/pyspark_interview_project/unity_catalog.py:15:1
   |
13 | import os
14 | from datetime import datetime
15 | from typing import Any, Dict, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
16 |
17 | from pyspark.sql import DataFrame, SparkSession
   |

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/pyspark_interview_project/unity_catalog.py:118:35
    |
116 |     def create_table(self, catalog_name: str, schema_name: str, table_name: str,
117 |                     df: DataFrame, description: str = "", owner: str = None,
118 |                     partition_by: List[str] = None, properties: Dict[str, str] = None) -> bool:
    |                                   ^^^^
119 |         """
120 |         Create a Unity Catalog table from DataFrame.
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/unity_catalog.py:118:65
    |
116 |     def create_table(self, catalog_name: str, schema_name: str, table_name: str,
117 |                     df: DataFrame, description: str = "", owner: str = None,
118 |                     partition_by: List[str] = None, properties: Dict[str, str] = None) -> bool:
    |                                                                 ^^^^
119 |         """
120 |         Create a Unity Catalog table from DataFrame.
    |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/pyspark_interview_project/unity_catalog.py:165:55
    |
164 |     def grant_permissions(self, catalog_name: str, schema_name: str, table_name: str,
165 |                          principal: str, permissions: List[str], grant_option: bool = False) -> bool:
    |                                                       ^^^^
166 |         """
167 |         Grant permissions on Unity Catalog table.
    |
help: Replace with `list`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/pyspark_interview_project/unity_catalog.py:242:49
    |
241 |     def share_table(self, catalog_name: str, schema_name: str, table_name: str,
242 |                    share_name: str, recipients: List[str], comment: str = "") -> bool:
    |                                                 ^^^^
243 |         """
244 |         Share a table with other workspaces/users.
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/unity_catalog.py:284:90
    |
282 |             return False
283 |
284 |     def get_table_lineage(self, catalog_name: str, schema_name: str, table_name: str) -> Dict[str, Any]:
    |                                                                                          ^^^^
285 |         """
286 |         Get lineage information for a table.
    |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/pyspark_interview_project/unity_catalog.py:368:44
    |
367 |     def create_data_governance_policy(self, policy_name: str, description: str,
368 |                                     rules: List[Dict[str, Any]]) -> bool:
    |                                            ^^^^
369 |         """
370 |         Create a data governance policy.
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/unity_catalog.py:368:49
    |
367 |     def create_data_governance_policy(self, policy_name: str, description: str,
368 |                                     rules: List[Dict[str, Any]]) -> bool:
    |                                                 ^^^^
369 |         """
370 |         Create a data governance policy.
    |
help: Replace with `dict`

F841 Local variable `policy_config` is assigned to but never used
   --> src/project_a/pyspark_interview_project/unity_catalog.py:386:13
    |
385 |             # Create policy using dbutils
386 |             policy_config = {
    |             ^^^^^^^^^^^^^
387 |                 "name": policy_name,
388 |                 "description": description,
    |
help: Remove assignment to unused variable `policy_config`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/unity_catalog.py:447:41
    |
445 |             return False
446 |
447 |     def get_governance_summary(self) -> Dict[str, Any]:
    |                                         ^^^^
448 |         """
449 |         Get summary of governance status across all catalogs.
    |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/pyspark_interview_project/unity_catalog.py:568:32
    |
566 |             return False
567 |
568 |     def list_catalogs(self) -> List[str]:
    |                                ^^^^
569 |         """
570 |         List all available catalogs.
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/unity_catalog.py:583:65
    |
583 | def setup_unity_catalog_governance(spark: SparkSession, config: Dict[str, Any]) -> UnityCatalogManager:
    |                                                                 ^^^^
584 |     """
585 |     Setup Unity Catalog governance for the project.
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/unity_catalog.py:624:59
    |
624 | def migrate_to_unity_catalog(spark: SparkSession, config: Dict[str, Any],
    |                                                           ^^^^
625 |                            uc_manager: UnityCatalogManager) -> bool:
626 |     """
    |
help: Replace with `dict`

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/utils/__init__.py:14:1
   |
12 |   """
13 |
14 | / from .spark_session import build_spark
15 | | from .config import load_conf
16 | | from .logging import setup_json_logging
17 | | from .io import read_delta, write_delta
18 | | from .path_resolver import (
19 | |     resolve_lake_path,
20 | |     bronze_path,
21 | |     silver_path,
22 | |     gold_path
23 | | )
24 | | from .dq_utils import (
25 | |     run_dq_suite,
26 | |     validate_not_null,
27 | |     validate_unique,
28 | |     generate_dq_report
29 | | )
30 | | from .metrics import (
31 | |     emit_metric,
32 | |     track_job_start,
33 | |     track_job_complete,
34 | |     track_records_processed,
35 | |     track_dq_check
36 | | )
   | |_^
37 |
38 |   # Add alias for backward compatibility
   |
help: Organize imports

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/utils/checkpoint.py:7:1
   |
 5 |   for incremental and streaming jobs.
 6 |   """
 7 | / import json
 8 | | import logging
 9 | | from typing import Optional, Any, Dict
10 | | from datetime import datetime
11 | | import boto3
12 | | from botocore.exceptions import ClientError
   | |___________________________________________^
13 |
14 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/utils/checkpoint.py:9:1
   |
 7 | import json
 8 | import logging
 9 | from typing import Optional, Any, Dict
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 | from datetime import datetime
11 | import boto3
   |

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/utils/checkpoint.py:21:6
   |
19 |     key: str,
20 |     region: str = "us-east-1"
21 | ) -> Optional[str]:
   |      ^^^^^^^^^^^^^
22 |     """
23 |     Get last processed batch ID from S3 checkpoint.
   |
help: Convert to `X | None`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/checkpoint.py:24:1
   |
22 |     """
23 |     Get last processed batch ID from S3 checkpoint.
24 |     
   | ^^^^
25 |     Args:
26 |         bucket: S3 bucket name
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/checkpoint.py:29:1
   |
27 |         key: S3 key for checkpoint file
28 |         region: AWS region
29 |         
   | ^^^^^^^^
30 |     Returns:
31 |         Last processed batch ID, or None if not found
   |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/utils/checkpoint.py:51:15
   |
49 |     key: str,
50 |     batch_id: str,
51 |     metadata: Optional[Dict[str, Any]] = None,
   |               ^^^^^^^^^^^^^^^^^^^^^^^^
52 |     region: str = "us-east-1"
53 | ) -> None:
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/utils/checkpoint.py:51:24
   |
49 |     key: str,
50 |     batch_id: str,
51 |     metadata: Optional[Dict[str, Any]] = None,
   |                        ^^^^
52 |     region: str = "us-east-1"
53 | ) -> None:
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/checkpoint.py:56:1
   |
54 |     """
55 |     Set last processed batch ID in S3 checkpoint.
56 |     
   | ^^^^
57 |     Args:
58 |         bucket: S3 bucket name
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/checkpoint.py:66:1
   |
64 |     try:
65 |         s3 = boto3.client("s3", region_name=region)
66 |         
   | ^^^^^^^^
67 |         payload = {
68 |             "batch_id": batch_id,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/checkpoint.py:71:1
   |
69 |             "timestamp_utc": datetime.utcnow().isoformat(),
70 |         }
71 |         
   | ^^^^^^^^
72 |         if metadata:
73 |             payload["metadata"] = metadata
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/checkpoint.py:74:1
   |
72 |         if metadata:
73 |             payload["metadata"] = metadata
74 |         
   | ^^^^^^^^
75 |         s3.put_object(
76 |             Bucket=bucket,
   |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/utils/checkpoint.py:91:6
   |
89 |     key: str,
90 |     region: str = "us-east-1"
91 | ) -> Optional[datetime]:
   |      ^^^^^^^^^^^^^^^^^^
92 |     """
93 |     Get watermark timestamp from checkpoint.
   |
help: Convert to `X | None`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/checkpoint.py:94:1
   |
92 |     """
93 |     Get watermark timestamp from checkpoint.
94 |     
   | ^^^^
95 |     Args:
96 |         bucket: S3 bucket name
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/checkpoint.py:99:1
    |
 97 |         key: S3 key for checkpoint file
 98 |         region: AWS region
 99 |         
    | ^^^^^^^^
100 |     Returns:
101 |         Watermark datetime, or None if not found
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/checkpoint.py:107:1
    |
105 |         obj = s3.get_object(Bucket=bucket, Key=key)
106 |         content = json.loads(obj["Body"].read().decode("utf-8"))
107 |         
    | ^^^^^^^^
108 |         watermark_str = content.get("watermark")
109 |         if watermark_str:
    |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/utils/checkpoint.py:123:15
    |
121 |     key: str,
122 |     watermark: datetime,
123 |     metadata: Optional[Dict[str, Any]] = None,
    |               ^^^^^^^^^^^^^^^^^^^^^^^^
124 |     region: str = "us-east-1"
125 | ) -> None:
    |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/utils/checkpoint.py:123:24
    |
121 |     key: str,
122 |     watermark: datetime,
123 |     metadata: Optional[Dict[str, Any]] = None,
    |                        ^^^^
124 |     region: str = "us-east-1"
125 | ) -> None:
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/checkpoint.py:128:1
    |
126 |     """
127 |     Set watermark timestamp in checkpoint.
128 |     
    | ^^^^
129 |     Args:
130 |         bucket: S3 bucket name
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/checkpoint.py:138:1
    |
136 |     try:
137 |         s3 = boto3.client("s3", region_name=region)
138 |         
    | ^^^^^^^^
139 |         payload = {
140 |             "watermark": watermark.isoformat(),
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/checkpoint.py:143:1
    |
141 |             "timestamp_utc": datetime.utcnow().isoformat(),
142 |         }
143 |         
    | ^^^^^^^^
144 |         if metadata:
145 |             payload["metadata"] = metadata
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/checkpoint.py:146:1
    |
144 |         if metadata:
145 |             payload["metadata"] = metadata
146 |         
    | ^^^^^^^^
147 |         s3.put_object(
148 |             Bucket=bucket,
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/utils/cloudwatch_metrics.py:6:1
   |
 4 |   Emits custom metrics to CloudWatch for EMR job monitoring.
 5 |   """
 6 | / import logging
 7 | | import boto3
 8 | | from typing import Dict, Optional
 9 | | from botocore.exceptions import ClientError
   | |___________________________________________^
10 |
11 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
 --> src/project_a/pyspark_interview_project/utils/cloudwatch_metrics.py:8:1
  |
6 | import logging
7 | import boto3
8 | from typing import Dict, Optional
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
9 | from botocore.exceptions import ClientError
  |

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/utils/cloudwatch_metrics.py:18:17
   |
16 |     metric_name: str,
17 |     value: float,
18 |     dimensions: Optional[Dict[str, str]] = None,
   |                 ^^^^^^^^^^^^^^^^^^^^^^^^
19 |     unit: str = "Count"
20 | ) -> None:
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/utils/cloudwatch_metrics.py:18:26
   |
16 |     metric_name: str,
17 |     value: float,
18 |     dimensions: Optional[Dict[str, str]] = None,
   |                          ^^^^
19 |     unit: str = "Count"
20 | ) -> None:
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/cloudwatch_metrics.py:23:1
   |
21 |     """
22 |     Put a custom metric to CloudWatch.
23 |     
   | ^^^^
24 |     Args:
25 |         namespace: CloudWatch namespace (e.g., "ProjectA/EMR")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/cloudwatch_metrics.py:33:1
   |
31 |     try:
32 |         cw = boto3.client("cloudwatch", region_name="us-east-1")
33 |         
   | ^^^^^^^^
34 |         metric_data = {
35 |             "MetricName": metric_name,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/cloudwatch_metrics.py:39:1
   |
37 |             "Unit": unit,
38 |         }
39 |         
   | ^^^^^^^^
40 |         if dimensions:
41 |             metric_data["Dimensions"] = [
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/cloudwatch_metrics.py:44:1
   |
42 |                 {"Name": k, "Value": v} for k, v in dimensions.items()
43 |             ]
44 |         
   | ^^^^^^^^
45 |         cw.put_metric_data(
46 |             Namespace=namespace,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/cloudwatch_metrics.py:49:1
   |
47 |             MetricData=[metric_data]
48 |         )
49 |         
   | ^^^^^^^^
50 |         logger.debug(f"âœ… Emitted metric: {namespace}/{metric_name}={value}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/cloudwatch_metrics.py:51:1
   |
50 |         logger.debug(f"âœ… Emitted metric: {namespace}/{metric_name}={value}")
51 |         
   | ^^^^^^^^
52 |     except ClientError as e:
53 |         logger.warning(f"âš ï¸  Failed to emit metric {metric_name}: {e}")
   |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/utils/cloudwatch_metrics.py:62:21
   |
60 |     duration_seconds: float,
61 |     env: str = "dev",
62 |     rows_processed: Optional[int] = None
   |                     ^^^^^^^^^^^^^
63 | ) -> None:
64 |     """
   |
help: Convert to `X | None`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/cloudwatch_metrics.py:66:1
   |
64 |     """
65 |     Emit metrics for a successful job run.
66 |     
   | ^^^^
67 |     Args:
68 |         job_name: Name of the job (e.g., "bronze_to_silver")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/cloudwatch_metrics.py:77:1
   |
75 |         "Env": env
76 |     }
77 |     
   | ^^^^
78 |     # Emit success metric
79 |     put_metric(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/cloudwatch_metrics.py:85:1
   |
83 |         dimensions=dimensions
84 |     )
85 |     
   | ^^^^
86 |     # Emit duration metric
87 |     put_metric(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/cloudwatch_metrics.py:94:1
   |
92 |         unit="Seconds"
93 |     )
94 |     
   | ^^^^
95 |     # Emit rows processed if provided
96 |     if rows_processed is not None:
   |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/utils/cloudwatch_metrics.py:109:17
    |
107 |     job_name: str,
108 |     env: str = "dev",
109 |     error_type: Optional[str] = None
    |                 ^^^^^^^^^^^^^
110 | ) -> None:
111 |     """
    |
help: Convert to `X | None`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/cloudwatch_metrics.py:113:1
    |
111 |     """
112 |     Emit metrics for a failed job run.
113 |     
    | ^^^^
114 |     Args:
115 |         job_name: Name of the job
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/cloudwatch_metrics.py:123:1
    |
121 |         "Env": env
122 |     }
123 |     
    | ^^^^
124 |     if error_type:
125 |         dimensions["ErrorType"] = error_type
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/cloudwatch_metrics.py:126:1
    |
124 |     if error_type:
125 |         dimensions["ErrorType"] = error_type
126 |     
    | ^^^^
127 |     # Emit failure metric
128 |     put_metric(
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/cloudwatch_metrics.py:144:1
    |
142 |     """
143 |     Emit data quality check metrics.
144 |     
    | ^^^^
145 |     Args:
146 |         table_name: Name of the table checked
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/cloudwatch_metrics.py:156:1
    |
154 |         "Env": env
155 |     }
156 |     
    | ^^^^
157 |     metric_name = "DQCheckPassed" if passed else "DQCheckFailed"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/cloudwatch_metrics.py:158:1
    |
157 |     metric_name = "DQCheckPassed" if passed else "DQCheckFailed"
158 |     
    | ^^^^
159 |     put_metric(
160 |         namespace="ProjectA/DQ",
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/utils/config.py:5:1
   |
 3 |   """
 4 |
 5 | / import os
 6 | | import yaml
 7 | | import logging
 8 | | from typing import Dict, Any
 9 | | from pathlib import Path
   | |________________________^
10 |
11 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
 --> src/project_a/pyspark_interview_project/utils/config.py:8:1
  |
6 | import yaml
7 | import logging
8 | from typing import Dict, Any
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
9 | from pathlib import Path
  |

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/utils/config.py:14:33
   |
14 | def load_conf(env_path: str) -> Dict[str, Any]:
   |                                 ^^^^
15 |     """
16 |     Load configuration from YAML file and overlay with environment variables.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/config.py:17:1
   |
15 |     """
16 |     Load configuration from YAML file and overlay with environment variables.
17 |     
   | ^^^^
18 |     Args:
19 |         env_path: Path to configuration file
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/config.py:20:1
   |
18 |     Args:
19 |         env_path: Path to configuration file
20 |         
   | ^^^^^^^^
21 |     Returns:
22 |         Resolved configuration dictionary
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/config.py:23:1
   |
21 |     Returns:
22 |         Resolved configuration dictionary
23 |         
   | ^^^^^^^^
24 |     Raises:
25 |         FileNotFoundError: If config file doesn't exist
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/config.py:31:1
   |
29 |     if not config_path.exists():
30 |         raise FileNotFoundError(f"Configuration file not found: {env_path}")
31 |     
   | ^^^^
32 |     # Load YAML configuration
33 |     with open(config_path, 'r') as f:
   |
help: Remove whitespace from blank line

UP015 [*] Unnecessary mode argument
  --> src/project_a/pyspark_interview_project/utils/config.py:33:28
   |
32 |     # Load YAML configuration
33 |     with open(config_path, 'r') as f:
   |                            ^^^
34 |         config = yaml.safe_load(f)
   |
help: Remove mode argument

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/config.py:35:1
   |
33 |     with open(config_path, 'r') as f:
34 |         config = yaml.safe_load(f)
35 |     
   | ^^^^
36 |     # Overlay environment variables
37 |     config = _overlay_env_vars(config)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/config.py:38:1
   |
36 |     # Overlay environment variables
37 |     config = _overlay_env_vars(config)
38 |     
   | ^^^^
39 |     # Validate required keys
40 |     _validate_config(config)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/config.py:41:1
   |
39 |     # Validate required keys
40 |     _validate_config(config)
41 |     
   | ^^^^
42 |     logger.info(f"Configuration loaded from {env_path}")
43 |     return config
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/utils/config.py:46:31
   |
46 | def _overlay_env_vars(config: Dict[str, Any]) -> Dict[str, Any]:
   |                               ^^^^
47 |     """Overlay environment variables on configuration and resolve template variables."""
48 |     def _resolve_vars(obj):
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/utils/config.py:46:50
   |
46 | def _overlay_env_vars(config: Dict[str, Any]) -> Dict[str, Any]:
   |                                                  ^^^^
47 |     """Overlay environment variables on configuration and resolve template variables."""
48 |     def _resolve_vars(obj):
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/config.py:63:1
   |
61 |         else:
62 |             return obj
63 |     
   | ^^^^
64 |     return _resolve_vars(config)
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/utils/config.py:67:53
   |
67 | def _resolve_template_variables(value: str, config: Dict[str, Any]) -> str:
   |                                                     ^^^^
68 |     """Resolve template variables like ${project}-${environment} in string values."""
69 |     import re
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/config.py:70:1
   |
68 |     """Resolve template variables like ${project}-${environment} in string values."""
69 |     import re
70 |     
   | ^^^^
71 |     def replace_var(match):
72 |         var_path = match.group(1)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/config.py:88:1
   |
86 |                 return str(config[var_path])
87 |             return match.group(0)  # Return original if not found
88 |     
   | ^^^^
89 |     return re.sub(r'\$\{([^}]+)\}', replace_var, value)
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/utils/config.py:92:30
   |
92 | def _validate_config(config: Dict[str, Any]) -> None:
   |                              ^^^^
93 |     """Validate required configuration keys."""
94 |     required_keys = [
   |
help: Replace with `dict`

W291 [*] Trailing whitespace
  --> src/project_a/pyspark_interview_project/utils/config.py:97:28
   |
95 |         "env",
96 |         "lake.bronze_path",
97 |         "lake.silver_path", 
   |                            ^
98 |         "lake.gold_path",
99 |         "runtime.shuffle_partitions",
   |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/config.py:102:1
    |
100 |         "runtime.app_name"
101 |     ]
102 |     
    | ^^^^
103 |     missing_keys = []
104 |     for key in required_keys:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/config.py:107:1
    |
105 |         if not _get_nested_value(config, key):
106 |             missing_keys.append(key)
107 |     
    | ^^^^
108 |     if missing_keys:
109 |         raise ValueError(f"Missing required configuration keys: {missing_keys}")
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/utils/config.py:112:31
    |
112 | def _get_nested_value(config: Dict[str, Any], key: str) -> Any:
    |                               ^^^^
113 |     """Get nested value from configuration using dot notation."""
114 |     keys = key.split('.')
    |
help: Replace with `dict`

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/utils/config_loader.py:8:1
   |
 6 |   """
 7 |
 8 | / import os
 9 | | import re
10 | | import logging
11 | | from pathlib import Path
12 | | from typing import Any, Optional, Dict
13 | |
14 | | import yaml
   | |___________^
15 |
16 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/utils/config_loader.py:12:1
   |
10 | import logging
11 | from pathlib import Path
12 | from typing import Any, Optional, Dict
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
13 |
14 | import yaml
   |

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/utils/config_loader.py:31:46
   |
31 | def _resolve_config_vars(value: str, config: Dict[str, Any]) -> str:
   |                                              ^^^^
32 |     """Resolve ${paths.bronze_root} style variable references."""
33 |     def replace_var(match: re.Match) -> str:
   |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/utils/config_loader.py:51:40
   |
51 | def _resolve_value(value: Any, config: Optional[Dict[str, Any]] = None) -> Any:
   |                                        ^^^^^^^^^^^^^^^^^^^^^^^^
52 |     """Resolve environment variables and config references in a value."""
53 |     if not isinstance(value, str):
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/utils/config_loader.py:51:49
   |
51 | def _resolve_value(value: Any, config: Optional[Dict[str, Any]] = None) -> Any:
   |                                                 ^^^^
52 |     """Resolve environment variables and config references in a value."""
53 |     if not isinstance(value, str):
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/config_loader.py:55:1
   |
53 |     if not isinstance(value, str):
54 |         return value
55 |     
   | ^^^^
56 |     # First resolve environment variables
57 |     value = _resolve_env_vars(value)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/config_loader.py:58:1
   |
56 |     # First resolve environment variables
57 |     value = _resolve_env_vars(value)
58 |     
   | ^^^^
59 |     # Then resolve config variable references (requires config to be loaded)
60 |     if config:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/config_loader.py:62:1
   |
60 |     if config:
61 |         value = _resolve_config_vars(value, config)
62 |     
   | ^^^^
63 |     return value
   |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/utils/config_loader.py:66:40
   |
66 | def _resolve_secrets(obj: Any, config: Optional[Dict[str, Any]] = None) -> Any:
   |                                        ^^^^^^^^^^^^^^^^^^^^^^^^
67 |     """Recursively resolve secrets and variables in config."""
68 |     if isinstance(obj, dict):
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/utils/config_loader.py:66:49
   |
66 | def _resolve_secrets(obj: Any, config: Optional[Dict[str, Any]] = None) -> Any:
   |                                                 ^^^^
67 |     """Recursively resolve secrets and variables in config."""
68 |     if isinstance(obj, dict):
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/utils/config_loader.py:81:46
   |
81 | def load_config_from_s3(config_path: str) -> Dict[str, Any]:
   |                                              ^^^^
82 |     """
83 |     Load configuration from S3 using boto3.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/config_loader.py:84:1
   |
82 |     """
83 |     Load configuration from S3 using boto3.
84 |     
   | ^^^^
85 |     Args:
86 |         config_path: S3 path (s3://bucket/key)
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/config_loader.py:87:1
   |
85 |     Args:
86 |         config_path: S3 path (s3://bucket/key)
87 |         
   | ^^^^^^^^
88 |     Returns:
89 |         Configuration dictionary
   |
help: Remove whitespace from blank line

F401 `botocore.exceptions.ClientError` imported but unused; consider using `importlib.util.find_spec` to test for availability
  --> src/project_a/pyspark_interview_project/utils/config_loader.py:93:41
   |
91 |     try:
92 |         import boto3
93 |         from botocore.exceptions import ClientError
   |                                         ^^^^^^^^^^^
94 |         
95 |         s3 = boto3.client("s3")
   |
help: Remove unused import: `botocore.exceptions.ClientError`

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/config_loader.py:94:1
   |
92 |         import boto3
93 |         from botocore.exceptions import ClientError
94 |         
   | ^^^^^^^^
95 |         s3 = boto3.client("s3")
96 |         path_parts = config_path.replace("s3://", "").split("/", 1)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/config_loader.py:99:1
    |
 97 |         bucket = path_parts[0]
 98 |         key = path_parts[1] if len(path_parts) > 1 else ""
 99 |         
    | ^^^^^^^^
100 |         obj = s3.get_object(Bucket=bucket, Key=key)
101 |         content = obj["Body"].read().decode("utf-8")
    |
help: Remove whitespace from blank line

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> src/project_a/pyspark_interview_project/utils/config_loader.py:116:13
    |
114 |             return cfg
115 |         except Exception as e:
116 |             raise ValueError(f"Failed to load config from S3: {e}")
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
117 |     except Exception as e:
118 |         raise ValueError(f"Failed to load config from S3: {e}")
    |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> src/project_a/pyspark_interview_project/utils/config_loader.py:118:9
    |
116 |             raise ValueError(f"Failed to load config from S3: {e}")
117 |     except Exception as e:
118 |         raise ValueError(f"Failed to load config from S3: {e}")
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/utils/config_loader.py:121:39
    |
121 | def load_config_resolved(config_path: Optional[str] = None, env: Optional[str] = None) -> Dict[str, Any]:
    |                                       ^^^^^^^^^^^^^
122 |     """
123 |     Load and resolve configuration from file (local or S3).
    |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/utils/config_loader.py:121:66
    |
121 | def load_config_resolved(config_path: Optional[str] = None, env: Optional[str] = None) -> Dict[str, Any]:
    |                                                                  ^^^^^^^^^^^^^
122 |     """
123 |     Load and resolve configuration from file (local or S3).
    |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/utils/config_loader.py:121:91
    |
121 | def load_config_resolved(config_path: Optional[str] = None, env: Optional[str] = None) -> Dict[str, Any]:
    |                                                                                           ^^^^
122 |     """
123 |     Load and resolve configuration from file (local or S3).
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/config_loader.py:124:1
    |
122 |     """
123 |     Load and resolve configuration from file (local or S3).
124 |     
    | ^^^^
125 |     Args:
126 |         config_path: Path to config file (local or s3://...)
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/config_loader.py:128:1
    |
126 |         config_path: Path to config file (local or s3://...)
127 |         env: Environment name (dev/staging/prod/local)
128 |         
    | ^^^^^^^^
129 |     Returns:
130 |         Resolved configuration dictionary
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/config_loader.py:142:1
    |
140 |         }
141 |         config_path = candidates.get(env, "config/dev.yaml")
142 |     
    | ^^^^
143 |     # Load config
144 |     if config_path.startswith("s3://"):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/config_loader.py:151:1
    |
149 |         if not config_file.exists():
150 |             raise FileNotFoundError(f"Config file not found: {config_path}")
151 |         
    | ^^^^^^^^
152 |         logger.info(f"Loading config from local file: {config_path}")
153 |         with open(config_file) as f:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/config_loader.py:155:1
    |
153 |         with open(config_file) as f:
154 |             cfg = yaml.safe_load(f)
155 |     
    | ^^^^
156 |     # Resolve secrets and variables (two-pass: first load, then resolve vars)
157 |     cfg = _resolve_secrets(cfg, cfg)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/config_loader.py:158:1
    |
156 |     # Resolve secrets and variables (two-pass: first load, then resolve vars)
157 |     cfg = _resolve_secrets(cfg, cfg)
158 |     
    | ^^^^
159 |     return cfg
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/utils/config_loader.py:162:30
    |
162 | def get_config_value(config: Dict[str, Any], key_path: str, default: Any = None) -> Any:
    |                              ^^^^
163 |     """
164 |     Get a config value using dot-notation path.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/config_loader.py:165:1
    |
163 |     """
164 |     Get a config value using dot-notation path.
165 |     
    | ^^^^
166 |     Args:
167 |         config: Configuration dictionary
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/config_loader.py:170:1
    |
168 |         key_path: Dot-notation path (e.g., "paths.bronze_root")
169 |         default: Default value if not found
170 |         
    | ^^^^^^^^
171 |     Returns:
172 |         Config value or default
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/utils/contracts.py:7:1
   |
 5 |   """
 6 |
 7 | / import json
 8 | | import logging
 9 | | from typing import Dict, Any, List, Tuple, Optional
10 | | from pathlib import Path
11 | | from pyspark.sql import DataFrame, SparkSession
12 | | from pyspark.sql import functions as F, types as T
   | |__________________________________________________^
13 |
14 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/utils/contracts.py:9:1
   |
 7 | import json
 8 | import logging
 9 | from typing import Dict, Any, List, Tuple, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 | from pathlib import Path
11 | from pyspark.sql import DataFrame, SparkSession
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/pyspark_interview_project/utils/contracts.py:9:1
   |
 7 | import json
 8 | import logging
 9 | from typing import Dict, Any, List, Tuple, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 | from pathlib import Path
11 | from pyspark.sql import DataFrame, SparkSession
   |

UP035 `typing.Tuple` is deprecated, use `tuple` instead
  --> src/project_a/pyspark_interview_project/utils/contracts.py:9:1
   |
 7 | import json
 8 | import logging
 9 | from typing import Dict, Any, List, Tuple, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 | from pathlib import Path
11 | from pyspark.sql import DataFrame, SparkSession
   |

F401 [*] `pathlib.Path` imported but unused
  --> src/project_a/pyspark_interview_project/utils/contracts.py:10:21
   |
 8 | import logging
 9 | from typing import Dict, Any, List, Tuple, Optional
10 | from pathlib import Path
   |                     ^^^^
11 | from pyspark.sql import DataFrame, SparkSession
12 | from pyspark.sql import functions as F, types as T
   |
help: Remove unused import: `pathlib.Path`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/utils/contracts.py:17:49
   |
17 | def load_schema_contract(contract_path: str) -> Dict[str, Any]:
   |                                                 ^^^^
18 |     """
19 |     Load a schema contract from JSON file.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/contracts.py:20:1
   |
18 |     """
19 |     Load a schema contract from JSON file.
20 |     
   | ^^^^
21 |     Args:
22 |         contract_path: Path to schema contract JSON file
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/contracts.py:23:1
   |
21 |     Args:
22 |         contract_path: Path to schema contract JSON file
23 |         
   | ^^^^^^^^
24 |     Returns:
25 |         Schema contract dictionary
   |
help: Remove whitespace from blank line

UP015 [*] Unnecessary mode argument
  --> src/project_a/pyspark_interview_project/utils/contracts.py:28:34
   |
26 |     """
27 |     try:
28 |         with open(contract_path, 'r') as f:
   |                                  ^^^
29 |             contract = json.load(f)
30 |         logger.info(f"âœ… Loaded schema contract: {contract_path}")
   |
help: Remove mode argument

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/utils/contracts.py:37:39
   |
37 | def contract_to_struct_type(contract: Dict[str, Any]) -> T.StructType:
   |                                       ^^^^
38 |     """
39 |     Convert schema contract to PySpark StructType.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/contracts.py:40:1
   |
38 |     """
39 |     Convert schema contract to PySpark StructType.
40 |     
   | ^^^^
41 |     Args:
42 |         contract: Schema contract dictionary
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/contracts.py:43:1
   |
41 |     Args:
42 |         contract: Schema contract dictionary
43 |         
   | ^^^^^^^^
44 |     Returns:
45 |         PySpark StructType
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/contracts.py:48:1
   |
46 |     """
47 |     fields = []
48 |     
   | ^^^^
49 |     for col_name, col_def in contract.get("columns", {}).items():
50 |         py_type = _py_type_mapping(col_def.get("type", "string"))
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/contracts.py:52:1
   |
50 |         py_type = _py_type_mapping(col_def.get("type", "string"))
51 |         nullable = col_def.get("nullable", True)
52 |         
   | ^^^^^^^^
53 |         fields.append(T.StructField(col_name, py_type, nullable))
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/contracts.py:54:1
   |
53 |         fields.append(T.StructField(col_name, py_type, nullable))
54 |     
   | ^^^^
55 |     return T.StructType(fields)
   |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/pyspark_interview_project/utils/contracts.py:61:20
   |
59 |     df: DataFrame,
60 |     struct: T.StructType,
61 |     required_cols: List[str],
   |                    ^^^^
62 |     extra_cols_to_metadata: bool = True
63 | ) -> Tuple[DataFrame, Dict[str, Any]]:
   |
help: Replace with `list`

UP006 [*] Use `tuple` instead of `Tuple` for type annotation
  --> src/project_a/pyspark_interview_project/utils/contracts.py:63:6
   |
61 |     required_cols: List[str],
62 |     extra_cols_to_metadata: bool = True
63 | ) -> Tuple[DataFrame, Dict[str, Any]]:
   |      ^^^^^
64 |     """
65 |     Align DataFrame to schema contract with validation.
   |
help: Replace with `tuple`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/utils/contracts.py:63:23
   |
61 |     required_cols: List[str],
62 |     extra_cols_to_metadata: bool = True
63 | ) -> Tuple[DataFrame, Dict[str, Any]]:
   |                       ^^^^
64 |     """
65 |     Align DataFrame to schema contract with validation.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/contracts.py:66:1
   |
64 |     """
65 |     Align DataFrame to schema contract with validation.
66 |     
   | ^^^^
67 |     This is the core production function for schema enforcement:
68 |     - Adds missing columns as nulls
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/contracts.py:72:1
   |
70 |     - Moves extra columns to metadata
71 |     - Validates required columns are not null
72 |     
   | ^^^^
73 |     Args:
74 |         df: Input DataFrame
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/contracts.py:78:1
   |
76 |         required_cols: List of required columns (must not be null)
77 |         extra_cols_to_metadata: If True, move extra columns to metadata_extra JSON field
78 |         
   | ^^^^^^^^
79 |     Returns:
80 |         Tuple of (aligned DataFrame, DQ dictionary with null counts)
   |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
  --> src/project_a/pyspark_interview_project/utils/contracts.py:82:17
   |
80 |         Tuple of (aligned DataFrame, DQ dictionary with null counts)
81 |     """
82 |     logger.info(f"Aligning DataFrame to schema contract...")
   |                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
83 |     logger.info(f"  Input columns: {df.columns}")
84 |     logger.info(f"  Expected columns: {struct.fieldNames()}")
   |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/contracts.py:85:1
   |
83 |     logger.info(f"  Input columns: {df.columns}")
84 |     logger.info(f"  Expected columns: {struct.fieldNames()}")
85 |     
   | ^^^^
86 |     # Track DQ issues
87 |     dq = {
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/contracts.py:93:1
   |
91 |         "extra_columns": []
92 |     }
93 |     
   | ^^^^
94 |     # 1. Add missing columns as nulls with correct type
95 |     for field in struct.fields:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/contracts.py:103:1
    |
101 |             dq["columns_added"] += 1
102 |             logger.debug(f"  + Added missing column: {field.name} as {field.dataType}")
103 |     
    | ^^^^
104 |     # 2. Handle extra columns
105 |     extra_cols = set(df.columns) - set(struct.fieldNames())
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/contracts.py:110:1
    |
108 |         logger.warning(f"  Found extra columns: {extra_cols} - moving to metadata_extra")
109 |         dq["extra_columns"] = list(extra_cols)
110 |         
    | ^^^^^^^^
111 |         # Create struct of extra columns
112 |         extra_struct_cols = [F.col(c).alias(c) for c in extra_cols]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/contracts.py:117:1
    |
115 |             F.to_json(F.struct(*extra_struct_cols))
116 |         )
117 |         
    | ^^^^^^^^
118 |         # Drop original extra columns
119 |         df = df.drop(*extra_cols)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/contracts.py:121:1
    |
119 |         df = df.drop(*extra_cols)
120 |         dq["columns_removed"] = len(extra_cols)
121 |     
    | ^^^^
122 |     # 3. Cast all columns to correct types
123 |     for field in struct.fields:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/contracts.py:130:1
    |
128 |             )
129 |             logger.debug(f"  âœ“ Cast {field.name} to {field.dataType}")
130 |     
    | ^^^^
131 |     # 4. Validate required columns are not null
132 |     for col_name in required_cols:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/contracts.py:137:1
    |
135 |             dq["required_null_counts"][col_name] = null_count
136 |             logger.warning(f"  âš ï¸  {col_name} has {null_count} null values (required)")
137 |     
    | ^^^^
138 |     # 5. Reorder columns to match schema
139 |     ordered_cols = [field.name for field in struct.fields]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/contracts.py:143:1
    |
141 |         ordered_cols.append("metadata_extra")
142 |     df = df.select(*ordered_cols)
143 |     
    | ^^^^
144 |     logger.info(f"âœ… Schema alignment complete")
145 |     logger.info(f"  Columns added: {dq['columns_added']}")
    |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
   --> src/project_a/pyspark_interview_project/utils/contracts.py:144:17
    |
142 |     df = df.select(*ordered_cols)
143 |     
144 |     logger.info(f"âœ… Schema alignment complete")
    |                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
145 |     logger.info(f"  Columns added: {dq['columns_added']}")
146 |     logger.info(f"  Columns removed: {dq['columns_removed']}")
    |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/contracts.py:148:1
    |
146 |     logger.info(f"  Columns removed: {dq['columns_removed']}")
147 |     logger.info(f"  Required null counts: {sum(dq['required_null_counts'].values())}")
148 |     
    | ^^^^
149 |     return df, dq
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/utils/contracts.py:155:15
    |
153 |     spark: SparkSession,
154 |     df: DataFrame,
155 |     contract: Dict[str, Any],
    |               ^^^^
156 |     error_lane_path: Optional[str] = None
157 | ) -> Tuple[DataFrame, DataFrame, Dict[str, Any]]:
    |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/utils/contracts.py:156:22
    |
154 |     df: DataFrame,
155 |     contract: Dict[str, Any],
156 |     error_lane_path: Optional[str] = None
    |                      ^^^^^^^^^^^^^
157 | ) -> Tuple[DataFrame, DataFrame, Dict[str, Any]]:
158 |     """
    |
help: Convert to `X | None`

UP006 [*] Use `tuple` instead of `Tuple` for type annotation
   --> src/project_a/pyspark_interview_project/utils/contracts.py:157:6
    |
155 |     contract: Dict[str, Any],
156 |     error_lane_path: Optional[str] = None
157 | ) -> Tuple[DataFrame, DataFrame, Dict[str, Any]]:
    |      ^^^^^
158 |     """
159 |     Validate DataFrame against contract and quarantine bad rows.
    |
help: Replace with `tuple`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/utils/contracts.py:157:34
    |
155 |     contract: Dict[str, Any],
156 |     error_lane_path: Optional[str] = None
157 | ) -> Tuple[DataFrame, DataFrame, Dict[str, Any]]:
    |                                  ^^^^
158 |     """
159 |     Validate DataFrame against contract and quarantine bad rows.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/contracts.py:160:1
    |
158 |     """
159 |     Validate DataFrame against contract and quarantine bad rows.
160 |     
    | ^^^^
161 |     This implements the P0 error lane pattern:
162 |     - Validates against schema
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/contracts.py:166:1
    |
164 |     - Writes bad rows to error lane
165 |     - Returns clean DataFrame
166 |     
    | ^^^^
167 |     Args:
168 |         spark: SparkSession
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/contracts.py:172:1
    |
170 |         contract: Schema contract dictionary
171 |         error_lane_path: Path to write error rows (s3://bucket/_errors/table/dd=YYYY-MM-DD)
172 |         
    | ^^^^^^^^
173 |     Returns:
174 |         Tuple of (clean DataFrame, quarantined DataFrame, validation results)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/contracts.py:177:1
    |
175 |     """
176 |     logger.info("Starting validation and quarantine process...")
177 |     
    | ^^^^
178 |     # Get schema and required columns
179 |     struct = contract_to_struct_type(contract)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/contracts.py:181:1
    |
179 |     struct = contract_to_struct_type(contract)
180 |     required_cols = contract.get("required_columns", [])
181 |     
    | ^^^^
182 |     # Align to schema
183 |     aligned_df, dq = align_to_schema(df, struct, required_cols)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/contracts.py:184:1
    |
182 |     # Align to schema
183 |     aligned_df, dq = align_to_schema(df, struct, required_cols)
184 |     
    | ^^^^
185 |     # Identify bad rows (rows with nulls in required columns)
186 |     bad_rows_mask = None
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/contracts.py:193:1
    |
191 |         else:
192 |             bad_rows_mask = bad_rows_mask | col_mask
193 |     
    | ^^^^
194 |     # Split into clean and bad
195 |     if bad_rows_mask is not None:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/contracts.py:201:1
    |
199 |         clean_df = aligned_df
200 |         quarantined_df = spark.createDataFrame([], aligned_df.schema)
201 |     
    | ^^^^
202 |     quarantine_count = quarantined_df.count()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/contracts.py:203:1
    |
202 |     quarantine_count = quarantined_df.count()
203 |     
    | ^^^^
204 |     # Write quarantined rows to error lane if path provided
205 |     if quarantine_count > 0 and error_lane_path:
    |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
   --> src/project_a/pyspark_interview_project/utils/contracts.py:212:21
    |
210 |             .option("timestampFormat", "yyyy-MM-dd'T'HH:mm:ss'Z'") \
211 |             .save(error_lane_path)
212 |         logger.info(f"âœ… Quarantined rows written to error lane")
    |                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
213 |     
214 |     validation_results = {
    |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/contracts.py:213:1
    |
211 |             .save(error_lane_path)
212 |         logger.info(f"âœ… Quarantined rows written to error lane")
213 |     
    | ^^^^
214 |     validation_results = {
215 |         "total_rows": df.count(),
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/contracts.py:221:1
    |
219 |         "dq_issues": dq
220 |     }
221 |     
    | ^^^^
222 |     logger.info(f"âœ… Validation complete: {validation_results['clean_rows']}/{validation_results['total_rows']} rows passed")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/contracts.py:223:1
    |
222 |     logger.info(f"âœ… Validation complete: {validation_results['clean_rows']}/{validation_results['total_rows']} rows passed")
223 |     
    | ^^^^
224 |     return clean_df, quarantined_df, validation_results
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/contracts.py:230:1
    |
228 |     """
229 |     Map string type to PySpark DataType.
230 |     
    | ^^^^
231 |     Args:
232 |         type_str: Type string (e.g., "string", "decimal(10,2)")
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/contracts.py:233:1
    |
231 |     Args:
232 |         type_str: Type string (e.g., "string", "decimal(10,2)")
233 |         
    | ^^^^^^^^
234 |     Returns:
235 |         PySpark DataType
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/contracts.py:238:1
    |
236 |     """
237 |     type_lower = type_str.lower().strip()
238 |     
    | ^^^^
239 |     # Handle parameterized types
240 |     if type_lower.startswith("decimal") or type_lower.startswith("numeric"):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/contracts.py:248:1
    |
246 |             return T.DecimalType(precision, scale)
247 |         return T.DecimalType(10, 2)  # Default
248 |     
    | ^^^^
249 |     if type_lower.startswith("varchar"):
250 |         import re
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/contracts.py:256:1
    |
254 |             return T.StringType()
255 |         return T.StringType()
256 |     
    | ^^^^
257 |     # Simple type mappings
258 |     type_map = {
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/contracts.py:274:1
    |
272 |         "struct": T.StructType(),
273 |     }
274 |     
    | ^^^^
275 |     return type_map.get(type_lower, T.StringType())
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/contracts.py:286:1
    |
284 |     """
285 |     Add standard run metadata columns to DataFrame.
286 |     
    | ^^^^
287 |     Args:
288 |         df: Input DataFrame
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/contracts.py:292:1
    |
290 |         source_system: Source system name (e.g., 'snowflake', 'redshift')
291 |         run_date: Run date YYYY-MM-DD format
292 |         
    | ^^^^^^^^
293 |     Returns:
294 |         DataFrame with metadata columns added
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/contracts.py:302:1
    |
300 |     needs_source = "_source_system" not in existing_cols
301 |     needs_run_date = "_run_date" not in existing_cols
302 |     
    | ^^^^
303 |     # Add metadata columns
304 |     if needs_ingest_ts:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/contracts.py:312:1
    |
310 |     if needs_run_date:
311 |         df = df.withColumn("_run_date", F.lit(run_date))
312 |     
    | ^^^^
313 |     return df
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/utils/dq_utils.py:8:1
   |
 6 |   """
 7 |
 8 | / import logging
 9 | | from typing import Dict, Any, List
10 | | from pyspark.sql import SparkSession, DataFrame
   | |_______________________________________________^
11 |
12 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/utils/dq_utils.py:9:1
   |
 8 | import logging
 9 | from typing import Dict, Any, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 | from pyspark.sql import SparkSession, DataFrame
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/pyspark_interview_project/utils/dq_utils.py:9:1
   |
 8 | import logging
 9 | from typing import Dict, Any, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 | from pyspark.sql import SparkSession, DataFrame
   |

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/pyspark_interview_project/utils/dq_utils.py:19:19
   |
17 |     df: DataFrame,
18 |     suite_name: str,
19 |     expectations: List[Dict[str, Any]]
   |                   ^^^^
20 | ) -> Dict[str, Any]:
21 |     """
   |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/utils/dq_utils.py:19:24
   |
17 |     df: DataFrame,
18 |     suite_name: str,
19 |     expectations: List[Dict[str, Any]]
   |                        ^^^^
20 | ) -> Dict[str, Any]:
21 |     """
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/utils/dq_utils.py:20:6
   |
18 |     suite_name: str,
19 |     expectations: List[Dict[str, Any]]
20 | ) -> Dict[str, Any]:
   |      ^^^^
21 |     """
22 |     Run a data quality suite on a DataFrame.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/dq_utils.py:23:1
   |
21 |     """
22 |     Run a data quality suite on a DataFrame.
23 |     
   | ^^^^
24 |     Args:
25 |         spark: SparkSession
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/dq_utils.py:29:1
   |
27 |         suite_name: Name of the DQ suite
28 |         expectations: List of expectation dictionaries
29 |         
   | ^^^^^^^^
30 |     Returns:
31 |         Dictionary with validation results
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/dq_utils.py:34:1
   |
32 |     """
33 |     logger.info(f"Running DQ suite: {suite_name}")
34 |     
   | ^^^^
35 |     results = {
36 |         "suite_name": suite_name,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/dq_utils.py:42:1
   |
40 |         "expectations": []
41 |     }
42 |     
   | ^^^^
43 |     for expectation in expectations:
44 |         exp_type = expectation.get("expectation_type")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/dq_utils.py:46:1
   |
44 |         exp_type = expectation.get("expectation_type")
45 |         kwargs = expectation.get("kwargs", {})
46 |         
   | ^^^^^^^^
47 |         try:
48 |             # Placeholder for actual DQ checks
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/dq_utils.py:56:1
   |
54 |             }
55 |             results["expectations"].append(result)
56 |             
   | ^^^^^^^^^^^^
57 |             if passed:
58 |                 results["passed"] += 1
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/dq_utils.py:62:1
   |
60 |                 results["failed"] += 1
61 |                 logger.warning(f"DQ check failed: {exp_type}")
62 |                 
   | ^^^^^^^^^^^^^^^^
63 |         except Exception as e:
64 |             logger.error(f"DQ check error: {exp_type}: {e}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/dq_utils.py:71:1
   |
69 |                 "error": str(e)
70 |             })
71 |     
   | ^^^^
72 |     logger.info(f"DQ suite {suite_name}: {results['passed']}/{results['total_expectations']} passed")
73 |     return results
   |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/pyspark_interview_project/utils/dq_utils.py:82:45
   |
82 | def validate_unique(df: DataFrame, columns: List[str]) -> bool:
   |                                             ^^^^
83 |     """Validate that columns are unique."""
84 |     total_rows = df.count()
   |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/utils/dq_utils.py:89:33
   |
89 | def generate_dq_report(results: Dict[str, Any]) -> str:
   |                                 ^^^^
90 |     """
91 |     Generate a human-readable DQ report.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/dq_utils.py:92:1
   |
90 |     """
91 |     Generate a human-readable DQ report.
92 |     
   | ^^^^
93 |     Args:
94 |         results: DQ validation results
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/dq_utils.py:95:1
   |
93 |     Args:
94 |         results: DQ validation results
95 |         
   | ^^^^^^^^
96 |     Returns:
97 |         Formatted report string
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/dq_utils.py:106:1
    |
104 |     report += f"Failed: {results['failed']}\n"
105 |     report += f"{'='*60}\n\n"
106 |     
    | ^^^^
107 |     for exp in results['expectations']:
108 |         status = "âœ…" if exp.get('passed', False) else "âŒ"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/dq_utils.py:110:1
    |
108 |         status = "âœ…" if exp.get('passed', False) else "âŒ"
109 |         report += f"{status} {exp.get('expectation_type', 'unknown')}\n"
110 |     
    | ^^^^
111 |     return report
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/utils/error_lanes.py:8:1
   |
 6 |   """
 7 |
 8 | / import logging
 9 | | from typing import Tuple, Dict, Any
10 | | from datetime import datetime
11 | | from pyspark.sql import DataFrame, SparkSession
12 | | from pyspark.sql import functions as F
   | |______________________________________^
13 |
14 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Tuple` is deprecated, use `tuple` instead
  --> src/project_a/pyspark_interview_project/utils/error_lanes.py:9:1
   |
 8 | import logging
 9 | from typing import Tuple, Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 | from datetime import datetime
11 | from pyspark.sql import DataFrame, SparkSession
   |

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/utils/error_lanes.py:9:1
   |
 8 | import logging
 9 | from typing import Tuple, Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 | from datetime import datetime
11 | from pyspark.sql import DataFrame, SparkSession
   |

F401 [*] `pyspark.sql.SparkSession` imported but unused
  --> src/project_a/pyspark_interview_project/utils/error_lanes.py:11:36
   |
 9 | from typing import Tuple, Dict, Any
10 | from datetime import datetime
11 | from pyspark.sql import DataFrame, SparkSession
   |                                    ^^^^^^^^^^^^
12 | from pyspark.sql import functions as F
   |
help: Remove unused import: `pyspark.sql.SparkSession`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/error_lanes.py:20:1
   |
18 |     """
19 |     Handler for quarantining bad rows to error lanes.
20 |     
   | ^^^^
21 |     Error lanes follow this pattern:
22 |     s3://bucket/_errors/<layer>/<table>/dt=YYYY-MM-DD/run_id=<uuid>/*.json
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/error_lanes.py:24:1
   |
22 |     s3://bucket/_errors/<layer>/<table>/dt=YYYY-MM-DD/run_id=<uuid>/*.json
23 |     """
24 |     
   | ^^^^
25 |     def __init__(self, lake_root: str):
26 |         """
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/error_lanes.py:28:1
   |
26 |         """
27 |         Initialize error lane handler.
28 |         
   | ^^^^^^^^
29 |         Args:
30 |             lake_root: Root data lake path (e.g., "s3://bucket" or "data/lakehouse_delta")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/error_lanes.py:34:1
   |
32 |         self.lake_root = lake_root.rstrip('/')
33 |         self.errors_prefix = f"{self.lake_root}/_errors"
34 |     
   | ^^^^
35 |     def get_error_lane_path(
36 |         self,
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/error_lanes.py:44:1
   |
42 |         """
43 |         Generate error lane path for quarantined rows.
44 |         
   | ^^^^^^^^
45 |         Args:
46 |             layer: Data layer (bronze, silver, gold)
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/error_lanes.py:50:1
   |
48 |             run_id: Unique run identifier
49 |             partition_date: Date partition (YYYY-MM-DD), defaults to today
50 |             
   | ^^^^^^^^^^^^
51 |         Returns:
52 |             Error lane path
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/error_lanes.py:55:1
   |
53 |         """
54 |         from datetime import datetime
55 |         
   | ^^^^^^^^
56 |         if partition_date is None:
57 |             partition_date = datetime.now().strftime("%Y-%m-%d")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/error_lanes.py:58:1
   |
56 |         if partition_date is None:
57 |             partition_date = datetime.now().strftime("%Y-%m-%d")
58 |         
   | ^^^^^^^^
59 |         path = f"{self.errors_prefix}/{layer}/{table}/dt={partition_date}/run_id={run_id}"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/error_lanes.py:60:1
   |
59 |         path = f"{self.errors_prefix}/{layer}/{table}/dt={partition_date}/run_id={run_id}"
60 |         
   | ^^^^^^^^
61 |         logger.debug(f"Error lane path: {path}")
62 |         return path
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/error_lanes.py:63:1
   |
61 |         logger.debug(f"Error lane path: {path}")
62 |         return path
63 |     
   | ^^^^
64 |     def quarantine_bad_rows(
65 |         self,
   |
help: Remove whitespace from blank line

UP006 [*] Use `tuple` instead of `Tuple` for type annotation
  --> src/project_a/pyspark_interview_project/utils/error_lanes.py:71:10
   |
69 |         table: str,
70 |         run_id: str
71 |     ) -> Tuple[DataFrame, Dict[str, Any]]:
   |          ^^^^^
72 |         """
73 |         Quarantine bad rows to error lane.
   |
help: Replace with `tuple`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/utils/error_lanes.py:71:27
   |
69 |         table: str,
70 |         run_id: str
71 |     ) -> Tuple[DataFrame, Dict[str, Any]]:
   |                           ^^^^
72 |         """
73 |         Quarantine bad rows to error lane.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/error_lanes.py:74:1
   |
72 |         """
73 |         Quarantine bad rows to error lane.
74 |         
   | ^^^^^^^^
75 |         Args:
76 |             df: Input DataFrame
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/error_lanes.py:81:1
   |
79 |             table: Table name
80 |             run_id: Unique run identifier
81 |             
   | ^^^^^^^^^^^^
82 |         Returns:
83 |             Tuple of (clean DataFrame, quarantine stats)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/error_lanes.py:88:1
   |
86 |             logger.info("No data to quarantine")
87 |             return df, {"total_rows": 0, "quarantined": 0, "clean_rows": 0}
88 |         
   | ^^^^^^^^
89 |         # Get error lane path
90 |         error_path = self.get_error_lane_path(layer, table, run_id)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/error_lanes.py:91:1
   |
89 |         # Get error lane path
90 |         error_path = self.get_error_lane_path(layer, table, run_id)
91 |         
   | ^^^^^^^^
92 |         # Split into clean and bad rows
93 |         # Bad rows have validation errors
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/error_lanes.py:97:1
   |
95 |             # Extract error details
96 |             bad_row_ids = validation_errors.select("row_id").distinct()
97 |             
   | ^^^^^^^^^^^^
98 |             # Join to get bad rows
99 |             quarantined_df = df.join(bad_row_ids, on="row_id", how="inner")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/error_lanes.py:101:1
    |
 99 |             quarantined_df = df.join(bad_row_ids, on="row_id", how="inner")
100 |             clean_df = df.join(bad_row_ids, on="row_id", how="left_anti")
101 |             
    | ^^^^^^^^^^^^
102 |             # Add error metadata
103 |             quarantined_df = quarantined_df \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/error_lanes.py:106:1
    |
104 |                 .withColumn("_error_timestamp", F.current_timestamp()) \
105 |                 .withColumn("_error_lane_path", F.lit(error_path))
106 |             
    | ^^^^^^^^^^^^
107 |             # Write to error lane
108 |             logger.warning(f"âš ï¸  Quarantining {quarantined_df.count()} bad rows to {error_path}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/error_lanes.py:109:1
    |
107 |             # Write to error lane
108 |             logger.warning(f"âš ï¸  Quarantining {quarantined_df.count()} bad rows to {error_path}")
109 |             
    | ^^^^^^^^^^^^
110 |             quarantined_df \
111 |                 .write \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/error_lanes.py:116:1
    |
114 |                 .option("timestampFormat", "yyyy-MM-dd'T'HH:mm:ss'Z'") \
115 |                 .save(error_path)
116 |             
    | ^^^^^^^^^^^^
117 |             logger.info(f"âœ… Bad rows written to error lane")
118 |         else:
    |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
   --> src/project_a/pyspark_interview_project/utils/error_lanes.py:117:25
    |
115 |                 .save(error_path)
116 |             
117 |             logger.info(f"âœ… Bad rows written to error lane")
    |                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
118 |         else:
119 |             clean_df = df
    |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/error_lanes.py:121:1
    |
119 |             clean_df = df
120 |             quarantined_df = df.filter(F.lit(False))  # Empty DataFrame
121 |         
    | ^^^^^^^^
122 |         # Stats
123 |         stats = {
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/error_lanes.py:129:1
    |
127 |             "error_lane_path": error_path
128 |         }
129 |         
    | ^^^^^^^^
130 |         logger.info(f"âœ… Quarantine complete: {stats['clean_rows']}/{stats['total_rows']} rows passed")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/error_lanes.py:131:1
    |
130 |         logger.info(f"âœ… Quarantine complete: {stats['clean_rows']}/{stats['total_rows']} rows passed")
131 |         
    | ^^^^^^^^
132 |         return clean_df, stats
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/error_lanes.py:133:1
    |
132 |         return clean_df, stats
133 |     
    | ^^^^
134 |     def quarantine_schema_violations(
135 |         self,
    |
help: Remove whitespace from blank line

UP006 [*] Use `tuple` instead of `Tuple` for type annotation
   --> src/project_a/pyspark_interview_project/utils/error_lanes.py:142:10
    |
140 |         required_cols: list,
141 |         date_col: str = None
142 |     ) -> Tuple[DataFrame, Dict[str, Any]]:
    |          ^^^^^
143 |         """
144 |         Quarantine rows with null values in required columns.
    |
help: Replace with `tuple`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/utils/error_lanes.py:142:27
    |
140 |         required_cols: list,
141 |         date_col: str = None
142 |     ) -> Tuple[DataFrame, Dict[str, Any]]:
    |                           ^^^^
143 |         """
144 |         Quarantine rows with null values in required columns.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/error_lanes.py:145:1
    |
143 |         """
144 |         Quarantine rows with null values in required columns.
145 |         
    | ^^^^^^^^
146 |         Args:
147 |             df: Input DataFrame
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/error_lanes.py:153:1
    |
151 |             required_cols: List of required column names
152 |             date_col: Optional date column for partitioning
153 |             
    | ^^^^^^^^^^^^
154 |         Returns:
155 |             Tuple of (clean DataFrame, quarantine stats)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/error_lanes.py:166:1
    |
164 |                 else:
165 |                     bad_rows_mask = bad_rows_mask | col_mask
166 |         
    | ^^^^^^^^
167 |         # Split clean vs bad
168 |         if bad_rows_mask is not None:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/error_lanes.py:174:1
    |
172 |             clean_df = df
173 |             quarantined_df = df.filter(F.lit(False))
174 |         
    | ^^^^^^^^
175 |         # Write quarantined rows
176 |         quarantine_count = quarantined_df.count()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/error_lanes.py:179:1
    |
177 |         if quarantine_count > 0:
178 |             error_path = self.get_error_lane_path(layer, table, run_id)
179 |             
    | ^^^^^^^^^^^^
180 |             logger.warning(f"âš ï¸  Quarantining {quarantine_count} schema violations")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/error_lanes.py:181:1
    |
180 |             logger.warning(f"âš ï¸  Quarantining {quarantine_count} schema violations")
181 |             
    | ^^^^^^^^^^^^
182 |             quarantined_df \
183 |                 .withColumn("_error_timestamp", F.current_timestamp()) \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/error_lanes.py:189:1
    |
187 |                 .mode("append") \
188 |                 .save(error_path)
189 |         
    | ^^^^^^^^
190 |         stats = {
191 |             "total_rows": df.count(),
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/error_lanes.py:196:1
    |
194 |             "error_type": "schema_violation"
195 |         }
196 |         
    | ^^^^^^^^
197 |         return clean_df, stats
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/error_lanes.py:203:1
    |
201 |     """
202 |     Add a unique row ID to DataFrame for error tracking.
203 |     
    | ^^^^
204 |     Args:
205 |         df: Input DataFrame
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/error_lanes.py:206:1
    |
204 |     Args:
205 |         df: Input DataFrame
206 |         
    | ^^^^^^^^
207 |     Returns:
208 |         DataFrame with row_id column
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
   --> src/project_a/pyspark_interview_project/utils/error_lanes.py:210:5
    |
208 |           DataFrame with row_id column
209 |       """
210 | /     from pyspark.sql import functions as F
211 | |     import uuid
    | |_______________^
212 |       
213 |       return df.withColumn(
    |
help: Organize imports

F401 [*] `uuid` imported but unused
   --> src/project_a/pyspark_interview_project/utils/error_lanes.py:211:12
    |
209 |     """
210 |     from pyspark.sql import functions as F
211 |     import uuid
    |            ^^^^
212 |     
213 |     return df.withColumn(
    |
help: Remove unused import: `uuid`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/error_lanes.py:212:1
    |
210 |     from pyspark.sql import functions as F
211 |     import uuid
212 |     
    | ^^^^
213 |     return df.withColumn(
214 |         "row_id",
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/utils/error_lanes.py:224:20
    |
222 |     run_id: str,
223 |     error_type: str,
224 |     error_details: Dict[str, Any]
    |                    ^^^^
225 | ) -> Dict[str, Any]:
226 |     """
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/utils/error_lanes.py:225:6
    |
223 |     error_type: str,
224 |     error_details: Dict[str, Any]
225 | ) -> Dict[str, Any]:
    |      ^^^^
226 |     """
227 |     Create error lane metadata document.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/error_lanes.py:228:1
    |
226 |     """
227 |     Create error lane metadata document.
228 |     
    | ^^^^
229 |     Args:
230 |         layer: Data layer
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/error_lanes.py:235:1
    |
233 |         error_type: Type of error
234 |         error_details: Error details
235 |         
    | ^^^^^^^^
236 |     Returns:
237 |         Error metadata dictionary
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/utils/io.py:5:1
   |
 3 |   """
 4 |
 5 | / import logging
 6 | | import os
 7 | | from pathlib import Path
 8 | | from typing import Optional, List
 9 | | from pyspark.sql import DataFrame, SparkSession
   | |_______________________________________________^
10 |
11 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.List` is deprecated, use `list` instead
 --> src/project_a/pyspark_interview_project/utils/io.py:8:1
  |
6 | import os
7 | from pathlib import Path
8 | from typing import Optional, List
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
9 | from pyspark.sql import DataFrame, SparkSession
  |

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/io.py:17:1
   |
15 |     """
16 |     Read Delta table with error handling and retry logic.
17 |     
   | ^^^^
18 |     Args:
19 |         spark: Spark session
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/io.py:21:1
   |
19 |         spark: Spark session
20 |         path: Path to Delta table
21 |         
   | ^^^^^^^^
22 |     Returns:
23 |         DataFrame from Delta table
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/io.py:24:1
   |
22 |     Returns:
23 |         DataFrame from Delta table
24 |         
   | ^^^^^^^^
25 |     Raises:
26 |         Exception: If read fails after retries
   |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
  --> src/project_a/pyspark_interview_project/utils/io.py:45:19
   |
44 | def write_delta(
45 |     df: DataFrame, 
   |                   ^
46 |     path: str, 
47 |     mode: str = "overwrite", 
   |
help: Remove trailing whitespace

W291 [*] Trailing whitespace
  --> src/project_a/pyspark_interview_project/utils/io.py:46:15
   |
44 | def write_delta(
45 |     df: DataFrame, 
46 |     path: str, 
   |               ^
47 |     mode: str = "overwrite", 
48 |     partitionBy: Optional[List[str]] = None
   |
help: Remove trailing whitespace

W291 [*] Trailing whitespace
  --> src/project_a/pyspark_interview_project/utils/io.py:47:29
   |
45 |     df: DataFrame, 
46 |     path: str, 
47 |     mode: str = "overwrite", 
   |                             ^
48 |     partitionBy: Optional[List[str]] = None
49 | ) -> None:
   |
help: Remove trailing whitespace

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/utils/io.py:48:18
   |
46 |     path: str, 
47 |     mode: str = "overwrite", 
48 |     partitionBy: Optional[List[str]] = None
   |                  ^^^^^^^^^^^^^^^^^^^
49 | ) -> None:
50 |     """
   |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/pyspark_interview_project/utils/io.py:48:27
   |
46 |     path: str, 
47 |     mode: str = "overwrite", 
48 |     partitionBy: Optional[List[str]] = None
   |                           ^^^^
49 | ) -> None:
50 |     """
   |
help: Replace with `list`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/io.py:52:1
   |
50 |     """
51 |     Write DataFrame to Delta table with idempotency and retry logic.
52 |     
   | ^^^^
53 |     Args:
54 |         df: DataFrame to write
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/io.py:58:1
   |
56 |         mode: Write mode (overwrite, append, etc.)
57 |         partitionBy: Columns to partition by
58 |         
   | ^^^^^^^^
59 |     Raises:
60 |         Exception: If write fails after retries
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/io.py:65:1
   |
63 |         # Normalize path for local filesystem
64 |         normalized_path = _normalize_path(path)
65 |         
   | ^^^^^^^^
66 |         # Ensure parent directories exist for local paths
67 |         if normalized_path.startswith("file://"):
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/io.py:70:1
   |
68 |             parent_dir = Path(normalized_path[7:]).parent
69 |             parent_dir.mkdir(parents=True, exist_ok=True)
70 |         
   | ^^^^^^^^
71 |         logger.info(f"Writing DataFrame to Delta table: {normalized_path} (mode: {mode})")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/io.py:72:1
   |
71 |         logger.info(f"Writing DataFrame to Delta table: {normalized_path} (mode: {mode})")
72 |         
   | ^^^^^^^^
73 |         writer = df.write.format("delta").mode(mode)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/io.py:74:1
   |
73 |         writer = df.write.format("delta").mode(mode)
74 |         
   | ^^^^^^^^
75 |         if partitionBy:
76 |             writer = writer.partitionBy(*partitionBy)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/io.py:78:1
   |
76 |             writer = writer.partitionBy(*partitionBy)
77 |             logger.info(f"Partitioning by: {partitionBy}")
78 |         
   | ^^^^^^^^
79 |         writer.save(normalized_path)
80 |         logger.info(f"Successfully wrote Delta table: {normalized_path}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/io.py:81:1
   |
79 |         writer.save(normalized_path)
80 |         logger.info(f"Successfully wrote Delta table: {normalized_path}")
81 |         
   | ^^^^^^^^
82 |     except Exception as e:
83 |         logger.warning(f"Delta write failed, trying Parquet: {e}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/io.py:87:1
   |
85 |             # Fallback to Parquet
86 |             writer = df.write.format("parquet").mode(mode)
87 |             
   | ^^^^^^^^^^^^
88 |             if partitionBy:
89 |                 writer = writer.partitionBy(*partitionBy)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/io.py:90:1
   |
88 |             if partitionBy:
89 |                 writer = writer.partitionBy(*partitionBy)
90 |             
   | ^^^^^^^^^^^^
91 |             writer.save(normalized_path)
92 |             logger.info(f"Successfully wrote Parquet table: {normalized_path}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/io.py:93:1
   |
91 |             writer.save(normalized_path)
92 |             logger.info(f"Successfully wrote Parquet table: {normalized_path}")
93 |             
   | ^^^^^^^^^^^^
94 |         except Exception as e2:
95 |             logger.error(f"Failed to write table {normalized_path}: {e2}")
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/io.py:102:1
    |
100 |     """
101 |     Normalize file path for Spark.
102 |     
    | ^^^^
103 |     Args:
104 |         path: Input path
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/io.py:105:1
    |
103 |     Args:
104 |         path: Input path
105 |         
    | ^^^^^^^^
106 |     Returns:
107 |         Normalized path
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/io.py:124:1
    |
122 |     """
123 |     Ensure directory exists for local file paths.
124 |     
    | ^^^^
125 |     Args:
126 |         path: File path
    |
help: Remove whitespace from blank line

W292 [*] No newline at end of file
   --> src/project_a/pyspark_interview_project/utils/io.py:131:62
    |
129 |         dir_path = Path(path[7:])
130 |         dir_path.mkdir(parents=True, exist_ok=True)
131 |         logger.debug(f"Ensured directory exists: {dir_path}")
    |                                                              ^
    |
help: Add trailing newline

I001 [*] Import block is un-sorted or un-formatted
 --> src/project_a/pyspark_interview_project/utils/logging.py:5:1
  |
3 |   """
4 |
5 | / import json
6 | | import logging
7 | | import uuid
8 | | from typing import Any, Dict, Optional
9 | | from datetime import datetime
  | |_____________________________^
  |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
 --> src/project_a/pyspark_interview_project/utils/logging.py:8:1
  |
6 | import logging
7 | import uuid
8 | from typing import Any, Dict, Optional
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
9 | from datetime import datetime
  |

F401 [*] `typing.Any` imported but unused
 --> src/project_a/pyspark_interview_project/utils/logging.py:8:20
  |
6 | import logging
7 | import uuid
8 | from typing import Any, Dict, Optional
  |                    ^^^
9 | from datetime import datetime
  |
help: Remove unused import

F401 [*] `typing.Dict` imported but unused
 --> src/project_a/pyspark_interview_project/utils/logging.py:8:25
  |
6 | import logging
7 | import uuid
8 | from typing import Any, Dict, Optional
  |                         ^^^^
9 | from datetime import datetime
  |
help: Remove unused import

F401 [*] `typing.Optional` imported but unused
 --> src/project_a/pyspark_interview_project/utils/logging.py:8:31
  |
6 | import logging
7 | import uuid
8 | from typing import Any, Dict, Optional
  |                               ^^^^^^^^
9 | from datetime import datetime
  |
help: Remove unused import

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/logging.py:15:1
   |
13 |     """
14 |     Setup structured JSON logging.
15 |     
   | ^^^^
16 |     Args:
17 |         level: Logging level
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/logging.py:19:1
   |
17 |         level: Logging level
18 |         include_trace_id: Whether to include trace ID in logs
19 |         
   | ^^^^^^^^
20 |     Returns:
21 |         Configured logger
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/logging.py:31:1
   |
29 |                 "message": record.getMessage(),
30 |             }
31 |             
   | ^^^^^^^^^^^^
32 |             if include_trace_id and hasattr(record, 'trace_id'):
33 |                 log_entry["trace_id"] = record.trace_id
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/logging.py:34:1
   |
32 |             if include_trace_id and hasattr(record, 'trace_id'):
33 |                 log_entry["trace_id"] = record.trace_id
34 |             
   | ^^^^^^^^^^^^
35 |             if record.exc_info:
36 |                 log_entry["exception"] = self.formatException(record.exc_info)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/logging.py:37:1
   |
35 |             if record.exc_info:
36 |                 log_entry["exception"] = self.formatException(record.exc_info)
37 |             
   | ^^^^^^^^^^^^
38 |             # Add any extra fields
39 |             for key, value in record.__dict__.items():
   |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
  --> src/project_a/pyspark_interview_project/utils/logging.py:40:90
   |
38 |             # Add any extra fields
39 |             for key, value in record.__dict__.items():
40 |                 if key not in ['name', 'msg', 'args', 'levelname', 'levelno', 'pathname', 
   |                                                                                          ^
41 |                               'filename', 'module', 'lineno', 'funcName', 'created', 
42 |                               'msecs', 'relativeCreated', 'thread', 'threadName', 
   |
help: Remove trailing whitespace

W291 [*] Trailing whitespace
  --> src/project_a/pyspark_interview_project/utils/logging.py:41:85
   |
39 |             for key, value in record.__dict__.items():
40 |                 if key not in ['name', 'msg', 'args', 'levelname', 'levelno', 'pathname', 
41 |                               'filename', 'module', 'lineno', 'funcName', 'created', 
   |                                                                                     ^
42 |                               'msecs', 'relativeCreated', 'thread', 'threadName', 
43 |                               'processName', 'process', 'getMessage', 'exc_info', 
   |
help: Remove trailing whitespace

W291 [*] Trailing whitespace
  --> src/project_a/pyspark_interview_project/utils/logging.py:42:82
   |
40 |                 if key not in ['name', 'msg', 'args', 'levelname', 'levelno', 'pathname', 
41 |                               'filename', 'module', 'lineno', 'funcName', 'created', 
42 |                               'msecs', 'relativeCreated', 'thread', 'threadName', 
   |                                                                                  ^
43 |                               'processName', 'process', 'getMessage', 'exc_info', 
44 |                               'exc_text', 'stack_info', 'trace_id']:
   |
help: Remove trailing whitespace

W291 [*] Trailing whitespace
  --> src/project_a/pyspark_interview_project/utils/logging.py:43:82
   |
41 |                               'filename', 'module', 'lineno', 'funcName', 'created', 
42 |                               'msecs', 'relativeCreated', 'thread', 'threadName', 
43 |                               'processName', 'process', 'getMessage', 'exc_info', 
   |                                                                                  ^
44 |                               'exc_text', 'stack_info', 'trace_id']:
45 |                     log_entry[key] = value
   |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/logging.py:46:1
   |
44 |                               'exc_text', 'stack_info', 'trace_id']:
45 |                     log_entry[key] = value
46 |             
   | ^^^^^^^^^^^^
47 |             return json.dumps(log_entry)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/logging.py:48:1
   |
47 |             return json.dumps(log_entry)
48 |     
   | ^^^^
49 |     # Configure root logger
50 |     root_logger = logging.getLogger()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/logging.py:52:1
   |
50 |     root_logger = logging.getLogger()
51 |     root_logger.setLevel(getattr(logging, level.upper()))
52 |     
   | ^^^^
53 |     # Remove existing handlers
54 |     for handler in root_logger.handlers[:]:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/logging.py:56:1
   |
54 |     for handler in root_logger.handlers[:]:
55 |         root_logger.removeHandler(handler)
56 |     
   | ^^^^
57 |     # Add console handler with JSON formatter
58 |     handler = logging.StreamHandler()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/logging.py:61:1
   |
59 |     handler.setFormatter(JSONFormatter())
60 |     root_logger.addHandler(handler)
61 |     
   | ^^^^
62 |     return root_logger
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/logging.py:73:1
   |
71 |     """
72 |     Log message with trace ID.
73 |     
   | ^^^^
74 |     Args:
75 |         logger: Logger instance
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/utils/metrics.py:8:1
   |
 6 |   """
 7 |
 8 | / import logging
 9 | | from typing import Dict, Any
10 | | from datetime import datetime
   | |_____________________________^
11 |
12 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/utils/metrics.py:9:1
   |
 8 | import logging
 9 | from typing import Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 | from datetime import datetime
   |

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/utils/metrics.py:19:17
   |
17 |     value: float,
18 |     unit: str = "Count",
19 |     dimensions: Dict[str, str] = None
   |                 ^^^^
20 | ) -> None:
21 |     """
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/metrics.py:23:1
   |
21 |     """
22 |     Emit a metric to monitoring systems.
23 |     
   | ^^^^
24 |     Args:
25 |         metric_name: Name of the metric
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/metrics.py:31:1
   |
29 |     """
30 |     dimensions = dimensions or {}
31 |     
   | ^^^^
32 |     metric_data = {
33 |         "timestamp": datetime.utcnow().isoformat(),
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/metrics.py:39:1
   |
37 |         "dimensions": dimensions
38 |     }
39 |     
   | ^^^^
40 |     logger.info(f"Metric: {metric_data}")
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/utils/metrics.py:43:44
   |
43 | def track_job_start(job_name: str, config: Dict[str, Any]) -> str:
   |                                            ^^^^
44 |     """Track job start event and return job ID."""
45 |     job_id = f"{job_name}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
   |
help: Replace with `dict`

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/utils/path_resolver.py:8:1
   |
 6 |   """
 7 |
 8 | / import logging
 9 | | from typing import Dict, Any
   | |____________________________^
10 |
11 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/utils/path_resolver.py:9:1
   |
 8 | import logging
 9 | from typing import Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 |
11 | logger = logging.getLogger(__name__)
   |

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/path_resolver.py:17:1
   |
15 |     """
16 |     Resolve a logical path to physical path.
17 |     
   | ^^^^
18 |     Args:
19 |         logical_path: Logical path like 'lake://bronze', 'lake://silver/customer_behavior', or absolute path
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/path_resolver.py:22:1
   |
20 |         *args: Optional table name (e.g., 'customer_behavior')
21 |         **kwargs: Optional 'run_id' for timestamped paths
22 |         
   | ^^^^^^^^
23 |     Returns:
24 |         Physical path like 's3a://bucket/bronze/customer_behavior' or 'data/lakehouse_delta/bronze/customer_behavior'
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/path_resolver.py:33:1
   |
31 |             result = f"{result}/{'/'.join(args)}"
32 |         return result
33 |     
   | ^^^^
34 |     try:
35 |         # Handle lake:// prefix
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/path_resolver.py:39:1
   |
37 |             layer = logical_path.split("/")[1] if "/" in logical_path[7:] else logical_path.replace("lake://", "")
38 |             rest_of_path = "/".join(logical_path.split("/")[2:]) if "/" in logical_path[7:] else ""
39 |             
   | ^^^^^^^^^^^^
40 |             # Get base path for layer from config
41 |             paths_cfg = config.get("paths", {})
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/path_resolver.py:43:1
   |
41 |             paths_cfg = config.get("paths", {})
42 |             base_path = paths_cfg.get(layer, "")
43 |             
   | ^^^^^^^^^^^^
44 |             if not base_path:
45 |                 # Fallback: try data_lake paths
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/path_resolver.py:53:1
   |
51 |                 elif layer == "gold":
52 |                     base_path = data_lake.get("gold_path", "data/lakehouse_delta/gold")
53 |             
   | ^^^^^^^^^^^^
54 |             # Clean trailing slashes
55 |             if base_path.endswith("/"):
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/path_resolver.py:59:1
   |
57 |             if rest_of_path and not rest_of_path.startswith("/"):
58 |                 rest_of_path = "/" + rest_of_path
59 |             
   | ^^^^^^^^^^^^
60 |             physical_path = f"{base_path}{rest_of_path}"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/path_resolver.py:61:1
   |
60 |             physical_path = f"{base_path}{rest_of_path}"
61 |             
   | ^^^^^^^^^^^^
62 |             # Add any additional args (table names)
63 |             if args:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/path_resolver.py:66:1
   |
64 |                 for arg in args:
65 |                     physical_path = f"{physical_path}/{arg}"
66 |             
   | ^^^^^^^^^^^^
67 |             logger.debug(f"Resolved {logical_path} â†’ {physical_path}")
68 |             return physical_path
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/path_resolver.py:75:1
   |
73 |                 result = f"{result}/{'/'.join(args)}"
74 |             return result
75 |             
   | ^^^^^^^^^^^^
76 |     except Exception as e:
77 |         logger.error(f"Failed to resolve path {logical_path}: {e}")
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/utils/path_resolver.py:81:50
   |
81 | def resolve_lake_path(logical_path: str, config: Dict[str, Any]) -> str:
   |                                                  ^^^^
82 |     """
83 |     Legacy alias for resolve_path - maintained for backward compatibility.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/path_resolver.py:84:1
   |
82 |     """
83 |     Legacy alias for resolve_path - maintained for backward compatibility.
84 |     
   | ^^^^
85 |     Args:
86 |         logical_path: Logical path like 'lake://bronze/source/table'
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/path_resolver.py:88:1
   |
86 |         logical_path: Logical path like 'lake://bronze/source/table'
87 |         config: Configuration dictionary with lake paths
88 |         
   | ^^^^^^^^
89 |     Returns:
90 |         Physical S3 path like 's3a://bucket/bronze/source/table'
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/path_resolver.py:98:1
    |
 96 |     """
 97 |     Generate bronze layer S3 path.
 98 |     
    | ^^^^
 99 |     Args:
100 |         bucket: S3 bucket name
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/path_resolver.py:103:1
    |
101 |         source: Source system name (e.g., 'salesforce', 'snowflake')
102 |         object_name: Object/table name
103 |         
    | ^^^^^^^^
104 |     Returns:
105 |         S3 path string
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/path_resolver.py:113:1
    |
111 |     """
112 |     Generate silver layer S3 path.
113 |     
    | ^^^^
114 |     Args:
115 |         bucket: S3 bucket name
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/path_resolver.py:117:1
    |
115 |         bucket: S3 bucket name
116 |         table_name: Table name
117 |         
    | ^^^^^^^^
118 |     Returns:
119 |         S3 path string
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/path_resolver.py:127:1
    |
125 |     """
126 |     Generate gold layer S3 path.
127 |     
    | ^^^^
128 |     Args:
129 |         bucket: S3 bucket name
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/path_resolver.py:131:1
    |
129 |         bucket: S3 bucket name
130 |         table_name: Table name
131 |         
    | ^^^^^^^^
132 |     Returns:
133 |         S3 path string
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/utils/pii_utils.py:7:1
   |
 5 |   """
 6 |
 7 | / import os
 8 | | import hashlib
 9 | | import re
10 | | import logging
11 | | from typing import Optional
12 | | from pyspark.sql import DataFrame
13 | | from pyspark.sql.functions import col, regexp_replace, sha2, lit, when, concat
14 | | from pyspark.sql.types import StringType
   | |________________________________________^
15 |
16 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

F401 [*] `hashlib` imported but unused
  --> src/project_a/pyspark_interview_project/utils/pii_utils.py:8:8
   |
 7 | import os
 8 | import hashlib
   |        ^^^^^^^
 9 | import re
10 | import logging
   |
help: Remove unused import: `hashlib`

F401 [*] `re` imported but unused
  --> src/project_a/pyspark_interview_project/utils/pii_utils.py:9:8
   |
 7 | import os
 8 | import hashlib
 9 | import re
   |        ^^
10 | import logging
11 | from typing import Optional
   |
help: Remove unused import: `re`

F401 [*] `pyspark.sql.types.StringType` imported but unused
  --> src/project_a/pyspark_interview_project/utils/pii_utils.py:14:31
   |
12 | from pyspark.sql import DataFrame
13 | from pyspark.sql.functions import col, regexp_replace, sha2, lit, when, concat
14 | from pyspark.sql.types import StringType
   |                               ^^^^^^^^^^
15 |
16 | logger = logging.getLogger(__name__)
   |
help: Remove unused import: `pyspark.sql.types.StringType`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/utils/pii_utils.py:24:59
   |
24 | def mask_email(df: DataFrame, email_col: str, output_col: Optional[str] = None) -> DataFrame:
   |                                                           ^^^^^^^^^^^^^
25 |     """
26 |     Mask email addresses: user@domain.com -> u***@d***.com
   |
help: Convert to `X | None`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/pii_utils.py:27:1
   |
25 |     """
26 |     Mask email addresses: user@domain.com -> u***@d***.com
27 |     
   | ^^^^
28 |     Args:
29 |         df: Input DataFrame
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/pii_utils.py:32:1
   |
30 |         email_col: Column name containing email
31 |         output_col: Output column name (defaults to email_col)
32 |         
   | ^^^^^^^^
33 |     Returns:
34 |         DataFrame with masked email
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/pii_utils.py:37:1
   |
35 |     """
36 |     output_col = output_col or email_col
37 |     
   | ^^^^
38 |     # Pattern: keep first char, mask rest before @, keep domain structure
39 |     masked_expr = regexp_replace(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/pii_utils.py:44:1
   |
42 |         r'\1***@\2***.\3'
43 |     )
44 |     
   | ^^^^
45 |     return df.withColumn(output_col, masked_expr)
   |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/utils/pii_utils.py:48:59
   |
48 | def mask_phone(df: DataFrame, phone_col: str, output_col: Optional[str] = None) -> DataFrame:
   |                                                           ^^^^^^^^^^^^^
49 |     """
50 |     Mask phone numbers: +1234567890 -> +1***7890
   |
help: Convert to `X | None`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/pii_utils.py:51:1
   |
49 |     """
50 |     Mask phone numbers: +1234567890 -> +1***7890
51 |     
   | ^^^^
52 |     Args:
53 |         df: Input DataFrame
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/pii_utils.py:56:1
   |
54 |         phone_col: Column name containing phone
55 |         output_col: Output column name (defaults to phone_col)
56 |         
   | ^^^^^^^^
57 |     Returns:
58 |         DataFrame with masked phone
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/pii_utils.py:61:1
   |
59 |     """
60 |     output_col = output_col or phone_col
61 |     
   | ^^^^
62 |     # Pattern: keep country code and last 4 digits, mask middle
63 |     masked_expr = regexp_replace(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/pii_utils.py:68:1
   |
66 |         r'\1***\3'
67 |     )
68 |     
   | ^^^^
69 |     # If pattern doesn't match, just mask all but last 4
70 |     masked_expr = when(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/pii_utils.py:74:1
   |
72 |         regexp_replace(col(phone_col), r'^(.+?)(\d{4})$', r'***\2')
73 |     ).otherwise(col(phone_col))
74 |     
   | ^^^^
75 |     return df.withColumn(output_col, masked_expr)
   |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/utils/pii_utils.py:78:58
   |
78 | def hash_value(df: DataFrame, col_name: str, output_col: Optional[str] = None, salt_from_env: bool = True) -> DataFrame:
   |                                                          ^^^^^^^^^^^^^
79 |     """
80 |     Hash a column value using SHA-256.
   |
help: Convert to `X | None`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/pii_utils.py:81:1
   |
79 |     """
80 |     Hash a column value using SHA-256.
81 |     
   | ^^^^
82 |     Args:
83 |         df: Input DataFrame
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/pii_utils.py:87:1
   |
85 |         output_col: Output column name (defaults to col_name)
86 |         salt_from_env: Use salt from environment
87 |         
   | ^^^^^^^^
88 |     Returns:
89 |         DataFrame with hashed column
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/pii_utils.py:92:1
   |
90 |     """
91 |     output_col = output_col or col_name
92 |     
   | ^^^^
93 |     # Create salted value
94 |     salt = get_salt() if salt_from_env else ""
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/pii_utils.py:95:1
   |
93 |     # Create salted value
94 |     salt = get_salt() if salt_from_env else ""
95 |     
   | ^^^^
96 |     # Hash using SHA-256
97 |     # Note: In real implementation, would concatenate salt with value
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/pii_utils.py:102:1
    |
100 |         256
101 |     )
102 |     
    | ^^^^
103 |     return df.withColumn(output_col, hashed_expr)
    |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/utils/pii_utils.py:106:57
    |
106 | def mask_name(df: DataFrame, name_col: str, output_col: Optional[str] = None) -> DataFrame:
    |                                                         ^^^^^^^^^^^^^
107 |     """
108 |     Mask name: "John Doe" -> "J*** D***"
    |
help: Convert to `X | None`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/pii_utils.py:109:1
    |
107 |     """
108 |     Mask name: "John Doe" -> "J*** D***"
109 |     
    | ^^^^
110 |     Args:
111 |         df: Input DataFrame
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/pii_utils.py:114:1
    |
112 |         name_col: Column name containing name
113 |         output_col: Output column name (defaults to name_col)
114 |         
    | ^^^^^^^^
115 |     Returns:
116 |         DataFrame with masked name
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/pii_utils.py:119:1
    |
117 |     """
118 |     output_col = output_col or name_col
119 |     
    | ^^^^
120 |     # Mask first name: keep first char
121 |     # Mask last name: keep first char
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/pii_utils.py:128:1
    |
126 |         r'\1***'
127 |     )
128 |     
    | ^^^^
129 |     return df.withColumn(output_col, masked_expr)
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/pii_utils.py:139:1
    |
137 |     """
138 |     Apply PII masking based on configuration.
139 |     
    | ^^^^
140 |     Args:
141 |         df: Input DataFrame
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/pii_utils.py:144:1
    |
142 |         pii_config: PII configuration dict (from config/dq.yaml)
143 |         layer: Data layer (bronze/silver/gold) - only mask in gold
144 |         
    | ^^^^^^^^
145 |     Returns:
146 |         DataFrame with PII masked
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/pii_utils.py:152:1
    |
150 |         logger.info(f"Skipping PII masking in {layer} layer (only applied in gold)")
151 |         return df
152 |     
    | ^^^^
153 |     logger.info("Applying PII masking to gold layer")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/pii_utils.py:154:1
    |
153 |     logger.info("Applying PII masking to gold layer")
154 |     
    | ^^^^
155 |     result_df = df
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/pii_utils.py:156:1
    |
155 |     result_df = df
156 |     
    | ^^^^
157 |     # Get PII columns from config
158 |     pii_fields = pii_config.get("pii_fields", {})
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/pii_utils.py:159:1
    |
157 |     # Get PII columns from config
158 |     pii_fields = pii_config.get("pii_fields", {})
159 |     
    | ^^^^
160 |     for col_name, pii_info in pii_fields.items():
161 |         if col_name not in df.columns:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/pii_utils.py:164:1
    |
162 |             logger.warning(f"PII column {col_name} not found in DataFrame")
163 |             continue
164 |         
    | ^^^^^^^^
165 |         pii_type = pii_info.get("type", "hash")
166 |         mask_mode = pii_info.get("mask_mode", "hash")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/pii_utils.py:167:1
    |
165 |         pii_type = pii_info.get("type", "hash")
166 |         mask_mode = pii_info.get("mask_mode", "hash")
167 |         
    | ^^^^^^^^
168 |         if mask_mode == "hash":
169 |             result_df = hash_value(result_df, col_name)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/pii_utils.py:187:1
    |
185 |             result_df = hash_value(result_df, col_name)
186 |             logger.info(f"âœ… Hashed PII column (default): {col_name}")
187 |     
    | ^^^^
188 |     return result_df
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/utils/run_audit.py:7:1
   |
 5 |   for audit and observability without requiring a separate database.
 6 |   """
 7 | / import json
 8 | | import uuid
 9 | | import logging
10 | | from datetime import datetime
11 | | from typing import Optional, Dict, Any, List
12 | | import boto3
13 | | from botocore.exceptions import ClientError
   | |___________________________________________^
14 |
15 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/utils/run_audit.py:11:1
   |
 9 | import logging
10 | from datetime import datetime
11 | from typing import Optional, Dict, Any, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
12 | import boto3
13 | from botocore.exceptions import ClientError
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/pyspark_interview_project/utils/run_audit.py:11:1
   |
 9 | import logging
10 | from datetime import datetime
11 | from typing import Optional, Dict, Any, List
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
12 | import boto3
13 | from botocore.exceptions import ClientError
   |

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/utils/run_audit.py:27:13
   |
25 |     rows_out: int,
26 |     status: str,
27 |     run_id: Optional[str] = None,
   |             ^^^^^^^^^^^^^
28 |     error_message: Optional[str] = None,
29 |     duration_ms: Optional[float] = None,
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/utils/run_audit.py:28:20
   |
26 |     status: str,
27 |     run_id: Optional[str] = None,
28 |     error_message: Optional[str] = None,
   |                    ^^^^^^^^^^^^^
29 |     duration_ms: Optional[float] = None,
30 |     config: Optional[Dict[str, Any]] = None
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/utils/run_audit.py:29:18
   |
27 |     run_id: Optional[str] = None,
28 |     error_message: Optional[str] = None,
29 |     duration_ms: Optional[float] = None,
   |                  ^^^^^^^^^^^^^^^
30 |     config: Optional[Dict[str, Any]] = None
31 | ) -> str:
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/utils/run_audit.py:30:13
   |
28 |     error_message: Optional[str] = None,
29 |     duration_ms: Optional[float] = None,
30 |     config: Optional[Dict[str, Any]] = None
   |             ^^^^^^^^^^^^^^^^^^^^^^^^
31 | ) -> str:
32 |     """
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/utils/run_audit.py:30:22
   |
28 |     error_message: Optional[str] = None,
29 |     duration_ms: Optional[float] = None,
30 |     config: Optional[Dict[str, Any]] = None
   |                      ^^^^
31 | ) -> str:
32 |     """
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/run_audit.py:34:1
   |
32 |     """
33 |     Write run audit record to S3.
34 |     
   | ^^^^
35 |     Args:
36 |         bucket: S3 bucket name
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/run_audit.py:48:1
   |
46 |         duration_ms: Optional job duration in milliseconds
47 |         config: Optional config dictionary for reference
48 |         
   | ^^^^^^^^
49 |     Returns:
50 |         S3 key where audit record was written
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/run_audit.py:54:1
   |
52 |     if run_id is None:
53 |         run_id = str(uuid.uuid4())
54 |     
   | ^^^^
55 |     timestamp_utc = datetime.utcnow()
56 |     date_str = timestamp_utc.strftime("%Y-%m-%d")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/run_audit.py:57:1
   |
55 |     timestamp_utc = datetime.utcnow()
56 |     date_str = timestamp_utc.strftime("%Y-%m-%d")
57 |     
   | ^^^^
58 |     # S3 key structure: _audit/{env}/{job_name}/{date}/{run_id}.json
59 |     key = f"_audit/{env}/{job_name}/{date_str}/{run_id}.json"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/run_audit.py:60:1
   |
58 |     # S3 key structure: _audit/{env}/{job_name}/{date}/{run_id}.json
59 |     key = f"_audit/{env}/{job_name}/{date_str}/{run_id}.json"
60 |     
   | ^^^^
61 |     payload = {
62 |         "run_id": run_id,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/run_audit.py:72:1
   |
70 |         "status": status,
71 |     }
72 |     
   | ^^^^
73 |     if error_message:
74 |         payload["error_message"] = error_message
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/run_audit.py:75:1
   |
73 |     if error_message:
74 |         payload["error_message"] = error_message
75 |     
   | ^^^^
76 |     if duration_ms is not None:
77 |         payload["duration_ms"] = duration_ms
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/run_audit.py:78:1
   |
76 |     if duration_ms is not None:
77 |         payload["duration_ms"] = duration_ms
78 |     
   | ^^^^
79 |     if config:
80 |         # Include relevant config sections (sanitize sensitive data)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/run_audit.py:85:1
   |
83 |             "paths": config.get("paths", {}),
84 |         }
85 |     
   | ^^^^
86 |     try:
87 |         s3 = boto3.client("s3")
   |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/utils/run_audit.py:106:11
    |
104 |     job_name: str,
105 |     env: str,
106 |     date: Optional[str] = None
    |           ^^^^^^^^^^^^^
107 | ) -> List[Dict[str, Any]]:
108 |     """
    |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/pyspark_interview_project/utils/run_audit.py:107:6
    |
105 |     env: str,
106 |     date: Optional[str] = None
107 | ) -> List[Dict[str, Any]]:
    |      ^^^^
108 |     """
109 |     Read run audit records for a job.
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/utils/run_audit.py:107:11
    |
105 |     env: str,
106 |     date: Optional[str] = None
107 | ) -> List[Dict[str, Any]]:
    |           ^^^^
108 |     """
109 |     Read run audit records for a job.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/run_audit.py:110:1
    |
108 |     """
109 |     Read run audit records for a job.
110 |     
    | ^^^^
111 |     Args:
112 |         bucket: S3 bucket name
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/run_audit.py:116:1
    |
114 |         env: Environment
115 |         date: Optional date (YYYY-MM-DD). Defaults to today.
116 |         
    | ^^^^^^^^
117 |     Returns:
118 |         List of audit records
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/run_audit.py:122:1
    |
120 |     if date is None:
121 |         date = datetime.utcnow().strftime("%Y-%m-%d")
122 |     
    | ^^^^
123 |     prefix = f"_audit/{env}/{job_name}/{date}/"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/run_audit.py:124:1
    |
123 |     prefix = f"_audit/{env}/{job_name}/{date}/"
124 |     
    | ^^^^
125 |     try:
126 |         s3 = boto3.client("s3")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/run_audit.py:128:1
    |
126 |         s3 = boto3.client("s3")
127 |         response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)
128 |         
    | ^^^^^^^^
129 |         records = []
130 |         if "Contents" in response:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/run_audit.py:139:1
    |
137 |                 except Exception as e:
138 |                     logger.warning(f"Failed to read audit record {obj['Key']}: {e}")
139 |         
    | ^^^^^^^^
140 |         return sorted(records, key=lambda x: x.get("timestamp_utc", ""), reverse=True)
141 |     except ClientError as e:
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/utils/safe_writer.py:8:1
   |
 6 |   """
 7 |
 8 | / import logging
 9 | | from typing import Optional, List, Dict, Any
10 | | from datetime import datetime
11 | | from pyspark.sql import DataFrame, SparkSession
12 | | from pyspark.sql import functions as F
13 | | from delta.tables import DeltaTable
   | |___________________________________^
14 |
15 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/pyspark_interview_project/utils/safe_writer.py:9:1
   |
 8 | import logging
 9 | from typing import Optional, List, Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 | from datetime import datetime
11 | from pyspark.sql import DataFrame, SparkSession
   |

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/utils/safe_writer.py:9:1
   |
 8 | import logging
 9 | from typing import Optional, List, Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
10 | from datetime import datetime
11 | from pyspark.sql import DataFrame, SparkSession
   |

F401 [*] `datetime.datetime` imported but unused
  --> src/project_a/pyspark_interview_project/utils/safe_writer.py:10:22
   |
 8 | import logging
 9 | from typing import Optional, List, Dict, Any
10 | from datetime import datetime
   |                      ^^^^^^^^
11 | from pyspark.sql import DataFrame, SparkSession
12 | from pyspark.sql import functions as F
   |
help: Remove unused import: `datetime.datetime`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/safe_writer.py:21:1
   |
19 |     """
20 |     Production-safe Delta Lake writer with multiple write strategies.
21 |     
   | ^^^^
22 |     Features:
23 |     - Prevents unsafe overwrites
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/safe_writer.py:29:1
   |
27 |     - Emits data quality metrics
28 |     """
29 |     
   | ^^^^
30 |     def __init__(self, spark: SparkSession):
31 |         self.spark = spark
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/safe_writer.py:32:1
   |
30 |     def __init__(self, spark: SparkSession):
31 |         self.spark = spark
32 |         
   | ^^^^^^^^
33 |     def write_with_merge(
34 |         self,
   |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/pyspark_interview_project/utils/safe_writer.py:37:21
   |
35 |         df: DataFrame,
36 |         target_path: str,
37 |         merge_keys: List[str],
   |                     ^^^^
38 |         partition_cols: Optional[List[str]] = None,
39 |         update_condition: Optional[str] = None
   |
help: Replace with `list`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/utils/safe_writer.py:38:25
   |
36 |         target_path: str,
37 |         merge_keys: List[str],
38 |         partition_cols: Optional[List[str]] = None,
   |                         ^^^^^^^^^^^^^^^^^^^
39 |         update_condition: Optional[str] = None
40 |     ) -> Dict[str, Any]:
   |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/pyspark_interview_project/utils/safe_writer.py:38:34
   |
36 |         target_path: str,
37 |         merge_keys: List[str],
38 |         partition_cols: Optional[List[str]] = None,
   |                                  ^^^^
39 |         update_condition: Optional[str] = None
40 |     ) -> Dict[str, Any]:
   |
help: Replace with `list`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/utils/safe_writer.py:39:27
   |
37 |         merge_keys: List[str],
38 |         partition_cols: Optional[List[str]] = None,
39 |         update_condition: Optional[str] = None
   |                           ^^^^^^^^^^^^^
40 |     ) -> Dict[str, Any]:
41 |         """
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/utils/safe_writer.py:40:10
   |
38 |         partition_cols: Optional[List[str]] = None,
39 |         update_condition: Optional[str] = None
40 |     ) -> Dict[str, Any]:
   |          ^^^^
41 |         """
42 |         Safe upsert using Delta MERGE operation.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/safe_writer.py:43:1
   |
41 |         """
42 |         Safe upsert using Delta MERGE operation.
43 |         
   | ^^^^^^^^
44 |         Args:
45 |             df: Source DataFrame
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/safe_writer.py:50:1
   |
48 |             partition_cols: Optional partition columns
49 |             update_condition: Optional additional update condition
50 |             
   | ^^^^^^^^^^^^
51 |         Returns:
52 |             Dictionary with write metrics
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/safe_writer.py:57:1
   |
55 |             source_count = df.count()
56 |             logger.info(f"Starting MERGE operation: {source_count} source records")
57 |             
   | ^^^^^^^^^^^^
58 |             # Check if target exists
59 |             try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/safe_writer.py:67:1
   |
65 |                 before_count = 0
66 |                 logger.info(f"Target table does not exist, creating new: {target_path}")
67 |             
   | ^^^^^^^^^^^^
68 |             if not target_exists:
69 |                 # Initial write for new table
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/safe_writer.py:74:1
   |
72 |                     writer = writer.partitionBy(*partition_cols)
73 |                 writer.save(target_path)
74 |                 
   | ^^^^^^^^^^^^^^^^
75 |                 return {
76 |                     "success": True,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/safe_writer.py:85:1
   |
83 |                     "records_deleted": 0
84 |                 }
85 |             
   | ^^^^^^^^^^^^
86 |             # Build merge condition
87 |             merge_condition = " AND ".join([
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/safe_writer.py:90:1
   |
88 |                 f"target.{key} = source.{key}" for key in merge_keys
89 |             ])
90 |             
   | ^^^^^^^^^^^^
91 |             if update_condition:
92 |                 merge_condition = f"({merge_condition}) AND ({update_condition})"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/safe_writer.py:93:1
   |
91 |             if update_condition:
92 |                 merge_condition = f"({merge_condition}) AND ({update_condition})"
93 |             
   | ^^^^^^^^^^^^
94 |             # Execute MERGE
95 |             merge_builder = (
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:101:1
    |
 99 |                 .whenNotMatchedInsertAll()
100 |             )
101 |             
    | ^^^^^^^^^^^^
102 |             merge_builder.execute()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:103:1
    |
102 |             merge_builder.execute()
103 |             
    | ^^^^^^^^^^^^
104 |             # Get metrics
105 |             after_count = target_table.toDF().count()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:106:1
    |
104 |             # Get metrics
105 |             after_count = target_table.toDF().count()
106 |             
    | ^^^^^^^^^^^^
107 |             # Calculate approximate changes (not exact due to updates)
108 |             records_inserted = max(0, after_count - before_count)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:109:1
    |
107 |             # Calculate approximate changes (not exact due to updates)
108 |             records_inserted = max(0, after_count - before_count)
109 |             
    | ^^^^^^^^^^^^
110 |             metrics = {
111 |                 "success": True,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:120:1
    |
118 |                 "records_deleted": 0
119 |             }
120 |             
    | ^^^^^^^^^^^^
121 |             logger.info(f"MERGE completed successfully: {metrics}")
122 |             return metrics
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:123:1
    |
121 |             logger.info(f"MERGE completed successfully: {metrics}")
122 |             return metrics
123 |             
    | ^^^^^^^^^^^^
124 |         except Exception as e:
125 |             logger.error(f"MERGE operation failed: {e}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:127:1
    |
125 |             logger.error(f"MERGE operation failed: {e}")
126 |             raise RuntimeError(f"Delta MERGE failed: {e}") from e
127 |     
    | ^^^^
128 |     def write_partition_overwrite(
129 |         self,
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:135:10
    |
133 |         partition_value: str,
134 |         validate_partition: bool = True
135 |     ) -> Dict[str, Any]:
    |          ^^^^
136 |         """
137 |         Safely overwrite a single partition using replaceWhere.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:138:1
    |
136 |         """
137 |         Safely overwrite a single partition using replaceWhere.
138 |         
    | ^^^^^^^^
139 |         Args:
140 |             df: Source DataFrame (should contain only target partition data)
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:145:1
    |
143 |             partition_value: Partition value to overwrite
144 |             validate_partition: Whether to validate source data matches partition
145 |             
    | ^^^^^^^^^^^^
146 |         Returns:
147 |             Dictionary with write metrics
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:155:1
    |
153 |                 f"{source_count} records"
154 |             )
155 |             
    | ^^^^^^^^^^^^
156 |             # Validate source data only contains target partition
157 |             if validate_partition:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:163:1
    |
161 |                     .collect()
162 |                 )
163 |                 
    | ^^^^^^^^^^^^^^^^
164 |                 if len(distinct_partitions) == 0:
165 |                     raise ValueError("Source DataFrame is empty")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:166:1
    |
164 |                 if len(distinct_partitions) == 0:
165 |                     raise ValueError("Source DataFrame is empty")
166 |                 
    | ^^^^^^^^^^^^^^^^
167 |                 if len(distinct_partitions) > 1:
168 |                     raise ValueError(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:172:1
    |
170 |                         f"{[row[0] for row in distinct_partitions]}"
171 |                     )
172 |                 
    | ^^^^^^^^^^^^^^^^
173 |                 actual_value = str(distinct_partitions[0][0])
174 |                 if actual_value != partition_value:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:179:1
    |
177 |                         f"target partition '{partition_value}'"
178 |                     )
179 |             
    | ^^^^^^^^^^^^
180 |             # Get before count for this partition
181 |             try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:188:1
    |
186 |             except Exception:
187 |                 before_count = 0
188 |             
    | ^^^^^^^^^^^^
189 |             # Perform safe partition overwrite
190 |             (
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:197:1
    |
195 |                 .save(target_path)
196 |             )
197 |             
    | ^^^^^^^^^^^^
198 |             # Validate after write
199 |             after_df = self.spark.read.format("delta").load(target_path)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:203:1
    |
201 |                 F.col(partition_col) == F.lit(partition_value)
202 |             ).count()
203 |             
    | ^^^^^^^^^^^^
204 |             if after_count != source_count:
205 |                 logger.error(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:212:1
    |
210 |                     f"got {after_count}"
211 |                 )
212 |             
    | ^^^^^^^^^^^^
213 |             metrics = {
214 |                 "success": True,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:224:1
    |
222 |                 "records_written": source_count
223 |             }
224 |             
    | ^^^^^^^^^^^^
225 |             logger.info(f"Partition overwrite completed successfully: {metrics}")
226 |             return metrics
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:227:1
    |
225 |             logger.info(f"Partition overwrite completed successfully: {metrics}")
226 |             return metrics
227 |             
    | ^^^^^^^^^^^^
228 |         except Exception as e:
229 |             logger.error(f"Partition overwrite failed: {e}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:231:1
    |
229 |             logger.error(f"Partition overwrite failed: {e}")
230 |             raise RuntimeError(f"Delta partition overwrite failed: {e}") from e
231 |     
    | ^^^^
232 |     def write_append(
233 |         self,
    |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:236:25
    |
234 |         df: DataFrame,
235 |         target_path: str,
236 |         partition_cols: Optional[List[str]] = None,
    |                         ^^^^^^^^^^^^^^^^^^^
237 |         validate_duplicates: bool = False,
238 |         duplicate_keys: Optional[List[str]] = None
    |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:236:34
    |
234 |         df: DataFrame,
235 |         target_path: str,
236 |         partition_cols: Optional[List[str]] = None,
    |                                  ^^^^
237 |         validate_duplicates: bool = False,
238 |         duplicate_keys: Optional[List[str]] = None
    |
help: Replace with `list`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:238:25
    |
236 |         partition_cols: Optional[List[str]] = None,
237 |         validate_duplicates: bool = False,
238 |         duplicate_keys: Optional[List[str]] = None
    |                         ^^^^^^^^^^^^^^^^^^^
239 |     ) -> Dict[str, Any]:
240 |         """
    |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:238:34
    |
236 |         partition_cols: Optional[List[str]] = None,
237 |         validate_duplicates: bool = False,
238 |         duplicate_keys: Optional[List[str]] = None
    |                                  ^^^^
239 |     ) -> Dict[str, Any]:
240 |         """
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:239:10
    |
237 |         validate_duplicates: bool = False,
238 |         duplicate_keys: Optional[List[str]] = None
239 |     ) -> Dict[str, Any]:
    |          ^^^^
240 |         """
241 |         Safely append data with optional duplicate checking.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:242:1
    |
240 |         """
241 |         Safely append data with optional duplicate checking.
242 |         
    | ^^^^^^^^
243 |         Args:
244 |             df: Source DataFrame
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:249:1
    |
247 |             validate_duplicates: Whether to check for duplicates
248 |             duplicate_keys: Keys to use for duplicate checking
249 |             
    | ^^^^^^^^^^^^
250 |         Returns:
251 |             Dictionary with write metrics
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:256:1
    |
254 |             source_count = df.count()
255 |             logger.info(f"Starting append operation: {source_count} records")
256 |             
    | ^^^^^^^^^^^^
257 |             # Check for duplicates if requested
258 |             if validate_duplicates and duplicate_keys:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:264:1
    |
262 |                     .filter(F.col("count") > 1)
263 |                 )
264 |                 
    | ^^^^^^^^^^^^^^^^
265 |                 duplicate_count = duplicates.count()
266 |                 if duplicate_count > 0:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:271:1
    |
269 |                         f"Source data contains {duplicate_count} duplicate keys"
270 |                     )
271 |             
    | ^^^^^^^^^^^^
272 |             # Get before count
273 |             try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:277:1
    |
275 |             except Exception:
276 |                 before_count = 0
277 |             
    | ^^^^^^^^^^^^
278 |             # Append data
279 |             writer = df.write.format("delta").mode("append")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:283:1
    |
281 |                 writer = writer.partitionBy(*partition_cols)
282 |             writer.save(target_path)
283 |             
    | ^^^^^^^^^^^^
284 |             # Validate after write
285 |             after_count = self.spark.read.format("delta").load(target_path).count()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:287:1
    |
285 |             after_count = self.spark.read.format("delta").load(target_path).count()
286 |             expected_count = before_count + source_count
287 |             
    | ^^^^^^^^^^^^
288 |             if after_count != expected_count:
289 |                 logger.error(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:296:1
    |
294 |                     f"got {after_count}"
295 |                 )
296 |             
    | ^^^^^^^^^^^^
297 |             metrics = {
298 |                 "success": True,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:305:1
    |
303 |                 "records_appended": source_count
304 |             }
305 |             
    | ^^^^^^^^^^^^
306 |             logger.info(f"Append completed successfully: {metrics}")
307 |             return metrics
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:308:1
    |
306 |             logger.info(f"Append completed successfully: {metrics}")
307 |             return metrics
308 |             
    | ^^^^^^^^^^^^
309 |         except Exception as e:
310 |             logger.error(f"Append operation failed: {e}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:312:1
    |
310 |             logger.error(f"Append operation failed: {e}")
311 |             raise RuntimeError(f"Delta append failed: {e}") from e
312 |     
    | ^^^^
313 |     def validate_and_log_metrics(
314 |         self,
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:317:18
    |
315 |         df: DataFrame,
316 |         table_name: str,
317 |         metrics: Dict[str, Any]
    |                  ^^^^
318 |     ) -> None:
319 |         """
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:321:1
    |
319 |         """
320 |         Validate data quality and log metrics.
321 |         
    | ^^^^^^^^
322 |         Args:
323 |             df: DataFrame to validate
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:331:1
    |
329 |             row_count = df.count()
330 |             column_count = len(df.columns)
331 |             
    | ^^^^^^^^^^^^
332 |             # Null counts
333 |             null_counts = {}
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:338:1
    |
336 |                 if null_count > 0:
337 |                     null_counts[col] = null_count
338 |             
    | ^^^^^^^^^^^^
339 |             # Log comprehensive metrics
340 |             logger.info(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:350:1
    |
348 |                 }
349 |             )
350 |             
    | ^^^^^^^^^^^^
351 |         except Exception as e:
352 |             logger.warning(f"Failed to collect metrics: {e}")
    |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:360:17
    |
358 |     target_path: str,
359 |     mode: str = "merge",
360 |     merge_keys: Optional[List[str]] = None,
    |                 ^^^^^^^^^^^^^^^^^^^
361 |     partition_col: Optional[str] = None,
362 |     partition_value: Optional[str] = None,
    |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:360:26
    |
358 |     target_path: str,
359 |     mode: str = "merge",
360 |     merge_keys: Optional[List[str]] = None,
    |                          ^^^^
361 |     partition_col: Optional[str] = None,
362 |     partition_value: Optional[str] = None,
    |
help: Replace with `list`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:361:20
    |
359 |     mode: str = "merge",
360 |     merge_keys: Optional[List[str]] = None,
361 |     partition_col: Optional[str] = None,
    |                    ^^^^^^^^^^^^^
362 |     partition_value: Optional[str] = None,
363 |     partition_cols: Optional[List[str]] = None
    |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:362:22
    |
360 |     merge_keys: Optional[List[str]] = None,
361 |     partition_col: Optional[str] = None,
362 |     partition_value: Optional[str] = None,
    |                      ^^^^^^^^^^^^^
363 |     partition_cols: Optional[List[str]] = None
364 | ) -> Dict[str, Any]:
    |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:363:21
    |
361 |     partition_col: Optional[str] = None,
362 |     partition_value: Optional[str] = None,
363 |     partition_cols: Optional[List[str]] = None
    |                     ^^^^^^^^^^^^^^^^^^^
364 | ) -> Dict[str, Any]:
365 |     """
    |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:363:30
    |
361 |     partition_col: Optional[str] = None,
362 |     partition_value: Optional[str] = None,
363 |     partition_cols: Optional[List[str]] = None
    |                              ^^^^
364 | ) -> Dict[str, Any]:
365 |     """
    |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:364:6
    |
362 |     partition_value: Optional[str] = None,
363 |     partition_cols: Optional[List[str]] = None
364 | ) -> Dict[str, Any]:
    |      ^^^^
365 |     """
366 |     Convenience function for safe Delta writes.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:367:1
    |
365 |     """
366 |     Convenience function for safe Delta writes.
367 |     
    | ^^^^
368 |     Args:
369 |         spark: SparkSession
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:377:1
    |
375 |         partition_value: Partition value for partition_overwrite
376 |         partition_cols: Partition columns for initial table creation
377 |         
    | ^^^^^^^^
378 |     Returns:
379 |         Dictionary with write metrics
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:382:1
    |
380 |     """
381 |     writer = SafeDeltaWriter(spark)
382 |     
    | ^^^^
383 |     if mode == "merge":
384 |         if not merge_keys:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:387:1
    |
385 |             raise ValueError("merge_keys required for merge mode")
386 |         return writer.write_with_merge(df, target_path, merge_keys, partition_cols)
387 |     
    | ^^^^
388 |     elif mode == "partition_overwrite":
389 |         if not partition_col or not partition_value:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:396:1
    |
394 |             df, target_path, partition_col, partition_value
395 |         )
396 |     
    | ^^^^
397 |     elif mode == "append":
398 |         return writer.write_append(df, target_path, partition_cols)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/safe_writer.py:399:1
    |
397 |     elif mode == "append":
398 |         return writer.write_append(df, target_path, partition_cols)
399 |     
    | ^^^^
400 |     else:
401 |         raise ValueError(f"Unsupported mode: {mode}")
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/utils/schema_validator.py:8:1
   |
 6 |   """
 7 |
 8 | / import json
 9 | | import logging
10 | | import yaml
11 | | from pathlib import Path
12 | | from typing import Dict, Any, List, Optional, Tuple, Literal
13 | | from datetime import datetime
14 | | from pyspark.sql import DataFrame, SparkSession
15 | | from pyspark.sql.functions import col, lit, current_timestamp, to_timestamp, struct
16 | | from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, MapType
   | |__________________________________________________________________________________________________________________^
17 |
18 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

F401 [*] `yaml` imported but unused
  --> src/project_a/pyspark_interview_project/utils/schema_validator.py:10:8
   |
 8 | import json
 9 | import logging
10 | import yaml
   |        ^^^^
11 | from pathlib import Path
12 | from typing import Dict, Any, List, Optional, Tuple, Literal
   |
help: Remove unused import: `yaml`

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/utils/schema_validator.py:12:1
   |
10 | import yaml
11 | from pathlib import Path
12 | from typing import Dict, Any, List, Optional, Tuple, Literal
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
13 | from datetime import datetime
14 | from pyspark.sql import DataFrame, SparkSession
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/pyspark_interview_project/utils/schema_validator.py:12:1
   |
10 | import yaml
11 | from pathlib import Path
12 | from typing import Dict, Any, List, Optional, Tuple, Literal
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
13 | from datetime import datetime
14 | from pyspark.sql import DataFrame, SparkSession
   |

UP035 `typing.Tuple` is deprecated, use `tuple` instead
  --> src/project_a/pyspark_interview_project/utils/schema_validator.py:12:1
   |
10 | import yaml
11 | from pathlib import Path
12 | from typing import Dict, Any, List, Optional, Tuple, Literal
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
13 | from datetime import datetime
14 | from pyspark.sql import DataFrame, SparkSession
   |

F401 [*] `pyspark.sql.functions.to_timestamp` imported but unused
  --> src/project_a/pyspark_interview_project/utils/schema_validator.py:15:64
   |
13 | from datetime import datetime
14 | from pyspark.sql import DataFrame, SparkSession
15 | from pyspark.sql.functions import col, lit, current_timestamp, to_timestamp, struct
   |                                                                ^^^^^^^^^^^^
16 | from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, MapType
   |
help: Remove unused import

F401 [*] `pyspark.sql.functions.struct` imported but unused
  --> src/project_a/pyspark_interview_project/utils/schema_validator.py:15:78
   |
13 | from datetime import datetime
14 | from pyspark.sql import DataFrame, SparkSession
15 | from pyspark.sql.functions import col, lit, current_timestamp, to_timestamp, struct
   |                                                                              ^^^^^^
16 | from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, MapType
   |
help: Remove unused import

F401 [*] `pyspark.sql.types.StructType` imported but unused
  --> src/project_a/pyspark_interview_project/utils/schema_validator.py:16:31
   |
14 | from pyspark.sql import DataFrame, SparkSession
15 | from pyspark.sql.functions import col, lit, current_timestamp, to_timestamp, struct
16 | from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, MapType
   |                               ^^^^^^^^^^
17 |
18 | logger = logging.getLogger(__name__)
   |
help: Remove unused import

F401 [*] `pyspark.sql.types.StructField` imported but unused
  --> src/project_a/pyspark_interview_project/utils/schema_validator.py:16:43
   |
14 | from pyspark.sql import DataFrame, SparkSession
15 | from pyspark.sql.functions import col, lit, current_timestamp, to_timestamp, struct
16 | from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, MapType
   |                                           ^^^^^^^^^^^
17 |
18 | logger = logging.getLogger(__name__)
   |
help: Remove unused import

F401 [*] `pyspark.sql.types.MapType` imported but unused
  --> src/project_a/pyspark_interview_project/utils/schema_validator.py:16:108
   |
14 | from pyspark.sql import DataFrame, SparkSession
15 | from pyspark.sql.functions import col, lit, current_timestamp, to_timestamp, struct
16 | from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, MapType
   |                                                                                                            ^^^^^^^
17 |
18 | logger = logging.getLogger(__name__)
   |
help: Remove unused import

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/schema_validator.py:23:1
   |
21 | class SchemaValidator:
22 |     """Validates DataFrames against schema registry contracts."""
23 |     
   | ^^^^
24 |     def __init__(self, schema_path: str):
25 |         """
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/schema_validator.py:27:1
   |
25 |         """
26 |         Initialize validator with schema registry file.
27 |         
   | ^^^^^^^^
28 |         Args:
29 |             schema_path: Path to JSON schema registry file
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/schema_validator.py:33:1
   |
31 |         self.schema_path = schema_path
32 |         self.schema_def = self._load_schema()
33 |     
   | ^^^^
34 |     def _load_schema(self) -> Dict[str, Any]:
35 |         """Load schema definition from JSON file."""
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/utils/schema_validator.py:34:31
   |
32 |         self.schema_def = self._load_schema()
33 |     
34 |     def _load_schema(self) -> Dict[str, Any]:
   |                               ^^^^
35 |         """Load schema definition from JSON file."""
36 |         try:
   |
help: Replace with `dict`

UP015 [*] Unnecessary mode argument
  --> src/project_a/pyspark_interview_project/utils/schema_validator.py:37:41
   |
35 |         """Load schema definition from JSON file."""
36 |         try:
37 |             with open(self.schema_path, 'r') as f:
   |                                         ^^^
38 |                 return json.load(f)
39 |         except Exception as e:
   |
help: Remove mode argument

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
  --> src/project_a/pyspark_interview_project/utils/schema_validator.py:40:13
   |
38 |                 return json.load(f)
39 |         except Exception as e:
40 |             raise ValueError(f"Failed to load schema from {self.schema_path}: {str(e)}")
   |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
41 |     
42 |     def validate_required_fields(self, df: DataFrame) -> List[str]:
   |

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/schema_validator.py:41:1
   |
39 |         except Exception as e:
40 |             raise ValueError(f"Failed to load schema from {self.schema_path}: {str(e)}")
41 |     
   | ^^^^
42 |     def validate_required_fields(self, df: DataFrame) -> List[str]:
43 |         """
   |
help: Remove whitespace from blank line

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/pyspark_interview_project/utils/schema_validator.py:42:58
   |
40 |             raise ValueError(f"Failed to load schema from {self.schema_path}: {str(e)}")
41 |     
42 |     def validate_required_fields(self, df: DataFrame) -> List[str]:
   |                                                          ^^^^
43 |         """
44 |         Check if all required fields are present.
   |
help: Replace with `list`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/schema_validator.py:45:1
   |
43 |         """
44 |         Check if all required fields are present.
45 |         
   | ^^^^^^^^
46 |         Args:
47 |             df: DataFrame to validate
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/schema_validator.py:48:1
   |
46 |         Args:
47 |             df: DataFrame to validate
48 |             
   | ^^^^^^^^^^^^
49 |         Returns:
50 |             List of missing required fields
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/schema_validator.py:55:1
   |
53 |         existing_columns = set(df.columns)
54 |         missing_fields = [f for f in required_fields if f not in existing_columns]
55 |         
   | ^^^^^^^^
56 |         if missing_fields:
57 |             logger.warning(f"Missing required fields: {missing_fields}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/schema_validator.py:58:1
   |
56 |         if missing_fields:
57 |             logger.warning(f"Missing required fields: {missing_fields}")
58 |         
   | ^^^^^^^^
59 |         return missing_fields
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/schema_validator.py:60:1
   |
59 |         return missing_fields
60 |     
   | ^^^^
61 |     def handle_schema_drift(self, df: DataFrame, spark: SparkSession) -> DataFrame:
62 |         """
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/schema_validator.py:64:1
   |
62 |         """
63 |         Handle schema drift by adding missing columns and logging extra columns.
64 |         
   | ^^^^^^^^
65 |         Args:
66 |             df: Input DataFrame
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/schema_validator.py:68:1
   |
66 |             df: Input DataFrame
67 |             spark: Spark session
68 |             
   | ^^^^^^^^^^^^
69 |         Returns:
70 |             DataFrame with aligned schema
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/schema_validator.py:74:1
   |
72 |         schema_fields = self.schema_def.get('fields', [])
73 |         required_columns = {f['name'] for f in schema_fields if f.get('required', False)}
74 |         
   | ^^^^^^^^
75 |         # Get existing columns
76 |         existing_columns = set(df.columns)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/schema_validator.py:77:1
   |
75 |         # Get existing columns
76 |         existing_columns = set(df.columns)
77 |         
   | ^^^^^^^^
78 |         # Find missing required columns
79 |         missing_columns = required_columns - existing_columns
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/schema_validator.py:81:1
   |
79 |         missing_columns = required_columns - existing_columns
80 |         extra_columns = existing_columns - {f['name'] for f in schema_fields}
81 |         
   | ^^^^^^^^
82 |         result_df = df
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/schema_validator.py:83:1
   |
82 |         result_df = df
83 |         
   | ^^^^^^^^
84 |         # Add missing required columns as null
85 |         for col_name in missing_columns:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/schema_validator.py:97:1
   |
95 |                 else:
96 |                     result_df = result_df.withColumn(col_name, lit(None).cast(StringType()))
97 |                 
   | ^^^^^^^^^^^^^^^^
98 |                 logger.info(f"Added missing required column: {col_name}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:99:1
    |
 98 |                 logger.info(f"Added missing required column: {col_name}")
 99 |         
    | ^^^^^^^^
100 |         # Log extra columns (schema drift tolerance)
101 |         if extra_columns:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:105:1
    |
103 |             # Optionally, collect extra columns into a JSON struct
104 |             # For now, we just log and preserve them
105 |         
    | ^^^^^^^^
106 |         return result_df
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:107:1
    |
106 |         return result_df
107 |     
    | ^^^^
108 |     def validate_data_quality(self, df: DataFrame) -> Dict[str, Any]:
109 |         """
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:108:55
    |
106 |         return result_df
107 |     
108 |     def validate_data_quality(self, df: DataFrame) -> Dict[str, Any]:
    |                                                       ^^^^
109 |         """
110 |         Run data quality checks defined in schema validation rules.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:111:1
    |
109 |         """
110 |         Run data quality checks defined in schema validation rules.
111 |         
    | ^^^^^^^^
112 |         Args:
113 |             df: DataFrame to validate
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:114:1
    |
112 |         Args:
113 |             df: DataFrame to validate
114 |             
    | ^^^^^^^^^^^^
115 |         Returns:
116 |             Dictionary with validation results
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:123:1
    |
121 |             "issues": []
122 |         }
123 |         
    | ^^^^^^^^
124 |         # Check required fields are not null
125 |         not_null_fields = validation_rules.get('not_null_fields', [])
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:132:1
    |
130 |                     results["passed"] = False
131 |                     results["issues"].append(f"{field}: {null_count} null values found")
132 |         
    | ^^^^^^^^
133 |         # Check format patterns (if defined)
134 |         format_checks = validation_rules.get('format_checks', {})
    |
help: Remove whitespace from blank line

F841 Local variable `format_checks` is assigned to but never used
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:134:9
    |
133 |         # Check format patterns (if defined)
134 |         format_checks = validation_rules.get('format_checks', {})
    |         ^^^^^^^^^^^^^
135 |         # Note: Format checking would require regex UDF - simplified here
    |
help: Remove assignment to unused variable `format_checks`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:136:1
    |
134 |         format_checks = validation_rules.get('format_checks', {})
135 |         # Note: Format checking would require regex UDF - simplified here
136 |         
    | ^^^^^^^^
137 |         # Check ranges (if defined)
138 |         range_checks = validation_rules.get('range_checks', {})
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:149:1
    |
147 |                     if above_max > 0:
148 |                         results["issues"].append(f"{field}: {above_max} values above maximum {range_def['max']}")
149 |         
    | ^^^^^^^^
150 |         return results
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:151:1
    |
150 |         return results
151 |     
    | ^^^^
152 |     def validate_and_prepare(self, df: DataFrame, spark: SparkSession) -> Tuple[DataFrame, Dict[str, Any]]:
153 |         """
    |
help: Remove whitespace from blank line

UP006 [*] Use `tuple` instead of `Tuple` for type annotation
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:152:75
    |
150 |         return results
151 |     
152 |     def validate_and_prepare(self, df: DataFrame, spark: SparkSession) -> Tuple[DataFrame, Dict[str, Any]]:
    |                                                                           ^^^^^
153 |         """
154 |         Complete validation and schema alignment.
    |
help: Replace with `tuple`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:152:92
    |
150 |         return results
151 |     
152 |     def validate_and_prepare(self, df: DataFrame, spark: SparkSession) -> Tuple[DataFrame, Dict[str, Any]]:
    |                                                                                            ^^^^
153 |         """
154 |         Complete validation and schema alignment.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:155:1
    |
153 |         """
154 |         Complete validation and schema alignment.
155 |         
    | ^^^^^^^^
156 |         Args:
157 |             df: Input DataFrame
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:159:1
    |
157 |             df: Input DataFrame
158 |             spark: Spark session
159 |             
    | ^^^^^^^^^^^^
160 |         Returns:
161 |             Tuple of (aligned DataFrame, validation results)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:165:1
    |
163 |         # Check required fields
164 |         missing_fields = self.validate_required_fields(df)
165 |         
    | ^^^^^^^^
166 |         if missing_fields:
167 |             raise ValueError(f"Missing required fields: {missing_fields}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:168:1
    |
166 |         if missing_fields:
167 |             raise ValueError(f"Missing required fields: {missing_fields}")
168 |         
    | ^^^^^^^^
169 |         # Handle schema drift
170 |         aligned_df = self.handle_schema_drift(df, spark)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:171:1
    |
169 |         # Handle schema drift
170 |         aligned_df = self.handle_schema_drift(df, spark)
171 |         
    | ^^^^^^^^
172 |         # Add metadata fields if not present
173 |         if '_ingestion_ts' not in aligned_df.columns:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:177:1
    |
175 |         if '_source_system' not in aligned_df.columns:
176 |             aligned_df = aligned_df.withColumn('_source_system', lit(self.schema_def.get('source_system', 'unknown')))
177 |         
    | ^^^^^^^^
178 |         # Validate data quality
179 |         dq_results = self.validate_data_quality(aligned_df)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:180:1
    |
178 |         # Validate data quality
179 |         dq_results = self.validate_data_quality(aligned_df)
180 |         
    | ^^^^^^^^
181 |         return aligned_df, dq_results
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:186:22
    |
184 | def validate_schema(
185 |     df: DataFrame,
186 |     expected_schema: Dict[str, Any],
    |                      ^^^^
187 |     mode: Literal["strict", "allow_new"] = "strict",
188 |     config: Optional[Dict[str, Any]] = None,
    |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:188:13
    |
186 |     expected_schema: Dict[str, Any],
187 |     mode: Literal["strict", "allow_new"] = "strict",
188 |     config: Optional[Dict[str, Any]] = None,
    |             ^^^^^^^^^^^^^^^^^^^^^^^^
189 |     spark: Optional[SparkSession] = None
190 | ) -> Tuple[DataFrame, Dict[str, Any]]:
    |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:188:22
    |
186 |     expected_schema: Dict[str, Any],
187 |     mode: Literal["strict", "allow_new"] = "strict",
188 |     config: Optional[Dict[str, Any]] = None,
    |                      ^^^^
189 |     spark: Optional[SparkSession] = None
190 | ) -> Tuple[DataFrame, Dict[str, Any]]:
    |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:189:12
    |
187 |     mode: Literal["strict", "allow_new"] = "strict",
188 |     config: Optional[Dict[str, Any]] = None,
189 |     spark: Optional[SparkSession] = None
    |            ^^^^^^^^^^^^^^^^^^^^^^
190 | ) -> Tuple[DataFrame, Dict[str, Any]]:
191 |     """
    |
help: Convert to `X | None`

UP006 [*] Use `tuple` instead of `Tuple` for type annotation
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:190:6
    |
188 |     config: Optional[Dict[str, Any]] = None,
189 |     spark: Optional[SparkSession] = None
190 | ) -> Tuple[DataFrame, Dict[str, Any]]:
    |      ^^^^^
191 |     """
192 |     Validate schema with evolution support.
    |
help: Replace with `tuple`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:190:23
    |
188 |     config: Optional[Dict[str, Any]] = None,
189 |     spark: Optional[SparkSession] = None
190 | ) -> Tuple[DataFrame, Dict[str, Any]]:
    |                       ^^^^
191 |     """
192 |     Validate schema with evolution support.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:193:1
    |
191 |     """
192 |     Validate schema with evolution support.
193 |     
    | ^^^^
194 |     Args:
195 |         df: DataFrame to validate
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:200:1
    |
198 |         config: Configuration dict (for drift logging path)
199 |         spark: SparkSession (for drift logging)
200 |         
    | ^^^^^^^^
201 |     Returns:
202 |         Tuple of (validated DataFrame, validation results)
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:203:1
    |
201 |     Returns:
202 |         Tuple of (validated DataFrame, validation results)
203 |         
    | ^^^^^^^^
204 |     Raises:
205 |         ValueError: If strict mode and unexpected columns found
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:209:1
    |
207 |     expected_columns = {col_def["name"] for col_def in expected_schema.get("columns", [])}
208 |     actual_columns = set(df.columns)
209 |     
    | ^^^^
210 |     missing_columns = expected_columns - actual_columns
211 |     extra_columns = actual_columns - expected_columns
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:212:1
    |
210 |     missing_columns = expected_columns - actual_columns
211 |     extra_columns = actual_columns - expected_columns
212 |     
    | ^^^^
213 |     validation_results = {
214 |         "passed": True,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:219:1
    |
217 |         "drift_detected": len(extra_columns) > 0 or len(missing_columns) > 0
218 |     }
219 |     
    | ^^^^
220 |     result_df = df
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:221:1
    |
220 |     result_df = df
221 |     
    | ^^^^
222 |     # Handle missing columns (add as null)
223 |     if missing_columns:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:230:1
    |
228 |                 col_type = col_def.get("type", "string")
229 |                 result_df = result_df.withColumn(col_name, lit(None).cast(col_type))
230 |     
    | ^^^^
231 |     # Handle extra columns (schema drift)
232 |     if extra_columns:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:234:1
    |
232 |     if extra_columns:
233 |         logger.warning(f"Unexpected columns detected (schema drift): {extra_columns}")
234 |         
    | ^^^^^^^^
235 |         if mode == "strict":
236 |             raise ValueError(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:250:1
    |
248 |                 "actual_columns": list(actual_columns)
249 |             }
250 |             
    | ^^^^^^^^^^^^
251 |             # Log to S3 or local
252 |             log_schema_drift(drift_data, config, spark)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:253:1
    |
251 |             # Log to S3 or local
252 |             log_schema_drift(drift_data, config, spark)
253 |             
    | ^^^^^^^^^^^^
254 |             # Preserve extra columns (they'll be written normally)
255 |             logger.info(f"Preserving extra columns in allow_new mode: {extra_columns}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:257:1
    |
255 |             logger.info(f"Preserving extra columns in allow_new mode: {extra_columns}")
256 |             validation_results["passed"] = True  # Allow to continue
257 |     
    | ^^^^
258 |     return result_df, validation_results
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:261:34
    |
261 | def log_schema_drift(drift_data: Dict[str, Any], config: Optional[Dict[str, Any]] = None, spark: Optional[SparkSession] = None):
    |                                  ^^^^
262 |     """Log schema drift to S3 or local file."""
263 |     date_str = datetime.utcnow().strftime("%Y-%m-%d")
    |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:261:58
    |
261 | def log_schema_drift(drift_data: Dict[str, Any], config: Optional[Dict[str, Any]] = None, spark: Optional[SparkSession] = None):
    |                                                          ^^^^^^^^^^^^^^^^^^^^^^^^
262 |     """Log schema drift to S3 or local file."""
263 |     date_str = datetime.utcnow().strftime("%Y-%m-%d")
    |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:261:67
    |
261 | def log_schema_drift(drift_data: Dict[str, Any], config: Optional[Dict[str, Any]] = None, spark: Optional[SparkSession] = None):
    |                                                                   ^^^^
262 |     """Log schema drift to S3 or local file."""
263 |     date_str = datetime.utcnow().strftime("%Y-%m-%d")
    |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:261:98
    |
261 | def log_schema_drift(drift_data: Dict[str, Any], config: Optional[Dict[str, Any]] = None, spark: Optional[SparkSession] = None):
    |                                                                                                  ^^^^^^^^^^^^^^^^^^^^^^
262 |     """Log schema drift to S3 or local file."""
263 |     date_str = datetime.utcnow().strftime("%Y-%m-%d")
    |
help: Convert to `X | None`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:264:1
    |
262 |     """Log schema drift to S3 or local file."""
263 |     date_str = datetime.utcnow().strftime("%Y-%m-%d")
264 |     
    | ^^^^
265 |     if config and spark:
266 |         drift_path = config.get("data_lake", {}).get("drift_prefix", "s3://bucket/meta/schema_drifts")
    |
help: Remove whitespace from blank line

F841 Local variable `drift_json` is assigned to but never used
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:271:17
    |
269 |                 # Write drift JSON to S3
270 |                 import json
271 |                 drift_json = json.dumps(drift_data, indent=2)
    |                 ^^^^^^^^^^
272 |                 # Use boto3 or Spark to write to S3
273 |                 logger.info(f"Schema drift logged to {drift_path}/{date_str}.json")
    |
help: Remove assignment to unused variable `drift_json`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:277:1
    |
275 |             except Exception as e:
276 |                 logger.warning(f"Could not log to S3: {e}, falling back to local")
277 |     
    | ^^^^
278 |     # Fallback to local
279 |     local_path = Path(f"data/metrics/schema_drifts/{date_str}.json")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:281:1
    |
279 |     local_path = Path(f"data/metrics/schema_drifts/{date_str}.json")
280 |     local_path.parent.mkdir(parents=True, exist_ok=True)
281 |     
    | ^^^^
282 |     import json
283 |     with open(local_path, "a") as f:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:285:1
    |
283 |     with open(local_path, "a") as f:
284 |         f.write(json.dumps(drift_data) + "\n")
285 |     
    | ^^^^
286 |     logger.info(f"Schema drift logged to {local_path}")
    |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:295:13
    |
293 |     schemas_dir: str = "schemas",
294 |     mode: Literal["strict", "allow_new"] = "allow_new",
295 |     config: Optional[Dict[str, Any]] = None
    |             ^^^^^^^^^^^^^^^^^^^^^^^^
296 | ) -> Tuple[DataFrame, Dict[str, Any]]:
297 |     """
    |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:295:22
    |
293 |     schemas_dir: str = "schemas",
294 |     mode: Literal["strict", "allow_new"] = "allow_new",
295 |     config: Optional[Dict[str, Any]] = None
    |                      ^^^^
296 | ) -> Tuple[DataFrame, Dict[str, Any]]:
297 |     """
    |
help: Replace with `dict`

UP006 [*] Use `tuple` instead of `Tuple` for type annotation
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:296:6
    |
294 |     mode: Literal["strict", "allow_new"] = "allow_new",
295 |     config: Optional[Dict[str, Any]] = None
296 | ) -> Tuple[DataFrame, Dict[str, Any]]:
    |      ^^^^^
297 |     """
298 |     Convenience function to validate bronze ingestion with schema evolution.
    |
help: Replace with `tuple`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:296:23
    |
294 |     mode: Literal["strict", "allow_new"] = "allow_new",
295 |     config: Optional[Dict[str, Any]] = None
296 | ) -> Tuple[DataFrame, Dict[str, Any]]:
    |                       ^^^^
297 |     """
298 |     Convenience function to validate bronze ingestion with schema evolution.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:299:1
    |
297 |     """
298 |     Convenience function to validate bronze ingestion with schema evolution.
299 |     
    | ^^^^
300 |     Args:
301 |         df: Input DataFrame
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:307:1
    |
305 |         mode: Schema validation mode ("strict" or "allow_new")
306 |         config: Configuration dict
307 |         
    | ^^^^^^^^
308 |     Returns:
309 |         Tuple of (validated DataFrame, validation results)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:312:1
    |
310 |     """
311 |     schema_path = Path(schemas_dir) / f"{schema_name}.schema.json"
312 |     
    | ^^^^
313 |     if not schema_path.exists():
314 |         logger.warning(f"Schema file not found: {schema_path}, skipping validation")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:316:1
    |
314 |         logger.warning(f"Schema file not found: {schema_path}, skipping validation")
315 |         return df, {"passed": True, "issues": ["Schema file not found"]}
316 |     
    | ^^^^
317 |     with open(schema_path) as f:
318 |         expected_schema = json.load(f)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/schema_validator.py:319:1
    |
317 |     with open(schema_path) as f:
318 |         expected_schema = json.load(f)
319 |     
    | ^^^^
320 |     return validate_schema(df, expected_schema, mode=mode, config=config, spark=spark)
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/utils/secrets.py:7:1
   |
 5 |   """
 6 |
 7 | / import os
 8 | | import json
 9 | | import logging
10 | | from typing import Dict, Any, Optional
   | |______________________________________^
11 |
12 |   try:
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/utils/secrets.py:10:1
   |
 8 | import json
 9 | import logging
10 | from typing import Dict, Any, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
11 |
12 | try:
   |

F401 [*] `typing.Optional` imported but unused
  --> src/project_a/pyspark_interview_project/utils/secrets.py:10:31
   |
 8 | import json
 9 | import logging
10 | from typing import Dict, Any, Optional
   |                               ^^^^^^^^
11 |
12 | try:
   |
help: Remove unused import: `typing.Optional`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/utils/secrets.py:24:82
   |
24 | def get_secret_from_manager(secret_name: str, region_name: str = "us-east-1") -> Dict[str, Any]:
   |                                                                                  ^^^^
25 |     """
26 |     Retrieve secret from AWS Secrets Manager.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/secrets.py:27:1
   |
25 |     """
26 |     Retrieve secret from AWS Secrets Manager.
27 |     
   | ^^^^
28 |     Args:
29 |         secret_name: Name of secret in Secrets Manager
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/secrets.py:31:1
   |
29 |         secret_name: Name of secret in Secrets Manager
30 |         region_name: AWS region
31 |         
   | ^^^^^^^^
32 |     Returns:
33 |         Secret dictionary (typically JSON-parsed)
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/secrets.py:34:1
   |
32 |     Returns:
33 |         Secret dictionary (typically JSON-parsed)
34 |         
   | ^^^^^^^^
35 |     Raises:
36 |         ValueError: If secret not found or access denied
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/secrets.py:41:1
   |
39 |         logger.warning("boto3 not available, using environment variables")
40 |         return _get_secret_from_env(secret_name)
41 |     
   | ^^^^
42 |     try:
43 |         client = boto3.client('secretsmanager', region_name=region_name)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/secrets.py:44:1
   |
42 |     try:
43 |         client = boto3.client('secretsmanager', region_name=region_name)
44 |         
   | ^^^^^^^^
45 |         response = client.get_secret_value(SecretId=secret_name)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/secrets.py:46:1
   |
45 |         response = client.get_secret_value(SecretId=secret_name)
46 |         
   | ^^^^^^^^
47 |         secret_string = response.get('SecretString')
48 |         if secret_string:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/secrets.py:53:1
   |
51 |             # Binary secret
52 |             return {"secret_binary": response.get('SecretBinary')}
53 |             
   | ^^^^^^^^^^^^
54 |     except ClientError as e:
55 |         error_code = e.response['Error']['Code']
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/secrets.py:56:1
   |
54 |     except ClientError as e:
55 |         error_code = e.response['Error']['Code']
56 |         
   | ^^^^^^^^
57 |         if error_code == 'ResourceNotFoundException':
58 |             logger.warning(f"Secret {secret_name} not found, falling back to environment variables")
   |
help: Remove whitespace from blank line

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
  --> src/project_a/pyspark_interview_project/utils/secrets.py:62:13
   |
60 |         elif error_code == 'AccessDeniedException':
61 |             logger.error(f"Access denied to secret {secret_name}")
62 |             raise ValueError(f"Cannot access secret {secret_name}: Access Denied")
   |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
63 |         else:
64 |             logger.error(f"Error retrieving secret {secret_name}: {e}")
   |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
  --> src/project_a/pyspark_interview_project/utils/secrets.py:65:13
   |
63 |         else:
64 |             logger.error(f"Error retrieving secret {secret_name}: {e}")
65 |             raise ValueError(f"Failed to retrieve secret: {e}")
   |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
66 |     except Exception as e:
67 |         logger.warning(f"Error accessing Secrets Manager: {e}, falling back to env vars")
   |

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/secrets.py:74:1
   |
72 |     """
73 |     Retrieve parameter from AWS SSM Parameter Store.
74 |     
   | ^^^^
75 |     Args:
76 |         parameter_name: Name of parameter (e.g., '/data-platform/snowflake/password')
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/secrets.py:79:1
   |
77 |         region_name: AWS region
78 |         decrypt: Whether to decrypt SecureString parameters
79 |         
   | ^^^^^^^^
80 |     Returns:
81 |         Parameter value as string
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/secrets.py:87:1
   |
85 |         env_key = parameter_name.replace('/', '_').upper()
86 |         return os.getenv(env_key, "")
87 |     
   | ^^^^
88 |     try:
89 |         client = boto3.client('ssm', region_name=region_name)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/secrets.py:90:1
   |
88 |     try:
89 |         client = boto3.client('ssm', region_name=region_name)
90 |         
   | ^^^^^^^^
91 |         response = client.get_parameter(
92 |             Name=parameter_name,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/secrets.py:95:1
   |
93 |             WithDecryption=decrypt
94 |         )
95 |         
   | ^^^^^^^^
96 |         return response['Parameter']['Value']
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/secrets.py:97:1
   |
96 |         return response['Parameter']['Value']
97 |         
   | ^^^^^^^^
98 |     except ClientError as e:
99 |         error_code = e.response['Error']['Code']
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/secrets.py:100:1
    |
 98 |     except ClientError as e:
 99 |         error_code = e.response['Error']['Code']
100 |         
    | ^^^^^^^^
101 |         if error_code == 'ParameterNotFound':
102 |             logger.warning(f"Parameter {parameter_name} not found, checking environment")
    |
help: Remove whitespace from blank line

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> src/project_a/pyspark_interview_project/utils/secrets.py:107:13
    |
105 |         else:
106 |             logger.error(f"Error retrieving parameter {parameter_name}: {e}")
107 |             raise ValueError(f"Failed to retrieve parameter: {e}")
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/utils/secrets.py:110:47
    |
110 | def _get_secret_from_env(secret_name: str) -> Dict[str, Any]:
    |                                               ^^^^
111 |     """Fallback to environment variables if Secrets Manager unavailable."""
112 |     # Map common secret names to env var patterns
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/secrets.py:130:1
    |
128 |         }
129 |     }
130 |     
    | ^^^^
131 |     # Try to infer secret type from name
132 |     secret_type = None
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/secrets.py:137:1
    |
135 |     elif "redshift" in secret_name.lower():
136 |         secret_type = "redshift"
137 |     
    | ^^^^
138 |     if secret_type and secret_type in env_mapping:
139 |         result = {}
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/secrets.py:145:1
    |
143 |                 result[key] = value
144 |         return result if result else {}
145 |     
    | ^^^^
146 |     # Generic fallback: try secret name as env var
147 |     env_key = secret_name.replace('-', '_').upper()
    |
help: Remove whitespace from blank line

E722 Do not use bare `except`
   --> src/project_a/pyspark_interview_project/utils/secrets.py:152:9
    |
150 |         try:
151 |             return json.loads(value)
152 |         except:
    |         ^^^^^^
153 |             return {"value": value}
    |

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/secrets.py:154:1
    |
152 |         except:
153 |             return {"value": value}
154 |     
    | ^^^^
155 |     return {}
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/utils/secrets.py:158:39
    |
158 | def get_snowflake_credentials(config: Dict[str, Any]) -> Dict[str, str]:
    |                                       ^^^^
159 |     """
160 |     Get Snowflake credentials from Secrets Manager (Phase 2 format).
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/utils/secrets.py:158:58
    |
158 | def get_snowflake_credentials(config: Dict[str, Any]) -> Dict[str, str]:
    |                                                          ^^^^
159 |     """
160 |     Get Snowflake credentials from Secrets Manager (Phase 2 format).
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/secrets.py:161:1
    |
159 |     """
160 |     Get Snowflake credentials from Secrets Manager (Phase 2 format).
161 |     
    | ^^^^
162 |     Args:
163 |         config: Configuration dictionary
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/secrets.py:164:1
    |
162 |     Args:
163 |         config: Configuration dictionary
164 |         
    | ^^^^^^^^
165 |     Returns:
166 |         Dictionary with Snowflake connection parameters
    |
help: Remove whitespace from blank line

F401 [*] `boto3` imported but unused
   --> src/project_a/pyspark_interview_project/utils/secrets.py:168:12
    |
166 |         Dictionary with Snowflake connection parameters
167 |     """
168 |     import boto3
    |            ^^^^^
169 |     
170 |     region = config.get('region', 'us-east-1')
    |
help: Remove unused import: `boto3`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/secrets.py:169:1
    |
167 |     """
168 |     import boto3
169 |     
    | ^^^^
170 |     region = config.get('region', 'us-east-1')
171 |     secret_name = "project-a-dev/snowflake/conn"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/secrets.py:172:1
    |
170 |     region = config.get('region', 'us-east-1')
171 |     secret_name = "project-a-dev/snowflake/conn"
172 |     
    | ^^^^
173 |     try:
174 |         logger.info(f"Retrieving Snowflake credentials from Secrets Manager: {secret_name}")
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/utils/secrets.py:182:38
    |
182 | def get_redshift_credentials(config: Dict[str, Any]) -> Dict[str, str]:
    |                                      ^^^^
183 |     """
184 |     Get Redshift credentials from Secrets Manager (Phase 2 format).
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/utils/secrets.py:182:57
    |
182 | def get_redshift_credentials(config: Dict[str, Any]) -> Dict[str, str]:
    |                                                         ^^^^
183 |     """
184 |     Get Redshift credentials from Secrets Manager (Phase 2 format).
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/secrets.py:185:1
    |
183 |     """
184 |     Get Redshift credentials from Secrets Manager (Phase 2 format).
185 |     
    | ^^^^
186 |     Args:
187 |         config: Configuration dictionary
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/secrets.py:188:1
    |
186 |     Args:
187 |         config: Configuration dictionary
188 |         
    | ^^^^^^^^
189 |     Returns:
190 |         Dictionary with Redshift connection parameters
    |
help: Remove whitespace from blank line

F401 [*] `boto3` imported but unused
   --> src/project_a/pyspark_interview_project/utils/secrets.py:192:12
    |
190 |         Dictionary with Redshift connection parameters
191 |     """
192 |     import boto3
    |            ^^^^^
193 |     
194 |     region = config.get('region', 'us-east-1')
    |
help: Remove unused import: `boto3`

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/secrets.py:193:1
    |
191 |     """
192 |     import boto3
193 |     
    | ^^^^
194 |     region = config.get('region', 'us-east-1')
195 |     secret_name = "project-a-dev/redshift/conn"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/secrets.py:196:1
    |
194 |     region = config.get('region', 'us-east-1')
195 |     secret_name = "project-a-dev/redshift/conn"
196 |     
    | ^^^^
197 |     try:
198 |         logger.info(f"Retrieving Redshift credentials from Secrets Manager: {secret_name}")
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
 --> src/project_a/pyspark_interview_project/utils/spark_session.py:1:1
  |
1 | / import logging
2 | | from typing import Optional, Dict, Any
3 | |
4 | | from pyspark.sql import SparkSession
  | |____________________________________^
5 |
6 |   logger = logging.getLogger(__name__)
  |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
 --> src/project_a/pyspark_interview_project/utils/spark_session.py:2:1
  |
1 | import logging
2 | from typing import Optional, Dict, Any
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
3 |
4 | from pyspark.sql import SparkSession
  |

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/utils/spark_session.py:11:13
   |
 9 | def build_spark(
10 |     app_name: str = "project_a",
11 |     config: Optional[Dict[str, Any]] = None,
   |             ^^^^^^^^^^^^^^^^^^^^^^^^
12 | ) -> SparkSession:
13 |     """
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/utils/spark_session.py:11:22
   |
 9 | def build_spark(
10 |     app_name: str = "project_a",
11 |     config: Optional[Dict[str, Any]] = None,
   |                      ^^^^
12 | ) -> SparkSession:
13 |     """
   |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/spark_session.py:36:1
   |
34 |     environment = (config or {}).get("environment") or (config or {}).get("env", "local")
35 |     is_local = environment in ("local", "dev_local")
36 |     
   | ^^^^
37 |     # Enable Delta only if explicitly requested OR if not local
38 |     enable_delta = cfg.get("enable_delta", not is_local)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/spark_session.py:39:1
   |
37 |     # Enable Delta only if explicitly requested OR if not local
38 |     enable_delta = cfg.get("enable_delta", not is_local)
39 |     
   | ^^^^
40 |     if enable_delta:
41 |         builder = (
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/pyspark_interview_project/utils/spark_session.py:57:17
   |
56 |     # Apply any extra configs from YAML (spark.extra_conf)
57 |     extra_conf: Dict[str, str] = cfg.get("extra_conf", {}) if cfg else {}
   |                 ^^^^
58 |     for k, v in extra_conf.items():
59 |         builder = builder.config(k, v)
   |
help: Replace with `dict`

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/utils/state_store.py:8:1
   |
 6 |   """
 7 |
 8 | / import json
 9 | | import logging
10 | | from datetime import datetime, timezone
11 | | from typing import Optional, Dict, Any
12 | | from pathlib import Path
   | |________________________^
13 |
14 |   try:
   |
help: Organize imports

F401 [*] `datetime.timezone` imported but unused
  --> src/project_a/pyspark_interview_project/utils/state_store.py:10:32
   |
 8 | import json
 9 | import logging
10 | from datetime import datetime, timezone
   |                                ^^^^^^^^
11 | from typing import Optional, Dict, Any
12 | from pathlib import Path
   |
help: Remove unused import: `datetime.timezone`

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/pyspark_interview_project/utils/state_store.py:11:1
   |
 9 | import logging
10 | from datetime import datetime, timezone
11 | from typing import Optional, Dict, Any
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
12 | from pathlib import Path
   |

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/state_store.py:28:1
   |
26 |     """
27 |     Manages state files (watermarks, checkpoints) in S3 or local filesystem.
28 |     
   | ^^^^
29 |     Uses S3 for production and local filesystem for development/testing.
30 |     """
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/state_store.py:31:1
   |
29 |     Uses S3 for production and local filesystem for development/testing.
30 |     """
31 |     
   | ^^^^
32 |     def __init__(self, bucket: Optional[str] = None, prefix: str = "_state", s3_client=None):
33 |         """
   |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/utils/state_store.py:32:32
   |
30 |     """
31 |     
32 |     def __init__(self, bucket: Optional[str] = None, prefix: str = "_state", s3_client=None):
   |                                ^^^^^^^^^^^^^
33 |         """
34 |         Initialize state store.
   |
help: Convert to `X | None`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/state_store.py:35:1
   |
33 |         """
34 |         Initialize state store.
35 |         
   | ^^^^^^^^
36 |         Args:
37 |             bucket: S3 bucket name (if None, uses local filesystem)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/state_store.py:44:1
   |
42 |         self.prefix = prefix
43 |         self.use_s3 = bucket is not None and BOTO3_AVAILABLE
44 |         
   | ^^^^^^^^
45 |         if self.use_s3:
46 |             self.s3_client = s3_client or boto3.client('s3')
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/state_store.py:53:1
   |
51 |             self.local_dir = local_dir
52 |             logger.info(f"StateStore initialized with local directory: {self.local_dir}")
53 |     
   | ^^^^
54 |     def _get_key(self, source_name: str) -> str:
55 |         """Get S3 key or local file path for a source."""
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/state_store.py:57:1
   |
55 |         """Get S3 key or local file path for a source."""
56 |         return f"{source_name}.json"
57 |     
   | ^^^^
58 |     def get_watermark(self, source_name: str) -> Optional[str]:
59 |         """
   |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/utils/state_store.py:58:50
   |
56 |         return f"{source_name}.json"
57 |     
58 |     def get_watermark(self, source_name: str) -> Optional[str]:
   |                                                  ^^^^^^^^^^^^^
59 |         """
60 |         Get watermark value for a source.
   |
help: Convert to `X | None`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/state_store.py:61:1
   |
59 |         """
60 |         Get watermark value for a source.
61 |         
   | ^^^^^^^^
62 |         Args:
63 |             source_name: Name of the data source (e.g., 'crm_contacts')
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/state_store.py:64:1
   |
62 |         Args:
63 |             source_name: Name of the data source (e.g., 'crm_contacts')
64 |             
   | ^^^^^^^^^^^^
65 |         Returns:
66 |             ISO8601 timestamp string or None if not found
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/state_store.py:69:1
   |
67 |         """
68 |         key = self._get_key(source_name)
69 |         
   | ^^^^^^^^
70 |         if self.use_s3:
71 |             try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/state_store.py:93:1
   |
91 |                 logger.info(f"No watermark found for {source_name} at {file_path} (first run)")
92 |                 return None
93 |             
   | ^^^^^^^^^^^^
94 |             try:
95 |                 with open(file_path, 'r') as f:
   |
help: Remove whitespace from blank line

UP015 [*] Unnecessary mode argument
  --> src/project_a/pyspark_interview_project/utils/state_store.py:95:38
   |
94 |             try:
95 |                 with open(file_path, 'r') as f:
   |                                      ^^^
96 |                     data = json.load(f)
97 |                 watermark = data.get('watermark')
   |
help: Remove mode argument

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/state_store.py:103:1
    |
101 |                 logger.warning(f"Failed to read local watermark: {e}")
102 |                 return None
103 |     
    | ^^^^
104 |     def set_watermark(self, source_name: str, value: str) -> None:
105 |         """
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/state_store.py:107:1
    |
105 |         """
106 |         Set watermark value for a source.
107 |         
    | ^^^^^^^^
108 |         Args:
109 |             source_name: Name of the data source
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/state_store.py:118:1
    |
116 |             "updated_at": datetime.utcnow().isoformat() + "Z"
117 |         }
118 |         
    | ^^^^^^^^
119 |         if self.use_s3:
120 |             try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/state_store.py:141:1
    |
139 |                 logger.error(f"Failed to write local watermark: {e}")
140 |                 raise
141 |     
    | ^^^^
142 |     def get_checkpoint(self, stream_name: str) -> Optional[Dict[str, Any]]:
143 |         """
    |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/utils/state_store.py:142:51
    |
140 |                 raise
141 |     
142 |     def get_checkpoint(self, stream_name: str) -> Optional[Dict[str, Any]]:
    |                                                   ^^^^^^^^^^^^^^^^^^^^^^^^
143 |         """
144 |         Get checkpoint data for a streaming job.
    |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/utils/state_store.py:142:60
    |
140 |                 raise
141 |     
142 |     def get_checkpoint(self, stream_name: str) -> Optional[Dict[str, Any]]:
    |                                                            ^^^^
143 |         """
144 |         Get checkpoint data for a streaming job.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/state_store.py:145:1
    |
143 |         """
144 |         Get checkpoint data for a streaming job.
145 |         
    | ^^^^^^^^
146 |         Args:
147 |             stream_name: Name of the stream (e.g., 'kafka_orders')
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/state_store.py:148:1
    |
146 |         Args:
147 |             stream_name: Name of the stream (e.g., 'kafka_orders')
148 |             
    | ^^^^^^^^^^^^
149 |         Returns:
150 |             Checkpoint dictionary or None
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/state_store.py:153:1
    |
151 |         """
152 |         key = f"{stream_name}_checkpoint.json"
153 |         
    | ^^^^^^^^
154 |         if self.use_s3:
155 |             try:
    |
help: Remove whitespace from blank line

UP015 [*] Unnecessary mode argument
   --> src/project_a/pyspark_interview_project/utils/state_store.py:169:38
    |
167 |                 return None
168 |             try:
169 |                 with open(file_path, 'r') as f:
    |                                      ^^^
170 |                     return json.load(f)
171 |             except Exception as e:
    |
help: Remove mode argument

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/state_store.py:174:1
    |
172 |                 logger.warning(f"Failed to read checkpoint: {e}")
173 |                 return None
174 |     
    | ^^^^
175 |     def set_checkpoint(self, stream_name: str, checkpoint_data: Dict[str, Any]) -> None:
176 |         """
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/utils/state_store.py:175:65
    |
173 |                 return None
174 |     
175 |     def set_checkpoint(self, stream_name: str, checkpoint_data: Dict[str, Any]) -> None:
    |                                                                 ^^^^
176 |         """
177 |         Set checkpoint data for a streaming job.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/state_store.py:178:1
    |
176 |         """
177 |         Set checkpoint data for a streaming job.
178 |         
    | ^^^^^^^^
179 |         Args:
180 |             stream_name: Name of the stream
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/state_store.py:189:1
    |
187 |             "updated_at": datetime.utcnow().isoformat() + "Z"
188 |         }
189 |         
    | ^^^^^^^^
190 |         if self.use_s3:
191 |             try:
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/pyspark_interview_project/utils/state_store.py:211:29
    |
211 | def get_state_store(config: Dict[str, Any]) -> StateStore:
    |                             ^^^^
212 |     """
213 |     Factory function to create StateStore from config.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/state_store.py:214:1
    |
212 |     """
213 |     Factory function to create StateStore from config.
214 |     
    | ^^^^
215 |     Args:
216 |         config: Configuration dictionary
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/state_store.py:217:1
    |
215 |     Args:
216 |         config: Configuration dictionary
217 |         
    | ^^^^^^^^
218 |     Returns:
219 |         StateStore instance
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/state_store.py:223:1
    |
221 |     bucket = config.get('data_lake', {}).get('bucket') or config.get('s3', {}).get('bucket')
222 |     prefix = config.get('data_lake', {}).get('state_prefix', '_state')
223 |     
    | ^^^^
224 |     return StateStore(bucket=bucket, prefix=prefix)
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
 --> src/project_a/pyspark_interview_project/utils/watermark.py:5:1
  |
3 |   Stores/retrieves ISO8601 timestamps in a lightweight JSON file under checkpoints.
4 |   """
5 | / import os
6 | | import json
7 | | from datetime import datetime
8 | | from typing import Optional
  | |___________________________^
  |
help: Organize imports

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/utils/watermark.py:16:46
   |
16 | def load_watermark(root: str, table: str) -> Optional[str]:
   |                                              ^^^^^^^^^^^^^
17 |     fp = _path(root, table)
18 |     if not os.path.exists(fp):
   |
help: Convert to `X | None`

UP015 [*] Unnecessary mode argument
  --> src/project_a/pyspark_interview_project/utils/watermark.py:21:23
   |
19 |         return None
20 |     try:
21 |         with open(fp, "r") as f:
   |                       ^^^
22 |             data = json.load(f)
23 |         return data.get("last_modified_at")
   |
help: Remove mode argument

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/utils/watermark_utils.py:8:1
   |
 6 |   """
 7 |
 8 | / import os
 9 | | import json
10 | | import logging
11 | | from datetime import datetime, timezone
12 | | from typing import Optional
13 | | from pathlib import Path
14 | | from pyspark.sql import SparkSession, DataFrame
15 | | from pyspark.sql.functions import max as spark_max, lit, current_timestamp
   | |__________________________________________________________________________^
16 |
17 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

F401 [*] `os` imported but unused
  --> src/project_a/pyspark_interview_project/utils/watermark_utils.py:8:8
   |
 6 | """
 7 |
 8 | import os
   |        ^^
 9 | import json
10 | import logging
   |
help: Remove unused import: `os`

F401 [*] `pyspark.sql.functions.lit` imported but unused
  --> src/project_a/pyspark_interview_project/utils/watermark_utils.py:15:53
   |
13 | from pathlib import Path
14 | from pyspark.sql import SparkSession, DataFrame
15 | from pyspark.sql.functions import max as spark_max, lit, current_timestamp
   |                                                     ^^^
16 |
17 | logger = logging.getLogger(__name__)
   |
help: Remove unused import

F401 [*] `pyspark.sql.functions.current_timestamp` imported but unused
  --> src/project_a/pyspark_interview_project/utils/watermark_utils.py:15:58
   |
13 | from pathlib import Path
14 | from pyspark.sql import SparkSession, DataFrame
15 | from pyspark.sql.functions import max as spark_max, lit, current_timestamp
   |                                                          ^^^^^^^^^^^^^^^^^
16 |
17 | logger = logging.getLogger(__name__)
   |
help: Remove unused import

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/pyspark_interview_project/utils/watermark_utils.py:20:89
   |
20 | def get_watermark(source_name: str, config: dict = None, spark: SparkSession = None) -> Optional[datetime]:
   |                                                                                         ^^^^^^^^^^^^^^^^^^
21 |     """
22 |     Get the latest watermark for a data source.
   |
help: Convert to `X | None`

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/watermark_utils.py:23:1
   |
21 |     """
22 |     Get the latest watermark for a data source.
23 |     
   | ^^^^
24 |     Args:
25 |         source_name: Name of the data source (e.g., 'redshift_behavior', 'snowflake_orders')
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/watermark_utils.py:28:1
   |
26 |         config: Configuration dict with S3 paths
27 |         spark: SparkSession (optional, for S3 reads)
28 |         
   | ^^^^^^^^
29 |     Returns:
30 |         Latest watermark datetime or None if not found
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/watermark_utils.py:44:1
   |
42 |             except Exception as e:
43 |                 logger.warning(f"Could not read watermark from S3: {e}, falling back to local")
44 |     
   | ^^^^
45 |     # Fallback to local CSV/JSON
46 |     local_path = Path("data/checkpoints/watermarks")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/watermark_utils.py:48:1
   |
46 |     local_path = Path("data/checkpoints/watermarks")
47 |     local_path.mkdir(parents=True, exist_ok=True)
48 |     
   | ^^^^
49 |     watermark_file = local_path / f"{source_name}_watermark.json"
50 |     if watermark_file.exists():
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/watermark_utils.py:59:1
   |
57 |         except Exception as e:
58 |             logger.warning(f"Could not read local watermark: {e}")
59 |     
   | ^^^^
60 |     return None
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/watermark_utils.py:66:1
   |
64 |     """
65 |     Update watermark for a data source.
66 |     
   | ^^^^
67 |     Args:
68 |         source_name: Name of the data source
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/pyspark_interview_project/utils/watermark_utils.py:73:5
   |
71 |         spark: SparkSession
72 |     """
73 |     from pyspark.sql.types import StructType, StructField, StringType, TimestampType
   |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
74 |     
75 |     schema = StructType([
   |
help: Organize imports

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/watermark_utils.py:74:1
   |
72 |     """
73 |     from pyspark.sql.types import StructType, StructField, StringType, TimestampType
74 |     
   | ^^^^
75 |     schema = StructType([
76 |         StructField("source_name", StringType(), False),
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/watermark_utils.py:80:1
   |
78 |         StructField("updated_at", TimestampType(), False)
79 |     ])
80 |     
   | ^^^^
81 |     watermark_df = spark.createDataFrame(
82 |         [(source_name, value, datetime.now(timezone.utc))],
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/watermark_utils.py:85:1
   |
83 |         schema=schema
84 |     )
85 |     
   | ^^^^
86 |     watermark_path = None
87 |     if config:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/watermark_utils.py:89:1
   |
87 |     if config:
88 |         watermark_path = config.get("data_lake", {}).get("watermark_prefix", "s3://bucket/meta/watermarks")
89 |     
   | ^^^^
90 |     if watermark_path and (watermark_path.startswith("s3://") or watermark_path.startswith("s3a://")) and spark:
91 |         # Write to S3 Delta table
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/pyspark_interview_project/utils/watermark_utils.py:94:1
   |
92 |         try:
93 |             full_path = f"{watermark_path}/{source_name}"
94 |             
   | ^^^^^^^^^^^^
95 |             # Check if Delta table exists
96 |             from delta.tables import DeltaTable
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/watermark_utils.py:107:1
    |
105 |                 # Create new table
106 |                 watermark_df.write.format("delta").mode("overwrite").save(full_path)
107 |             
    | ^^^^^^^^^^^^
108 |             logger.info(f"âœ… Updated watermark for {source_name} to {value.isoformat()} in S3")
109 |             return
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/watermark_utils.py:112:1
    |
110 |         except Exception as e:
111 |             logger.warning(f"Could not write watermark to S3: {e}, falling back to local")
112 |     
    | ^^^^
113 |     # Fallback to local JSON
114 |     local_path = Path("data/checkpoints/watermarks")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/watermark_utils.py:116:1
    |
114 |     local_path = Path("data/checkpoints/watermarks")
115 |     local_path.mkdir(parents=True, exist_ok=True)
116 |     
    | ^^^^
117 |     watermark_file = local_path / f"{source_name}_watermark.json"
118 |     with open(watermark_file, "w") as f:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/watermark_utils.py:124:1
    |
122 |             "updated_at": datetime.now(timezone.utc).isoformat()
123 |         }, f, indent=2)
124 |     
    | ^^^^
125 |     logger.info(f"âœ… Updated watermark for {source_name} to {value.isoformat()} (local)")
    |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/pyspark_interview_project/utils/watermark_utils.py:128:90
    |
128 | def get_latest_timestamp_from_df(df: DataFrame, timestamp_col: str = "_ingestion_ts") -> Optional[datetime]:
    |                                                                                          ^^^^^^^^^^^^^^^^^^
129 |     """
130 |     Extract the latest timestamp from a DataFrame.
    |
help: Convert to `X | None`

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/watermark_utils.py:131:1
    |
129 |     """
130 |     Extract the latest timestamp from a DataFrame.
131 |     
    | ^^^^
132 |     Args:
133 |         df: DataFrame with timestamp column
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/watermark_utils.py:135:1
    |
133 |         df: DataFrame with timestamp column
134 |         timestamp_col: Name of timestamp column
135 |         
    | ^^^^^^^^
136 |     Returns:
137 |         Latest timestamp or None
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/pyspark_interview_project/utils/watermark_utils.py:142:1
    |
140 |         if df.isEmpty():
141 |             return None
142 |         
    | ^^^^^^^^
143 |         max_ts = df.agg(spark_max(col(timestamp_col))).collect()[0][0]
144 |         return max_ts
    |
help: Remove whitespace from blank line

F821 Undefined name `col`
   --> src/project_a/pyspark_interview_project/utils/watermark_utils.py:143:35
    |
141 |             return None
142 |         
143 |         max_ts = df.agg(spark_max(col(timestamp_col))).collect()[0][0]
    |                                   ^^^
144 |         return max_ts
145 |     except Exception as e:
    |

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/utils/cloudwatch_metrics.py:7:1
   |
 5 |   """
 6 |
 7 | / import logging
 8 | | from typing import Optional, Dict
 9 | |
10 | | import boto3
11 | | from botocore.exceptions import ClientError
   | |___________________________________________^
12 |
13 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/utils/cloudwatch_metrics.py:8:1
   |
 7 | import logging
 8 | from typing import Optional, Dict
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 9 |
10 | import boto3
   |

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/utils/cloudwatch_metrics.py:20:17
   |
18 |     metric_name: str,
19 |     value: float,
20 |     dimensions: Optional[Dict[str, str]] = None,
   |                 ^^^^^^^^^^^^^^^^^^^^^^^^
21 |     unit: str = "Count",
22 | ) -> None:
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/utils/cloudwatch_metrics.py:20:26
   |
18 |     metric_name: str,
19 |     value: float,
20 |     dimensions: Optional[Dict[str, str]] = None,
   |                          ^^^^
21 |     unit: str = "Count",
22 | ) -> None:
   |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/utils/cloudwatch_metrics.py:56:79
   |
55 | def emit_job_success(
56 |     job_name: str, duration_seconds: float, env: str = "dev", rows_processed: Optional[int] = None
   |                                                                               ^^^^^^^^^^^^^
57 | ) -> None:
58 |     """
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/utils/cloudwatch_metrics.py:94:67
   |
94 | def emit_job_failure(job_name: str, env: str = "dev", error_type: Optional[str] = None) -> None:
   |                                                                   ^^^^^^^^^^^^^
95 |     """
96 |     Emit metrics for a failed job run.
   |
help: Convert to `X | None`

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/utils/config.py:7:1
   |
 5 |   """
 6 |
 7 | / from __future__ import annotations
 8 | |
 9 | | import os
10 | | import re
11 | | from pathlib import Path
12 | | from typing import Any, Optional, Dict
13 | |
14 | | import boto3
15 | | import yaml
16 | | from botocore.exceptions import ClientError
   | |___________________________________________^
17 |
18 |   # Matches ${ENV:VAR} or ${SECRET:scope:key} or ${paths.bronze_root} style variable references
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/utils/config.py:12:1
   |
10 | import re
11 | from pathlib import Path
12 | from typing import Any, Optional, Dict
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
13 |
14 | import boto3
   |

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/utils/config.py:23:40
   |
23 | def _resolve_value(value: Any, config: Optional[Dict[str, Any]] = None) -> Any:
   |                                        ^^^^^^^^^^^^^^^^^^^^^^^^
24 |     """Resolve secret and variable references in a value."""
25 |     if not isinstance(value, str):
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/utils/config.py:23:49
   |
23 | def _resolve_value(value: Any, config: Optional[Dict[str, Any]] = None) -> Any:
   |                                                 ^^^^
24 |     """Resolve secret and variable references in a value."""
25 |     if not isinstance(value, str):
   |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/utils/config.py:59:40
   |
59 | def _resolve_secrets(obj: Any, config: Optional[Dict[str, Any]] = None) -> Any:
   |                                        ^^^^^^^^^^^^^^^^^^^^^^^^
60 |     """Recursively resolve secrets and variables in config."""
61 |     if isinstance(obj, dict):
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/utils/config.py:59:49
   |
59 | def _resolve_secrets(obj: Any, config: Optional[Dict[str, Any]] = None) -> Any:
   |                                                 ^^^^
60 |     """Recursively resolve secrets and variables in config."""
61 |     if isinstance(obj, dict):
   |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/utils/config.py:71:56
   |
71 | def load_config(config_path: str, env: str = "dev") -> Dict[str, Any]:
   |                                                        ^^^^
72 |     """
73 |     Load configuration from file (local or S3).
   |
help: Replace with `dict`

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
  --> src/project_a/utils/config.py:94:13
   |
92 |             cfg = yaml.safe_load(content)
93 |         except ClientError as e:
94 |             raise ValueError(f"Failed to load config from S3: {e}")
   |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
95 |     else:
96 |         # Local file
   |

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/utils/config.py:109:39
    |
109 | def load_config_resolved(config_path: Optional[str] = None, env: Optional[str] = None) -> Dict[str, Any]:
    |                                       ^^^^^^^^^^^^^
110 |     """
111 |     Load config from a path, or pick by environment if path not provided.
    |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/utils/config.py:109:66
    |
109 | def load_config_resolved(config_path: Optional[str] = None, env: Optional[str] = None) -> Dict[str, Any]:
    |                                                                  ^^^^^^^^^^^^^
110 |     """
111 |     Load config from a path, or pick by environment if path not provided.
    |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/utils/config.py:109:91
    |
109 | def load_config_resolved(config_path: Optional[str] = None, env: Optional[str] = None) -> Dict[str, Any]:
    |                                                                                           ^^^^
110 |     """
111 |     Load config from a path, or pick by environment if path not provided.
    |
help: Replace with `dict`

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/utils/contracts.py:8:1
   |
 6 |   """
 7 |
 8 | / from __future__ import annotations
 9 | |
10 | | import json
11 | | import logging
12 | | from dataclasses import dataclass
13 | | from pathlib import Path
14 | |
15 | | from pyspark.sql import DataFrame
16 | | from pyspark.sql import functions as F
17 | | from pyspark.sql.types import StructType, StructField
18 | | from typing import List, Dict, Optional
   | |_______________________________________^
19 |
20 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

F401 [*] `pyspark.sql.types.StructField` imported but unused
  --> src/project_a/utils/contracts.py:17:43
   |
15 | from pyspark.sql import DataFrame
16 | from pyspark.sql import functions as F
17 | from pyspark.sql.types import StructType, StructField
   |                                           ^^^^^^^^^^^
18 | from typing import List, Dict, Optional
   |
help: Remove unused import: `pyspark.sql.types.StructField`

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/utils/contracts.py:18:1
   |
16 | from pyspark.sql import functions as F
17 | from pyspark.sql.types import StructType, StructField
18 | from typing import List, Dict, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
19 |
20 | logger = logging.getLogger(__name__)
   |

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/utils/contracts.py:18:1
   |
16 | from pyspark.sql import functions as F
17 | from pyspark.sql.types import StructType, StructField
18 | from typing import List, Dict, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
19 |
20 | logger = logging.getLogger(__name__)
   |

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/utils/contracts.py:28:18
   |
27 |     name: str
28 |     primary_key: List[str]
   |                  ^^^^
29 |     columns: Dict[str, str]
30 |     required: List[str]
   |
help: Replace with `list`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/utils/contracts.py:29:14
   |
27 |     name: str
28 |     primary_key: List[str]
29 |     columns: Dict[str, str]
   |              ^^^^
30 |     required: List[str]
   |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/utils/contracts.py:30:15
   |
28 |     primary_key: List[str]
29 |     columns: Dict[str, str]
30 |     required: List[str]
   |               ^^^^
31 |
32 |     @classmethod
   |
help: Replace with `list`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/utils/contracts.py:43:50
   |
43 | def load_contract(contract_name: str, base_path: Optional[str] = None) -> TableContract:
   |                                                  ^^^^^^^^^^^^^
44 |     """
45 |     Load a contract from config/schema_definitions/bronze/.
   |
help: Convert to `X | None`

UP006 [*] Use `list` instead of `List` for type annotation
  --> src/project_a/utils/contracts.py:83:43
   |
83 | def enforce_not_null(df: DataFrame, cols: List[str]) -> DataFrame:
   |                                           ^^^^
84 |     """
85 |     Filter out rows where any of the given columns are NULL.
   |
help: Replace with `list`

W293 Blank line contains whitespace
   --> src/project_a/utils/contracts.py:131:1
    |
129 |     """
130 |     Validate that a DataFrame matches an expected StructType schema.
131 |     
    | ^^^^
132 |     Checks:
133 |     - Required columns exist
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/utils/contracts.py:135:1
    |
133 |     - Required columns exist
134 |     - Data types match (using simpleString() comparison)
135 |     
    | ^^^^
136 |     Args:
137 |         df: DataFrame to validate
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/utils/contracts.py:141:1
    |
139 |         table_name: Logical table name for error messages
140 |         fail_on_mismatch: If True, raise exception on mismatch; if False, log warning
141 |         
    | ^^^^^^^^
142 |     Raises:
143 |         ValueError: If schema validation fails and fail_on_mismatch=True
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/utils/contracts.py:148:1
    |
146 |     df_cols = {f.name: f.dataType.simpleString() for f in df_schema.fields}
147 |     expected_cols = {f.name: f.dataType.simpleString() for f in expected_schema.fields}
148 |     
    | ^^^^
149 |     # Check for missing columns
150 |     missing_cols = set(expected_cols.keys()) - set(df_cols.keys())
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/utils/contracts.py:158:1
    |
156 |             logger.warning(error_msg)
157 |             return
158 |     
    | ^^^^
159 |     # Check for type mismatches in common columns
160 |     type_mismatches = []
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/utils/contracts.py:167:1
    |
165 |                     f"{col_name}: expected {expected_cols[col_name]}, got {df_cols[col_name]}"
166 |                 )
167 |     
    | ^^^^
168 |     if type_mismatches:
169 |         error_msg = f"Schema validation FAILED for {table_name}: Type mismatches: {', '.join(type_mismatches)}"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/utils/contracts.py:175:1
    |
173 |             logger.warning(error_msg)
174 |             return
175 |     
    | ^^^^
176 |     logger.info(f"âœ… Schema validation PASSED for {table_name}: {len(expected_cols)} columns match")
    |
help: Remove whitespace from blank line

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/utils/error_lanes.py:10:1
   |
 8 | import logging
 9 | from datetime import datetime
10 | from typing import Any, Dict, List, Tuple
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
11 |
12 | from pyspark.sql import DataFrame, SparkSession
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> src/project_a/utils/error_lanes.py:10:1
   |
 8 | import logging
 9 | from datetime import datetime
10 | from typing import Any, Dict, List, Tuple
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
11 |
12 | from pyspark.sql import DataFrame, SparkSession
   |

UP035 `typing.Tuple` is deprecated, use `tuple` instead
  --> src/project_a/utils/error_lanes.py:10:1
   |
 8 | import logging
 9 | from datetime import datetime
10 | from typing import Any, Dict, List, Tuple
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
11 |
12 | from pyspark.sql import DataFrame, SparkSession
   |

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/utils/error_lanes.py:26:53
   |
24 |     """
25 |
26 |     def __init__(self, spark: SparkSession, config: Dict[str, Any]):
   |                                                     ^^^^
27 |         """
28 |         Initialize error lane handler.
   |
help: Replace with `dict`

UP006 [*] Use `tuple` instead of `Tuple` for type annotation
  --> src/project_a/utils/error_lanes.py:70:10
   |
68 |     def quarantine_bad_rows(
69 |         self, df: DataFrame, validation_errors: DataFrame, layer: str, table: str, run_id: str
70 |     ) -> Tuple[DataFrame, Dict[str, Any]]:
   |          ^^^^^
71 |         """
72 |         Quarantine bad rows to error lane.
   |
help: Replace with `tuple`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/utils/error_lanes.py:70:27
   |
68 |     def quarantine_bad_rows(
69 |         self, df: DataFrame, validation_errors: DataFrame, layer: str, table: str, run_id: str
70 |     ) -> Tuple[DataFrame, Dict[str, Any]]:
   |                           ^^^^
71 |         """
72 |         Quarantine bad rows to error lane.
   |
help: Replace with `dict`

UP006 [*] Use `list` instead of `List` for type annotation
   --> src/project_a/utils/error_lanes.py:138:24
    |
136 |         table: str,
137 |         run_id: str,
138 |         required_cols: List[str],
    |                        ^^^^
139 |         date_col: str = None,
140 |     ) -> Tuple[DataFrame, Dict[str, Any]]:
    |
help: Replace with `list`

UP006 [*] Use `tuple` instead of `Tuple` for type annotation
   --> src/project_a/utils/error_lanes.py:140:10
    |
138 |         required_cols: List[str],
139 |         date_col: str = None,
140 |     ) -> Tuple[DataFrame, Dict[str, Any]]:
    |          ^^^^^
141 |         """
142 |         Quarantine rows with null values in required columns.
    |
help: Replace with `tuple`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/utils/error_lanes.py:140:27
    |
138 |         required_cols: List[str],
139 |         date_col: str = None,
140 |     ) -> Tuple[DataFrame, Dict[str, Any]]:
    |                           ^^^^
141 |         """
142 |         Quarantine rows with null values in required columns.
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/utils/error_lanes.py:211:74
    |
210 | def create_error_lane_metadata(
211 |     layer: str, table: str, run_id: str, error_type: str, error_details: Dict[str, Any]
    |                                                                          ^^^^
212 | ) -> Dict[str, Any]:
213 |     """
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/utils/error_lanes.py:212:6
    |
210 | def create_error_lane_metadata(
211 |     layer: str, table: str, run_id: str, error_type: str, error_details: Dict[str, Any]
212 | ) -> Dict[str, Any]:
    |      ^^^^
213 |     """
214 |     Create error lane metadata document.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/utils/path_resolver.py:9:1
   |
 7 | Usage:
 8 |     from project_a.utils.path_resolver import resolve_data_path
 9 |     
   | ^^^^
10 |     # Works on both local and AWS
11 |     bronze_path = resolve_data_path(config, "bronze", "crm", "accounts")
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/utils/path_resolver.py:15:1
   |
13 |       # AWS: s3://bucket/bronze/crm/accounts
14 |   """
15 | / import logging
16 | | import os
17 | | from pathlib import Path
18 | | from typing import Dict, Any, Optional
   | |______________________________________^
19 |
20 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

F401 [*] `os` imported but unused
  --> src/project_a/utils/path_resolver.py:16:8
   |
14 | """
15 | import logging
16 | import os
   |        ^^
17 | from pathlib import Path
18 | from typing import Dict, Any, Optional
   |
help: Remove unused import: `os`

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/utils/path_resolver.py:18:1
   |
16 | import os
17 | from pathlib import Path
18 | from typing import Dict, Any, Optional
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
19 |
20 | logger = logging.getLogger(__name__)
   |

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/utils/path_resolver.py:24:13
   |
23 | def resolve_data_path(
24 |     config: Dict[str, Any],
   |             ^^^^
25 |     layer: str,
26 |     source: Optional[str] = None,
   |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/utils/path_resolver.py:26:13
   |
24 |     config: Dict[str, Any],
25 |     layer: str,
26 |     source: Optional[str] = None,
   |             ^^^^^^^^^^^^^
27 |     table: Optional[str] = None
28 | ) -> str:
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/utils/path_resolver.py:27:12
   |
25 |     layer: str,
26 |     source: Optional[str] = None,
27 |     table: Optional[str] = None
   |            ^^^^^^^^^^^^^
28 | ) -> str:
29 |     """
   |
help: Convert to `X | None`

W293 Blank line contains whitespace
  --> src/project_a/utils/path_resolver.py:31:1
   |
29 |     """
30 |     Resolve a logical data path to physical path (local or S3).
31 |     
   | ^^^^
32 |     Args:
33 |         config: Configuration dictionary with paths section
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/utils/path_resolver.py:37:1
   |
35 |         source: Source system name (for bronze layer, e.g., 'crm', 'snowflake')
36 |         table: Table name (e.g., 'customers', 'orders')
37 |     
   | ^^^^
38 |     Returns:
39 |         Physical path (file:// for local, s3:// for AWS)
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/utils/path_resolver.py:40:1
   |
38 |     Returns:
39 |         Physical path (file:// for local, s3:// for AWS)
40 |     
   | ^^^^
41 |     Examples:
42 |         >>> resolve_data_path(config, "bronze", "crm", "accounts")
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/utils/path_resolver.py:44:1
   |
42 |         >>> resolve_data_path(config, "bronze", "crm", "accounts")
43 |         's3://bucket/bronze/crm/accounts'  # or 'file:///path/to/bronze/crm/accounts'
44 |         
   | ^^^^^^^^
45 |         >>> resolve_data_path(config, "silver", table="customers")
46 |         's3://bucket/silver/customers'  # or 'file:///path/to/silver/customers'
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/utils/path_resolver.py:50:1
   |
48 |     paths_cfg = config.get("paths", {})
49 |     environment = config.get("environment", config.get("env", "local"))
50 |     
   | ^^^^
51 |     # Get base path for layer
52 |     layer_key = f"{layer}_root"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/utils/path_resolver.py:54:1
   |
52 |     layer_key = f"{layer}_root"
53 |     base_path = paths_cfg.get(layer_key, "")
54 |     
   | ^^^^
55 |     if not base_path:
56 |         raise ValueError(f"Path configuration missing for {layer_key}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/utils/path_resolver.py:57:1
   |
55 |     if not base_path:
56 |         raise ValueError(f"Path configuration missing for {layer_key}")
57 |     
   | ^^^^
58 |     # Build path components
59 |     path_parts = [base_path.rstrip("/")]
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/utils/path_resolver.py:60:1
   |
58 |     # Build path components
59 |     path_parts = [base_path.rstrip("/")]
60 |     
   | ^^^^
61 |     if source:
62 |         path_parts.append(source)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/utils/path_resolver.py:65:1
   |
63 |     if table:
64 |         path_parts.append(table)
65 |     
   | ^^^^
66 |     physical_path = "/".join(path_parts)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/utils/path_resolver.py:67:1
   |
66 |     physical_path = "/".join(path_parts)
67 |     
   | ^^^^
68 |     # Ensure proper protocol and absolute paths for local
69 |     if not physical_path.startswith(("s3://", "file://", "s3a://")):
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/utils/path_resolver.py:82:1
   |
80 |             # Default to S3 for AWS
81 |             physical_path = f"s3://{physical_path}"
82 |     
   | ^^^^
83 |     logger.debug(f"Resolved {layer}/{source}/{table} â†’ {physical_path}")
84 |     return physical_path
   |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/utils/path_resolver.py:88:13
   |
87 | def resolve_source_file_path(
88 |     config: Dict[str, Any],
   |             ^^^^
89 |     source_name: str,
90 |     file_key: str
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/utils/path_resolver.py:94:1
   |
92 |     """
93 |     Resolve a source file path from config.
94 |     
   | ^^^^
95 |     Works identically on local and AWS by handling relative paths correctly.
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/utils/path_resolver.py:96:1
   |
95 |     Works identically on local and AWS by handling relative paths correctly.
96 |     
   | ^^^^
97 |     Args:
98 |         config: Configuration dictionary
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/utils/path_resolver.py:101:1
    |
 99 |         source_name: Source system name (e.g., 'crm', 'snowflake', 'fx')
100 |         file_key: File key from sources.{source_name}.files (e.g., 'accounts', 'orders')
101 |     
    | ^^^^
102 |     Returns:
103 |         Full path to source file (file:// for local, s3:// for AWS)
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/utils/path_resolver.py:104:1
    |
102 |     Returns:
103 |         Full path to source file (file:// for local, s3:// for AWS)
104 |     
    | ^^^^
105 |     Examples:
106 |         >>> resolve_source_file_path(config, "crm", "accounts")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/utils/path_resolver.py:112:1
    |
110 |     sources_cfg = config.get("sources", {})
111 |     source_cfg = sources_cfg.get(source_name, {})
112 |     
    | ^^^^
113 |     base_path = source_cfg.get("base_path", "")
114 |     files = source_cfg.get("files", {})
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/utils/path_resolver.py:116:1
    |
114 |     files = source_cfg.get("files", {})
115 |     filename = files.get(file_key, "")
116 |     
    | ^^^^
117 |     if not base_path:
118 |         raise ValueError(f"Source {source_name} has no base_path in config")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/utils/path_resolver.py:121:1
    |
119 |     if not filename:
120 |         raise ValueError(f"Source {source_name} has no file for key {file_key}")
121 |     
    | ^^^^
122 |     full_path = f"{base_path.rstrip('/')}/{filename}"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/utils/path_resolver.py:123:1
    |
122 |     full_path = f"{base_path.rstrip('/')}/{filename}"
123 |     
    | ^^^^
124 |     # Handle local vs AWS path resolution
125 |     environment = config.get("environment", config.get("env", "local"))
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/utils/path_resolver.py:127:1
    |
125 |     environment = config.get("environment", config.get("env", "local"))
126 |     is_local = environment in ("local", "dev_local")
127 |     
    | ^^^^
128 |     if is_local:
129 |         # Local: convert relative paths to absolute file:// paths
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/utils/path_resolver.py:142:1
    |
140 |         if not full_path.startswith(("s3://", "s3a://")):
141 |             full_path = f"s3://{full_path}"
142 |     
    | ^^^^
143 |     logger.debug(f"Resolved source file {source_name}.{file_key} â†’ {full_path} (env={environment})")
144 |     return full_path
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/utils/path_resolver.py:147:34
    |
147 | def is_local_environment(config: Dict[str, Any]) -> bool:
    |                                  ^^^^
148 |     """
149 |     Check if running in local environment.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/utils/path_resolver.py:150:1
    |
148 |     """
149 |     Check if running in local environment.
150 |     
    | ^^^^
151 |     Args:
152 |         config: Configuration dictionary
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/utils/path_resolver.py:153:1
    |
151 |     Args:
152 |         config: Configuration dictionary
153 |     
    | ^^^^
154 |     Returns:
155 |         True if local, False if AWS/EMR
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/utils/path_resolver.py:161:41
    |
161 | def get_kafka_bootstrap_servers(config: Dict[str, Any]) -> str:
    |                                         ^^^^
162 |     """
163 |     Get Kafka bootstrap servers from config (local or MSK).
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/utils/path_resolver.py:164:1
    |
162 |     """
163 |     Get Kafka bootstrap servers from config (local or MSK).
164 |     
    | ^^^^
165 |     Args:
166 |         config: Configuration dictionary
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/utils/path_resolver.py:167:1
    |
165 |     Args:
166 |         config: Configuration dictionary
167 |     
    | ^^^^
168 |     Returns:
169 |         Bootstrap servers string (e.g., 'localhost:9092' or 'broker1:9092,broker2:9092')
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/utils/path_resolver.py:172:1
    |
170 |     """
171 |     kafka_cfg = config.get("sources", {}).get("kafka", {})
172 |     
    | ^^^^
173 |     if is_local_environment(config):
174 |         # Local Kafka
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/utils/run_audit.py:8:1
   |
 6 |   """
 7 |
 8 | / import json
 9 | | import logging
10 | | import uuid
11 | | from datetime import datetime
12 | | from typing import Any, Optional, Dict
13 | |
14 | | import boto3
15 | | from botocore.exceptions import ClientError
   | |___________________________________________^
16 |
17 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> src/project_a/utils/run_audit.py:12:1
   |
10 | import uuid
11 | from datetime import datetime
12 | from typing import Any, Optional, Dict
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
13 |
14 | import boto3
   |

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/utils/run_audit.py:29:13
   |
27 |     rows_out: int,
28 |     status: str,
29 |     run_id: Optional[str] = None,
   |             ^^^^^^^^^^^^^
30 |     error_message: Optional[str] = None,
31 |     duration_ms: Optional[float] = None,
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/utils/run_audit.py:30:20
   |
28 |     status: str,
29 |     run_id: Optional[str] = None,
30 |     error_message: Optional[str] = None,
   |                    ^^^^^^^^^^^^^
31 |     duration_ms: Optional[float] = None,
32 |     config: Optional[Dict[str, Any]] = None,
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/utils/run_audit.py:31:18
   |
29 |     run_id: Optional[str] = None,
30 |     error_message: Optional[str] = None,
31 |     duration_ms: Optional[float] = None,
   |                  ^^^^^^^^^^^^^^^
32 |     config: Optional[Dict[str, Any]] = None,
33 | ) -> str:
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/utils/run_audit.py:32:13
   |
30 |     error_message: Optional[str] = None,
31 |     duration_ms: Optional[float] = None,
32 |     config: Optional[Dict[str, Any]] = None,
   |             ^^^^^^^^^^^^^^^^^^^^^^^^
33 | ) -> str:
34 |     """
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/utils/run_audit.py:32:22
   |
30 |     error_message: Optional[str] = None,
31 |     duration_ms: Optional[float] = None,
32 |     config: Optional[Dict[str, Any]] = None,
   |                      ^^^^
33 | ) -> str:
34 |     """
   |
help: Replace with `dict`

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/utils/run_audit.py:105:49
    |
104 | def read_run_audit(
105 |     bucket: str, job_name: str, env: str, date: Optional[str] = None
    |                                                 ^^^^^^^^^^^^^
106 | ) -> list:
107 |     """
    |
help: Convert to `X | None`

I001 [*] Import block is un-sorted or un-formatted
  --> src/project_a/utils/spark_session.py:6:1
   |
 4 |   Creates and configures SparkSession with Delta Lake support.
 5 |   """
 6 | / import logging
 7 | | import os
 8 | | from typing import Dict, Optional
 9 | | from pyspark.sql import SparkSession
   | |____________________________________^
10 |
11 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
 --> src/project_a/utils/spark_session.py:8:1
  |
6 | import logging
7 | import os
8 | from typing import Dict, Optional
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
9 | from pyspark.sql import SparkSession
  |

W293 [*] Blank line contains whitespace
  --> src/project_a/utils/spark_session.py:29:1
   |
27 |         self._jsc = self._sc._jsc
28 |         self._jvm = self._sc._jvm
29 |         
   | ^^^^^^^^
30 |         assert self._jvm is not None
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/utils/spark_session.py:31:1
   |
30 |         assert self._jvm is not None
31 |         
   | ^^^^^^^^
32 |         if jsparkSession is None:
33 |             # Check for default session first (this path doesn't use HashMap constructor)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/utils/spark_session.py:52:1
   |
50 |             # jsparkSession provided - skip applying options (HashMap bug)
51 |             pass
52 |         
   | ^^^^^^^^
53 |         # Set the Java session
54 |         self._jsparkSession = jsparkSession
   |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
  --> src/project_a/utils/spark_session.py:65:54
   |
65 | def build_spark(app_name: str = "project_a", config: Optional[Dict] = None) -> SparkSession:
   |                                                      ^^^^^^^^^^^^^^
66 |     """
67 |     Create and configure SparkSession with Delta Lake support.
   |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> src/project_a/utils/spark_session.py:65:63
   |
65 | def build_spark(app_name: str = "project_a", config: Optional[Dict] = None) -> SparkSession:
   |                                                               ^^^^
66 |     """
67 |     Create and configure SparkSession with Delta Lake support.
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> src/project_a/utils/spark_session.py:68:1
   |
66 |     """
67 |     Create and configure SparkSession with Delta Lake support.
68 |     
   | ^^^^
69 |     Args:
70 |         app_name: Application name
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> src/project_a/utils/spark_session.py:72:1
   |
70 |         app_name: Application name
71 |         config: Configuration dictionary
72 |         
   | ^^^^^^^^
73 |     Returns:
74 |         Configured SparkSession
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/utils/spark_session.py:78:1
   |
76 |     if config is None:
77 |         config = {}
78 |     
   | ^^^^
79 |     app_name = config.get("app_name", app_name)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/utils/spark_session.py:80:1
   |
79 |     app_name = config.get("app_name", app_name)
80 |     
   | ^^^^
81 |     # Build SparkSession
82 |     builder = SparkSession.builder.appName(app_name)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/utils/spark_session.py:83:1
   |
81 |     # Build SparkSession
82 |     builder = SparkSession.builder.appName(app_name)
83 |     
   | ^^^^
84 |     # Configure Delta Lake
85 |     spark_env = os.getenv("SPARK_ENV") or config.get("environment") or config.get("env") or "local"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> src/project_a/utils/spark_session.py:86:1
   |
84 |     # Configure Delta Lake
85 |     spark_env = os.getenv("SPARK_ENV") or config.get("environment") or config.get("env") or "local"
86 |     
   | ^^^^
87 |     # Set master for local execution
88 |     if spark_env == "local":
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/utils/spark_session.py:102:1
    |
100 |         )
101 |         logger.info(f"Delta Lake extensions enabled for environment: {spark_env}")
102 |     
    | ^^^^
103 |     # Add extra Spark configs from config dict
104 |     for k, v in config.get("spark", {}).items():
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/utils/spark_session.py:106:1
    |
104 |     for k, v in config.get("spark", {}).items():
105 |         builder = builder.config(k, v)
106 |     
    | ^^^^
107 |     # Set default Spark optimizations
108 |     builder = (
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/utils/spark_session.py:116:1
    |
114 |         .config("spark.sql.autoBroadcastJoinThreshold", 64 * 1024 * 1024)
115 |     )
116 |     
    | ^^^^
117 |     # Create SparkSession
118 |     # CRITICAL: PySpark 3.5.0 has a bug with Java 17 where getOrCreate() fails
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/utils/spark_session.py:122:1
    |
120 |     try:
121 |         from pyspark import SparkContext
122 |         
    | ^^^^^^^^
123 |         # CRITICAL: Stop ALL existing Spark sessions and contexts
124 |         # This must be done BEFORE any builder calls
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/utils/spark_session.py:135:1
    |
133 |         except Exception:
134 |             pass
135 |         
    | ^^^^^^^^
136 |         try:
137 |             sc = SparkContext._active_spark_context
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/utils/spark_session.py:145:1
    |
143 |         except Exception:
144 |             pass
145 |         
    | ^^^^^^^^
146 |         # For local mode, use a workaround that avoids the HashMap bug
147 |         if spark_env == "local":
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/utils/spark_session.py:151:1
    |
149 |             os.environ.setdefault("PYSPARK_PYTHON", "python3")
150 |             os.environ.setdefault("PYSPARK_DRIVER_PYTHON", "python3")
151 |             
    | ^^^^^^^^^^^^
152 |             # Use builder with minimal config - this should work if no existing context
153 |             # The key is ensuring we have NO existing SparkContext
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/utils/spark_session.py:168:1
    |
166 |             # For EMR, use the full builder
167 |             spark = builder.getOrCreate()
168 |         
    | ^^^^^^^^
169 |         logger.info(f"SparkSession created successfully ({spark_env} mode)")
170 |     except Exception as e:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/utils/spark_session.py:175:1
    |
173 |         logger.error("Workaround: Use PySpark 3.4.2 or Java 11")
174 |         raise
175 |     
    | ^^^^
176 |     # Configure AWS S3 for both local and AWS execution
177 |     # Check if any S3 paths are used (bronze_root, silver_root, etc.)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/utils/spark_session.py:184:1
    |
182 |             has_s3_paths = True
183 |             break
184 |     
    | ^^^^
185 |     # Also check sources for S3 paths
186 |     sources = config.get("sources", {})
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/utils/spark_session.py:193:1
    |
191 |                     has_s3_paths = True
192 |                     break
193 |     
    | ^^^^
194 |     if has_s3_paths:
195 |         try:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> src/project_a/utils/spark_session.py:212:1
    |
210 |         except Exception as e:
211 |             logger.warning(f"AWS S3 configuration failed: {str(e)}")
212 |     
    | ^^^^
213 |     return spark
    |
help: Remove whitespace from blank line

UP045 [*] Use `X | None` for type annotations
   --> src/project_a/utils/spark_session.py:216:52
    |
216 | def get_spark(app_name: str = "project_a", config: Optional[Dict] = None) -> SparkSession:
    |                                                    ^^^^^^^^^^^^^^
217 |     """
218 |     Alias for build_spark for consistency.
    |
help: Convert to `X | None`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> src/project_a/utils/spark_session.py:216:61
    |
216 | def get_spark(app_name: str = "project_a", config: Optional[Dict] = None) -> SparkSession:
    |                                                             ^^^^
217 |     """
218 |     Alias for build_spark for consistency.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> src/project_a/utils/spark_session.py:219:1
    |
217 |     """
218 |     Alias for build_spark for consistency.
219 |     
    | ^^^^
220 |     Args:
221 |         app_name: Application name
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> src/project_a/utils/spark_session.py:223:1
    |
221 |         app_name: Application name
222 |         config: Configuration dictionary
223 |         
    | ^^^^^^^^
224 |     Returns:
225 |         Configured SparkSession
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> tests/conftest.py:5:1
   |
 3 |   """
 4 |
 5 | / import pytest
 6 | | import os
 7 | | import sys
 8 | | from pyspark.sql import SparkSession
   | |____________________________________^
 9 |
10 |   # Add src to Python path for imports
   |
help: Organize imports

W293 [*] Blank line contains whitespace
  --> tests/conftest.py:27:1
   |
25 |             .config("spark.driver.bindAddress", "127.0.0.1") \
26 |             .getOrCreate()
27 |         
   | ^^^^^^^^
28 |         yield spark
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/conftest.py:29:1
   |
28 |         yield spark
29 |         
   | ^^^^^^^^
30 |         spark.stop()
31 |     except Exception as e:
   |
help: Remove whitespace from blank line

F841 [*] Local variable `e` is assigned to but never used
  --> tests/conftest.py:31:25
   |
30 |         spark.stop()
31 |     except Exception as e:
   |                         ^
32 |         # If Spark initialization fails, create a mock
33 |         from unittest.mock import MagicMock
   |
help: Remove assignment to unused variable `e`

W292 [*] No newline at end of file
  --> tests/conftest.py:68:44
   |
66 |     """Setup logging for tests."""
67 |     import logging
68 |     logging.basicConfig(level=logging.INFO)
   |                                            ^
   |
help: Add trailing newline

I001 [*] Import block is un-sorted or un-formatted
 --> tests/dev_secret_probe.py:6:1
  |
4 |   Proves Spark can fetch secrets without logging values.
5 |   """
6 | / import json
7 | | import boto3
8 | | from pyspark.sql import SparkSession
  | |____________________________________^
  |
help: Organize imports

W293 [*] Blank line contains whitespace
  --> tests/dev_secret_probe.py:20:1
   |
18 | if __name__ == "__main__":
19 |     spark = SparkSession.builder.appName("dev-secret-probe").getOrCreate()
20 |     
   | ^^^^
21 |     try:
22 |         # Fetch secret
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/dev_secret_probe.py:24:1
   |
22 |         # Fetch secret
23 |         snow = fetch_secret("project-a-dev/snowflake/conn")
24 |         
   | ^^^^^^^^
25 |         # Print keys only - NEVER print secret values
26 |         print("âœ… Fetched keys only:", list(snow.keys()))
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/dev_secret_probe.py:27:1
   |
25 |         # Print keys only - NEVER print secret values
26 |         print("âœ… Fetched keys only:", list(snow.keys()))
27 |         
   | ^^^^^^^^
28 |         # Verify we can access the structure
29 |         assert "account" in snow, "Missing 'account' key"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/dev_secret_probe.py:32:1
   |
30 |         assert "user" in snow, "Missing 'user' key"
31 |         assert "password" in snow, "Missing 'password' key"
32 |         
   | ^^^^^^^^
33 |         print("âœ… Secret structure validated")
34 |         print("âœ… No secret values logged (security check passed)")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/dev_secret_probe.py:35:1
   |
33 |         print("âœ… Secret structure validated")
34 |         print("âœ… No secret values logged (security check passed)")
35 |         
   | ^^^^^^^^
36 |     except Exception as e:
37 |         print(f"âŒ Error: {e}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/dev_secret_probe.py:39:1
   |
37 |         print(f"âŒ Error: {e}")
38 |         raise
39 |     
   | ^^^^
40 |     finally:
41 |         spark.stop()
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> tests/test_bronze_to_silver.py:5:1
   |
 3 |   """
 4 |
 5 | / import pytest
 6 | | from pyspark.sql import SparkSession
 7 | | from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType
 8 | |
 9 | | from project_a.transforms.bronze_to_silver import (
10 | |     transform_customers_bronze_to_silver,
11 | |     transform_orders_bronze_to_silver,
12 | |     transform_products_bronze_to_silver
13 | | )
   | |_^
   |
help: Organize imports

W293 [*] Blank line contains whitespace
  --> tests/test_bronze_to_silver.py:43:1
   |
41 |         StructField("age", StringType(), True)
42 |     ])
43 |     
   | ^^^^
44 |     data = [
45 |         ("C001", "John", "Doe", "john@example.com", "123 Main St", "City", "State", "Country", "12345", "123-456-7890", "01/01/2023", â€¦
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_bronze_to_silver.py:49:1
   |
47 |         (None, "Bob", "Johnson", "bob@example.com", "789 Pine Rd", "Village", "Region", "Country", "11111", "555-123-4567", "03/01/202â€¦
48 |     ]
49 |     
   | ^^^^
50 |     return spark.createDataFrame(data, schema)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_bronze_to_silver.py:67:1
   |
65 |         StructField("payment_method", StringType(), True)
66 |     ])
67 |     
   | ^^^^
68 |     data = [
69 |         ("O001", "C001", "P001", "2", "10.50", "21.00", "2023-01-01", "completed", "credit_card"),
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_bronze_to_silver.py:73:1
   |
71 |         ("O003", "C003", "P003", "3", "15.75", "47.25", "2023-01-03", "completed", "debit_card")
72 |     ]
73 |     
   | ^^^^
74 |     return spark.createDataFrame(data, schema)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_bronze_to_silver.py:80:1
   |
78 |     """Test customers bronze to silver transformation."""
79 |     result_df = transform_customers_bronze_to_silver(spark, sample_customers_df)
80 |     
   | ^^^^
81 |     # Check that null customer_id rows are filtered out
82 |     assert result_df.count() == 2  # Only 2 valid customers
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_bronze_to_silver.py:83:1
   |
81 |     # Check that null customer_id rows are filtered out
82 |     assert result_df.count() == 2  # Only 2 valid customers
83 |     
   | ^^^^
84 |     # Check that data types are correct
85 |     assert result_df.schema["age"].dataType == IntegerType()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_bronze_to_silver.py:87:1
   |
85 |     assert result_df.schema["age"].dataType == IntegerType()
86 |     assert result_df.schema["first_name"].dataType == StringType()
87 |     
   | ^^^^
88 |     # Check that duplicates are removed
89 |     result_df_with_duplicates = sample_customers_df.union(sample_customers_df)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_bronze_to_silver.py:97:1
   |
95 |     """Test orders bronze to silver transformation."""
96 |     result_df = transform_orders_bronze_to_silver(spark, sample_orders_df)
97 |     
   | ^^^^
98 |     # Check that all orders are processed
99 |     assert result_df.count() == 3
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_bronze_to_silver.py:100:1
    |
 98 |     # Check that all orders are processed
 99 |     assert result_df.count() == 3
100 |     
    | ^^^^
101 |     # Check that data types are correct
102 |     assert result_df.schema["quantity"].dataType == IntegerType()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_bronze_to_silver.py:105:1
    |
103 |     assert result_df.schema["unit_price"].dataType == DoubleType()
104 |     assert result_df.schema["total_amount"].dataType == DoubleType()
105 |     
    | ^^^^
106 |     # Check that negative amounts are filtered out
107 |     negative_amount_data = [
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_bronze_to_silver.py:125:1
    |
123 |         StructField("stock_quantity", StringType(), True)
124 |     ])
125 |     
    | ^^^^
126 |     data = [
127 |         ("P001", "Widget A", "Electronics", "BrandX", "29.99", "100"),
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_bronze_to_silver.py:131:1
    |
129 |         ("P003", None, "Books", "BrandZ", "19.99", "25")  # Null product_name
130 |     ]
131 |     
    | ^^^^
132 |     products_df = spark.createDataFrame(data, schema)
133 |     result_df = transform_products_bronze_to_silver(spark, products_df)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_bronze_to_silver.py:134:1
    |
132 |     products_df = spark.createDataFrame(data, schema)
133 |     result_df = transform_products_bronze_to_silver(spark, products_df)
134 |     
    | ^^^^
135 |     # Check that null product_name rows are filtered out
136 |     assert result_df.count() == 2  # Only 2 valid products
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_bronze_to_silver.py:137:1
    |
135 |     # Check that null product_name rows are filtered out
136 |     assert result_df.count() == 2  # Only 2 valid products
137 |     
    | ^^^^
138 |     # Check that data types are correct
139 |     assert result_df.schema["price"].dataType == DoubleType()
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
 --> tests/test_bronze_to_silver_behavior.py:4:1
  |
2 |   Tests for bronze_to_silver_behavior job.
3 |   """
4 | / import pytest
5 | | from unittest.mock import Mock, patch, MagicMock
6 | | from pyspark.sql import SparkSession
7 | | from pyspark.sql.types import StructType, StructField, StringType, TimestampType
8 | |
9 | | from jobs.bronze_to_silver_behavior import transform_bronze_to_silver_behavior
  | |______________________________________________________________________________^
  |
help: Organize imports

F401 [*] `unittest.mock.MagicMock` imported but unused
 --> tests/test_bronze_to_silver_behavior.py:5:40
  |
3 | """
4 | import pytest
5 | from unittest.mock import Mock, patch, MagicMock
  |                                        ^^^^^^^^^
6 | from pyspark.sql import SparkSession
7 | from pyspark.sql.types import StructType, StructField, StringType, TimestampType
  |
help: Remove unused import: `unittest.mock.MagicMock`

F401 [*] `pyspark.sql.SparkSession` imported but unused
 --> tests/test_bronze_to_silver_behavior.py:6:25
  |
4 | import pytest
5 | from unittest.mock import Mock, patch, MagicMock
6 | from pyspark.sql import SparkSession
  |                         ^^^^^^^^^^^^
7 | from pyspark.sql.types import StructType, StructField, StringType, TimestampType
  |
help: Remove unused import: `pyspark.sql.SparkSession`

F401 [*] `pyspark.sql.types.TimestampType` imported but unused
 --> tests/test_bronze_to_silver_behavior.py:7:68
  |
5 | from unittest.mock import Mock, patch, MagicMock
6 | from pyspark.sql import SparkSession
7 | from pyspark.sql.types import StructType, StructField, StringType, TimestampType
  |                                                                    ^^^^^^^^^^^^^
8 |
9 | from jobs.bronze_to_silver_behavior import transform_bronze_to_silver_behavior
  |
help: Remove unused import: `pyspark.sql.types.TimestampType`

W293 [*] Blank line contains whitespace
  --> tests/test_bronze_to_silver_behavior.py:68:1
   |
66 |     mock_ge_instance.run_checkpoint.return_value = {"success": True}
67 |     mock_ge_runner.return_value = mock_ge_instance
68 |     
   | ^^^^
69 |     # Write sample data to bronze path
70 |     bronze_path = tmp_path / "bronze" / "redshift" / "behavior"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_bronze_to_silver_behavior.py:72:1
   |
70 |     bronze_path = tmp_path / "bronze" / "redshift" / "behavior"
71 |     bronze_path.mkdir(parents=True)
72 |     
   | ^^^^
73 |     sample_bronze_behavior_data.write \
74 |         .format("delta") \
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_bronze_to_silver_behavior.py:77:1
   |
75 |         .mode("overwrite") \
76 |         .save(str(bronze_path))
77 |     
   | ^^^^
78 |     # Update config to use temp path
79 |     mock_config["data_lake"]["bronze_path"] = str(tmp_path / "bronze")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_bronze_to_silver_behavior.py:81:1
   |
79 |     mock_config["data_lake"]["bronze_path"] = str(tmp_path / "bronze")
80 |     mock_config["data_lake"]["silver_path"] = str(tmp_path / "silver")
81 |     
   | ^^^^
82 |     # Execute transformation
83 |     result_df = transform_bronze_to_silver_behavior(mock_spark, mock_config)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_bronze_to_silver_behavior.py:84:1
   |
82 |     # Execute transformation
83 |     result_df = transform_bronze_to_silver_behavior(mock_spark, mock_config)
84 |     
   | ^^^^
85 |     # Assertions
86 |     assert result_df is not None
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_bronze_to_silver_behavior.py:90:1
   |
88 |     mock_emit_complete.assert_called_once()
89 |     mock_emit_metrics.assert_called_once()
90 |     
   | ^^^^
91 |     # Verify data transformations
92 |     assert "event_name" in result_df.columns
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_bronze_to_silver_behavior.py:107:1
    |
105 |     mock_config["data_lake"]["bronze_path"] = str(tmp_path / "nonexistent" / "bronze")
106 |     mock_config["data_lake"]["silver_path"] = str(tmp_path / "silver")
107 |     
    | ^^^^
108 |     # Should handle missing data
109 |     with pytest.raises(Exception):
    |
help: Remove whitespace from blank line

B017 Do not assert blind exception: `Exception`
   --> tests/test_bronze_to_silver_behavior.py:109:10
    |
108 |     # Should handle missing data
109 |     with pytest.raises(Exception):
    |          ^^^^^^^^^^^^^^^^^^^^^^^^
110 |         transform_bronze_to_silver_behavior(mock_spark, mock_config)
    |

W293 [*] Blank line contains whitespace
   --> tests/test_bronze_to_silver_behavior.py:111:1
    |
109 |     with pytest.raises(Exception):
110 |         transform_bronze_to_silver_behavior(mock_spark, mock_config)
111 |     
    | ^^^^
112 |     # Should emit fail event
113 |     mock_emit_fail.assert_called_once()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_bronze_to_silver_behavior.py:129:1
    |
127 |     mock_ge_instance.run_checkpoint.return_value = {"success": False}
128 |     mock_ge_runner.return_value = mock_ge_instance
129 |     
    | ^^^^
130 |     bronze_path = tmp_path / "bronze" / "redshift" / "behavior"
131 |     bronze_path.mkdir(parents=True)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_bronze_to_silver_behavior.py:133:1
    |
131 |     bronze_path.mkdir(parents=True)
132 |     sample_bronze_behavior_data.write.format("delta").mode("overwrite").save(str(bronze_path))
133 |     
    | ^^^^
134 |     mock_config["data_lake"]["bronze_path"] = str(tmp_path / "bronze")
135 |     mock_config["data_lake"]["silver_path"] = str(tmp_path / "silver")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_bronze_to_silver_behavior.py:136:1
    |
134 |     mock_config["data_lake"]["bronze_path"] = str(tmp_path / "bronze")
135 |     mock_config["data_lake"]["silver_path"] = str(tmp_path / "silver")
136 |     
    | ^^^^
137 |     # Should raise error on GE failure
138 |     with pytest.raises(RuntimeError):
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
 --> tests/test_bronze_to_silver_orders.py:4:1
  |
2 |   Test bronze to silver transformation for orders.
3 |   """
4 | / import pytest
5 | | from pyspark.sql import SparkSession
6 | | from pyspark.sql import functions as F
7 | |
8 | | from project_a.utils.contracts import load_contract, validate_schema, enforce_not_null
9 | | from project_a.dq.gate import run_not_null_checks, run_dq_gate
  | |______________________________________________________________^
  |
help: Organize imports

F401 [*] `project_a.utils.contracts.validate_schema` imported but unused
 --> tests/test_bronze_to_silver_orders.py:8:54
  |
6 | from pyspark.sql import functions as F
7 |
8 | from project_a.utils.contracts import load_contract, validate_schema, enforce_not_null
  |                                                      ^^^^^^^^^^^^^^^
9 | from project_a.dq.gate import run_not_null_checks, run_dq_gate
  |
help: Remove unused import: `project_a.utils.contracts.validate_schema`

F401 [*] `project_a.dq.gate.run_not_null_checks` imported but unused
 --> tests/test_bronze_to_silver_orders.py:9:31
  |
8 | from project_a.utils.contracts import load_contract, validate_schema, enforce_not_null
9 | from project_a.dq.gate import run_not_null_checks, run_dq_gate
  |                               ^^^^^^^^^^^^^^^^^^^
  |
help: Remove unused import: `project_a.dq.gate.run_not_null_checks`

W293 [*] Blank line contains whitespace
  --> tests/test_bronze_to_silver_orders.py:21:1
   |
19 |     """Test that null order_id rows are dropped or quarantined."""
20 |     contract = load_contract("snowflake_orders.schema.json")
21 |     
   | ^^^^
22 |     # Create DataFrame with null order_id
23 |     df = spark.createDataFrame(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_bronze_to_silver_orders.py:31:1
   |
29 |         ["order_id", "customer_id", "product_id", "order_date", "order_amount", "currency", "status"],
30 |     )
31 |     
   | ^^^^
32 |     # Filter nulls in required columns
33 |     df_clean = enforce_not_null(df, contract.required)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_bronze_to_silver_orders.py:34:1
   |
32 |     # Filter nulls in required columns
33 |     df_clean = enforce_not_null(df, contract.required)
34 |     
   | ^^^^
35 |     # Should only have 2 rows (o1 and o3)
36 |     assert df_clean.count() == 2
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_bronze_to_silver_orders.py:37:1
   |
35 |     # Should only have 2 rows (o1 and o3)
36 |     assert df_clean.count() == 2
37 |     
   | ^^^^
38 |     # Verify no null order_ids remain
39 |     null_count = df_clean.filter(F.col("order_id").isNull()).count()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_bronze_to_silver_orders.py:46:1
   |
44 |     """Test that DQ gate passes for valid orders data."""
45 |     contract = load_contract("snowflake_orders.schema.json")
46 |     
   | ^^^^
47 |     # Create valid DataFrame
48 |     df = spark.createDataFrame(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_bronze_to_silver_orders.py:55:1
   |
53 |         ["order_id", "customer_id", "product_id", "order_date", "order_amount", "currency", "status"],
54 |     )
55 |     
   | ^^^^
56 |     # Run DQ gate
57 |     result = run_dq_gate(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_bronze_to_silver_orders.py:65:1
   |
63 |         check_uniqueness=True,
64 |     )
65 |     
   | ^^^^
66 |     assert result.passed is True
67 |     assert result.total_rows == 2
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_bronze_to_silver_orders.py:73:1
   |
71 |     """Test that DQ gate fails when required columns have nulls."""
72 |     contract = load_contract("snowflake_orders.schema.json")
73 |     
   | ^^^^
74 |     # Create DataFrame with nulls in required columns
75 |     df = spark.createDataFrame(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_bronze_to_silver_orders.py:82:1
   |
80 |         ["order_id", "customer_id", "product_id", "order_date", "order_amount", "currency", "status"],
81 |     )
82 |     
   | ^^^^
83 |     # Run DQ gate - should raise ValueError
84 |     with pytest.raises(ValueError, match="DQ Gate FAILED"):
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
 --> tests/test_collect_run_summary.py:4:1
  |
2 |   Tests for collect_run_summary job.
3 |   """
4 | / import pytest
5 | | from unittest.mock import Mock, patch
6 | | from pyspark.sql import SparkSession
7 | |
8 | | from jobs.collect_run_summary import collect_run_summary
  | |________________________________________________________^
  |
help: Organize imports

F401 [*] `unittest.mock.Mock` imported but unused
 --> tests/test_collect_run_summary.py:5:27
  |
3 | """
4 | import pytest
5 | from unittest.mock import Mock, patch
  |                           ^^^^
6 | from pyspark.sql import SparkSession
  |
help: Remove unused import

F401 [*] `unittest.mock.patch` imported but unused
 --> tests/test_collect_run_summary.py:5:33
  |
3 | """
4 | import pytest
5 | from unittest.mock import Mock, patch
  |                                 ^^^^^
6 | from pyspark.sql import SparkSession
  |
help: Remove unused import

F401 [*] `pyspark.sql.SparkSession` imported but unused
 --> tests/test_collect_run_summary.py:6:25
  |
4 | import pytest
5 | from unittest.mock import Mock, patch
6 | from pyspark.sql import SparkSession
  |                         ^^^^^^^^^^^^
7 |
8 | from jobs.collect_run_summary import collect_run_summary
  |
help: Remove unused import: `pyspark.sql.SparkSession`

W293 [*] Blank line contains whitespace
  --> tests/test_collect_run_summary.py:30:1
   |
28 |     silver_path = tmp_path / "silver"
29 |     gold_path = tmp_path / "gold"
30 |     
   | ^^^^
31 |     for path in [bronze_path, silver_path, gold_path]:
32 |         path.mkdir(parents=True)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_collect_run_summary.py:33:1
   |
31 |     for path in [bronze_path, silver_path, gold_path]:
32 |         path.mkdir(parents=True)
33 |     
   | ^^^^
34 |     # Create empty DataFrames and save
35 |     empty_df = spark.createDataFrame([], "id STRING")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_collect_run_summary.py:39:1
   |
37 |     empty_df.write.format("delta").mode("overwrite").save(str(silver_path / "behavior"))
38 |     empty_df.write.format("delta").mode("overwrite").save(str(gold_path / "customer_360"))
39 |     
   | ^^^^
40 |     mock_config["data_lake"]["bronze_path"] = str(bronze_path)
41 |     mock_config["data_lake"]["silver_path"] = str(silver_path)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_collect_run_summary.py:43:1
   |
41 |     mock_config["data_lake"]["silver_path"] = str(silver_path)
42 |     mock_config["data_lake"]["gold_path"] = str(gold_path)
43 |     
   | ^^^^
44 |     # Execute
45 |     summary = collect_run_summary(spark, mock_config, "test_run_123", "2025-01-01")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_collect_run_summary.py:46:1
   |
44 |     # Execute
45 |     summary = collect_run_summary(spark, mock_config, "test_run_123", "2025-01-01")
46 |     
   | ^^^^
47 |     # Assertions
48 |     assert summary is not None
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_collect_run_summary.py:60:1
   |
58 |     mock_config["data_lake"]["silver_path"] = str(tmp_path / "nonexistent" / "silver")
59 |     mock_config["data_lake"]["gold_path"] = str(tmp_path / "nonexistent" / "gold")
60 |     
   | ^^^^
61 |     # Should handle missing data
62 |     summary = collect_run_summary(spark, mock_config, "test_run_456", "2025-01-02")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_collect_run_summary.py:63:1
   |
61 |     # Should handle missing data
62 |     summary = collect_run_summary(spark, mock_config, "test_run_456", "2025-01-02")
63 |     
   | ^^^^
64 |     # Should still return summary with errors
65 |     assert summary is not None
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
 --> tests/test_config.py:4:1
  |
2 |   Tests for config utilities.
3 |   """
4 | / import pytest
5 | | from unittest.mock import Mock, patch, MagicMock
6 | | import yaml
7 | | from pathlib import Path
8 | |
9 | | from project_a.utils.config import load_conf
  | |____________________________________________^
  |
help: Organize imports

F401 [*] `unittest.mock.Mock` imported but unused
 --> tests/test_config.py:5:27
  |
3 | """
4 | import pytest
5 | from unittest.mock import Mock, patch, MagicMock
  |                           ^^^^
6 | import yaml
7 | from pathlib import Path
  |
help: Remove unused import

F401 [*] `unittest.mock.patch` imported but unused
 --> tests/test_config.py:5:33
  |
3 | """
4 | import pytest
5 | from unittest.mock import Mock, patch, MagicMock
  |                                 ^^^^^
6 | import yaml
7 | from pathlib import Path
  |
help: Remove unused import

F401 [*] `unittest.mock.MagicMock` imported but unused
 --> tests/test_config.py:5:40
  |
3 | """
4 | import pytest
5 | from unittest.mock import Mock, patch, MagicMock
  |                                        ^^^^^^^^^
6 | import yaml
7 | from pathlib import Path
  |
help: Remove unused import

F401 [*] `pathlib.Path` imported but unused
 --> tests/test_config.py:7:21
  |
5 | from unittest.mock import Mock, patch, MagicMock
6 | import yaml
7 | from pathlib import Path
  |                     ^^^^
8 |
9 | from project_a.utils.config import load_conf
  |
help: Remove unused import: `pathlib.Path`

W293 [*] Blank line contains whitespace
  --> tests/test_config.py:30:1
   |
28 |     """Test loading config from file."""
29 |     result = load_conf(sample_config_file)
30 |     
   | ^^^^
31 |     assert result is not None
32 |     assert result["app_name"] == "test_app"
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> tests/test_config_validation.py:7:1
   |
 5 |   """
 6 |
 7 | / import pytest
 8 | | import yaml
 9 | | from pathlib import Path
10 | | from project_a.utils.config import load_conf
   | |____________________________________________^
   |
help: Organize imports

W293 [*] Blank line contains whitespace
  --> tests/test_config_validation.py:33:1
   |
31 |     """Test that config has data_lake.bucket or s3.bucket."""
32 |     config = load_conf(str(config_file))
33 |     
   | ^^^^
34 |     has_lake_bucket = config.get("data_lake", {}).get("bucket") is not None
35 |     has_s3_bucket = config.get("s3", {}).get("bucket") is not None
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_config_validation.py:36:1
   |
34 |     has_lake_bucket = config.get("data_lake", {}).get("bucket") is not None
35 |     has_s3_bucket = config.get("s3", {}).get("bucket") is not None
36 |     
   | ^^^^
37 |     assert has_lake_bucket or has_s3_bucket, \
38 |         f"Config {config_file} must have data_lake.bucket or s3.bucket"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_config_validation.py:45:1
   |
43 |     """Test that config has AWS region."""
44 |     config = load_conf(str(config_file))
45 |     
   | ^^^^
46 |     has_aws_region = config.get("aws", {}).get("region") is not None
47 |     has_region = config.get("region") is not None
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_config_validation.py:48:1
   |
46 |     has_aws_region = config.get("aws", {}).get("region") is not None
47 |     has_region = config.get("region") is not None
48 |     
   | ^^^^
49 |     assert has_aws_region or has_region, \
50 |         f"Config {config_file} must have aws.region or region"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_config_validation.py:56:1
   |
54 |     """Test that local.yaml has all required paths."""
55 |     config = load_conf("config/local.yaml")
56 |     
   | ^^^^
57 |     required_paths = [
58 |         "data_lake.bronze_path",
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_config_validation.py:62:1
   |
60 |         "data_lake.gold_path"
61 |     ]
62 |     
   | ^^^^
63 |     for path in required_paths:
64 |         keys = path.split(".")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_config_validation.py:75:1
   |
73 |     """Test that local.yaml has ingestion configuration."""
74 |     config = load_conf("config/local.yaml")
75 |     
   | ^^^^
76 |     assert "ingestion" in config, "Missing ingestion section"
77 |     assert "mode" in config["ingestion"], "Missing ingestion.mode"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_config_validation.py:80:1
   |
78 |     assert config["ingestion"]["mode"] in ["schema_on_write", "schema_on_read"], \
79 |         "ingestion.mode must be schema_on_write or schema_on_read"
80 |     
   | ^^^^
81 |     assert "on_unknown_column" in config["ingestion"], "Missing ingestion.on_unknown_column"
82 |     assert config["ingestion"]["on_unknown_column"] in ["quarantine", "drop", "fail"], \
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_config_validation.py:89:1
   |
87 |     """Test that config files have valid YAML syntax."""
88 |     config_dir = Path("config")
89 |     
   | ^^^^
90 |     for config_file in config_dir.glob("*.yaml"):
91 |         try:
   |
help: Remove whitespace from blank line

UP015 [*] Unnecessary mode argument
  --> tests/test_config_validation.py:92:36
   |
90 |     for config_file in config_dir.glob("*.yaml"):
91 |         try:
92 |             with open(config_file, 'r') as f:
   |                                    ^^^
93 |                 yaml.safe_load(f)
94 |         except yaml.YAMLError as e:
   |
help: Remove mode argument

W293 [*] Blank line contains whitespace
   --> tests/test_config_validation.py:102:1
    |
100 |     try:
101 |         config = load_conf("config/prod.yaml")
102 |         
    | ^^^^^^^^
103 |         # Check that sensitive values are not hardcoded
104 |         snowflake_config = config.get("data_sources", {}).get("snowflake", {})
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_config_validation.py:105:1
    |
103 |         # Check that sensitive values are not hardcoded
104 |         snowflake_config = config.get("data_sources", {}).get("snowflake", {})
105 |         
    | ^^^^^^^^
106 |         # Should use secret references, not actual passwords
107 |         if "password" in snowflake_config:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_config_validation.py:118:1
    |
116 |     """Test that configs have valid Spark settings."""
117 |     config_files = ["config/local.yaml"]
118 |     
    | ^^^^
119 |     for config_file in config_files:
120 |         config = load_conf(config_file)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_config_validation.py:121:1
    |
119 |     for config_file in config_files:
120 |         config = load_conf(config_file)
121 |         
    | ^^^^^^^^
122 |         if "spark" in config:
123 |             assert "master" in config["spark"] or "environment" in config, \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_config_validation.py:125:1
    |
123 |             assert "master" in config["spark"] or "environment" in config, \
124 |                 "Spark config should have master or environment"
125 |             
    | ^^^^^^^^^^^^
126 |             if "shuffle_partitions" in config.get("spark", {}):
127 |                 partitions = config["spark"]["shuffle_partitions"]
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
 --> tests/test_contracts_customers.py:4:1
  |
2 |   Test data contracts for customers table.
3 |   """
4 | / import pytest
5 | | from pyspark.sql import SparkSession
6 | |
7 | | from project_a.utils.contracts import load_contract, validate_schema, enforce_not_null
  | |______________________________________________________________________________________^
  |
help: Organize imports

W293 [*] Blank line contains whitespace
  --> tests/test_contracts_customers.py:19:1
   |
17 |     """Test that customers DataFrame validates against contract."""
18 |     contract = load_contract("snowflake_customers.schema.json")
19 |     
   | ^^^^
20 |     # Create test DataFrame with valid data
21 |     df = spark.createDataFrame(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_contracts_customers.py:28:1
   |
26 |         ["customer_id", "customer_name", "email", "country"],
27 |     )
28 |     
   | ^^^^
29 |     # Should not raise
30 |     validate_schema(df, contract)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_contracts_customers.py:31:1
   |
29 |     # Should not raise
30 |     validate_schema(df, contract)
31 |     
   | ^^^^
32 |     # Test not-null enforcement
33 |     df_clean = enforce_not_null(df, contract.required)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_contracts_customers.py:40:1
   |
38 |     """Test that missing required columns raise ValueError."""
39 |     contract = load_contract("snowflake_customers.schema.json")
40 |     
   | ^^^^
41 |     # Create DataFrame missing required column
42 |     df = spark.createDataFrame(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_contracts_customers.py:49:1
   |
47 |         ["customer_name", "email"],  # Missing customer_id
48 |     )
49 |     
   | ^^^^
50 |     # Should raise ValueError
51 |     with pytest.raises(ValueError, match="missing required columns"):
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_contracts_customers.py:58:1
   |
56 |     """Test that null values in required columns are filtered."""
57 |     contract = load_contract("snowflake_customers.schema.json")
58 |     
   | ^^^^
59 |     # Create DataFrame with nulls in required columns
60 |     df = spark.createDataFrame(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_contracts_customers.py:68:1
   |
66 |         ["customer_id", "customer_name", "email", "country"],
67 |     )
68 |     
   | ^^^^
69 |     # Filter nulls
70 |     df_clean = enforce_not_null(df, contract.required)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_contracts_customers.py:71:1
   |
69 |     # Filter nulls
70 |     df_clean = enforce_not_null(df, contract.required)
71 |     
   | ^^^^
72 |     # Should only have 1 row (c1, Alice)
73 |     assert df_clean.count() == 1
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> tests/test_dag_imports.py:4:1
   |
 2 |   Test DAG imports for enterprise-grade validation.
 3 |   """
 4 | / import pkgutil
 5 | | import importlib
 6 | | import time
 7 | | import pathlib
 8 | | import pytest
 9 | | import logging
   | |______________^
10 |
11 |   logger = logging.getLogger(__name__)
   |
help: Organize imports

W293 [*] Blank line contains whitespace
  --> tests/test_dag_imports.py:17:1
   |
15 |     start_time = time.time()
16 |     dags_dir = pathlib.Path("airflow/dags")
17 |     
   | ^^^^
18 |     if not dags_dir.exists():
19 |         pytest.skip("Airflow DAGs directory not found")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_dag_imports.py:20:1
   |
18 |     if not dags_dir.exists():
19 |         pytest.skip("Airflow DAGs directory not found")
20 |     
   | ^^^^
21 |     imported_dags = []
22 |     failed_imports = []
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_dag_imports.py:23:1
   |
21 |     imported_dags = []
22 |     failed_imports = []
23 |     
   | ^^^^
24 |     for mod in pkgutil.iter_modules([str(dags_dir)]):
25 |         try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_dag_imports.py:32:1
   |
30 |             failed_imports.append((mod.name, str(e)))
31 |             logger.error(f"âŒ DAG import failed: {mod.name} - {e}")
32 |     
   | ^^^^
33 |     elapsed_time = time.time() - start_time
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_dag_imports.py:34:1
   |
33 |     elapsed_time = time.time() - start_time
34 |     
   | ^^^^
35 |     # Assertions
36 |     assert len(failed_imports) == 0, f"Failed to import DAGs: {failed_imports}"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_dag_imports.py:39:1
   |
37 |     assert elapsed_time < 2.0, f"DAG import took too long: {elapsed_time:.2f}s"
38 |     assert len(imported_dags) > 0, "No DAGs were imported"
39 |     
   | ^^^^
40 |     logger.info(f"âœ… All {len(imported_dags)} DAGs imported in {elapsed_time:.2f}s")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_dag_imports.py:45:1
   |
43 |     """Test DAG syntax validation."""
44 |     dags_dir = pathlib.Path("airflow/dags")
45 |     
   | ^^^^
46 |     if not dags_dir.exists():
47 |         pytest.skip("Airflow DAGs directory not found")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_dag_imports.py:48:1
   |
46 |     if not dags_dir.exists():
47 |         pytest.skip("Airflow DAGs directory not found")
48 |     
   | ^^^^
49 |     for dag_file in dags_dir.glob("*.py"):
50 |         try:
   |
help: Remove whitespace from blank line

UP015 [*] Unnecessary mode argument
  --> tests/test_dag_imports.py:51:33
   |
49 |     for dag_file in dags_dir.glob("*.py"):
50 |         try:
51 |             with open(dag_file, 'r') as f:
   |                                 ^^^
52 |                 content = f.read()
   |
help: Remove mode argument

W293 [*] Blank line contains whitespace
  --> tests/test_dag_imports.py:53:1
   |
51 |             with open(dag_file, 'r') as f:
52 |                 content = f.read()
53 |             
   | ^^^^^^^^^^^^
54 |             # Basic syntax check
55 |             compile(content, str(dag_file), 'exec')
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_dag_imports.py:57:1
   |
55 |             compile(content, str(dag_file), 'exec')
56 |             logger.info(f"âœ… DAG syntax valid: {dag_file.name}")
57 |             
   | ^^^^^^^^^^^^
58 |         except SyntaxError as e:
59 |             pytest.fail(f"DAG syntax error in {dag_file.name}: {e}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_dag_imports.py:66:1
   |
64 |     """Test DAG configuration consistency."""
65 |     dags_dir = pathlib.Path("airflow/dags")
66 |     
   | ^^^^
67 |     if not dags_dir.exists():
68 |         pytest.skip("Airflow DAGs directory not found")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_dag_imports.py:69:1
   |
67 |     if not dags_dir.exists():
68 |         pytest.skip("Airflow DAGs directory not found")
69 |     
   | ^^^^
70 |     required_tags = ["project_a"]
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_dag_imports.py:71:1
   |
70 |     required_tags = ["project_a"]
71 |     
   | ^^^^
72 |     for dag_file in dags_dir.glob("*.py"):
73 |         try:
   |
help: Remove whitespace from blank line

UP015 [*] Unnecessary mode argument
  --> tests/test_dag_imports.py:74:33
   |
72 |     for dag_file in dags_dir.glob("*.py"):
73 |         try:
74 |             with open(dag_file, 'r') as f:
   |                                 ^^^
75 |                 content = f.read()
   |
help: Remove mode argument

W293 [*] Blank line contains whitespace
  --> tests/test_dag_imports.py:76:1
   |
74 |             with open(dag_file, 'r') as f:
75 |                 content = f.read()
76 |             
   | ^^^^^^^^^^^^
77 |             # Check for required tags
78 |             for tag in required_tags:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_dag_imports.py:80:1
   |
78 |             for tag in required_tags:
79 |                 assert tag in content, f"DAG {dag_file.name} missing required tag: {tag}"
80 |             
   | ^^^^^^^^^^^^
81 |             # Check for proper timeout configuration
82 |             assert "dagrun_timeout" in content, f"DAG {dag_file.name} missing dagrun_timeout"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_dag_imports.py:83:1
   |
81 |             # Check for proper timeout configuration
82 |             assert "dagrun_timeout" in content, f"DAG {dag_file.name} missing dagrun_timeout"
83 |             
   | ^^^^^^^^^^^^
84 |             logger.info(f"âœ… DAG configuration valid: {dag_file.name}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_dag_imports.py:85:1
   |
84 |             logger.info(f"âœ… DAG configuration valid: {dag_file.name}")
85 |             
   | ^^^^^^^^^^^^
86 |         except Exception as e:
87 |             pytest.fail(f"DAG configuration error in {dag_file.name}: {e}")
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
 --> tests/test_dq_policies.py:5:1
  |
3 |   """
4 |
5 | / import pytest
6 | | import yaml
7 | | import os
8 | | from pathlib import Path
  | |________________________^
  |
help: Organize imports

F401 [*] `os` imported but unused
 --> tests/test_dq_policies.py:7:8
  |
5 | import pytest
6 | import yaml
7 | import os
  |        ^^
8 | from pathlib import Path
  |
help: Remove unused import: `os`

W293 [*] Blank line contains whitespace
  --> tests/test_dq_policies.py:14:1
   |
12 |     """Test that DQ YAML files have valid schema structure."""
13 |     dq_dir = Path(__file__).parent.parent / "dq"
14 |     
   | ^^^^
15 |     if not dq_dir.exists():
16 |         pytest.skip("DQ directory not found")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_dq_policies.py:17:1
   |
15 |     if not dq_dir.exists():
16 |         pytest.skip("DQ directory not found")
17 |     
   | ^^^^
18 |     yaml_files = list(dq_dir.glob("**/*.yml")) + list(dq_dir.glob("**/*.yaml"))
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_dq_policies.py:19:1
   |
18 |     yaml_files = list(dq_dir.glob("**/*.yml")) + list(dq_dir.glob("**/*.yaml"))
19 |     
   | ^^^^
20 |     if not yaml_files:
21 |         pytest.skip("No DQ YAML files found")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_dq_policies.py:22:1
   |
20 |     if not yaml_files:
21 |         pytest.skip("No DQ YAML files found")
22 |     
   | ^^^^
23 |     required_schema_keys = [
24 |         'table',
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_dq_policies.py:27:1
   |
25 |         'rules'
26 |     ]
27 |     
   | ^^^^
28 |     rule_required_keys = [
29 |         'name',
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_dq_policies.py:32:1
   |
30 |         'type'
31 |     ]
32 |     
   | ^^^^
33 |     for yaml_file in yaml_files:
34 |         try:
   |
help: Remove whitespace from blank line

UP015 [*] Unnecessary mode argument
  --> tests/test_dq_policies.py:35:34
   |
33 |     for yaml_file in yaml_files:
34 |         try:
35 |             with open(yaml_file, 'r') as f:
   |                                  ^^^
36 |                 data = yaml.safe_load(f)
   |
help: Remove mode argument

W293 [*] Blank line contains whitespace
  --> tests/test_dq_policies.py:37:1
   |
35 |             with open(yaml_file, 'r') as f:
36 |                 data = yaml.safe_load(f)
37 |             
   | ^^^^^^^^^^^^
38 |             # Check top-level schema
39 |             for key in required_schema_keys:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_dq_policies.py:41:1
   |
39 |             for key in required_schema_keys:
40 |                 assert key in data, f"{yaml_file} missing required key: {key}"
41 |             
   | ^^^^^^^^^^^^
42 |             # Check rules structure
43 |             assert isinstance(data['rules'], list), f"{yaml_file} rules must be a list"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_dq_policies.py:44:1
   |
42 |             # Check rules structure
43 |             assert isinstance(data['rules'], list), f"{yaml_file} rules must be a list"
44 |             
   | ^^^^^^^^^^^^
45 |             for rule in data['rules']:
46 |                 for key in rule_required_keys:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_dq_policies.py:48:1
   |
46 |                 for key in rule_required_keys:
47 |                     assert key in rule, f"{yaml_file} rule missing required key: {key}"
48 |                 
   | ^^^^^^^^^^^^^^^^
49 |                 # Validate rule type
50 |                 valid_rule_types = [
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_dq_policies.py:59:1
   |
57 |                 ]
58 |                 assert rule['type'] in valid_rule_types, f"{yaml_file} invalid rule type: {rule['type']}"
59 |                 
   | ^^^^^^^^^^^^^^^^
60 |         except yaml.YAMLError as e:
61 |             pytest.fail(f"Invalid YAML in {yaml_file}: {e}")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_dq_policies.py:69:1
   |
67 |     """Test that DQ policy files follow naming conventions."""
68 |     dq_dir = Path(__file__).parent.parent / "dq"
69 |     
   | ^^^^
70 |     if not dq_dir.exists():
71 |         pytest.skip("DQ directory not found")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_dq_policies.py:72:1
   |
70 |     if not dq_dir.exists():
71 |         pytest.skip("DQ directory not found")
72 |     
   | ^^^^
73 |     yaml_files = list(dq_dir.glob("**/*.yml")) + list(dq_dir.glob("**/*.yaml"))
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_dq_policies.py:74:1
   |
73 |     yaml_files = list(dq_dir.glob("**/*.yml")) + list(dq_dir.glob("**/*.yaml"))
74 |     
   | ^^^^
75 |     for yaml_file in yaml_files:
76 |         filename = yaml_file.name
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_dq_policies.py:77:1
   |
75 |     for yaml_file in yaml_files:
76 |         filename = yaml_file.name
77 |         
   | ^^^^^^^^
78 |         # Should be lowercase with underscores
79 |         assert filename.islower() or filename.replace('.', '_').replace('-', '_').islower()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_dq_policies.py:80:1
   |
78 |         # Should be lowercase with underscores
79 |         assert filename.islower() or filename.replace('.', '_').replace('-', '_').islower()
80 |         
   | ^^^^^^^^
81 |         # Should end with .yml or .yaml
82 |         assert filename.endswith(('.yml', '.yaml'))
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_dq_policies.py:83:1
   |
81 |         # Should end with .yml or .yaml
82 |         assert filename.endswith(('.yml', '.yaml'))
83 |         
   | ^^^^^^^^
84 |         # Should not contain spaces
85 |         assert ' ' not in filename
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_dq_policies.py:91:1
   |
89 |     """Test that DQ rules have proper configuration."""
90 |     dq_dir = Path(__file__).parent.parent / "dq"
91 |     
   | ^^^^
92 |     if not dq_dir.exists():
93 |         pytest.skip("DQ directory not found")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_dq_policies.py:94:1
   |
92 |     if not dq_dir.exists():
93 |         pytest.skip("DQ directory not found")
94 |     
   | ^^^^
95 |     yaml_files = list(dq_dir.glob("**/*.yml")) + list(dq_dir.glob("**/*.yaml"))
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_dq_policies.py:96:1
   |
95 |     yaml_files = list(dq_dir.glob("**/*.yml")) + list(dq_dir.glob("**/*.yaml"))
96 |     
   | ^^^^
97 |     for yaml_file in yaml_files:
98 |         try:
   |
help: Remove whitespace from blank line

UP015 [*] Unnecessary mode argument
   --> tests/test_dq_policies.py:99:34
    |
 97 |     for yaml_file in yaml_files:
 98 |         try:
 99 |             with open(yaml_file, 'r') as f:
    |                                  ^^^
100 |                 data = yaml.safe_load(f)
    |
help: Remove mode argument

W293 [*] Blank line contains whitespace
   --> tests/test_dq_policies.py:101:1
    |
 99 |             with open(yaml_file, 'r') as f:
100 |                 data = yaml.safe_load(f)
101 |             
    | ^^^^^^^^^^^^
102 |             for rule in data.get('rules', []):
103 |                 rule_type = rule.get('type')
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_dq_policies.py:104:1
    |
102 |             for rule in data.get('rules', []):
103 |                 rule_type = rule.get('type')
104 |                 
    | ^^^^^^^^^^^^^^^^
105 |                 if rule_type == 'not_null':
106 |                     assert 'columns' in rule, f"{yaml_file} not_null rule missing columns"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_dq_policies.py:108:1
    |
106 |                     assert 'columns' in rule, f"{yaml_file} not_null rule missing columns"
107 |                     assert isinstance(rule['columns'], list), f"{yaml_file} columns must be a list"
108 |                 
    | ^^^^^^^^^^^^^^^^
109 |                 elif rule_type == 'unique':
110 |                     assert 'columns' in rule, f"{yaml_file} unique rule missing columns"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_dq_policies.py:112:1
    |
110 |                     assert 'columns' in rule, f"{yaml_file} unique rule missing columns"
111 |                     assert isinstance(rule['columns'], list), f"{yaml_file} columns must be a list"
112 |                 
    | ^^^^^^^^^^^^^^^^
113 |                 elif rule_type == 'range':
114 |                     assert 'column' in rule, f"{yaml_file} range rule missing column"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_dq_policies.py:116:1
    |
114 |                     assert 'column' in rule, f"{yaml_file} range rule missing column"
115 |                     assert 'min' in rule or 'max' in rule, f"{yaml_file} range rule missing min/max"
116 |                 
    | ^^^^^^^^^^^^^^^^
117 |                 elif rule_type == 'format':
118 |                     assert 'column' in rule, f"{yaml_file} format rule missing column"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_dq_policies.py:120:1
    |
118 |                     assert 'column' in rule, f"{yaml_file} format rule missing column"
119 |                     assert 'pattern' in rule, f"{yaml_file} format rule missing pattern"
120 |                 
    | ^^^^^^^^^^^^^^^^
121 |                 elif rule_type == 'referential_integrity':
122 |                     assert 'source_column' in rule, f"{yaml_file} referential_integrity missing source_column"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_dq_policies.py:125:1
    |
123 |                     assert 'target_table' in rule, f"{yaml_file} referential_integrity missing target_table"
124 |                     assert 'target_column' in rule, f"{yaml_file} referential_integrity missing target_column"
125 |                 
    | ^^^^^^^^^^^^^^^^
126 |                 # Check severity
127 |                 if 'severity' in rule:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_dq_policies.py:130:1
    |
128 |                     valid_severities = ['critical', 'warning', 'info']
129 |                     assert rule['severity'] in valid_severities, f"{yaml_file} invalid severity: {rule['severity']}"
130 |                 
    | ^^^^^^^^^^^^^^^^
131 |         except Exception as e:
132 |             pytest.fail(f"Error validating {yaml_file}: {e}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_dq_policies.py:138:1
    |
136 |     """Test that DQ table names are consistent with expected tables."""
137 |     dq_dir = Path(__file__).parent.parent / "dq"
138 |     
    | ^^^^
139 |     if not dq_dir.exists():
140 |         pytest.skip("DQ directory not found")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_dq_policies.py:141:1
    |
139 |     if not dq_dir.exists():
140 |         pytest.skip("DQ directory not found")
141 |     
    | ^^^^
142 |     yaml_files = list(dq_dir.glob("**/*.yml")) + list(dq_dir.glob("**/*.yaml"))
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_dq_policies.py:143:1
    |
142 |     yaml_files = list(dq_dir.glob("**/*.yml")) + list(dq_dir.glob("**/*.yaml"))
143 |     
    | ^^^^
144 |     expected_tables = [
145 |         'customers',
    |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
   --> tests/test_dq_policies.py:146:18
    |
144 |     expected_tables = [
145 |         'customers',
146 |         'orders', 
    |                  ^
147 |         'products',
148 |         'customer_behavior',
    |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
   --> tests/test_dq_policies.py:154:1
    |
152 |         'hubspot_deals'
153 |     ]
154 |     
    | ^^^^
155 |     found_tables = set()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_dq_policies.py:156:1
    |
155 |     found_tables = set()
156 |     
    | ^^^^
157 |     for yaml_file in yaml_files:
158 |         try:
    |
help: Remove whitespace from blank line

UP015 [*] Unnecessary mode argument
   --> tests/test_dq_policies.py:159:34
    |
157 |     for yaml_file in yaml_files:
158 |         try:
159 |             with open(yaml_file, 'r') as f:
    |                                  ^^^
160 |                 data = yaml.safe_load(f)
    |
help: Remove mode argument

W293 [*] Blank line contains whitespace
   --> tests/test_dq_policies.py:161:1
    |
159 |             with open(yaml_file, 'r') as f:
160 |                 data = yaml.safe_load(f)
161 |             
    | ^^^^^^^^^^^^
162 |             table_name = data.get('table')
163 |             if table_name:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_dq_policies.py:165:1
    |
163 |             if table_name:
164 |                 found_tables.add(table_name)
165 |                 
    | ^^^^^^^^^^^^^^^^
166 |                 # Check naming convention
167 |                 assert '_' in table_name or table_name.islower(), f"{yaml_file} table name should be lowercase with underscores"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_dq_policies.py:168:1
    |
166 |                 # Check naming convention
167 |                 assert '_' in table_name or table_name.islower(), f"{yaml_file} table name should be lowercase with underscores"
168 |                 
    | ^^^^^^^^^^^^^^^^
169 |         except Exception as e:
170 |             pytest.fail(f"Error processing {yaml_file}: {e}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_dq_policies.py:171:1
    |
169 |         except Exception as e:
170 |             pytest.fail(f"Error processing {yaml_file}: {e}")
171 |     
    | ^^^^
172 |     # Check that we have DQ policies for expected tables
173 |     missing_tables = set(expected_tables) - found_tables
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_dq_policies.py:181:1
    |
179 |     """Test that all DQ YAML files have valid syntax."""
180 |     dq_dir = Path(__file__).parent.parent / "dq"
181 |     
    | ^^^^
182 |     if not dq_dir.exists():
183 |         pytest.skip("DQ directory not found")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_dq_policies.py:184:1
    |
182 |     if not dq_dir.exists():
183 |         pytest.skip("DQ directory not found")
184 |     
    | ^^^^
185 |     yaml_files = list(dq_dir.glob("**/*.yml")) + list(dq_dir.glob("**/*.yaml"))
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_dq_policies.py:186:1
    |
185 |     yaml_files = list(dq_dir.glob("**/*.yml")) + list(dq_dir.glob("**/*.yaml"))
186 |     
    | ^^^^
187 |     for yaml_file in yaml_files:
188 |         try:
    |
help: Remove whitespace from blank line

UP015 [*] Unnecessary mode argument
   --> tests/test_dq_policies.py:189:34
    |
187 |     for yaml_file in yaml_files:
188 |         try:
189 |             with open(yaml_file, 'r') as f:
    |                                  ^^^
190 |                 yaml.safe_load(f)
191 |         except yaml.YAMLError as e:
    |
help: Remove mode argument

I001 [*] Import block is un-sorted or un-formatted
 --> tests/test_extract.py:1:1
  |
1 | / import os
2 | | import sys
  | |__________^
  |
help: Organize imports

C405 Unnecessary list literal (rewrite as a set literal)
  --> tests/test_extract.py:42:12
   |
40 |     p = "data/input_data/returns.json"
41 |     df = extract.extract_returns(spark, p)
42 |     assert set(["order_id", "return_date", "reason"]).issubset(set(df.columns))
   |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   |
help: Rewrite as a set literal

C405 Unnecessary list literal (rewrite as a set literal)
  --> tests/test_extract.py:48:12
   |
46 |     p = "data/input_data/exchange_rates.csv"
47 |     df = extract.extract_exchange_rates(spark, p)
48 |     assert set(["currency", "usd_rate"]).issubset(set(df.columns))
   |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
49 |     assert df.filter(df.currency == "USD").count() == 1
   |
help: Rewrite as a set literal

C405 Unnecessary list literal (rewrite as a set literal)
  --> tests/test_extract.py:55:12
   |
53 |     p = "data/input_data/inventory_snapshots.csv"
54 |     df = extract.extract_inventory_snapshots(spark, p)
55 |     assert set(["product_id", "on_hand", "warehouse"]).issubset(set(df.columns))
   |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   |
help: Rewrite as a set literal

I001 [*] Import block is un-sorted or un-formatted
  --> tests/test_fx_transform.py:5:1
   |
 3 |   """
 4 |
 5 | / import pytest
 6 | | from pyspark.sql import SparkSession
 7 | | from pyspark.sql.functions import col, lit
 8 | | from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType
 9 | |
10 | | from project_a.jobs.fx_bronze_to_silver import (
11 | |     deduplicate_fx_rates,
12 | |     add_rate_categories,
13 | |     validate_fx_rates
14 | | )
   | |_^
   |
help: Organize imports

F401 [*] `pyspark.sql.functions.lit` imported but unused
 --> tests/test_fx_transform.py:7:40
  |
5 | import pytest
6 | from pyspark.sql import SparkSession
7 | from pyspark.sql.functions import col, lit
  |                                        ^^^
8 | from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType
  |
help: Remove unused import: `pyspark.sql.functions.lit`

W293 [*] Blank line contains whitespace
  --> tests/test_fx_transform.py:38:1
   |
36 |         StructField("ingestion_timestamp", StringType(), False)
37 |     ])
38 |     
   | ^^^^
39 |     data = [
40 |         ("USD", 1.0, "2025-01-27", "USD", "2025-01-27T10:00:00"),
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_fx_transform.py:47:1
   |
45 |         ("EUR", 0.86, "2025-01-28", "USD", "2025-01-28T10:00:00"),
46 |     ]
47 |     
   | ^^^^
48 |     return spark_session.createDataFrame(data, schema)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_fx_transform.py:58:1
   |
56 |         ("EUR", 0.84, "2025-01-27", "USD", "2025-01-27T12:00:00"),  # Later timestamp
57 |     ]
58 |     
   | ^^^^
59 |     duplicate_df = spark_session.createDataFrame(duplicate_data, sample_fx_data.schema)
60 |     combined_df = sample_fx_data.union(duplicate_df)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_fx_transform.py:61:1
   |
59 |     duplicate_df = spark_session.createDataFrame(duplicate_data, sample_fx_data.schema)
60 |     combined_df = sample_fx_data.union(duplicate_df)
61 |     
   | ^^^^
62 |     # Test deduplication
63 |     result_df = deduplicate_fx_rates(spark_session, combined_df)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_fx_transform.py:64:1
   |
62 |     # Test deduplication
63 |     result_df = deduplicate_fx_rates(spark_session, combined_df)
64 |     
   | ^^^^
65 |     # Should have 5 unique records (USD and EUR for 2025-01-27 should be deduplicated)
66 |     assert result_df.count() == 5
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_fx_transform.py:67:1
   |
65 |     # Should have 5 unique records (USD and EUR for 2025-01-27 should be deduplicated)
66 |     assert result_df.count() == 5
67 |     
   | ^^^^
68 |     # Check that latest timestamps are kept
69 |     usd_records = result_df.filter(col("ccy") == "USD").collect()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_fx_transform.py:77:1
   |
75 |     """Test rate category assignment."""
76 |     result_df = add_rate_categories(spark_session, sample_fx_data)
77 |     
   | ^^^^
78 |     # Check categories
79 |     categories = result_df.select("ccy", "rate_category").collect()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_fx_transform.py:81:1
   |
79 |     categories = result_df.select("ccy", "rate_category").collect()
80 |     category_map = {row["ccy"]: row["rate_category"] for row in categories}
81 |     
   | ^^^^
82 |     assert category_map["USD"] == "major"
83 |     assert category_map["EUR"] == "major"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_fx_transform.py:104:1
    |
102 |         StructField("ingestion_timestamp", StringType(), True)
103 |     ])
104 |     
    | ^^^^
105 |     data = [
106 |         (None, 1.0, "2025-01-27", "USD", "2025-01-27T10:00:00"),  # Null ccy
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_fx_transform.py:110:1
    |
108 |         ("EUR", 0.85, None, "USD", "2025-01-27T10:00:00"),  # Null date
109 |     ]
110 |     
    | ^^^^
111 |     df = spark_session.createDataFrame(data, schema)
112 |     assert validate_fx_rates(df) is False
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_fx_transform.py:124:1
    |
122 |         StructField("ingestion_timestamp", StringType(), False)
123 |     ])
124 |     
    | ^^^^
125 |     data = [
126 |         ("USD", -1.0, "2025-01-27", "USD", "2025-01-27T10:00:00"),  # Negative rate
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_fx_transform.py:129:1
    |
127 |         ("EUR", 0.0, "2025-01-27", "USD", "2025-01-27T10:00:00"),   # Zero rate
128 |     ]
129 |     
    | ^^^^
130 |     df = spark_session.createDataFrame(data, schema)
131 |     assert validate_fx_rates(df) is False
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_fx_transform.py:143:1
    |
141 |         StructField("ingestion_timestamp", StringType(), False)
142 |     ])
143 |     
    | ^^^^
144 |     data = [
145 |         ("USD", 1.0, "2025-01-27", "USD", "2025-01-27T10:00:00"),
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_fx_transform.py:148:1
    |
146 |         ("USD", 1.1, "2025-01-27", "USD", "2025-01-27T11:00:00"),  # Duplicate ccy+date
147 |     ]
148 |     
    | ^^^^
149 |     df = spark_session.createDataFrame(data, schema)
150 |     assert validate_fx_rates(df) is False
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> tests/test_glue_catalog_contract.py:5:1
   |
 3 |   """
 4 |
 5 | / import pytest
 6 | | import os
 7 | | import sys
 8 | | from pathlib import Path
   | |________________________^
 9 |
10 |   # Add src to path
   |
help: Organize imports

F401 [*] `pytest` imported but unused
 --> tests/test_glue_catalog_contract.py:5:8
  |
3 | """
4 |
5 | import pytest
  |        ^^^^^^
6 | import os
7 | import sys
  |
help: Remove unused import: `pytest`

W291 [*] Trailing whitespace
  --> tests/test_glue_catalog_contract.py:19:31
   |
17 |     expected_databases = {
18 |         'bronze_db': 'bronze',
19 |         'silver_db': 'silver', 
   |                               ^
20 |         'gold_db': 'gold'
21 |     }
   |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
  --> tests/test_glue_catalog_contract.py:22:1
   |
20 |         'gold_db': 'gold'
21 |     }
22 |     
   | ^^^^
23 |     # Test environment variable defaults
24 |     assert os.getenv("GLUE_DB_SILVER", "silver_db") == "silver_db"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_glue_catalog_contract.py:26:1
   |
24 |     assert os.getenv("GLUE_DB_SILVER", "silver_db") == "silver_db"
25 |     assert os.getenv("GLUE_DB_GOLD", "gold_db") == "gold_db"
26 |     
   | ^^^^
27 |     # Test naming convention
28 |     for db_name, layer in expected_databases.items():
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_glue_catalog_contract.py:39:1
   |
37 |         'gold': 's3a://{bucket}/gold/'
38 |     }
39 |     
   | ^^^^
40 |     bucket = os.getenv("S3_LAKE_BUCKET", "data-lake-bucket")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_glue_catalog_contract.py:41:1
   |
40 |     bucket = os.getenv("S3_LAKE_BUCKET", "data-lake-bucket")
41 |     
   | ^^^^
42 |     for layer, path_template in expected_paths.items():
43 |         expected_path = path_template.format(bucket=bucket)
   |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
  --> tests/test_glue_catalog_contract.py:53:25
   |
51 |     bronze_tables = [
52 |         'hubspot_contacts',
53 |         'hubspot_deals', 
   |                         ^
54 |         'snowflake_customers',
55 |         'snowflake_orders',
   |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
  --> tests/test_glue_catalog_contract.py:61:1
   |
59 |         'kafka_events'
60 |     ]
61 |     
   | ^^^^
62 |     # Silver layer tables
63 |     silver_tables = [
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_glue_catalog_contract.py:70:1
   |
68 |         'fx_rates_conformed'
69 |     ]
70 |     
   | ^^^^
71 |     # Gold layer tables
72 |     gold_tables = [
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_glue_catalog_contract.py:78:1
   |
76 |         'customer_segments'
77 |     ]
78 |     
   | ^^^^
79 |     # Test naming conventions
80 |     for table in bronze_tables:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_glue_catalog_contract.py:83:1
   |
81 |         assert '_' in table  # Snake case
82 |         assert table.islower()  # Lowercase
83 |     
   | ^^^^
84 |     for table in silver_tables:
85 |         assert '_' in table  # Snake case
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_glue_catalog_contract.py:88:1
   |
86 |         assert table.islower()  # Lowercase
87 |         assert 'conformed' in table  # Silver layer suffix
88 |     
   | ^^^^
89 |     for table in gold_tables:
90 |         assert '_' in table  # Snake case
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_glue_catalog_contract.py:98:1
   |
96 |     checkpoint_prefix = os.getenv("S3_CHECKPOINT_PREFIX", "checkpoints")
97 |     bucket = os.getenv("S3_LAKE_BUCKET", "data-lake-bucket")
98 |     
   | ^^^^
99 |     expected_checkpoint_base = f"s3a://{bucket}/{checkpoint_prefix}"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_glue_catalog_contract.py:100:1
    |
 99 |     expected_checkpoint_base = f"s3a://{bucket}/{checkpoint_prefix}"
100 |     
    | ^^^^
101 |     # Test streaming checkpoint paths
102 |     streaming_checkpoints = [
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_glue_catalog_contract.py:107:1
    |
105 |         f"{expected_checkpoint_base}/orders_stream/raw_dlq"
106 |     ]
107 |     
    | ^^^^
108 |     for checkpoint_path in streaming_checkpoints:
109 |         assert checkpoint_path.startswith("s3a://")
    |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
   --> tests/test_glue_catalog_contract.py:119:26
    |
117 |         'EMR_APP_ID',
118 |         'EMR_JOB_ROLE_ARN',
119 |         'GLUE_DB_SILVER', 
    |                          ^
120 |         'GLUE_DB_GOLD',
121 |         'S3_LAKE_BUCKET',
    |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
   --> tests/test_glue_catalog_contract.py:124:1
    |
122 |         'S3_CHECKPOINT_PREFIX'
123 |     ]
124 |     
    | ^^^^
125 |     # Test naming convention
126 |     for var in required_variables:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_glue_catalog_contract.py:140:1
    |
138 |         'config/local.yaml'
139 |     ]
140 |     
    | ^^^^
141 |     for config_file in config_files:
142 |         config_path = Path(__file__).parent.parent / config_file
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> tests/test_gold_contract.py:5:1
   |
 3 |   """
 4 |
 5 | / import pytest
 6 | | import json
 7 | | import tempfile
 8 | | from pathlib import Path
 9 | | from pyspark.sql import SparkSession
10 | | from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType
11 | |
12 | | from project_a.schema.validator import SchemaValidator
   | |______________________________________________________^
   |
help: Organize imports

W293 [*] Blank line contains whitespace
  --> tests/test_gold_contract.py:63:1
   |
61 |         StructField("data", StringType(), True)
62 |     ])
63 |     
   | ^^^^
64 |     df = schema_validator.spark.createDataFrame([], schema)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_gold_contract.py:65:1
   |
64 |     df = schema_validator.spark.createDataFrame([], schema)
65 |     
   | ^^^^
66 |     # Bronze validation should always pass
67 |     assert schema_validator.validate_bronze_schema(df, "test_table") == True
   |
help: Remove whitespace from blank line

E712 Avoid equality comparisons to `True`; use `...:` for truth checks
  --> tests/test_gold_contract.py:67:12
   |
66 |     # Bronze validation should always pass
67 |     assert schema_validator.validate_bronze_schema(df, "test_table") == True
   |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   |
help: Replace comparison

W293 [*] Blank line contains whitespace
  --> tests/test_gold_contract.py:78:1
   |
76 |         StructField("age", IntegerType(), False)
77 |     ])
78 |     
   | ^^^^
79 |     # Create compatible schema (additive nullable field)
80 |     compatible_schema = StructType([
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_gold_contract.py:86:1
   |
84 |         StructField("email", StringType(), True)  # New nullable field
85 |     ])
86 |     
   | ^^^^
87 |     # Create incompatible schema (removed field)
88 |     incompatible_schema = StructType([
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_gold_contract.py:93:1
   |
91 |         # age field removed
92 |     ])
93 |     
   | ^^^^
94 |     # Test compatible schema
95 |     compatible_df = schema_validator.spark.createDataFrame([], compatible_schema)
   |
help: Remove whitespace from blank line

E712 Avoid equality comparisons to `True`; use `...:` for truth checks
   --> tests/test_gold_contract.py:101:16
    |
100 |     try:
101 |         assert schema_validator.validate_silver_schema(compatible_df, "test_table", existing_schema_path) == True
    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
102 |     finally:
103 |         if Path(existing_schema_path).exists():
    |
help: Replace comparison

W293 [*] Blank line contains whitespace
   --> tests/test_gold_contract.py:111:1
    |
109 |         json.dump(existing_schema.jsonValue(), f)
110 |         existing_schema_path = f.name
111 |     
    | ^^^^
112 |     try:
113 |         assert schema_validator.validate_silver_schema(incompatible_df, "test_table", existing_schema_path) == False
    |
help: Remove whitespace from blank line

E712 Avoid equality comparisons to `False`; use `not ...:` for false checks
   --> tests/test_gold_contract.py:113:16
    |
112 |     try:
113 |         assert schema_validator.validate_silver_schema(incompatible_df, "test_table", existing_schema_path) == False
    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
114 |     finally:
115 |         if Path(existing_schema_path).exists():
    |
help: Replace comparison

W293 [*] Blank line contains whitespace
   --> tests/test_gold_contract.py:125:1
    |
123 |         json.dump(fact_orders_contract, f)
124 |         contract_path = f.name
125 |     
    | ^^^^
126 |     try:
127 |         # Create matching schema
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_gold_contract.py:145:1
    |
143 |             StructField("created_at", TimestampType(), False)
144 |         ])
145 |         
    | ^^^^^^^^
146 |         matching_df = schema_validator.spark.createDataFrame([], matching_schema)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_gold_contract.py:147:1
    |
146 |         matching_df = schema_validator.spark.createDataFrame([], matching_schema)
147 |         
    | ^^^^^^^^
148 |         # This should fail due to type mismatch (date vs string)
149 |         assert schema_validator.validate_gold_schema(matching_df, "fact_orders", contract_path) == False
    |
help: Remove whitespace from blank line

E712 Avoid equality comparisons to `False`; use `not ...:` for false checks
   --> tests/test_gold_contract.py:149:16
    |
148 |         # This should fail due to type mismatch (date vs string)
149 |         assert schema_validator.validate_gold_schema(matching_df, "fact_orders", contract_path) == False
    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
150 |         
151 |         # Create non-matching schema (missing field)
    |
help: Replace comparison

W293 [*] Blank line contains whitespace
   --> tests/test_gold_contract.py:150:1
    |
148 |         # This should fail due to type mismatch (date vs string)
149 |         assert schema_validator.validate_gold_schema(matching_df, "fact_orders", contract_path) == False
150 |         
    | ^^^^^^^^
151 |         # Create non-matching schema (missing field)
152 |         non_matching_schema = StructType([
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_gold_contract.py:157:1
    |
155 |             # Missing other required fields
156 |         ])
157 |         
    | ^^^^^^^^
158 |         non_matching_df = schema_validator.spark.createDataFrame([], non_matching_schema)
159 |         assert schema_validator.validate_gold_schema(non_matching_df, "fact_orders", contract_path) == False
    |
help: Remove whitespace from blank line

E712 Avoid equality comparisons to `False`; use `not ...:` for false checks
   --> tests/test_gold_contract.py:159:16
    |
158 |         non_matching_df = schema_validator.spark.createDataFrame([], non_matching_schema)
159 |         assert schema_validator.validate_gold_schema(non_matching_df, "fact_orders", contract_path) == False
    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
160 |         
161 |     finally:
    |
help: Replace comparison

W293 [*] Blank line contains whitespace
   --> tests/test_gold_contract.py:160:1
    |
158 |         non_matching_df = schema_validator.spark.createDataFrame([], non_matching_schema)
159 |         assert schema_validator.validate_gold_schema(non_matching_df, "fact_orders", contract_path) == False
160 |         
    | ^^^^^^^^
161 |     finally:
162 |         Path(contract_path).unlink()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_gold_contract.py:170:1
    |
168 |         StructField("id", StringType(), False)
169 |     ])
170 |     
    | ^^^^
171 |     df = schema_validator.spark.createDataFrame([], schema)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_gold_contract.py:172:1
    |
171 |     df = schema_validator.spark.createDataFrame([], schema)
172 |     
    | ^^^^
173 |     # Should fail when contract file doesn't exist
174 |     assert schema_validator.validate_gold_schema(df, "test_table", "nonexistent.json") == False
    |
help: Remove whitespace from blank line

E712 Avoid equality comparisons to `False`; use `not ...:` for false checks
   --> tests/test_gold_contract.py:174:12
    |
173 |     # Should fail when contract file doesn't exist
174 |     assert schema_validator.validate_gold_schema(df, "test_table", "nonexistent.json") == False
    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |
help: Replace comparison

I001 [*] Import block is un-sorted or un-formatted
 --> tests/test_great_expectations_runner.py:4:1
  |
2 |   Tests for Great Expectations runner.
3 |   """
4 | / import pytest
5 | | from unittest.mock import Mock, patch, MagicMock
6 | | from pathlib import Path
7 | |
8 | | from project_a.dq.great_expectations_runner import GreatExpectationsRunner, run_dq_checkpoint
  | |_____________________________________________________________________________________________^
  |
help: Organize imports

F401 [*] `unittest.mock.MagicMock` imported but unused
 --> tests/test_great_expectations_runner.py:5:40
  |
3 | """
4 | import pytest
5 | from unittest.mock import Mock, patch, MagicMock
  |                                        ^^^^^^^^^
6 | from pathlib import Path
  |
help: Remove unused import: `unittest.mock.MagicMock`

F401 [*] `pathlib.Path` imported but unused
 --> tests/test_great_expectations_runner.py:6:21
  |
4 | import pytest
5 | from unittest.mock import Mock, patch, MagicMock
6 | from pathlib import Path
  |                     ^^^^
7 |
8 | from project_a.dq.great_expectations_runner import GreatExpectationsRunner, run_dq_checkpoint
  |
help: Remove unused import: `pathlib.Path`

W293 [*] Blank line contains whitespace
  --> tests/test_great_expectations_runner.py:25:1
   |
23 |     mock_path_instance.exists.return_value = True
24 |     mock_path.return_value = mock_path_instance
25 |     
   | ^^^^
26 |     mock_context = Mock()
27 |     mock_data_context.return_value = mock_context
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_great_expectations_runner.py:28:1
   |
26 |     mock_context = Mock()
27 |     mock_data_context.return_value = mock_context
28 |     
   | ^^^^
29 |     runner = GreatExpectationsRunner()
30 |     runner.init_context()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_great_expectations_runner.py:31:1
   |
29 |     runner = GreatExpectationsRunner()
30 |     runner.init_context()
31 |     
   | ^^^^
32 |     assert runner.context is not None
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_great_expectations_runner.py:41:1
   |
39 |     mock_path_instance.exists.return_value = False
40 |     mock_path.return_value = mock_path_instance
41 |     
   | ^^^^
42 |     runner = GreatExpectationsRunner()
43 |     runner.init_context()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_great_expectations_runner.py:44:1
   |
42 |     runner = GreatExpectationsRunner()
43 |     runner.init_context()
44 |     
   | ^^^^
45 |     # Context should be None if GE not configured
46 |     assert runner.context is None
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_great_expectations_runner.py:54:1
   |
52 |     runner = GreatExpectationsRunner()
53 |     runner.context = None
54 |     
   | ^^^^
55 |     result = runner.run_checkpoint("test_checkpoint", fail_on_error=False)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_great_expectations_runner.py:56:1
   |
55 |     result = runner.run_checkpoint("test_checkpoint", fail_on_error=False)
56 |     
   | ^^^^
57 |     assert result["skipped"] is True
58 |     assert result["success"] is True
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_great_expectations_runner.py:73:1
   |
71 |     }
72 |     runner.context = mock_context
73 |     
   | ^^^^
74 |     result = runner.run_checkpoint("test_checkpoint", fail_on_error=False)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_great_expectations_runner.py:75:1
   |
74 |     result = runner.run_checkpoint("test_checkpoint", fail_on_error=False)
75 |     
   | ^^^^
76 |     assert result["success"] is True
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_great_expectations_runner.py:91:1
   |
89 |     }
90 |     runner.context = mock_context
91 |     
   | ^^^^
92 |     with pytest.raises(RuntimeError):
93 |         runner.run_checkpoint("test_checkpoint", fail_on_error=True)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_great_expectations_runner.py:103:1
    |
101 |         mock_runner.run_checkpoint.return_value = {"success": True}
102 |         mock_runner_class.return_value = mock_runner
103 |         
    | ^^^^^^^^
104 |         result = run_dq_checkpoint("test_checkpoint", fail_on_error=False)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_great_expectations_runner.py:105:1
    |
104 |         result = run_dq_checkpoint("test_checkpoint", fail_on_error=False)
105 |         
    | ^^^^^^^^
106 |         assert result["success"] is True
107 |         mock_runner.init_context.assert_called_once()
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> tests/test_integration.py:7:1
   |
 5 |   """
 6 |
 7 | / import sys
 8 | | import os
 9 | | import logging
10 | | import pytest
11 | | import tempfile
12 | | from pathlib import Path
   | |________________________^
13 |
14 |   # Add src to path
   |
help: Organize imports

F401 [*] `os` imported but unused
  --> tests/test_integration.py:8:8
   |
 7 | import sys
 8 | import os
   |        ^^
 9 | import logging
10 | import pytest
   |
help: Remove unused import: `os`

F401 [*] `pytest` imported but unused
  --> tests/test_integration.py:10:8
   |
 8 | import os
 9 | import logging
10 | import pytest
   |        ^^^^^^
11 | import tempfile
12 | from pathlib import Path
   |
help: Remove unused import: `pytest`

F401 [*] `tempfile` imported but unused
  --> tests/test_integration.py:11:8
   |
 9 | import logging
10 | import pytest
11 | import tempfile
   |        ^^^^^^^^
12 | from pathlib import Path
   |
help: Remove unused import: `tempfile`

I001 [*] Import block is un-sorted or un-formatted
  --> tests/test_integration.py:17:1
   |
15 |   sys.path.insert(0, str(Path(__file__).parent.parent / "src"))
16 |
17 | / from project_a import (
18 | |     build_spark,
19 | |     load_config_resolved,
20 | |     extract_customers,
21 | |     extract_products,
22 | |     extract_orders_json,
23 | |     extract_returns,
24 | |     extract_exchange_rates,
25 | |     extract_inventory_snapshots,
26 | |     write_delta,
27 | |     write_parquet
28 | | )
   | |_^
29 |
30 |   # Setup logging
   |
help: Organize imports

F401 [*] `project_a.extract_products` imported but unused
  --> tests/test_integration.py:21:5
   |
19 |     load_config_resolved,
20 |     extract_customers,
21 |     extract_products,
   |     ^^^^^^^^^^^^^^^^
22 |     extract_orders_json,
23 |     extract_returns,
   |
help: Remove unused import

F401 [*] `project_a.extract_orders_json` imported but unused
  --> tests/test_integration.py:22:5
   |
20 |     extract_customers,
21 |     extract_products,
22 |     extract_orders_json,
   |     ^^^^^^^^^^^^^^^^^^^
23 |     extract_returns,
24 |     extract_exchange_rates,
   |
help: Remove unused import

F401 [*] `project_a.extract_returns` imported but unused
  --> tests/test_integration.py:23:5
   |
21 |     extract_products,
22 |     extract_orders_json,
23 |     extract_returns,
   |     ^^^^^^^^^^^^^^^
24 |     extract_exchange_rates,
25 |     extract_inventory_snapshots,
   |
help: Remove unused import

F401 [*] `project_a.extract_exchange_rates` imported but unused
  --> tests/test_integration.py:24:5
   |
22 |     extract_orders_json,
23 |     extract_returns,
24 |     extract_exchange_rates,
   |     ^^^^^^^^^^^^^^^^^^^^^^
25 |     extract_inventory_snapshots,
26 |     write_delta,
   |
help: Remove unused import

F401 [*] `project_a.extract_inventory_snapshots` imported but unused
  --> tests/test_integration.py:25:5
   |
23 |     extract_returns,
24 |     extract_exchange_rates,
25 |     extract_inventory_snapshots,
   |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
26 |     write_delta,
27 |     write_parquet
   |
help: Remove unused import

F401 [*] `project_a.write_delta` imported but unused
  --> tests/test_integration.py:26:5
   |
24 |     extract_exchange_rates,
25 |     extract_inventory_snapshots,
26 |     write_delta,
   |     ^^^^^^^^^^^
27 |     write_parquet
28 | )
   |
help: Remove unused import

F401 [*] `project_a.write_parquet` imported but unused
  --> tests/test_integration.py:27:5
   |
25 |     extract_inventory_snapshots,
26 |     write_delta,
27 |     write_parquet
   |     ^^^^^^^^^^^^^
28 | )
   |
help: Remove unused import

I001 [*] Import block is un-sorted or un-formatted
  --> tests/test_integration_comprehensive.py:7:1
   |
 5 |   """
 6 |
 7 | / import sys
 8 | | import os
 9 | | import json
10 | | import tempfile
11 | | import shutil
12 | | import logging
13 | | from pathlib import Path
14 | | from datetime import datetime, timedelta
   | |________________________________________^
15 |
16 |   # Add src to path
   |
help: Organize imports

I001 [*] Import block is un-sorted or un-formatted
  --> tests/test_integration_comprehensive.py:19:1
   |
17 |   sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))
18 |
19 | / from project_a import (
20 | |     build_spark,
21 | |     load_config_resolved,
22 | |     extract_customers,
23 | |     extract_products,
24 | |     extract_orders_json,
25 | |     extract_returns,
26 | |     extract_exchange_rates,
27 | |     extract_inventory_snapshots,
28 | |     write_delta,
29 | |     write_parquet
30 | | )
31 | |
32 | | from project_a.ingestion_pipeline import IngestionPipeline
33 | | from project_a.metrics_collector import MetricsCollector
34 | | from project_a.data_quality_suite import DataQualitySuite
35 | | from project_a.disaster_recovery import DisasterRecoveryExecutor
36 | | from project_a.data_contracts import DataContractManager
37 | | from project_a.performance_optimizer import PerformanceOptimizer
   | |________________________________________________________________^
38 |
39 |   # Setup logging
   |
help: Organize imports

F401 [*] `project_a.extract_customers` imported but unused
  --> tests/test_integration_comprehensive.py:22:5
   |
20 |     build_spark,
21 |     load_config_resolved,
22 |     extract_customers,
   |     ^^^^^^^^^^^^^^^^^
23 |     extract_products,
24 |     extract_orders_json,
   |
help: Remove unused import

F401 [*] `project_a.extract_products` imported but unused
  --> tests/test_integration_comprehensive.py:23:5
   |
21 |     load_config_resolved,
22 |     extract_customers,
23 |     extract_products,
   |     ^^^^^^^^^^^^^^^^
24 |     extract_orders_json,
25 |     extract_returns,
   |
help: Remove unused import

F401 [*] `project_a.extract_orders_json` imported but unused
  --> tests/test_integration_comprehensive.py:24:5
   |
22 |     extract_customers,
23 |     extract_products,
24 |     extract_orders_json,
   |     ^^^^^^^^^^^^^^^^^^^
25 |     extract_returns,
26 |     extract_exchange_rates,
   |
help: Remove unused import

F401 [*] `project_a.extract_returns` imported but unused
  --> tests/test_integration_comprehensive.py:25:5
   |
23 |     extract_products,
24 |     extract_orders_json,
25 |     extract_returns,
   |     ^^^^^^^^^^^^^^^
26 |     extract_exchange_rates,
27 |     extract_inventory_snapshots,
   |
help: Remove unused import

F401 [*] `project_a.extract_exchange_rates` imported but unused
  --> tests/test_integration_comprehensive.py:26:5
   |
24 |     extract_orders_json,
25 |     extract_returns,
26 |     extract_exchange_rates,
   |     ^^^^^^^^^^^^^^^^^^^^^^
27 |     extract_inventory_snapshots,
28 |     write_delta,
   |
help: Remove unused import

F401 [*] `project_a.extract_inventory_snapshots` imported but unused
  --> tests/test_integration_comprehensive.py:27:5
   |
25 |     extract_returns,
26 |     extract_exchange_rates,
27 |     extract_inventory_snapshots,
   |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
28 |     write_delta,
29 |     write_parquet
   |
help: Remove unused import

F401 [*] `project_a.write_delta` imported but unused
  --> tests/test_integration_comprehensive.py:28:5
   |
26 |     extract_exchange_rates,
27 |     extract_inventory_snapshots,
28 |     write_delta,
   |     ^^^^^^^^^^^
29 |     write_parquet
30 | )
   |
help: Remove unused import

F401 [*] `project_a.write_parquet` imported but unused
  --> tests/test_integration_comprehensive.py:29:5
   |
27 |     extract_inventory_snapshots,
28 |     write_delta,
29 |     write_parquet
   |     ^^^^^^^^^^^^^
30 | )
   |
help: Remove unused import

F821 Undefined name `EnterpriseDataPlatform`
  --> tests/test_integration_comprehensive.py:65:25
   |
64 |         # Create enterprise platform
65 |         self.platform = EnterpriseDataPlatform(self.spark, self.config)
   |                         ^^^^^^^^^^^^^^^^^^^^^^
66 |
67 |         # Create test directories
   |

F821 Undefined name `stream_orders_to_bronze`
   --> tests/test_integration_comprehensive.py:424:29
    |
422 |         try:
423 |             # Test streaming function import and basic functionality
424 |             assert callable(stream_orders_to_bronze), "Streaming function not callable"
    |                             ^^^^^^^^^^^^^^^^^^^^^^^
425 |
426 |             # Create sample streaming data
    |

I001 [*] Import block is un-sorted or un-formatted
 --> tests/test_lineage_emitter.py:4:1
  |
2 |   Tests for lineage emitter.
3 |   """
4 | / import pytest
5 | | from unittest.mock import Mock, patch, MagicMock
6 | |
7 | | from project_a.monitoring.lineage_emitter import (
8 | |     emit_lineage_event, emit_start, emit_complete, emit_fail
9 | | )
  | |_^
  |
help: Organize imports

F401 [*] `unittest.mock.MagicMock` imported but unused
 --> tests/test_lineage_emitter.py:5:40
  |
3 | """
4 | import pytest
5 | from unittest.mock import Mock, patch, MagicMock
  |                                        ^^^^^^^^^
6 |
7 | from project_a.monitoring.lineage_emitter import (
  |
help: Remove unused import: `unittest.mock.MagicMock`

W293 [*] Blank line contains whitespace
  --> tests/test_lineage_emitter.py:28:1
   |
26 |     """Test successful lineage event emission."""
27 |     mock_post.return_value.raise_for_status = Mock()
28 |     
   | ^^^^
29 |     result = emit_lineage_event(
30 |         event_type="START",
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_lineage_emitter.py:36:1
   |
34 |         config=mock_config
35 |     )
36 |     
   | ^^^^
37 |     assert result is True
38 |     mock_post.assert_called_once()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_lineage_emitter.py:44:1
   |
42 |     """Test lineage emission when disabled."""
43 |     mock_config["lineage"]["enabled"] = False
44 |     
   | ^^^^
45 |     result = emit_lineage_event(
46 |         event_type="START",
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_lineage_emitter.py:52:1
   |
50 |         config=mock_config
51 |     )
52 |     
   | ^^^^
53 |     assert result is False
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_lineage_emitter.py:59:1
   |
57 |     """Test lineage emission with no URL configured."""
58 |     mock_config["lineage"]["url"] = ""
59 |     
   | ^^^^
60 |     result = emit_lineage_event(
61 |         event_type="START",
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_lineage_emitter.py:67:1
   |
65 |         config=mock_config
66 |     )
67 |     
   | ^^^^
68 |     assert result is False
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_lineage_emitter.py:75:1
   |
73 |     """Test emit_start convenience function."""
74 |     mock_post.return_value.raise_for_status = Mock()
75 |     
   | ^^^^
76 |     result = emit_start(
77 |         job_name="test_job",
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_lineage_emitter.py:82:1
   |
80 |         config=mock_config
81 |     )
82 |     
   | ^^^^
83 |     assert result is True
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_lineage_emitter.py:90:1
   |
88 |     """Test emit_complete convenience function."""
89 |     mock_post.return_value.raise_for_status = Mock()
90 |     
   | ^^^^
91 |     result = emit_complete(
92 |         job_name="test_job",
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_lineage_emitter.py:98:1
   |
96 |         metadata={"rows": 100}
97 |     )
98 |     
   | ^^^^
99 |     assert result is True
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_lineage_emitter.py:106:1
    |
104 |     """Test emit_fail convenience function."""
105 |     mock_post.return_value.raise_for_status = Mock()
106 |     
    | ^^^^
107 |     result = emit_fail(
108 |         job_name="test_job",
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_lineage_emitter.py:114:1
    |
112 |         error="Test error"
113 |     )
114 |     
    | ^^^^
115 |     assert result is True
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
 --> tests/test_load.py:1:1
  |
1 | / import os
2 | |
3 | | from project_a.load import (write_avro, write_json,
4 | |                                             write_parquet)
  | |__________________________________________________________^
  |
help: Organize imports

I001 [*] Import block is un-sorted or un-formatted
 --> tests/test_metrics_collector.py:4:1
  |
2 |   Tests for metrics collector.
3 |   """
4 | / import pytest
5 | | from unittest.mock import Mock, patch, MagicMock
6 | |
7 | | from project_a.monitoring.metrics_collector import (
8 | |     emit_rowcount, emit_duration, emit_metrics
9 | | )
  | |_^
  |
help: Organize imports

F401 [*] `unittest.mock.MagicMock` imported but unused
 --> tests/test_metrics_collector.py:5:40
  |
3 | """
4 | import pytest
5 | from unittest.mock import Mock, patch, MagicMock
  |                                        ^^^^^^^^^
6 |
7 | from project_a.monitoring.metrics_collector import (
  |
help: Remove unused import: `unittest.mock.MagicMock`

W293 [*] Blank line contains whitespace
  --> tests/test_metrics_collector.py:33:1
   |
31 |     mock_client = Mock()
32 |     mock_boto3.client.return_value = mock_client
33 |     
   | ^^^^
34 |     emit_rowcount("test_metric", 100, {}, mock_config)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_metrics_collector.py:35:1
   |
34 |     emit_rowcount("test_metric", 100, {}, mock_config)
35 |     
   | ^^^^
36 |     # Verify CloudWatch put_metric_data called
37 |     mock_client.put_metric_data.assert_called_once()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_metrics_collector.py:43:1
   |
41 |     """Test emit_rowcount in local mode."""
42 |     mock_config["monitoring"]["metrics_enabled"] = False
43 |     
   | ^^^^
44 |     # Should not raise exception
45 |     emit_rowcount("test_metric", 100, {}, mock_config)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_metrics_collector.py:53:1
   |
51 |     mock_client = Mock()
52 |     mock_boto3.client.return_value = mock_client
53 |     
   | ^^^^
54 |     emit_duration("test_duration", 5.5, {}, mock_config)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_metrics_collector.py:55:1
   |
54 |     emit_duration("test_duration", 5.5, {}, mock_config)
55 |     
   | ^^^^
56 |     mock_client.put_metric_data.assert_called_once()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_metrics_collector.py:64:1
   |
62 |     mock_client = Mock()
63 |     mock_boto3.client.return_value = mock_client
64 |     
   | ^^^^
65 |     emit_metrics(
66 |         job_name="test_job",
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_metrics_collector.py:73:1
   |
71 |         config=mock_config
72 |     )
73 |     
   | ^^^^
74 |     # Should call put_metric_data multiple times
75 |     assert mock_client.put_metric_data.call_count > 0
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
 --> tests/test_module_imports.py:5:1
  |
3 |   """
4 |
5 | / import pytest
6 | | import sys
7 | | from pathlib import Path
  | |________________________^
8 |
9 |   # Add src to path
  |
help: Organize imports

I001 [*] Import block is un-sorted or un-formatted
  --> tests/test_module_imports.py:25:9
   |
23 |       """Test that pipeline modules import correctly."""
24 |       try:
25 | /         from project_a.pipeline import bronze_to_silver
26 | |         from project_a.pipeline import silver_to_gold
   | |_____________________________________________________^
27 |           assert bronze_to_silver is not None
28 |           assert silver_to_gold is not None
   |
help: Organize imports

I001 [*] Import block is un-sorted or un-formatted
  --> tests/test_module_imports.py:36:9
   |
34 |       """Test that utility modules import correctly."""
35 |       try:
36 | /         from project_a.utils import config
37 | |         from project_a.utils import spark
38 | |         from project_a.utils import logging
   | |___________________________________________^
39 |           assert config is not None
40 |           assert spark is not None
   |
help: Organize imports

I001 [*] Import block is un-sorted or un-formatted
  --> tests/test_module_imports.py:49:9
   |
47 |       """Test that job modules import correctly."""
48 |       try:
49 | /         from project_a.jobs import kafka_orders_stream
50 | |         from project_a.jobs import salesforce_to_bronze
51 | |         from project_a.jobs import snowflake_to_bronze
   | |______________________________________________________^
52 |           assert kafka_orders_stream is not None
53 |           assert salesforce_to_bronze is not None
   |
help: Organize imports

W293 [*] Blank line contains whitespace
  --> tests/test_module_imports.py:63:1
   |
61 |     # This test ensures we're not using src.* imports
62 |     import project_a.utils.config as config_module
63 |     
   | ^^^^
64 |     # Check that the module path doesn't contain 'src.'
65 |     assert 'src.' not in config_module.__name__
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
 --> tests/test_monitoring.py:4:1
  |
2 |   Unit tests for monitoring/metrics module.
3 |   """
4 | / import pytest
5 | | import time
6 | | import sys
7 | | import os
  | |_________^
8 |
9 |   # Add src to path
  |
help: Organize imports

I001 [*] Import block is un-sorted or un-formatted
  --> tests/test_monitoring.py:12:1
   |
10 |   sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))
11 |
12 | / from project_a.monitoring.metrics import (
13 | |     track_job_execution,
14 | |     track_stage_duration,
15 | |     record_dq_check,
16 | |     record_records_processed,
17 | |     record_records_failed,
18 | |     record_delta_table_metrics,
19 | |     record_delta_write,
20 | |     record_schema_drift,
21 | |     record_error,
22 | |     get_metrics_text,
23 | |     registry,
24 | | )
   | |_^
   |
help: Organize imports

F401 [*] `project_a.monitoring.metrics.registry` imported but unused
  --> tests/test_monitoring.py:23:5
   |
21 |     record_error,
22 |     get_metrics_text,
23 |     registry,
   |     ^^^^^^^^
24 | )
   |
help: Remove unused import: `project_a.monitoring.metrics.registry`

W293 [*] Blank line contains whitespace
  --> tests/test_monitoring.py:36:1
   |
34 |             time.sleep(0.1)
35 |             return "success"
36 |         
   | ^^^^^^^^
37 |         result = successful_job()
38 |         assert result == "success"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_monitoring.py:39:1
   |
37 |         result = successful_job()
38 |         assert result == "success"
39 |         
   | ^^^^^^^^
40 |         # Verify metrics were recorded
41 |         metrics = get_metrics_text().decode('utf-8')
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_monitoring.py:51:1
   |
49 |             time.sleep(0.1)
50 |             raise ValueError("Test error")
51 |         
   | ^^^^^^^^
52 |         with pytest.raises(ValueError, match="Test error"):
53 |             failing_job()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_monitoring.py:54:1
   |
52 |         with pytest.raises(ValueError, match="Test error"):
53 |             failing_job()
54 |         
   | ^^^^^^^^
55 |         # Verify error metrics were recorded
56 |         metrics = get_metrics_text().decode('utf-8')
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_monitoring.py:64:1
   |
62 |         with track_stage_duration("test_pipeline", "extract", "test"):
63 |             time.sleep(0.05)
64 |         
   | ^^^^^^^^
65 |         metrics = get_metrics_text().decode('utf-8')
66 |         assert 'etl_job_duration_seconds' in metrics
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_monitoring.py:77:1
   |
75 |         except RuntimeError:
76 |             pass
77 |         
   | ^^^^^^^^
78 |         metrics = get_metrics_text().decode('utf-8')
79 |         assert 'etl_errors_total' in metrics
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_monitoring.py:85:1
   |
83 |         """Test recording passed DQ check."""
84 |         record_dq_check("test_table", "null_check", passed=True)
85 |         
   | ^^^^^^^^
86 |         metrics = get_metrics_text().decode('utf-8')
87 |         assert 'etl_dq_checks_total{check_type="null_check",status="passed",table="test_table"}' in metrics
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_monitoring.py:92:1
   |
90 |         """Test recording failed DQ check with violations."""
91 |         record_dq_check("test_table", "range_check", passed=False, violations=10, severity="critical")
92 |         
   | ^^^^^^^^
93 |         metrics = get_metrics_text().decode('utf-8')
94 |         assert 'etl_dq_checks_total{check_type="range_check",status="failed",table="test_table"}' in metrics
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_monitoring.py:100:1
    |
 98 |         """Test recording processed records."""
 99 |         record_records_processed("test_job", "transform", "customers", 1000)
100 |         
    | ^^^^^^^^
101 |         metrics = get_metrics_text().decode('utf-8')
102 |         assert 'etl_records_processed_total' in metrics
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_monitoring.py:109:1
    |
107 |         """Test recording failed records."""
108 |         record_records_failed("test_job", "transform", "orders", 5, "ValidationError")
109 |         
    | ^^^^^^^^
110 |         metrics = get_metrics_text().decode('utf-8')
111 |         assert 'etl_records_failed_total' in metrics
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_monitoring.py:122:1
    |
120 |         }
121 |         record_delta_table_metrics("customers", "bronze", metrics_data)
122 |         
    | ^^^^^^^^
123 |         metrics = get_metrics_text().decode('utf-8')
124 |         assert 'delta_table_size_bytes{layer="bronze",table="customers"}' in metrics
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_monitoring.py:131:1
    |
129 |         """Test recording Delta write operation."""
130 |         record_delta_write("orders", "merge", "silver", 2.5)
131 |         
    | ^^^^^^^^
132 |         metrics = get_metrics_text().decode('utf-8')
133 |         assert 'delta_write_duration_seconds' in metrics
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_monitoring.py:140:1
    |
138 |         """Test recording schema drift."""
139 |         record_schema_drift("products", "column_added")
140 |         
    | ^^^^^^^^
141 |         metrics = get_metrics_text().decode('utf-8')
142 |         assert 'etl_schema_drift_detected_total' in metrics
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_monitoring.py:148:1
    |
146 |         """Test recording errors."""
147 |         record_error("etl_pipeline", "load", "ConnectionError", "critical")
148 |         
    | ^^^^^^^^
149 |         metrics = get_metrics_text().decode('utf-8')
150 |         assert 'etl_errors_total' in metrics
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_monitoring.py:158:1
    |
156 |         metrics = get_metrics_text()
157 |         assert isinstance(metrics, bytes)
158 |         
    | ^^^^^^^^
159 |         # Decode and verify contains some expected content
160 |         metrics_text = metrics.decode('utf-8')
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_monitoring.py:171:1
    |
169 |         record_records_failed("batch_job", "transform", "users", 10, "ParseError")
170 |         record_dq_check("users", "completeness", True)
171 |         
    | ^^^^^^^^
172 |         metrics = get_metrics_text().decode('utf-8')
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_monitoring.py:173:1
    |
172 |         metrics = get_metrics_text().decode('utf-8')
173 |         
    | ^^^^^^^^
174 |         # Verify all metrics are present
175 |         assert 'etl_records_processed_total' in metrics
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_monitoring.py:183:1
    |
181 |         # Should handle tables/jobs with underscores, hyphens
182 |         record_records_processed("my-etl-job_v2", "extract", "user_events", 1000)
183 |         
    | ^^^^^^^^
184 |         metrics = get_metrics_text().decode('utf-8')
185 |         assert 'user_events' in metrics
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
 --> tests/test_orders_transform.py:6:1
  |
4 |   """
5 |
6 | / import pytest
7 | | from pyspark.sql import SparkSession
8 | | from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType
9 | | from src.project_a.transforms.bronze_to_silver import transform_orders_bronze_to_silver
  | |_______________________________________________________________________________________^
  |
help: Organize imports

F401 [*] `pyspark.sql.types.TimestampType` imported but unused
 --> tests/test_orders_transform.py:8:93
  |
6 | import pytest
7 | from pyspark.sql import SparkSession
8 | from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType
  |                                                                                             ^^^^^^^^^^^^^
9 | from src.project_a.transforms.bronze_to_silver import transform_orders_bronze_to_silver
  |
help: Remove unused import: `pyspark.sql.types.TimestampType`

W293 [*] Blank line contains whitespace
  --> tests/test_orders_transform.py:30:1
   |
28 |         ("O3", "C3", "P3", 1, 75.0, 75.0, "2025-01-03T00:00:00", "Card", "PENDING"),
29 |     ]
30 |     
   | ^^^^
31 |     schema = StructType([
32 |         StructField("order_id", StringType(), True),
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_orders_transform.py:42:1
   |
40 |         StructField("status", StringType(), True),
41 |     ])
42 |     
   | ^^^^
43 |     df = spark.createDataFrame(test_data, schema)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_orders_transform.py:44:1
   |
43 |     df = spark.createDataFrame(test_data, schema)
44 |     
   | ^^^^
45 |     # Transform
46 |     result = transform_orders_bronze_to_silver(spark, df)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_orders_transform.py:47:1
   |
45 |     # Transform
46 |     result = transform_orders_bronze_to_silver(spark, df)
47 |     
   | ^^^^
48 |     # Assertions
49 |     assert result.count() == 3
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_orders_transform.py:51:1
   |
49 |     assert result.count() == 3
50 |     assert len(result.columns) == 9  # Expected silver schema columns
51 |     
   | ^^^^
52 |     # Check that all required columns exist
53 |     expected_columns = ["order_id", "customer_id", "product_id", "quantity", 
   |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
  --> tests/test_orders_transform.py:53:77
   |
52 |     # Check that all required columns exist
53 |     expected_columns = ["order_id", "customer_id", "product_id", "quantity", 
   |                                                                             ^
54 |                        "unit_price", "total_amount", "order_date", "payment_method", "status"]
55 |     for col in expected_columns:
   |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
  --> tests/test_orders_transform.py:57:1
   |
55 |     for col in expected_columns:
56 |         assert col in result.columns
57 |     
   | ^^^^
58 |     # Check data types
59 |     assert result.schema["order_id"].dataType == StringType()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_orders_transform.py:69:1
   |
67 |     """Test orders transformation with nested payment structure."""
68 |     from pyspark.sql.functions import struct
69 |     
   | ^^^^
70 |     # Create test data with nested payment
71 |     test_data = [
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_orders_transform.py:75:1
   |
73 |         ("O2", "C2", "P2", 2, 50.0, "2025-01-02T00:00:00", "Cash", "PENDING"),
74 |     ]
75 |     
   | ^^^^
76 |     schema = StructType([
77 |         StructField("order_id", StringType(), True),
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_orders_transform.py:86:1
   |
84 |         StructField("status", StringType(), True),
85 |     ])
86 |     
   | ^^^^
87 |     df = spark.createDataFrame(test_data, schema)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_orders_transform.py:88:1
   |
87 |     df = spark.createDataFrame(test_data, schema)
88 |     
   | ^^^^
89 |     # Add nested payment structure
90 |     df = df.withColumn("payment", struct(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_orders_transform.py:94:1
   |
92 |         df.status.alias("status")
93 |     ))
94 |     
   | ^^^^
95 |     # Transform
96 |     result = transform_orders_bronze_to_silver(spark, df)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_orders_transform.py:97:1
   |
95 |     # Transform
96 |     result = transform_orders_bronze_to_silver(spark, df)
97 |     
   | ^^^^
98 |     # Assertions
99 |     assert result.count() == 2
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_orders_transform.py:113:1
    |
111 |         ("O4", "C4", "P4", 1, -10.0, -10.0, "2025-01-04T00:00:00", "Card", "PAID"),   # Invalid: negative amount
112 |     ]
113 |     
    | ^^^^
114 |     schema = StructType([
115 |         StructField("order_id", StringType(), True),
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_orders_transform.py:125:1
    |
123 |         StructField("status", StringType(), True),
124 |     ])
125 |     
    | ^^^^
126 |     df = spark.createDataFrame(test_data, schema)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_orders_transform.py:127:1
    |
126 |     df = spark.createDataFrame(test_data, schema)
127 |     
    | ^^^^
128 |     # Transform
129 |     result = transform_orders_bronze_to_silver(spark, df)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_orders_transform.py:130:1
    |
128 |     # Transform
129 |     result = transform_orders_bronze_to_silver(spark, df)
130 |     
    | ^^^^
131 |     # Should only keep valid records
132 |     assert result.count() == 1  # Only O1 should pass
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_orders_transform.py:144:1
    |
142 |         ("O2", "C2", "P2", 2, 50.0, 100.0, "2025-01-02T00:00:00", "Cash", "PAID"),
143 |     ]
144 |     
    | ^^^^
145 |     schema = StructType([
146 |         StructField("order_id", StringType(), True),
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_orders_transform.py:156:1
    |
154 |         StructField("status", StringType(), True),
155 |     ])
156 |     
    | ^^^^
157 |     df = spark.createDataFrame(test_data, schema)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_orders_transform.py:158:1
    |
157 |     df = spark.createDataFrame(test_data, schema)
158 |     
    | ^^^^
159 |     # Transform
160 |     result = transform_orders_bronze_to_silver(spark, df)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_orders_transform.py:161:1
    |
159 |     # Transform
160 |     result = transform_orders_bronze_to_silver(spark, df)
161 |     
    | ^^^^
162 |     # Should remove duplicates
163 |     assert result.count() == 2  # O1 and O2, duplicate removed
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> tests/test_performance_optimization.py:13:1
   |
11 |   """
12 |
13 | / import pytest
14 | | import json
15 | | from unittest.mock import Mock, patch
16 | | from pyspark.sql import SparkSession
17 | | from pyspark.sql import DataFrame
18 | |
19 | | from project_a.performance_optimizer import (
20 | |     PerformanceOptimizer,
21 | |     CacheManager,
22 | |     PerformanceBenchmark,
23 | |     PerformanceMetrics,
24 | |     create_performance_optimizer
25 | | )
   | |_^
   |
help: Organize imports

F841 Local variable `optimized_df` is assigned to but never used
   --> tests/test_performance_optimization.py:256:9
    |
254 |         df.coalesce = Mock(return_value=df)
255 |
256 |         optimized_df = optimizer._optimize_shuffle_partitions(df)
    |         ^^^^^^^^^^^^
257 |
258 |         # Should call coalesce to reduce partitions
    |
help: Remove assignment to unused variable `optimized_df`

F841 Local variable `optimized_df` is assigned to but never used
   --> tests/test_performance_optimization.py:272:9
    |
270 |         df.repartition = Mock(return_value=df)
271 |
272 |         optimized_df = optimizer._optimize_file_compaction(df)
    |         ^^^^^^^^^^^^
273 |
274 |         # Should call repartition for compaction
    |
help: Remove assignment to unused variable `optimized_df`

F841 Local variable `cached_df` is assigned to but never used
   --> tests/test_performance_optimization.py:400:13
    |
398 |             df.persist = Mock(return_value=df)
399 |
400 |             cached_df = optimizer.cache_manager.smart_cache(df, f"dataset_{row_count}", "auto")
    |             ^^^^^^^^^
401 |
402 |             # Verify storage level selection
    |
help: Remove assignment to unused variable `cached_df`

I001 [*] Import block is un-sorted or un-formatted
  --> tests/test_pii_utils.py:4:1
   |
 2 |   Tests for PII utilities.
 3 |   """
 4 | / import pytest
 5 | | from unittest.mock import Mock, patch
 6 | | import hashlib
 7 | |
 8 | | from project_a.utils.pii_utils import (
 9 | |     mask_email, mask_phone, hash_value, mask_name, apply_pii_masking
10 | | )
   | |_^
   |
help: Organize imports

F401 [*] `pytest` imported but unused
 --> tests/test_pii_utils.py:4:8
  |
2 | Tests for PII utilities.
3 | """
4 | import pytest
  |        ^^^^^^
5 | from unittest.mock import Mock, patch
6 | import hashlib
  |
help: Remove unused import: `pytest`

F401 [*] `unittest.mock.Mock` imported but unused
 --> tests/test_pii_utils.py:5:27
  |
3 | """
4 | import pytest
5 | from unittest.mock import Mock, patch
  |                           ^^^^
6 | import hashlib
  |
help: Remove unused import

F401 [*] `unittest.mock.patch` imported but unused
 --> tests/test_pii_utils.py:5:33
  |
3 | """
4 | import pytest
5 | from unittest.mock import Mock, patch
  |                                 ^^^^^
6 | import hashlib
  |
help: Remove unused import

F401 [*] `hashlib` imported but unused
 --> tests/test_pii_utils.py:6:8
  |
4 | import pytest
5 | from unittest.mock import Mock, patch
6 | import hashlib
  |        ^^^^^^^
7 |
8 | from project_a.utils.pii_utils import (
  |
help: Remove unused import: `hashlib`

W293 [*] Blank line contains whitespace
  --> tests/test_pii_utils.py:16:1
   |
14 |     """Test email masking."""
15 |     result = mask_email("john.doe@example.com")
16 |     
   | ^^^^
17 |     assert "@" in result
18 |     assert "example.com" in result
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_pii_utils.py:25:1
   |
23 |     """Test phone masking."""
24 |     result = mask_phone("123-456-7890")
25 |     
   | ^^^^
26 |     assert len(result) > 0
27 |     # Should mask most digits
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_pii_utils.py:34:1
   |
32 |     """Test value hashing."""
33 |     result = hash_value("test_value")
34 |     
   | ^^^^
35 |     assert result is not None
36 |     assert len(result) > 0
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_pii_utils.py:44:1
   |
42 |     """Test name masking."""
43 |     result = mask_name("John Doe")
44 |     
   | ^^^^
45 |     assert result is not None
46 |     assert len(result) > 0
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> tests/test_pii_utils.py:51:5
   |
49 | def test_apply_pii_masking(spark):
50 |     """Test applying PII masking to DataFrame."""
51 |     from pyspark.sql.types import StructType, StructField, StringType
   |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
52 |     
53 |     data = [
   |
help: Organize imports

W293 [*] Blank line contains whitespace
  --> tests/test_pii_utils.py:52:1
   |
50 |     """Test applying PII masking to DataFrame."""
51 |     from pyspark.sql.types import StructType, StructField, StringType
52 |     
   | ^^^^
53 |     data = [
54 |         ("john@example.com", "123-456-7890", "John Doe"),
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_pii_utils.py:62:1
   |
60 |     ])
61 |     df = spark.createDataFrame(data, schema)
62 |     
   | ^^^^
63 |     # Define PII columns
64 |     pii_config = {
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_pii_utils.py:69:1
   |
67 |         "name": "hash"
68 |     }
69 |     
   | ^^^^
70 |     result_df = apply_pii_masking(df, pii_config)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_pii_utils.py:71:1
   |
70 |     result_df = apply_pii_masking(df, pii_config)
71 |     
   | ^^^^
72 |     assert result_df is not None
73 |     assert "email" in result_df.columns
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> tests/test_pipeline_components.py:4:1
   |
 2 |   Test the new pipeline components integration.
 3 |   """
 4 | / import pytest
 5 | | import tempfile
 6 | | import shutil
 7 | | from pathlib import Path
 8 | | from unittest.mock import Mock, patch
 9 | | import sys
10 | | import os
   | |_________^
11 |
12 |   # Add src to path for imports
   |
help: Organize imports

I001 [*] Import block is un-sorted or un-formatted
  --> tests/test_pipeline_components.py:15:1
   |
13 |   sys.path.insert(0, str(Path(__file__).parent.parent / "src"))
14 |
15 | / from project_a.pipeline_stages.bronze_to_silver import run as b2s_run
16 | | from project_a.pipeline_stages.silver_to_gold import run as s2g_run
17 | | from project_a.logging_setup import get_logger, new_correlation_id
18 | | from project_a.config.paths import BRONZE, SILVER, GOLD
   | |_______________________________________________________^
19 |
20 |   class TestPipelineComponents:
   |
help: Organize imports

I001 [*] Import block is un-sorted or un-formatted
 --> tests/test_publish_gold_to_snowflake.py:4:1
  |
2 |   Tests for publish_gold_to_snowflake job.
3 |   """
4 | / import pytest
5 | | from unittest.mock import Mock, patch, MagicMock
6 | | from pyspark.sql import SparkSession
7 | | from pyspark.sql.types import StructType, StructField, StringType, DecimalType
8 | |
9 | | from jobs.publish_gold_to_snowflake import load_customer_360_to_snowflake
  | |_________________________________________________________________________^
  |
help: Organize imports

F401 [*] `unittest.mock.Mock` imported but unused
 --> tests/test_publish_gold_to_snowflake.py:5:27
  |
3 | """
4 | import pytest
5 | from unittest.mock import Mock, patch, MagicMock
  |                           ^^^^
6 | from pyspark.sql import SparkSession
7 | from pyspark.sql.types import StructType, StructField, StringType, DecimalType
  |
help: Remove unused import

F401 [*] `unittest.mock.MagicMock` imported but unused
 --> tests/test_publish_gold_to_snowflake.py:5:40
  |
3 | """
4 | import pytest
5 | from unittest.mock import Mock, patch, MagicMock
  |                                        ^^^^^^^^^
6 | from pyspark.sql import SparkSession
7 | from pyspark.sql.types import StructType, StructField, StringType, DecimalType
  |
help: Remove unused import

F401 [*] `pyspark.sql.SparkSession` imported but unused
 --> tests/test_publish_gold_to_snowflake.py:6:25
  |
4 | import pytest
5 | from unittest.mock import Mock, patch, MagicMock
6 | from pyspark.sql import SparkSession
  |                         ^^^^^^^^^^^^
7 | from pyspark.sql.types import StructType, StructField, StringType, DecimalType
  |
help: Remove unused import: `pyspark.sql.SparkSession`

W293 [*] Blank line contains whitespace
  --> tests/test_publish_gold_to_snowflake.py:70:1
   |
68 |         "schema": "PUBLIC"
69 |     }
70 |     
   | ^^^^
71 |     # Write sample data
72 |     gold_path = tmp_path / "gold" / "customer_360"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_publish_gold_to_snowflake.py:75:1
   |
73 |     gold_path.mkdir(parents=True)
74 |     sample_gold_data.write.format("delta").mode("overwrite").save(str(gold_path))
75 |     
   | ^^^^
76 |     mock_config["data_lake"]["gold_path"] = str(tmp_path / "gold")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_publish_gold_to_snowflake.py:77:1
   |
76 |     mock_config["data_lake"]["gold_path"] = str(tmp_path / "gold")
77 |     
   | ^^^^
78 |     # Mock Snowflake write operations
79 |     with patch.object(spark.read.format("delta"), "load", return_value=sample_gold_data):
   |
help: Remove whitespace from blank line

F841 Local variable `mock_write` is assigned to but never used
  --> tests/test_publish_gold_to_snowflake.py:80:82
   |
78 |     # Mock Snowflake write operations
79 |     with patch.object(spark.read.format("delta"), "load", return_value=sample_gold_data):
80 |         with patch.object(sample_gold_data.write.format("snowflake"), "save") as mock_write:
   |                                                                                  ^^^^^^^^^^
81 |             with patch.object(spark.read.format("jdbc"), "load") as mock_jdbc:
82 |                 # Mock JDBC operations
   |
help: Remove assignment to unused variable `mock_write`

W293 [*] Blank line contains whitespace
  --> tests/test_publish_gold_to_snowflake.py:84:1
   |
82 |                 # Mock JDBC operations
83 |                 mock_jdbc.return_value.collect.return_value = []
84 |                 
   | ^^^^^^^^^^^^^^^^
85 |                 try:
86 |                     result = load_customer_360_to_snowflake(spark, mock_config)
   |
help: Remove whitespace from blank line

F841 Local variable `result` is assigned to but never used
  --> tests/test_publish_gold_to_snowflake.py:86:21
   |
85 |                 try:
86 |                     result = load_customer_360_to_snowflake(spark, mock_config)
   |                     ^^^^^^
87 |                 except Exception:
88 |                     # Expected to fail in test without real Snowflake, but verify mocks called
   |
help: Remove assignment to unused variable `result`

W293 [*] Blank line contains whitespace
  --> tests/test_publish_gold_to_snowflake.py:90:1
   |
88 |                     # Expected to fail in test without real Snowflake, but verify mocks called
89 |                     pass
90 |                 
   | ^^^^^^^^^^^^^^^^
91 |                 # Verify lineage events emitted
92 |                 mock_emit_start.assert_called_once()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_publish_gold_to_snowflake.py:106:1
    |
104 |         "warehouse": "test_warehouse"
105 |     }
106 |     
    | ^^^^
107 |     mock_config["data_lake"]["gold_path"] = str(tmp_path / "nonexistent" / "gold")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_publish_gold_to_snowflake.py:108:1
    |
107 |     mock_config["data_lake"]["gold_path"] = str(tmp_path / "nonexistent" / "gold")
108 |     
    | ^^^^
109 |     # Should raise exception for missing data
110 |     with pytest.raises(Exception):
    |
help: Remove whitespace from blank line

B017 Do not assert blind exception: `Exception`
   --> tests/test_publish_gold_to_snowflake.py:110:10
    |
109 |     # Should raise exception for missing data
110 |     with pytest.raises(Exception):
    |          ^^^^^^^^^^^^^^^^^^^^^^^^
111 |         load_customer_360_to_snowflake(spark, mock_config)
    |

I001 [*] Import block is un-sorted or un-formatted
  --> tests/test_quality_gate.py:8:1
   |
 6 |   """
 7 |
 8 | / import pytest
 9 | | from pathlib import Path
10 | | import json
11 | | from pyspark.sql import SparkSession
12 | | from project_a.dq.runner import run_suite
13 | | from project_a.utils.config import load_conf
14 | | from project_a.utils.spark_session import build_spark
   | |_____________________________________________________^
   |
help: Organize imports

F401 [*] `pathlib.Path` imported but unused
  --> tests/test_quality_gate.py:9:21
   |
 8 | import pytest
 9 | from pathlib import Path
   |                     ^^^^
10 | import json
11 | from pyspark.sql import SparkSession
   |
help: Remove unused import: `pathlib.Path`

F401 [*] `json` imported but unused
  --> tests/test_quality_gate.py:10:8
   |
 8 | import pytest
 9 | from pathlib import Path
10 | import json
   |        ^^^^
11 | from pyspark.sql import SparkSession
12 | from project_a.dq.runner import run_suite
   |
help: Remove unused import: `json`

F401 [*] `pyspark.sql.SparkSession` imported but unused
  --> tests/test_quality_gate.py:11:25
   |
 9 | from pathlib import Path
10 | import json
11 | from pyspark.sql import SparkSession
   |                         ^^^^^^^^^^^^
12 | from project_a.dq.runner import run_suite
13 | from project_a.utils.config import load_conf
   |
help: Remove unused import: `pyspark.sql.SparkSession`

W293 [*] Blank line contains whitespace
  --> tests/test_quality_gate.py:19:1
   |
17 | class TestQualityGate:
18 |     """Quality gate tests that must pass for deployment."""
19 |     
   | ^^^^
20 |     @pytest.fixture(scope="class")
21 |     def spark(self):
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_quality_gate.py:27:1
   |
25 |         yield spark
26 |         spark.stop()
27 |     
   | ^^^^
28 |     @pytest.fixture
29 |     def quality_thresholds(self):
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_quality_gate.py:36:1
   |
34 |             "critical_checks": ["not_null_keys", "referential_integrity"]
35 |         })
36 |     
   | ^^^^
37 |     def test_silver_orders_quality_gate(self, spark, quality_thresholds):
38 |         """
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_quality_gate.py:43:1
   |
41 |         """
42 |         silver_orders_path = "data/lakehouse_delta/silver/orders"
43 |         
   | ^^^^^^^^
44 |         # Skip if table doesn't exist (e.g., in CI without data)
45 |         try:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_quality_gate.py:51:1
   |
49 |         except Exception:
50 |             pytest.skip("Silver orders table does not exist - skipping quality gate")
51 |         
   | ^^^^^^^^
52 |         # Run DQ suite
53 |         result = run_suite("silver_orders_not_null_keys", silver_orders_path, spark)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_quality_gate.py:54:1
   |
52 |         # Run DQ suite
53 |         result = run_suite("silver_orders_not_null_keys", silver_orders_path, spark)
54 |         
   | ^^^^^^^^
55 |         # Assert critical checks pass
56 |         assert result.passed, f"Quality gate failed for silver.orders: {result.failures}"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_quality_gate.py:57:1
   |
55 |         # Assert critical checks pass
56 |         assert result.passed, f"Quality gate failed for silver.orders: {result.failures}"
57 |         
   | ^^^^^^^^
58 |         # Assert pass rate meets threshold
59 |         total_checks = len(result.checks)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_quality_gate.py:62:1
   |
60 |         passed_checks = total_checks - len(result.failures)
61 |         pass_rate = passed_checks / total_checks if total_checks > 0 else 0
62 |         
   | ^^^^^^^^
63 |         min_pass_rate = quality_thresholds.get("min_pass_rate", 0.95)
64 |         assert pass_rate >= min_pass_rate, (
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_quality_gate.py:68:1
   |
66 |             f"Failed checks: {result.failures}"
67 |         )
68 |     
   | ^^^^
69 |     def test_silver_fx_rates_quality_gate(self, spark, quality_thresholds):
70 |         """
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_quality_gate.py:74:1
   |
72 |         """
73 |         silver_fx_path = "data/lakehouse_delta/silver/fx_rates"
74 |         
   | ^^^^^^^^
75 |         try:
76 |             df = spark.read.format("delta").load(silver_fx_path)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_quality_gate.py:81:1
   |
79 |         except Exception:
80 |             pytest.skip("Silver FX rates table does not exist")
81 |         
   | ^^^^^^^^
82 |         result = run_suite("silver_fx_rates_not_null_keys", silver_fx_path, spark)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_quality_gate.py:83:1
   |
82 |         result = run_suite("silver_fx_rates_not_null_keys", silver_fx_path, spark)
83 |         
   | ^^^^^^^^
84 |         assert result.passed, f"Quality gate failed for silver.fx_rates: {result.failures}"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_quality_gate.py:85:1
   |
84 |         assert result.passed, f"Quality gate failed for silver.fx_rates: {result.failures}"
85 |         
   | ^^^^^^^^
86 |         min_pass_rate = quality_thresholds.get("min_pass_rate", 0.95)
87 |         total_checks = len(result.checks)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_quality_gate.py:90:1
   |
88 |         passed_checks = total_checks - len(result.failures)
89 |         pass_rate = passed_checks / total_checks if total_checks > 0 else 0
90 |         
   | ^^^^^^^^
91 |         assert pass_rate >= min_pass_rate, (
92 |             f"Quality gate pass rate {pass_rate:.2%} below threshold {min_pass_rate:.2%}"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_quality_gate.py:94:1
   |
92 |             f"Quality gate pass rate {pass_rate:.2%} below threshold {min_pass_rate:.2%}"
93 |         )
94 |     
   | ^^^^
95 |     def test_gold_revenue_quality_gate(self, spark, quality_thresholds):
96 |         """
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_quality_gate.py:100:1
    |
 98 |         """
 99 |         gold_revenue_path = "data/lakehouse_delta/gold/fact_revenue"
100 |         
    | ^^^^^^^^
101 |         try:
102 |             df = spark.read.format("delta").load(gold_revenue_path)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_quality_gate.py:107:1
    |
105 |         except Exception:
106 |             pytest.skip("Gold revenue table does not exist")
107 |         
    | ^^^^^^^^
108 |         result = run_suite("gold_revenue_not_null_keys", gold_revenue_path, spark)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_quality_gate.py:109:1
    |
108 |         result = run_suite("gold_revenue_not_null_keys", gold_revenue_path, spark)
109 |         
    | ^^^^^^^^
110 |         assert result.passed, f"Quality gate failed for gold.fact_revenue: {result.failures}"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_quality_gate.py:111:1
    |
110 |         assert result.passed, f"Quality gate failed for gold.fact_revenue: {result.failures}"
111 |         
    | ^^^^^^^^
112 |         min_pass_rate = quality_thresholds.get("min_pass_rate", 0.95)
113 |         total_checks = len(result.checks)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_quality_gate.py:116:1
    |
114 |         passed_checks = total_checks - len(result.failures)
115 |         pass_rate = passed_checks / total_checks if total_checks > 0 else 0
116 |         
    | ^^^^^^^^
117 |         assert pass_rate >= min_pass_rate, (
118 |             f"Quality gate pass rate {pass_rate:.2%} below threshold {min_pass_rate:.2%}"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_quality_gate.py:120:1
    |
118 |             f"Quality gate pass rate {pass_rate:.2%} below threshold {min_pass_rate:.2%}"
119 |         )
120 |     
    | ^^^^
121 |     def test_critical_checks_defined(self, quality_thresholds):
122 |         """
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_quality_gate.py:127:1
    |
125 |         critical_checks = quality_thresholds.get("critical_checks", [])
126 |         assert len(critical_checks) > 0, "No critical checks defined in quality gates config"
127 |         
    | ^^^^^^^^
128 |         # Verify critical check names are valid
129 |         valid_critical_checks = ["not_null_keys", "referential_integrity", "freshness", "volume"]
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> tests/test_safe_writer.py:4:1
   |
 2 |   Unit tests for SafeDeltaWriter module.
 3 |   """
 4 | / import pytest
 5 | | import tempfile
 6 | | import shutil
 7 | | from pathlib import Path
 8 | | from datetime import datetime, date
 9 | | from pyspark.sql import SparkSession, DataFrame
10 | | from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType
11 | | from delta.tables import DeltaTable
12 | | import sys
13 | | import os
   | |_________^
14 |
15 |   # Add src to path
   |
help: Organize imports

F401 [*] `datetime.datetime` imported but unused
  --> tests/test_safe_writer.py:8:22
   |
 6 | import shutil
 7 | from pathlib import Path
 8 | from datetime import datetime, date
   |                      ^^^^^^^^
 9 | from pyspark.sql import SparkSession, DataFrame
10 | from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType
   |
help: Remove unused import: `datetime.datetime`

F401 [*] `pyspark.sql.types.IntegerType` imported but unused
  --> tests/test_safe_writer.py:10:68
   |
 8 | from datetime import datetime, date
 9 | from pyspark.sql import SparkSession, DataFrame
10 | from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType
   |                                                                    ^^^^^^^^^^^
11 | from delta.tables import DeltaTable
12 | import sys
   |
help: Remove unused import: `pyspark.sql.types.IntegerType`

W293 [*] Blank line contains whitespace
  --> tests/test_safe_writer.py:34:1
   |
32 |         .getOrCreate()
33 |     )
34 |     
   | ^^^^
35 |     spark.sparkContext.setLogLevel("ERROR")
36 |     yield spark
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_safe_writer.py:83:1
   |
81 |         writer = SafeDeltaWriter(spark)
82 |         target_path = str(Path(temp_dir) / "customers")
83 |         
   | ^^^^^^^^
84 |         result = writer.write_with_merge(
85 |             df=sample_data,
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_safe_writer.py:90:1
   |
88 |             mode="append"
89 |         )
90 |         
   | ^^^^^^^^
91 |         assert result["success"] is True
92 |         assert result["final_row_count"] == 3
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_safe_writer.py:95:1
   |
93 |         assert result["initial_row_count"] == 0
94 |         assert result["records_written"] == 3
95 |         
   | ^^^^^^^^
96 |         # Verify table exists
97 |         delta_table = DeltaTable.forPath(spark, target_path)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_safe_writer.py:104:1
    |
102 |         writer = SafeDeltaWriter(spark)
103 |         target_path = str(Path(temp_dir) / "customers")
104 |         
    | ^^^^^^^^
105 |         # Initial write
106 |         writer.write_with_merge(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_safe_writer.py:112:1
    |
110 |             mode="append"
111 |         )
112 |         
    | ^^^^^^^^
113 |         # Update data
114 |         updated_data = [
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_safe_writer.py:119:1
    |
117 |         ]
118 |         df_update = spark.createDataFrame(updated_data, schema=sample_schema)
119 |         
    | ^^^^^^^^
120 |         # Merge
121 |         result = writer.write_with_merge(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_safe_writer.py:127:1
    |
125 |             mode="merge"
126 |         )
127 |         
    | ^^^^^^^^
128 |         assert result["success"] is True
129 |         assert result["final_row_count"] == 4  # 3 original + 1 new
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_safe_writer.py:130:1
    |
128 |         assert result["success"] is True
129 |         assert result["final_row_count"] == 4  # 3 original + 1 new
130 |         
    | ^^^^^^^^
131 |         # Verify merge results
132 |         delta_table = DeltaTable.forPath(spark, target_path)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_safe_writer.py:134:1
    |
132 |         delta_table = DeltaTable.forPath(spark, target_path)
133 |         result_df = delta_table.toDF()
134 |         
    | ^^^^^^^^
135 |         # Check updated record
136 |         alice = result_df.filter("customer_id = 'C001'").collect()[0]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_safe_writer.py:139:1
    |
137 |         assert alice["name"] == "Alice Updated"
138 |         assert alice["balance"] == 150.0
139 |         
    | ^^^^^^^^
140 |         # Check new record
141 |         david = result_df.filter("customer_id = 'C004'").collect()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_safe_writer.py:148:1
    |
146 |         writer = SafeDeltaWriter(spark)
147 |         target_path = str(Path(temp_dir) / "customers")
148 |         
    | ^^^^^^^^
149 |         # Initial write
150 |         writer.write_with_merge(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_safe_writer.py:156:1
    |
154 |             mode="append"
155 |         )
156 |         
    | ^^^^^^^^
157 |         # New data for same partition
158 |         new_data = [
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_safe_writer.py:162:1
    |
160 |         ]
161 |         df_new = spark.createDataFrame(new_data, schema=sample_schema)
162 |         
    | ^^^^^^^^
163 |         # Overwrite partition
164 |         result = writer.write_with_merge(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_safe_writer.py:171:1
    |
169 |             replace_where_condition="ingest_date = '2024-01-01'"
170 |         )
171 |         
    | ^^^^^^^^
172 |         assert result["success"] is True
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_safe_writer.py:173:1
    |
172 |         assert result["success"] is True
173 |         
    | ^^^^^^^^
174 |         # Verify partition was replaced
175 |         delta_table = DeltaTable.forPath(spark, target_path)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_safe_writer.py:177:1
    |
175 |         delta_table = DeltaTable.forPath(spark, target_path)
176 |         result_df = delta_table.toDF()
177 |         
    | ^^^^^^^^
178 |         # Should only have the new record for this partition
179 |         partition_data = result_df.filter("ingest_date = '2024-01-01'").collect()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_safe_writer.py:187:1
    |
185 |         writer = SafeDeltaWriter(spark)
186 |         target_path = str(Path(temp_dir) / "customers")
187 |         
    | ^^^^^^^^
188 |         # Initial write
189 |         writer.write_with_merge(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_safe_writer.py:195:1
    |
193 |             mode="append"
194 |         )
195 |         
    | ^^^^^^^^
196 |         # Try merge without keys
197 |         with pytest.raises(ValueError, match="Merge keys must be provided"):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_safe_writer.py:209:1
    |
207 |         writer = SafeDeltaWriter(spark)
208 |         target_path = str(Path(temp_dir) / "customers")
209 |         
    | ^^^^^^^^
210 |         # Initial write
211 |         writer.write_with_merge(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_safe_writer.py:217:1
    |
215 |             mode="append"
216 |         )
217 |         
    | ^^^^^^^^
218 |         # Try unsafe overwrite
219 |         with pytest.raises(ValueError, match="Unsupported write mode"):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_safe_writer.py:232:1
    |
230 |         writer = SafeDeltaWriter(spark)
231 |         target_path = str(Path(temp_dir) / "customers")
232 |         
    | ^^^^^^^^
233 |         def pre_hook(df: DataFrame) -> DataFrame:
234 |             """Add a timestamp column."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_safe_writer.py:237:1
    |
235 |             from pyspark.sql.functions import current_timestamp
236 |             return df.withColumn("processed_at", current_timestamp())
237 |         
    | ^^^^^^^^
238 |         result = writer.write_with_merge(
239 |             df=sample_data,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_safe_writer.py:245:1
    |
243 |             pre_write_hook=pre_hook
244 |         )
245 |         
    | ^^^^^^^^
246 |         assert result["success"] is True
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_safe_writer.py:247:1
    |
246 |         assert result["success"] is True
247 |         
    | ^^^^^^^^
248 |         # Verify hook was applied
249 |         delta_table = DeltaTable.forPath(spark, target_path)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_safe_writer.py:257:1
    |
255 |         writer = SafeDeltaWriter(spark)
256 |         target_path = str(Path(temp_dir) / "customers")
257 |         
    | ^^^^^^^^
258 |         hook_called = {"count": 0}
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_safe_writer.py:259:1
    |
258 |         hook_called = {"count": 0}
259 |         
    | ^^^^^^^^
260 |         def post_hook(df: DataFrame, row_count: int):
261 |             """Verify row count."""
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_safe_writer.py:263:1
    |
261 |             """Verify row count."""
262 |             hook_called["count"] = row_count
263 |         
    | ^^^^^^^^
264 |         result = writer.write_with_merge(
265 |             df=sample_data,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_safe_writer.py:271:1
    |
269 |             post_write_hook=post_hook
270 |         )
271 |         
    | ^^^^^^^^
272 |         assert result["success"] is True
273 |         assert hook_called["count"] == 3
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_safe_writer.py:279:1
    |
277 |         writer = SafeDeltaWriter(spark)
278 |         target_path = str(Path(temp_dir) / "customers")
279 |         
    | ^^^^^^^^
280 |         result = writer.write_with_merge(
281 |             df=sample_data,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_safe_writer.py:286:1
    |
284 |             mode="append"
285 |         )
286 |         
    | ^^^^^^^^
287 |         # Verify counts
288 |         assert result["initial_row_count"] == 0
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_safe_writer.py:296:1
    |
294 |         writer = SafeDeltaWriter(spark)
295 |         target_path = str(Path(temp_dir) / "customers")
296 |         
    | ^^^^^^^^
297 |         # Initial write
298 |         writer.write_with_merge(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_safe_writer.py:304:1
    |
302 |             mode="append"
303 |         )
304 |         
    | ^^^^^^^^
305 |         # Multiple updates
306 |         for i in range(3):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_safe_writer.py:311:1
    |
309 |             ]
310 |             df_update = spark.createDataFrame(updated_data, schema=sample_schema)
311 |             
    | ^^^^^^^^^^^^
312 |             result = writer.write_with_merge(
313 |                 df=df_update,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_safe_writer.py:318:1
    |
316 |                 mode="merge"
317 |             )
318 |             
    | ^^^^^^^^^^^^
319 |             assert result["success"] is True
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_safe_writer.py:320:1
    |
319 |             assert result["success"] is True
320 |         
    | ^^^^^^^^
321 |         # Verify final state
322 |         delta_table = DeltaTable.forPath(spark, target_path)
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> tests/test_scd2.py:8:1
   |
 6 |   """
 7 |
 8 | / import pytest
 9 | | from datetime import datetime
10 | | from pyspark.sql import SparkSession, functions as F
11 | | from pyspark.sql.types import StructType, StructField, StringType, TimestampType
12 | |
13 | | from project_a.common.scd2 import apply_scd2, SCD2Config, validate_scd2_table
   | |_____________________________________________________________________________^
   |
help: Organize imports

E712 Avoid equality comparisons to `True`; use `F.col("is_current"):` for truth checks
   --> tests/test_scd2.py:125:37
    |
124 |         # Check all records are current
125 |         current_records = df.filter(F.col("is_current") == True)
    |                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
126 |         assert current_records.count() == 3
    |
help: Replace with `F.col("is_current")`

E712 Avoid equality comparisons to `True`; use `F.col("is_current"):` for truth checks
   --> tests/test_scd2.py:153:37
    |
152 |         # Check current records
153 |         current_records = df.filter(F.col("is_current") == True)
    |                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
154 |         assert current_records.count() == 4  # 3 original + 1 new
    |
help: Replace with `F.col("is_current")`

E712 Avoid equality comparisons to `True`; use `F.col("is_current"):` for truth checks
   --> tests/test_scd2.py:374:43
    |
373 |         # Should have 4 current records
374 |         current_records = final_df.filter(F.col("is_current") == True)
    |                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
375 |         assert current_records.count() == 4
    |
help: Replace with `F.col("is_current")`

I001 [*] Import block is un-sorted or un-formatted
 --> tests/test_scd2_validation.py:5:1
  |
3 |   """
4 |
5 | / import pytest
6 | | from pyspark.sql import SparkSession, functions as F, Window as W
7 | | from pyspark.sql.types import StructType, StructField, StringType, DateType, BooleanType
  | |________________________________________________________________________________________^
  |
help: Organize imports

E712 Avoid equality comparisons to `True`; use `F.col("is_current"):` for truth checks
  --> tests/test_scd2_validation.py:54:30
   |
52 |         """Test that each business key has exactly one current record."""
53 |         violations = (sample_scd2_data
54 |                      .filter(F.col("is_current") == True)
   |                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
55 |                      .groupBy("customer_id")
56 |                      .count()
   |
help: Replace with `F.col("is_current")`

E712 Avoid equality comparisons to `True`; use `F.col("is_current"):` for truth checks
  --> tests/test_scd2_validation.py:74:14
   |
72 |         """Test that current records have null effective_to."""
73 |         violations = sample_scd2_data.filter(
74 |             (F.col("is_current") == True) & (F.col("effective_to").isNotNull())
   |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
75 |         )
   |
help: Replace with `F.col("is_current")`

E712 Avoid equality comparisons to `False`; use `not F.col("is_current"):` for false checks
  --> tests/test_scd2_validation.py:82:14
   |
80 |         """Test that historical records have non-null effective_to."""
81 |         violations = sample_scd2_data.filter(
82 |             (F.col("is_current") == False) & (F.col("effective_to").isNull())
   |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
83 |         )
   |
help: Replace with `not F.col("is_current")`

E712 Avoid equality comparisons to `False`; use `not F.col("is_current"):` for false checks
  --> tests/test_scd2_validation.py:90:14
   |
88 |         """Test that effective_from is before effective_to for historical records."""
89 |         violations = sample_scd2_data.filter(
90 |             (F.col("is_current") == False) &
   |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
91 |             (F.col("effective_from") >= F.col("effective_to"))
92 |         )
   |
help: Replace with `not F.col("is_current")`

E712 Avoid equality comparisons to `True`; use `F.col("is_current"):` for truth checks
   --> tests/test_scd2_validation.py:152:42
    |
150 |             "table_exists": True,
151 |             "total_records": df.count(),
152 |             "current_records": df.filter(F.col("is_current") == True).count(),
    |                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
153 |             "historical_records": df.filter(F.col("is_current") == False).count(),
154 |             "validation_passed": True,
    |
help: Replace with `F.col("is_current")`

E712 Avoid equality comparisons to `False`; use `not F.col("is_current"):` for false checks
   --> tests/test_scd2_validation.py:153:45
    |
151 |             "total_records": df.count(),
152 |             "current_records": df.filter(F.col("is_current") == True).count(),
153 |             "historical_records": df.filter(F.col("is_current") == False).count(),
    |                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
154 |             "validation_passed": True,
155 |             "errors": []
    |
help: Replace with `not F.col("is_current")`

E712 Avoid equality comparisons to `True`; use `F.col("is_current"):` for truth checks
   --> tests/test_scd2_validation.py:166:33
    |
165 |         # Check single current record per key
166 |         violations = (df.filter(F.col("is_current") == True)
    |                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^
167 |                      .groupBy(business_key)
168 |                      .count()
    |
help: Replace with `F.col("is_current")`

I001 [*] Import block is un-sorted or un-formatted
 --> tests/test_schema_validator.py:4:1
  |
2 |   Tests for schema validator.
3 |   """
4 | / import pytest
5 | | from unittest.mock import Mock, patch
6 | | from pyspark.sql import SparkSession
7 | | from pyspark.sql.types import StructType, StructField, StringType, IntegerType
  | |______________________________________________________________________________^
8 |
9 |   # Note: validate_schema may not be directly importable - test via module
  |
help: Organize imports

F401 [*] `unittest.mock.Mock` imported but unused
 --> tests/test_schema_validator.py:5:27
  |
3 | """
4 | import pytest
5 | from unittest.mock import Mock, patch
  |                           ^^^^
6 | from pyspark.sql import SparkSession
7 | from pyspark.sql.types import StructType, StructField, StringType, IntegerType
  |
help: Remove unused import

F401 [*] `unittest.mock.patch` imported but unused
 --> tests/test_schema_validator.py:5:33
  |
3 | """
4 | import pytest
5 | from unittest.mock import Mock, patch
  |                                 ^^^^^
6 | from pyspark.sql import SparkSession
7 | from pyspark.sql.types import StructType, StructField, StringType, IntegerType
  |
help: Remove unused import

F401 [*] `pyspark.sql.SparkSession` imported but unused
 --> tests/test_schema_validator.py:6:25
  |
4 | import pytest
5 | from unittest.mock import Mock, patch
6 | from pyspark.sql import SparkSession
  |                         ^^^^^^^^^^^^
7 | from pyspark.sql.types import StructType, StructField, StringType, IntegerType
  |
help: Remove unused import: `pyspark.sql.SparkSession`

W293 [*] Blank line contains whitespace
  --> tests/test_schema_validator.py:36:1
   |
34 |     data = [("1", "John", 30)]
35 |     df = spark.createDataFrame(data, actual_schema_match)
36 |     
   | ^^^^
37 |     # Should pass validation
38 |     result = validate_schema(df, expected_schema, mode="strict")
   |
help: Remove whitespace from blank line

F821 Undefined name `validate_schema`
  --> tests/test_schema_validator.py:38:14
   |
37 |     # Should pass validation
38 |     result = validate_schema(df, expected_schema, mode="strict")
   |              ^^^^^^^^^^^^^^^
39 |     
40 |     assert result is True or result is None  # Depending on implementation
   |

W293 [*] Blank line contains whitespace
  --> tests/test_schema_validator.py:39:1
   |
37 |     # Should pass validation
38 |     result = validate_schema(df, expected_schema, mode="strict")
39 |     
   | ^^^^
40 |     assert result is True or result is None  # Depending on implementation
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_schema_validator.py:52:1
   |
50 |     data = [("1", "John")]
51 |     df = spark.createDataFrame(data, actual_schema)
52 |     
   | ^^^^
53 |     # Should fail in strict mode
54 |     if validate_schema:
   |
help: Remove whitespace from blank line

F821 Undefined name `validate_schema`
  --> tests/test_schema_validator.py:54:8
   |
53 |     # Should fail in strict mode
54 |     if validate_schema:
   |        ^^^^^^^^^^^^^^^
55 |         result = validate_schema(df, expected_schema, mode="strict")
56 |         # Result may be False or raise exception
   |

F821 Undefined name `validate_schema`
  --> tests/test_schema_validator.py:55:18
   |
53 |     # Should fail in strict mode
54 |     if validate_schema:
55 |         result = validate_schema(df, expected_schema, mode="strict")
   |                  ^^^^^^^^^^^^^^^
56 |         # Result may be False or raise exception
57 |         assert result is False or True  # Implementation dependent
   |

I001 [*] Import block is un-sorted or un-formatted
  --> tests/test_schemas.py:4:1
   |
 2 |   Unit tests for production schemas module.
 3 |   """
 4 | / import pytest
 5 | | from datetime import datetime, date
 6 | | from pyspark.sql import SparkSession
 7 | | from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, DateType
 8 | | import sys
 9 | | import os
   | |_________^
10 |
11 |   # Add src to path
   |
help: Organize imports

I001 [*] Import block is un-sorted or un-formatted
  --> tests/test_schemas.py:14:1
   |
12 |   sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))
13 |
14 | / from project_a.schemas.production_schemas import (
15 | |     get_schema,
16 | |     validate_schema_drift,
17 | |     BRONZE_CUSTOMERS_SCHEMA,
18 | |     BRONZE_ORDERS_SCHEMA,
19 | |     SILVER_CUSTOMERS_SCHEMA,
20 | | )
   | |_^
   |
help: Organize imports

W293 [*] Blank line contains whitespace
  --> tests/test_schemas.py:44:1
   |
42 |         """Test retrieving bronze customers schema."""
43 |         schema = get_schema("bronze.customers")
44 |         
   | ^^^^^^^^
45 |         assert isinstance(schema, StructType)
46 |         assert len(schema.fields) == 11
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_schemas.py:47:1
   |
45 |         assert isinstance(schema, StructType)
46 |         assert len(schema.fields) == 11
47 |         
   | ^^^^^^^^
48 |         # Check key fields
49 |         field_names = [f.name for f in schema.fields]
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_schemas.py:59:1
   |
57 |         """Test retrieving bronze orders schema."""
58 |         schema = get_schema("bronze.orders")
59 |         
   | ^^^^^^^^
60 |         assert isinstance(schema, StructType)
61 |         assert len(schema.fields) == 12
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_schemas.py:62:1
   |
60 |         assert isinstance(schema, StructType)
61 |         assert len(schema.fields) == 12
62 |         
   | ^^^^^^^^
63 |         # Check key fields
64 |         field_names = [f.name for f in schema.fields]
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_schemas.py:72:1
   |
70 |         """Test retrieving silver customers schema."""
71 |         schema = get_schema("silver.customers")
72 |         
   | ^^^^^^^^
73 |         assert isinstance(schema, StructType)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_schemas.py:74:1
   |
73 |         assert isinstance(schema, StructType)
74 |         
   | ^^^^^^^^
75 |         # Check SCD Type 2 fields
76 |         field_names = [f.name for f in schema.fields]
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_schemas.py:90:1
   |
88 |         """Test bronze customers schema structure in detail."""
89 |         schema = BRONZE_CUSTOMERS_SCHEMA
90 |         
   | ^^^^^^^^
91 |         # Check nullable constraints
92 |         customer_id_field = next(f for f in schema.fields if f.name == "customer_id")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_schemas.py:94:1
   |
92 |         customer_id_field = next(f for f in schema.fields if f.name == "customer_id")
93 |         assert customer_id_field.nullable is False
94 |         
   | ^^^^^^^^
95 |         source_field = next(f for f in schema.fields if f.name == "_source")
96 |         assert source_field.nullable is False
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_schemas.py:97:1
   |
95 |         source_field = next(f for f in schema.fields if f.name == "_source")
96 |         assert source_field.nullable is False
97 |         
   | ^^^^^^^^
98 |         # Check data types
99 |         assert customer_id_field.dataType == StringType()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_schemas.py:100:1
    |
 98 |         # Check data types
 99 |         assert customer_id_field.dataType == StringType()
100 |         
    | ^^^^^^^^
101 |         balance_field = next(f for f in schema.fields if f.name == "account_balance")
102 |         assert balance_field.dataType == DoubleType()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_schemas.py:103:1
    |
101 |         balance_field = next(f for f in schema.fields if f.name == "account_balance")
102 |         assert balance_field.dataType == DoubleType()
103 |         
    | ^^^^^^^^
104 |         nation_field = next(f for f in schema.fields if f.name == "nation_key")
105 |         assert nation_field.dataType == IntegerType()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_schemas.py:110:1
    |
108 |         """Test schema drift detection with matching schemas."""
109 |         schema = get_schema("bronze.customers")
110 |         
    | ^^^^^^^^
111 |         # Create DataFrame with correct schema
112 |         data = [(
    |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
   --> tests/test_schemas.py:113:56
    |
111 |         # Create DataFrame with correct schema
112 |         data = [(
113 |             "C001", "Alice", "123 Main", 1, "555-1234", 
    |                                                        ^
114 |             100.0, "Retail", "Comment", "source1", 
115 |             datetime.now(), date.today()
    |
help: Remove trailing whitespace

W291 [*] Trailing whitespace
   --> tests/test_schemas.py:114:51
    |
112 |         data = [(
113 |             "C001", "Alice", "123 Main", 1, "555-1234", 
114 |             100.0, "Retail", "Comment", "source1", 
    |                                                   ^
115 |             datetime.now(), date.today()
116 |         )]
    |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
   --> tests/test_schemas.py:118:1
    |
116 |         )]
117 |         df = spark.createDataFrame(data, schema=schema)
118 |         
    | ^^^^^^^^
119 |         # Should not raise
120 |         validate_schema_drift(df.schema, schema, "bronze.customers")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_schemas.py:125:1
    |
123 |         """Test schema drift detection with missing column."""
124 |         expected_schema = get_schema("bronze.customers")
125 |         
    | ^^^^^^^^
126 |         # Create schema with missing column
127 |         incomplete_schema = StructType([
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_schemas.py:130:1
    |
128 |             f for f in expected_schema.fields if f.name != "nation_key"
129 |         ])
130 |         
    | ^^^^^^^^
131 |         data = [(
132 |             "C001", "Alice", "123 Main", "555-1234", 
    |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
   --> tests/test_schemas.py:132:53
    |
131 |         data = [(
132 |             "C001", "Alice", "123 Main", "555-1234", 
    |                                                     ^
133 |             100.0, "Retail", "Comment", "source1", 
134 |             datetime.now(), date.today()
    |
help: Remove trailing whitespace

W291 [*] Trailing whitespace
   --> tests/test_schemas.py:133:51
    |
131 |         data = [(
132 |             "C001", "Alice", "123 Main", "555-1234", 
133 |             100.0, "Retail", "Comment", "source1", 
    |                                                   ^
134 |             datetime.now(), date.today()
135 |         )]
    |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
   --> tests/test_schemas.py:137:1
    |
135 |         )]
136 |         df = spark.createDataFrame(data, schema=incomplete_schema)
137 |         
    | ^^^^^^^^
138 |         # Should raise RuntimeError
139 |         with pytest.raises(RuntimeError, match="Schema drift detected"):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_schemas.py:145:1
    |
143 |         """Test schema drift detection with type mismatch."""
144 |         expected_schema = get_schema("bronze.customers")
145 |         
    | ^^^^^^^^
146 |         # Create schema with type mismatch (nation_key as String instead of Integer)
147 |         modified_fields = []
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_schemas.py:153:1
    |
151 |             else:
152 |                 modified_fields.append(field)
153 |         
    | ^^^^^^^^
154 |         modified_schema = StructType(modified_fields)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_schemas.py:155:1
    |
154 |         modified_schema = StructType(modified_fields)
155 |         
    | ^^^^^^^^
156 |         data = [(
157 |             "C001", "Alice", "123 Main", "1", "555-1234", 
    |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
   --> tests/test_schemas.py:157:58
    |
156 |         data = [(
157 |             "C001", "Alice", "123 Main", "1", "555-1234", 
    |                                                          ^
158 |             100.0, "Retail", "Comment", "source1", 
159 |             datetime.now(), date.today()
    |
help: Remove trailing whitespace

W291 [*] Trailing whitespace
   --> tests/test_schemas.py:158:51
    |
156 |         data = [(
157 |             "C001", "Alice", "123 Main", "1", "555-1234", 
158 |             100.0, "Retail", "Comment", "source1", 
    |                                                   ^
159 |             datetime.now(), date.today()
160 |         )]
    |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
   --> tests/test_schemas.py:162:1
    |
160 |         )]
161 |         df = spark.createDataFrame(data, schema=modified_schema)
162 |         
    | ^^^^^^^^
163 |         # Should raise RuntimeError
164 |         with pytest.raises(RuntimeError, match="Schema drift detected"):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_schemas.py:170:1
    |
168 |         """Test schema drift detection with unexpected extra column."""
169 |         expected_schema = get_schema("bronze.customers")
170 |         
    | ^^^^^^^^
171 |         # Create schema with extra column
172 |         extra_fields = list(expected_schema.fields) + [
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_schemas.py:176:1
    |
174 |         ]
175 |         extended_schema = StructType(extra_fields)
176 |         
    | ^^^^^^^^
177 |         data = [(
178 |             "C001", "Alice", "123 Main", 1, "555-1234", 
    |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
   --> tests/test_schemas.py:178:56
    |
177 |         data = [(
178 |             "C001", "Alice", "123 Main", 1, "555-1234", 
    |                                                        ^
179 |             100.0, "Retail", "Comment", "source1", 
180 |             datetime.now(), date.today(), "extra"
    |
help: Remove trailing whitespace

W291 [*] Trailing whitespace
   --> tests/test_schemas.py:179:51
    |
177 |         data = [(
178 |             "C001", "Alice", "123 Main", 1, "555-1234", 
179 |             100.0, "Retail", "Comment", "source1", 
    |                                                   ^
180 |             datetime.now(), date.today(), "extra"
181 |         )]
    |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
   --> tests/test_schemas.py:183:1
    |
181 |         )]
182 |         df = spark.createDataFrame(data, schema=extended_schema)
183 |         
    | ^^^^^^^^
184 |         # Should log warning but might not raise (depending on policy)
185 |         # For now, just verify it doesn't crash
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_schemas.py:198:1
    |
196 |             "silver.customers",
197 |         ]
198 |         
    | ^^^^^^^^
199 |         for schema_name in schema_names:
200 |             schema = get_schema(schema_name)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_schemas.py:207:1
    |
205 |         """Test bronze orders schema structure in detail."""
206 |         schema = BRONZE_ORDERS_SCHEMA
207 |         
    | ^^^^^^^^
208 |         # Check required fields
209 |         order_id_field = next(f for f in schema.fields if f.name == "order_id")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_schemas.py:211:1
    |
209 |         order_id_field = next(f for f in schema.fields if f.name == "order_id")
210 |         assert order_id_field.nullable is False
211 |         
    | ^^^^^^^^
212 |         customer_id_field = next(f for f in schema.fields if f.name == "customer_id")
213 |         assert customer_id_field.nullable is False
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_schemas.py:214:1
    |
212 |         customer_id_field = next(f for f in schema.fields if f.name == "customer_id")
213 |         assert customer_id_field.nullable is False
214 |         
    | ^^^^^^^^
215 |         # Check data types
216 |         total_price_field = next(f for f in schema.fields if f.name == "total_price")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_schemas.py:218:1
    |
216 |         total_price_field = next(f for f in schema.fields if f.name == "total_price")
217 |         assert total_price_field.dataType == DoubleType()
218 |         
    | ^^^^^^^^
219 |         order_date_field = next(f for f in schema.fields if f.name == "order_date")
220 |         assert order_date_field.dataType == DateType()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_schemas.py:225:1
    |
223 |         """Test SCD Type 2 fields in silver customers schema."""
224 |         schema = SILVER_CUSTOMERS_SCHEMA
225 |         
    | ^^^^^^^^
226 |         scd2_fields = ["is_current", "effective_from", "effective_to", "_updated_at"]
227 |         field_names = [f.name for f in schema.fields]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_schemas.py:228:1
    |
226 |         scd2_fields = ["is_current", "effective_from", "effective_to", "_updated_at"]
227 |         field_names = [f.name for f in schema.fields]
228 |         
    | ^^^^^^^^
229 |         for field_name in scd2_fields:
230 |             assert field_name in field_names, f"Missing SCD2 field: {field_name}"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_schemas.py:231:1
    |
229 |         for field_name in scd2_fields:
230 |             assert field_name in field_names, f"Missing SCD2 field: {field_name}"
231 |         
    | ^^^^^^^^
232 |         # Check types
233 |         effective_from = next(f for f in schema.fields if f.name == "effective_from")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_schemas.py:235:1
    |
233 |         effective_from = next(f for f in schema.fields if f.name == "effective_from")
234 |         assert effective_from.dataType == TimestampType()
235 |         
    | ^^^^^^^^
236 |         effective_to = next(f for f in schema.fields if f.name == "effective_to")
237 |         assert effective_to.dataType == TimestampType()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_schemas.py:243:1
    |
241 |         schema = get_schema("bronze.customers")
242 |         field_names = [f.name for f in schema.fields]
243 |         
    | ^^^^^^^^
244 |         # Business fields should come first
245 |         assert field_names[0] == "customer_id"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_schemas.py:246:1
    |
244 |         # Business fields should come first
245 |         assert field_names[0] == "customer_id"
246 |         
    | ^^^^^^^^
247 |         # Metadata fields should come last
248 |         assert "_source" in field_names[-3:]
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> tests/test_secrets.py:4:1
   |
 2 |   Tests for secrets manager utilities.
 3 |   """
 4 | / import pytest
 5 | | from unittest.mock import Mock, patch, MagicMock
 6 | | import os
 7 | |
 8 | | from project_a.utils.secrets import (
 9 | |     get_secret_from_manager,
10 | |     get_parameter_from_ssm,
11 | |     get_snowflake_credentials,
12 | |     get_redshift_credentials
13 | | )
   | |_^
   |
help: Organize imports

F401 [*] `unittest.mock.MagicMock` imported but unused
 --> tests/test_secrets.py:5:40
  |
3 | """
4 | import pytest
5 | from unittest.mock import Mock, patch, MagicMock
  |                                        ^^^^^^^^^
6 | import os
  |
help: Remove unused import: `unittest.mock.MagicMock`

F401 [*] `project_a.utils.secrets.get_parameter_from_ssm` imported but unused
  --> tests/test_secrets.py:10:5
   |
 8 | from project_a.utils.secrets import (
 9 |     get_secret_from_manager,
10 |     get_parameter_from_ssm,
   |     ^^^^^^^^^^^^^^^^^^^^^^
11 |     get_snowflake_credentials,
12 |     get_redshift_credentials
   |
help: Remove unused import: `project_a.utils.secrets.get_parameter_from_ssm`

W293 [*] Blank line contains whitespace
  --> tests/test_secrets.py:53:1
   |
51 |         "SecretString": '{"user": "test_user", "password": "test_pass"}'
52 |     }
53 |     
   | ^^^^
54 |     result = get_secret_from_manager("test/secret")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_secrets.py:55:1
   |
54 |     result = get_secret_from_manager("test/secret")
55 |     
   | ^^^^
56 |     assert result["user"] == "test_user"
57 |     assert result["password"] == "test_pass"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_secrets.py:65:1
   |
63 |     mock_client = Mock()
64 |     mock_boto3.client.return_value = mock_client
65 |     
   | ^^^^
66 |     error_response = {"Error": {"Code": "ResourceNotFoundException"}}
67 |     mock_client.get_secret_value.side_effect = Mock(side_effect=Exception())
   |
help: Remove whitespace from blank line

F841 Local variable `error_response` is assigned to but never used
  --> tests/test_secrets.py:66:5
   |
64 |     mock_boto3.client.return_value = mock_client
65 |     
66 |     error_response = {"Error": {"Code": "ResourceNotFoundException"}}
   |     ^^^^^^^^^^^^^^
67 |     mock_client.get_secret_value.side_effect = Mock(side_effect=Exception())
68 |     mock_client.exceptions.ClientError = type('ClientError', (Exception,), {})
   |
help: Remove assignment to unused variable `error_response`

W293 [*] Blank line contains whitespace
  --> tests/test_secrets.py:69:1
   |
67 |     mock_client.get_secret_value.side_effect = Mock(side_effect=Exception())
68 |     mock_client.exceptions.ClientError = type('ClientError', (Exception,), {})
69 |     
   | ^^^^
70 |     # Should fall back to environment variables
71 |     with patch.dict(os.environ, {"SNOWFLAKE_USER": "env_user"}):
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_secrets.py:86:1
   |
84 |         "warehouse": "test_warehouse"
85 |     }
86 |     
   | ^^^^
87 |     result = get_snowflake_credentials(mock_config)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_secrets.py:88:1
   |
87 |     result = get_snowflake_credentials(mock_config)
88 |     
   | ^^^^
89 |     assert result["account"] == "test-account"
90 |     assert result["user"] == "test_user"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_secrets.py:97:1
   |
95 |     """Test getting Snowflake credentials from config."""
96 |     mock_config["secrets"]["snowflake"] = {}  # No secret name
97 |     
   | ^^^^
98 |     result = get_snowflake_credentials(mock_config)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_secrets.py:99:1
    |
 98 |     result = get_snowflake_credentials(mock_config)
 99 |     
    | ^^^^
100 |     assert result["account"] == "test-account"
101 |     assert result["user"] == "test_user"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_secrets.py:112:1
    |
110 |         "password": "test_pass"
111 |     }
112 |     
    | ^^^^
113 |     result = get_redshift_credentials(mock_config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_secrets.py:114:1
    |
113 |     result = get_redshift_credentials(mock_config)
114 |     
    | ^^^^
115 |     assert result["host"] == "test.redshift.amazonaws.com"
116 |     assert result["user"] == "test_user"
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> tests/test_silver_behavior_contract.py:7:1
   |
 5 |   """
 6 |
 7 | / import pytest
 8 | | from pyspark.sql import SparkSession
 9 | | from pyspark.sql.types import TimestampType, StringType
10 | | from pyspark.sql.functions import col
11 | |
12 | | from project_a.utils.spark_session import build_spark
13 | | from project_a.utils.config import load_conf
   | |____________________________________________^
   |
help: Organize imports

F401 [*] `pyspark.sql.SparkSession` imported but unused
  --> tests/test_silver_behavior_contract.py:8:25
   |
 7 | import pytest
 8 | from pyspark.sql import SparkSession
   |                         ^^^^^^^^^^^^
 9 | from pyspark.sql.types import TimestampType, StringType
10 | from pyspark.sql.functions import col
   |
help: Remove unused import: `pyspark.sql.SparkSession`

W293 [*] Blank line contains whitespace
  --> tests/test_silver_behavior_contract.py:48:1
   |
46 |         "_processing_ts"
47 |     }
48 |     
   | ^^^^
49 |     actual_columns = set(silver_behavior_df.columns)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_silver_behavior_contract.py:50:1
   |
49 |     actual_columns = set(silver_behavior_df.columns)
50 |     
   | ^^^^
51 |     # Check all expected columns exist
52 |     missing_columns = expected_columns - actual_columns
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_silver_behavior_contract.py:54:1
   |
52 |     missing_columns = expected_columns - actual_columns
53 |     assert not missing_columns, f"Missing expected columns: {missing_columns}"
54 |     
   | ^^^^
55 |     # Check critical columns exist
56 |     assert "event_id" in actual_columns, "event_id is required"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_silver_behavior_contract.py:66:1
   |
64 |     if "event_name" not in silver_behavior_df.columns:
65 |         pytest.skip("event_name column not found")
66 |     
   | ^^^^
67 |     # Check if any values are not lowercase
68 |     non_lowercase = silver_behavior_df.filter(
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_silver_behavior_contract.py:71:1
   |
69 |         col("event_name") != col("event_name").lower()
70 |     )
71 |     
   | ^^^^
72 |     count = non_lowercase.count()
73 |     assert count == 0, f"Found {count} records with non-lowercase event_name"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_silver_behavior_contract.py:80:1
   |
78 |     if "session_id" not in silver_behavior_df.columns:
79 |         pytest.skip("session_id column not found")
80 |     
   | ^^^^
81 |     # Count invalid session_ids (null is allowed, but if present must match pattern)
82 |     invalid_pattern = silver_behavior_df.filter(
   |
help: Remove whitespace from blank line

W291 [*] Trailing whitespace
  --> tests/test_silver_behavior_contract.py:83:40
   |
81 |     # Count invalid session_ids (null is allowed, but if present must match pattern)
82 |     invalid_pattern = silver_behavior_df.filter(
83 |         col("session_id").isNotNull() & 
   |                                        ^
84 |         ~col("session_id").rlike(r"^SESS-\d+$")
85 |     )
   |
help: Remove trailing whitespace

W293 [*] Blank line contains whitespace
  --> tests/test_silver_behavior_contract.py:86:1
   |
84 |         ~col("session_id").rlike(r"^SESS-\d+$")
85 |     )
86 |     
   | ^^^^
87 |     count = invalid_pattern.count()
88 |     assert count == 0, f"Found {count} records with invalid session_id pattern"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_silver_behavior_contract.py:95:1
   |
93 |     if "event_id" not in silver_behavior_df.columns:
94 |         pytest.skip("event_id column not found")
95 |     
   | ^^^^
96 |     duplicates = silver_behavior_df.groupBy("event_id").count().filter(col("count") > 1)
97 |     duplicate_count = duplicates.count()
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_silver_behavior_contract.py:98:1
   |
96 |     duplicates = silver_behavior_df.groupBy("event_id").count().filter(col("count") > 1)
97 |     duplicate_count = duplicates.count()
98 |     
   | ^^^^
99 |     assert duplicate_count == 0, f"Found {duplicate_count} duplicate event_ids"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_silver_behavior_contract.py:106:1
    |
104 |     if "event_ts" not in silver_behavior_df.columns:
105 |         pytest.skip("event_ts column not found")
106 |     
    | ^^^^
107 |     # Check data type
108 |     event_ts_field = next(
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_silver_behavior_contract.py:112:1
    |
110 |         None
111 |     )
112 |     
    | ^^^^
113 |     assert event_ts_field is not None, "event_ts column not found in schema"
114 |     assert isinstance(event_ts_field.dataType, TimestampType), \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_silver_behavior_contract.py:123:1
    |
121 |     expected_min_columns = 7  # event_id, customer_id, event_name, event_ts, session_id, page_url, referrer
122 |     actual_count = len(silver_behavior_df.columns)
123 |     
    | ^^^^
124 |     assert actual_count >= expected_min_columns, \
125 |         f"Expected at least {expected_min_columns} columns, got {actual_count}"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_silver_behavior_contract.py:136:1
    |
134 |         "event_ts": TimestampType,
135 |     }
136 |     
    | ^^^^
137 |     for col_name, expected_type in type_checks.items():
138 |         if col_name not in silver_behavior_df.columns:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_silver_behavior_contract.py:140:1
    |
138 |         if col_name not in silver_behavior_df.columns:
139 |             continue
140 |         
    | ^^^^^^^^
141 |         field = next(
142 |             (f for f in silver_behavior_df.schema.fields if f.name == col_name),
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_silver_behavior_contract.py:145:1
    |
143 |             None
144 |         )
145 |         
    | ^^^^^^^^
146 |         assert field is not None, f"Column {col_name} not found"
147 |         assert isinstance(field.dataType, expected_type), \
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_silver_behavior_contract.py:155:1
    |
153 |     min_rows = 1
154 |     actual_rows = silver_behavior_df.count()
155 |     
    | ^^^^
156 |     assert actual_rows >= min_rows, \
157 |         f"Expected at least {min_rows} rows, got {actual_rows}"
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
 --> tests/test_silver_build_customer_360.py:4:1
  |
2 |   Tests for silver_build_customer_360 job.
3 |   """
4 | / import pytest
5 | | from unittest.mock import Mock, patch, MagicMock
6 | | from pyspark.sql import SparkSession
7 | | from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DecimalType, IntegerType
8 | |
9 | | from jobs.silver_build_customer_360 import build_customer_360
  | |_____________________________________________________________^
  |
help: Organize imports

F401 [*] `unittest.mock.MagicMock` imported but unused
 --> tests/test_silver_build_customer_360.py:5:40
  |
3 | """
4 | import pytest
5 | from unittest.mock import Mock, patch, MagicMock
  |                                        ^^^^^^^^^
6 | from pyspark.sql import SparkSession
7 | from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DecimalType, IntegerType
  |
help: Remove unused import: `unittest.mock.MagicMock`

F401 [*] `pyspark.sql.SparkSession` imported but unused
 --> tests/test_silver_build_customer_360.py:6:25
  |
4 | import pytest
5 | from unittest.mock import Mock, patch, MagicMock
6 | from pyspark.sql import SparkSession
  |                         ^^^^^^^^^^^^
7 | from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DecimalType, IntegerType
  |
help: Remove unused import: `pyspark.sql.SparkSession`

F401 [*] `pyspark.sql.types.TimestampType` imported but unused
 --> tests/test_silver_build_customer_360.py:7:68
  |
5 | from unittest.mock import Mock, patch, MagicMock
6 | from pyspark.sql import SparkSession
7 | from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DecimalType, IntegerType
  |                                                                    ^^^^^^^^^^^^^
8 |
9 | from jobs.silver_build_customer_360 import build_customer_360
  |
help: Remove unused import: `pyspark.sql.types.TimestampType`

W293 [*] Blank line contains whitespace
   --> tests/test_silver_build_customer_360.py:113:1
    |
111 |     mock_ge_instance.run_checkpoint.return_value = {"success": True}
112 |     mock_ge_runner.return_value = mock_ge_instance
113 |     
    | ^^^^
114 |     # Write sample data to silver paths
115 |     silver_path = tmp_path / "silver"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_silver_build_customer_360.py:120:1
    |
118 |     (silver_path / "snowflake" / "orders").mkdir(parents=True)
119 |     (silver_path / "behavior").mkdir(parents=True)
120 |     
    | ^^^^
121 |     sample_contacts_data.write.format("delta").mode("overwrite").save(str(silver_path / "crm" / "contacts"))
122 |     sample_accounts_data.write.format("delta").mode("overwrite").save(str(silver_path / "crm" / "accounts"))
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_silver_build_customer_360.py:125:1
    |
123 |     sample_orders_data.write.format("delta").mode("overwrite").save(str(silver_path / "snowflake" / "orders"))
124 |     sample_behavior_data.write.format("delta").mode("overwrite").save(str(silver_path / "behavior"))
125 |     
    | ^^^^
126 |     mock_config["data_lake"]["silver_path"] = str(silver_path)
127 |     mock_config["data_lake"]["gold_path"] = str(tmp_path / "gold")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_silver_build_customer_360.py:128:1
    |
126 |     mock_config["data_lake"]["silver_path"] = str(silver_path)
127 |     mock_config["data_lake"]["gold_path"] = str(tmp_path / "gold")
128 |     
    | ^^^^
129 |     # Execute build
130 |     result_df = build_customer_360(spark, mock_config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_silver_build_customer_360.py:131:1
    |
129 |     # Execute build
130 |     result_df = build_customer_360(spark, mock_config)
131 |     
    | ^^^^
132 |     # Assertions
133 |     assert result_df is not None
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_silver_build_customer_360.py:145:1
    |
143 |     mock_config["data_lake"]["silver_path"] = str(tmp_path / "nonexistent" / "silver")
144 |     mock_config["data_lake"]["gold_path"] = str(tmp_path / "gold")
145 |     
    | ^^^^
146 |     # Should handle missing data gracefully
147 |     result_df = build_customer_360(spark, mock_config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tests/test_silver_build_customer_360.py:148:1
    |
146 |     # Should handle missing data gracefully
147 |     result_df = build_customer_360(spark, mock_config)
148 |     
    | ^^^^
149 |     # Should still return a DataFrame (even if empty)
150 |     assert result_df is not None
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
 --> tests/test_spark_session.py:4:1
  |
2 |   Tests for Spark session utilities.
3 |   """
4 | / import pytest
5 | | from unittest.mock import Mock, patch, MagicMock
6 | |
7 | | from project_a.utils.spark_session import build_spark
  | |_____________________________________________________^
  |
help: Organize imports

F401 [*] `unittest.mock.MagicMock` imported but unused
 --> tests/test_spark_session.py:5:40
  |
3 | """
4 | import pytest
5 | from unittest.mock import Mock, patch, MagicMock
  |                                        ^^^^^^^^^
6 |
7 | from project_a.utils.spark_session import build_spark
  |
help: Remove unused import: `unittest.mock.MagicMock`

W293 [*] Blank line contains whitespace
  --> tests/test_spark_session.py:38:1
   |
36 |     mock_builder.config.return_value = mock_builder
37 |     mock_builder.getOrCreate.return_value = mock_spark_instance
38 |     
   | ^^^^
39 |     result = build_spark("test_app", mock_config)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_spark_session.py:40:1
   |
39 |     result = build_spark("test_app", mock_config)
40 |     
   | ^^^^
41 |     assert result is not None
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_spark_session.py:47:1
   |
45 |     """Test Spark session with minimal config."""
46 |     minimal_config = {}
47 |     
   | ^^^^
48 |     # Should not raise exception
49 |     try:
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> tests/test_transform.py:8:1
   |
 6 |   sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "../src")))
 7 |
 8 | / from pyspark.sql import Row
 9 | |
10 | | from project_a.transform import (broadcast_join_demo,
11 | |                                                  build_customers_scd2,
12 | |                                                  build_fact_orders,
13 | |                                                  data_cleaning_examples,
14 | |                                                  enrich_products,
15 | |                                                  join_examples, join_inventory,
16 | |                                                  join_returns,
17 | |                                                  normalize_currency,
18 | |                                                  partitioning_examples,
19 | |                                                  select_and_filter,
20 | |                                                  skew_mitigation_demo,
21 | |                                                  sql_vs_dsl_demo, udf_examples,
22 | |                                                  window_functions_demo)
   | |_______________________________________________________________________^
   |
help: Organize imports

C405 Unnecessary list literal (rewrite as a set literal)
   --> tests/test_transform.py:238:12
    |
236 |     )
237 |     scd2 = build_customers_scd2(customers, changes)
238 |     assert set(["effective_from", "effective_to", "is_current"]).issubset(
    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
239 |         set(scd2.columns)
240 |     )
    |
help: Rewrite as a set literal

E712 Avoid equality comparisons to `True`; use `scd2.is_current:` for truth checks
   --> tests/test_transform.py:241:24
    |
239 |         set(scd2.columns)
240 |     )
241 |     assert scd2.filter(scd2.is_current == True).count() == 1
    |                        ^^^^^^^^^^^^^^^^^^^^^^^
    |
help: Replace with `scd2.is_current`

I001 [*] Import block is un-sorted or un-formatted
  --> tests/test_watermark_utils.py:4:1
   |
 2 |   Tests for watermark utilities.
 3 |   """
 4 | / import pytest
 5 | | from unittest.mock import Mock, patch, MagicMock
 6 | | from datetime import datetime, timezone
 7 | | from pyspark.sql import SparkSession
 8 | | from pyspark.sql.types import StructType, StructField, StringType, TimestampType
 9 | |
10 | | from project_a.utils.watermark_utils import (
11 | |     get_watermark, upsert_watermark, get_latest_timestamp_from_df
12 | | )
   | |_^
   |
help: Organize imports

F401 [*] `unittest.mock.Mock` imported but unused
 --> tests/test_watermark_utils.py:5:27
  |
3 | """
4 | import pytest
5 | from unittest.mock import Mock, patch, MagicMock
  |                           ^^^^
6 | from datetime import datetime, timezone
7 | from pyspark.sql import SparkSession
  |
help: Remove unused import

F401 [*] `unittest.mock.MagicMock` imported but unused
 --> tests/test_watermark_utils.py:5:40
  |
3 | """
4 | import pytest
5 | from unittest.mock import Mock, patch, MagicMock
  |                                        ^^^^^^^^^
6 | from datetime import datetime, timezone
7 | from pyspark.sql import SparkSession
  |
help: Remove unused import

F401 [*] `pyspark.sql.SparkSession` imported but unused
 --> tests/test_watermark_utils.py:7:25
  |
5 | from unittest.mock import Mock, patch, MagicMock
6 | from datetime import datetime, timezone
7 | from pyspark.sql import SparkSession
  |                         ^^^^^^^^^^^^
8 | from pyspark.sql.types import StructType, StructField, StringType, TimestampType
  |
help: Remove unused import: `pyspark.sql.SparkSession`

F401 [*] `pyspark.sql.types.TimestampType` imported but unused
  --> tests/test_watermark_utils.py:8:68
   |
 6 | from datetime import datetime, timezone
 7 | from pyspark.sql import SparkSession
 8 | from pyspark.sql.types import StructType, StructField, StringType, TimestampType
   |                                                                    ^^^^^^^^^^^^^
 9 |
10 | from project_a.utils.watermark_utils import (
   |
help: Remove unused import: `pyspark.sql.types.TimestampType`

W293 [*] Blank line contains whitespace
  --> tests/test_watermark_utils.py:31:1
   |
29 |     """Test getting watermark on first run (no watermark exists)."""
30 |     result = get_watermark("test_source", mock_config, spark)
31 |     
   | ^^^^
32 |     # Should return None on first run
33 |     assert result is None
   |
help: Remove whitespace from blank line

F401 [*] `pyspark.sql.functions.lit` imported but unused
  --> tests/test_watermark_utils.py:38:39
   |
36 | def test_get_latest_timestamp_from_df(spark):
37 |     """Test extracting latest timestamp from DataFrame."""
38 |     from pyspark.sql.functions import lit
   |                                       ^^^
39 |     
40 |     data = [
   |
help: Remove unused import: `pyspark.sql.functions.lit`

W293 [*] Blank line contains whitespace
  --> tests/test_watermark_utils.py:39:1
   |
37 |     """Test extracting latest timestamp from DataFrame."""
38 |     from pyspark.sql.functions import lit
39 |     
   | ^^^^
40 |     data = [
41 |         ("2025-01-01 10:00:00",),
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_watermark_utils.py:49:1
   |
47 |     ])
48 |     df = spark.createDataFrame(data, schema)
49 |     
   | ^^^^
50 |     # Should extract latest timestamp
51 |     result = get_latest_timestamp_from_df(df, timestamp_col="timestamp")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_watermark_utils.py:52:1
   |
50 |     # Should extract latest timestamp
51 |     result = get_latest_timestamp_from_df(df, timestamp_col="timestamp")
52 |     
   | ^^^^
53 |     # May return None or timestamp depending on implementation
54 |     assert result is None or isinstance(result, datetime)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_watermark_utils.py:61:1
   |
59 |     """Test upserting watermark."""
60 |     mock_exists.return_value = False  # First write
61 |     
   | ^^^^
62 |     timestamp = datetime(2025, 1, 1, 10, 0, 0, tzinfo=timezone.utc)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_watermark_utils.py:63:1
   |
62 |     timestamp = datetime(2025, 1, 1, 10, 0, 0, tzinfo=timezone.utc)
63 |     
   | ^^^^
64 |     # Should not raise exception
65 |     upsert_watermark("test_source", timestamp, mock_config, spark)
   |
help: Remove whitespace from blank line

F841 Local variable `watermark_data` is assigned to but never used
  --> tests/test_watermark_utils.py:71:5
   |
69 |     """Test getting existing watermark."""
70 |     # Create a simple watermark file
71 |     watermark_data = spark.createDataFrame(
   |     ^^^^^^^^^^^^^^
72 |         [("2025-01-01T10:00:00Z",)],
73 |         "watermark STRING"
   |
help: Remove assignment to unused variable `watermark_data`

W293 [*] Blank line contains whitespace
  --> tests/test_watermark_utils.py:75:1
   |
73 |         "watermark STRING"
74 |     )
75 |     
   | ^^^^
76 |     # Try to get watermark (may not exist, which is fine)
77 |     result = get_watermark("test_source", mock_config, spark)
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tests/test_watermark_utils.py:78:1
   |
76 |     # Try to get watermark (may not exist, which is fine)
77 |     result = get_watermark("test_source", mock_config, spark)
78 |     
   | ^^^^
79 |     # Result should be None or a datetime
80 |     assert result is None or isinstance(result, datetime)
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> tools/validate_aws_etl.py:17:1
   |
15 |     # On EMR Serverless (via Airflow DAG or EMR job):
16 |     python tools/validate_aws_etl.py --env dev --config s3://bucket/config/aws/config/dev.yaml
17 |     
   | ^^^^
18 |     # For local testing (uses local filesystem):
19 |     python tools/validate_local_etl.py --env local --config local/config/local.yaml
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> tools/validate_aws_etl.py:22:1
   |
20 |   """
21 |
22 | / import sys
23 | | import argparse
24 | | import logging
25 | | from pathlib import Path
26 | | from typing import Dict, Any, List, Tuple
   | |_________________________________________^
27 |
28 |   # Add src to path
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> tools/validate_aws_etl.py:26:1
   |
24 | import logging
25 | from pathlib import Path
26 | from typing import Dict, Any, List, Tuple
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
27 |
28 | # Add src to path
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> tools/validate_aws_etl.py:26:1
   |
24 | import logging
25 | from pathlib import Path
26 | from typing import Dict, Any, List, Tuple
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
27 |
28 | # Add src to path
   |

UP035 `typing.Tuple` is deprecated, use `tuple` instead
  --> tools/validate_aws_etl.py:26:1
   |
24 | import logging
25 | from pathlib import Path
26 | from typing import Dict, Any, List, Tuple
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
27 |
28 | # Add src to path
   |

F401 [*] `typing.List` imported but unused
  --> tools/validate_aws_etl.py:26:31
   |
24 | import logging
25 | from pathlib import Path
26 | from typing import Dict, Any, List, Tuple
   |                               ^^^^
27 |
28 | # Add src to path
   |
help: Remove unused import: `typing.List`

E402 Module level import not at top of file
  --> tools/validate_aws_etl.py:34:1
   |
32 |     sys.path.insert(0, str(src_path))
33 |
34 | from pyspark.sql import SparkSession, DataFrame
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
35 | from pyspark.sql import functions as F
36 | from pyspark.sql import types as T
   |

I001 [*] Import block is un-sorted or un-formatted
  --> tools/validate_aws_etl.py:34:1
   |
32 |       sys.path.insert(0, str(src_path))
33 |
34 | / from pyspark.sql import SparkSession, DataFrame
35 | | from pyspark.sql import functions as F
36 | | from pyspark.sql import types as T
37 | |
38 | | # Core utilities
39 | | from project_a.utils.spark_session import build_spark
40 | | from project_a.pyspark_interview_project.utils.config_loader import load_config_resolved
41 | | from project_a.utils.path_resolver import resolve_data_path
   | |___________________________________________________________^
42 |
43 |   logging.basicConfig(
   |
help: Organize imports

E402 Module level import not at top of file
  --> tools/validate_aws_etl.py:35:1
   |
34 | from pyspark.sql import SparkSession, DataFrame
35 | from pyspark.sql import functions as F
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
36 | from pyspark.sql import types as T
   |

E402 Module level import not at top of file
  --> tools/validate_aws_etl.py:36:1
   |
34 | from pyspark.sql import SparkSession, DataFrame
35 | from pyspark.sql import functions as F
36 | from pyspark.sql import types as T
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
37 |
38 | # Core utilities
   |

E402 Module level import not at top of file
  --> tools/validate_aws_etl.py:39:1
   |
38 | # Core utilities
39 | from project_a.utils.spark_session import build_spark
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
40 | from project_a.pyspark_interview_project.utils.config_loader import load_config_resolved
41 | from project_a.utils.path_resolver import resolve_data_path
   |

E402 Module level import not at top of file
  --> tools/validate_aws_etl.py:40:1
   |
38 | # Core utilities
39 | from project_a.utils.spark_session import build_spark
40 | from project_a.pyspark_interview_project.utils.config_loader import load_config_resolved
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
41 | from project_a.utils.path_resolver import resolve_data_path
   |

E402 Module level import not at top of file
  --> tools/validate_aws_etl.py:41:1
   |
39 | from project_a.utils.spark_session import build_spark
40 | from project_a.pyspark_interview_project.utils.config_loader import load_config_resolved
41 | from project_a.utils.path_resolver import resolve_data_path
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
42 |
43 | logging.basicConfig(
   |

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> tools/validate_aws_etl.py:54:13
   |
52 |     path: str,
53 |     table_name: str,
54 |     config: Dict[str, Any],
   |             ^^^^
55 |     *,
56 |     required: bool = True,
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> tools/validate_aws_etl.py:61:1
   |
59 |     """
60 |     Read a table from Silver or Gold layer with format-aware fallback.
61 |     
   | ^^^^
62 |     For AWS environment: delta first, then parquet fallback
63 |     For local environment: parquet only (no delta attempts)
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> tools/validate_aws_etl.py:64:1
   |
62 |     For AWS environment: delta first, then parquet fallback
63 |     For local environment: parquet only (no delta attempts)
64 |     
   | ^^^^
65 |     Args:
66 |         spark: SparkSession
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> tools/validate_aws_etl.py:72:1
   |
70 |         required: Whether this table is required (affects error logging)
71 |         schema: Optional schema for empty DataFrame fallback
72 |         
   | ^^^^^^^^
73 |     Returns:
74 |         DataFrame or empty DataFrame with schema if not found
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> tools/validate_aws_etl.py:76:5
   |
74 |           DataFrame or empty DataFrame with schema if not found
75 |       """
76 | /     import os
77 | |     from pyspark.sql.utils import AnalysisException
78 | |     from pyspark.sql import types as T
   | |______________________________________^
79 |       
80 |       environment = config.get("environment", config.get("env", "aws"))
   |
help: Organize imports

F401 [*] `os` imported but unused
  --> tools/validate_aws_etl.py:76:12
   |
74 |         DataFrame or empty DataFrame with schema if not found
75 |     """
76 |     import os
   |            ^^
77 |     from pyspark.sql.utils import AnalysisException
78 |     from pyspark.sql import types as T
   |
help: Remove unused import: `os`

W293 [*] Blank line contains whitespace
  --> tools/validate_aws_etl.py:79:1
   |
77 |     from pyspark.sql.utils import AnalysisException
78 |     from pyspark.sql import types as T
79 |     
   | ^^^^
80 |     environment = config.get("environment", config.get("env", "aws"))
81 |     # For AWS validation, we expect "emr", "aws", or "prod" - not "local"
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tools/validate_aws_etl.py:83:1
   |
81 |     # For AWS validation, we expect "emr", "aws", or "prod" - not "local"
82 |     is_local = environment in ("local", "dev_local")
83 |     
   | ^^^^
84 |     # Normalize path
85 |     path = path.rstrip("/")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tools/validate_aws_etl.py:86:1
   |
84 |     # Normalize path
85 |     path = path.rstrip("/")
86 |     
   | ^^^^
87 |     # Check if path is S3
88 |     is_s3_path = path.startswith("s3://")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tools/validate_aws_etl.py:89:1
   |
87 |     # Check if path is S3
88 |     is_s3_path = path.startswith("s3://")
89 |     
   | ^^^^
90 |     # If running locally but trying to read from S3, provide helpful error
91 |     if is_local and is_s3_path:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:106:1
    |
104 |             return spark.createDataFrame([], schema)
105 |         return spark.createDataFrame([], T.StructType([]))
106 |     
    | ^^^^
107 |     # Local filesystem: parquet only (no delta attempts)
108 |     if is_local:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:132:1
    |
130 |                 return spark.createDataFrame([], schema)
131 |             return spark.createDataFrame([], T.StructType([]))
132 |     
    | ^^^^
133 |     # AWS: delta first, then parquet fallback
134 |     try:
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> tools/validate_aws_etl.py:183:6
    |
181 |     show_samples: bool = True,
182 |     num_samples: int = 5
183 | ) -> Dict[str, Any]:
    |      ^^^^
184 |     """
185 |     Print a compact report for a table.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> tools/validate_aws_etl.py:186:1
    |
184 |     """
185 |     Print a compact report for a table.
186 |     
    | ^^^^
187 |     Args:
188 |         df: DataFrame to report on
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> tools/validate_aws_etl.py:192:1
    |
190 |         show_samples: Whether to show sample rows
191 |         num_samples: Number of sample rows to show
192 |         
    | ^^^^^^^^
193 |     Returns:
194 |         Dictionary with report metrics
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:199:1
    |
197 |     print(f"ðŸ“Š {table_name.upper()}")
198 |     print(f"{'='*80}")
199 |     
    | ^^^^
200 |     try:
201 |         row_count = df.count()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:203:1
    |
201 |         row_count = df.count()
202 |         print(f"Row Count: {row_count:,}")
203 |         
    | ^^^^^^^^
204 |         if row_count == 0:
205 |             print("âš ï¸  WARNING: Table is empty!")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:219:1
    |
217 |                 pass
218 |             return {"row_count": 0, "schema": schema_dict if schema_dict else None, "has_data": False}
219 |         
    | ^^^^^^^^
220 |         # Schema
221 |         print(f"\nSchema ({len(df.columns)} columns):")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:230:1
    |
228 |                 "nullable": field.nullable
229 |             }
230 |         
    | ^^^^^^^^
231 |         # Sample rows
232 |         if show_samples and row_count > 0:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:246:1
    |
244 |             except Exception as e:
245 |                 print(f"  âš ï¸  Could not show samples: {e}")
246 |         
    | ^^^^^^^^
247 |         return {
248 |             "row_count": row_count,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:252:1
    |
250 |             "has_data": True
251 |         }
252 |         
    | ^^^^^^^^
253 |     except Exception as e:
254 |         print(f"âŒ Error analyzing {table_name}: {e}")
    |
help: Remove whitespace from blank line

UP006 [*] Use `tuple` instead of `Tuple` for type annotation
   --> tools/validate_aws_etl.py:258:58
    |
258 | def check_null_percentage(df: DataFrame, column: str) -> Tuple[int, float]:
    |                                                          ^^^^^
259 |     """
260 |     Check null percentage for a column.
    |
help: Replace with `tuple`

W293 Blank line contains whitespace
   --> tools/validate_aws_etl.py:261:1
    |
259 |     """
260 |     Check null percentage for a column.
261 |     
    | ^^^^
262 |     Args:
263 |         df: DataFrame
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> tools/validate_aws_etl.py:265:1
    |
263 |         df: DataFrame
264 |         column: Column name
265 |         
    | ^^^^^^^^
266 |     Returns:
267 |         Tuple of (null_count, null_percentage)
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> tools/validate_aws_etl.py:287:6
    |
285 |     fact_name: str,
286 |     dim_name: str
287 | ) -> Dict[str, Any]:
    |      ^^^^
288 |     """
289 |     Check referential integrity between fact and dimension tables.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> tools/validate_aws_etl.py:290:1
    |
288 |     """
289 |     Check referential integrity between fact and dimension tables.
290 |     
    | ^^^^
291 |     Args:
292 |         fact_df: Fact table DataFrame
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> tools/validate_aws_etl.py:298:1
    |
296 |         fact_name: Fact table name
297 |         dim_name: Dimension table name
298 |         
    | ^^^^^^^^
299 |     Returns:
300 |         Dictionary with integrity check results
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:311:1
    |
309 |                 "status": "SKIPPED"
310 |             }
311 |         
    | ^^^^^^^^
312 |         # Get distinct keys from fact table
313 |         fact_keys = fact_df.select(fact_key).distinct()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:314:1
    |
312 |         # Get distinct keys from fact table
313 |         fact_keys = fact_df.select(fact_key).distinct()
314 |         
    | ^^^^^^^^
315 |         # Get distinct keys from dimension
316 |         dim_keys = dim_df.select(dim_key).distinct()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:317:1
    |
315 |         # Get distinct keys from dimension
316 |         dim_keys = dim_df.select(dim_key).distinct()
317 |         
    | ^^^^^^^^
318 |         # Find orphans (keys in fact but not in dim)
319 |         orphans = fact_keys.join(dim_keys, fact_keys[fact_key] == dim_keys[dim_key], "left_anti")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:322:1
    |
320 |         orphan_count = orphans.count()
321 |         orphan_pct = (orphan_count / fact_total) * 100.0 if fact_total > 0 else 0.0
322 |         
    | ^^^^^^^^
323 |         status = "PASS" if orphan_count == 0 else "WARN" if orphan_pct < 5.0 else "FAIL"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:324:1
    |
323 |         status = "PASS" if orphan_count == 0 else "WARN" if orphan_pct < 5.0 else "FAIL"
324 |         
    | ^^^^^^^^
325 |         return {
326 |             "total_fact_rows": fact_total,
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> tools/validate_aws_etl.py:349:13
    |
347 | def validate_silver_layer(
348 |     spark: SparkSession,
349 |     config: Dict[str, Any]
    |             ^^^^
350 | ) -> Dict[str, Any]:
351 |     """Validate Silver layer tables."""
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> tools/validate_aws_etl.py:350:6
    |
348 |     spark: SparkSession,
349 |     config: Dict[str, Any]
350 | ) -> Dict[str, Any]:
    |      ^^^^
351 |     """Validate Silver layer tables."""
352 |     print("\n" + "="*80)
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:355:1
    |
353 |     print("ðŸ” VALIDATING SILVER LAYER")
354 |     print("="*80)
355 |     
    | ^^^^
356 |     silver_root = resolve_data_path(config, "silver")
357 |     tables_config = config.get("tables", {}).get("silver", {})
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:358:1
    |
356 |     silver_root = resolve_data_path(config, "silver")
357 |     tables_config = config.get("tables", {}).get("silver", {})
358 |     
    | ^^^^
359 |     silver_tables = {
360 |         "customers_silver": (tables_config.get("customers", "customers_silver"), True, None),
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:367:1
    |
365 |         "order_events_silver": (tables_config.get("order_events", "order_events_silver"), True, None),
366 |     }
367 |     
    | ^^^^
368 |     results = {}
369 |     dfs = {}
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:370:1
    |
368 |     results = {}
369 |     dfs = {}
370 |     
    | ^^^^
371 |     for table_key, (table_name, required, schema) in silver_tables.items():
372 |         path = f"{silver_root}/{table_name}"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:376:1
    |
374 |         dfs[table_key] = df
375 |         results[table_key] = print_table_report(df, table_name, show_samples=True)
376 |         
    | ^^^^^^^^
377 |         # Special handling for fx_rates_silver: warn if empty but schema is present
378 |         if table_key == "fx_rates_silver" and results[table_key].get("row_count", 0) == 0:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:384:1
    |
382 |                 else:
383 |                     logger.warning(f"  âš  {table_name} is empty and no schema found (using provided schema)")
384 |     
    | ^^^^
385 |     # Sanity checks
386 |     print(f"\n{'='*80}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:389:1
    |
387 |     print("ðŸ” SILVER LAYER SANITY CHECKS")
388 |     print(f"{'='*80}")
389 |     
    | ^^^^
390 |     checks_passed = 0
391 |     checks_total = 0
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:392:1
    |
390 |     checks_passed = 0
391 |     checks_total = 0
392 |     
    | ^^^^
393 |     # Check critical tables have data
394 |     critical_tables = ["customers_silver", "orders_silver", "products_silver"]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:396:1
    |
394 |     critical_tables = ["customers_silver", "orders_silver", "products_silver"]
395 |     optional_tables = ["fx_rates_silver"]  # Optional tables that can be empty
396 |     
    | ^^^^
397 |     for table_key in critical_tables:
398 |         checks_total += 1
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:406:1
    |
404 |             else:
405 |                 print(f"âœ— {table_key}: 0 rows (FAIL)")
406 |     
    | ^^^^
407 |     # Check optional tables (warn if empty but don't fail)
408 |     for table_key in optional_tables:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:418:1
    |
416 |                 else:
417 |                     print(f"âš  {table_key}: 0 rows and no schema (WARN - may not exist)")
418 |     
    | ^^^^
419 |     # Check null percentages for key columns
420 |     if "customers_silver" in dfs and results["customers_silver"]["has_data"]:
    |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
   --> tools/validate_aws_etl.py:424:19
    |
422 |         null_count, null_pct = check_null_percentage(dfs["customers_silver"], "customer_id")
423 |         if null_pct == 0.0:
424 |             print(f"âœ“ customers_silver.customer_id: 0% null (PASS)")
    |                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
425 |             checks_passed += 1
426 |         else:
    |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:428:1
    |
426 |         else:
427 |             print(f"âš  customers_silver.customer_id: {null_pct:.2f}% null ({null_count:,} rows)")
428 |     
    | ^^^^
429 |     if "orders_silver" in dfs and results["orders_silver"]["has_data"]:
430 |         checks_total += 1
    |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
   --> tools/validate_aws_etl.py:433:19
    |
431 |         null_count, null_pct = check_null_percentage(dfs["orders_silver"], "order_id")
432 |         if null_pct == 0.0:
433 |             print(f"âœ“ orders_silver.order_id: 0% null (PASS)")
    |                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
434 |             checks_passed += 1
435 |         else:
    |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:437:1
    |
435 |         else:
436 |             print(f"âš  orders_silver.order_id: {null_pct:.2f}% null ({null_count:,} rows)")
437 |         
    | ^^^^^^^^
438 |         # Check customer_id FK
439 |         checks_total += 1
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:446:1
    |
444 |         else:
445 |             print(f"âš  orders_silver.customer_id: {null_pct:.2f}% null ({null_count:,} rows)")
446 |     
    | ^^^^
447 |     print(f"\nSilver Checks: {checks_passed}/{checks_total} passed")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:448:1
    |
447 |     print(f"\nSilver Checks: {checks_passed}/{checks_total} passed")
448 |     
    | ^^^^
449 |     return {
450 |         "tables": results,
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> tools/validate_aws_etl.py:459:13
    |
457 | def validate_gold_layer(
458 |     spark: SparkSession,
459 |     config: Dict[str, Any],
    |             ^^^^
460 |     silver_dfs: Dict[str, DataFrame]
461 | ) -> Dict[str, Any]:
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> tools/validate_aws_etl.py:460:17
    |
458 |     spark: SparkSession,
459 |     config: Dict[str, Any],
460 |     silver_dfs: Dict[str, DataFrame]
    |                 ^^^^
461 | ) -> Dict[str, Any]:
462 |     """Validate Gold layer tables."""
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> tools/validate_aws_etl.py:461:6
    |
459 |     config: Dict[str, Any],
460 |     silver_dfs: Dict[str, DataFrame]
461 | ) -> Dict[str, Any]:
    |      ^^^^
462 |     """Validate Gold layer tables."""
463 |     print("\n" + "="*80)
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:466:1
    |
464 |     print("ðŸ” VALIDATING GOLD LAYER")
465 |     print("="*80)
466 |     
    | ^^^^
467 |     gold_root = resolve_data_path(config, "gold")
468 |     tables_config = config.get("tables", {}).get("gold", {})
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:469:1
    |
467 |     gold_root = resolve_data_path(config, "gold")
468 |     tables_config = config.get("tables", {}).get("gold", {})
469 |     
    | ^^^^
470 |     gold_tables = {
471 |         "fact_orders": tables_config.get("fact_orders", "fact_orders"),
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:478:1
    |
476 |         "product_performance": tables_config.get("product_performance", "product_performance"),
477 |     }
478 |     
    | ^^^^
479 |     results = {}
480 |     dfs = {}
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:481:1
    |
479 |     results = {}
480 |     dfs = {}
481 |     
    | ^^^^
482 |     for table_key, table_name in gold_tables.items():
483 |         path = f"{gold_root}/{table_name}"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:487:1
    |
485 |         dfs[table_key] = df
486 |         results[table_key] = print_table_report(df, table_name, show_samples=True)
487 |     
    | ^^^^
488 |     # Sanity checks
489 |     print(f"\n{'='*80}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:492:1
    |
490 |     print("ðŸ” GOLD LAYER SANITY CHECKS")
491 |     print(f"{'='*80}")
492 |     
    | ^^^^
493 |     checks_passed = 0
494 |     checks_total = 0
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:495:1
    |
493 |     checks_passed = 0
494 |     checks_total = 0
495 |     
    | ^^^^
496 |     # Check critical tables have data
497 |     critical_tables = ["fact_orders", "dim_customer", "dim_product"]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:507:1
    |
505 |             else:
506 |                 print(f"âœ— {table_key}: 0 rows (FAIL)")
507 |     
    | ^^^^
508 |     # Check referential integrity
509 |     # Note: fact_orders uses surrogate keys (customer_sk, product_sk), not natural keys
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:515:1
    |
513 |             fact_df = dfs["fact_orders"]
514 |             total_fact = fact_df.count()
515 |             
    | ^^^^^^^^^^^^
516 |             # Check for missing customer_sk (should be rare, -1 indicates missing dim)
517 |             if "customer_sk" in fact_df.columns:
    |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
   --> tools/validate_aws_etl.py:522:27
    |
520 |                 missing_pct = (missing_customer / total_fact) * 100.0 if total_fact > 0 else 0.0
521 |                 if missing_pct == 0.0:
522 |                     print(f"âœ“ fact_orders.customer_sk: All rows have valid customer (PASS)")
    |                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
523 |                     checks_passed += 1
524 |                 elif missing_pct < 1.0:
    |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:529:1
    |
527 |                 else:
528 |                     print(f"âœ— fact_orders.customer_sk: {missing_pct:.2f}% missing customer (FAIL)")
529 |             
    | ^^^^^^^^^^^^
530 |             # Check for missing product_sk
531 |             if "product_sk" in fact_df.columns:
    |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
   --> tools/validate_aws_etl.py:536:27
    |
534 |                 missing_pct = (missing_product / total_fact) * 100.0 if total_fact > 0 else 0.0
535 |                 if missing_pct == 0.0:
536 |                     print(f"âœ“ fact_orders.product_sk: All rows have valid product (PASS)")
    |                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
537 |                     checks_passed += 1
538 |                 elif missing_pct < 1.0:
    |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:545:1
    |
543 |         except Exception as e:
544 |             print(f"âš  Could not check referential integrity: {e}")
545 |     
    | ^^^^
546 |     # Check key aggregations make sense
547 |     if "fact_orders" in dfs and results["fact_orders"]["has_data"]:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:558:1
    |
556 |         except Exception as e:
557 |             print(f"âš  Could not compute total revenue: {e}")
558 |     
    | ^^^^
559 |     print(f"\nGold Checks: {checks_passed}/{checks_total} passed")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:560:1
    |
559 |     print(f"\nGold Checks: {checks_passed}/{checks_total} passed")
560 |     
    | ^^^^
561 |     return {
562 |         "tables": results,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:577:1
    |
575 |     parser.add_argument("--skip-gold", action="store_true", help="Skip Gold validation")
576 |     args = parser.parse_args()
577 |     
    | ^^^^
578 |     # Load config
579 |     if args.config:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:587:1
    |
585 |         else:
586 |             config_path = f"aws/config/{env}.yaml"
587 |     
    | ^^^^
588 |     try:
589 |         config = load_config_resolved(config_path, env=args.env)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:593:1
    |
591 |         logger.error(f"Failed to load config: {e}")
592 |         sys.exit(1)
593 |     
    | ^^^^
594 |     # Set environment if not set
595 |     if not config.get("environment"):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:597:1
    |
595 |     if not config.get("environment"):
596 |         config["environment"] = args.env if args.env != "aws" else "aws"
597 |     
    | ^^^^
598 |     # Detect if running locally
599 |     import os
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:602:1
    |
600 |     spark_master = os.getenv("SPARK_MASTER", "local[*]")
601 |     is_local_run = spark_master.startswith("local") or config.get("environment") == "local"
602 |     
    | ^^^^
603 |     # Check if config has S3 paths
604 |     paths = config.get("paths", {})
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:609:1
    |
607 |         for v in paths.values()
608 |     )
609 |     
    | ^^^^
610 |     if is_local_run and has_s3_paths:
611 |         logger.error("="*80)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:635:1
    |
633 |         logger.error("Exiting. Please run this script on EMR Serverless or use validate_local_etl.py for local validation.")
634 |         sys.exit(1)
635 |     
    | ^^^^
636 |     # Build Spark session (with Delta enabled for AWS)
637 |     spark = build_spark(app_name="validate_aws_etl", config=config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:638:1
    |
636 |     # Build Spark session (with Delta enabled for AWS)
637 |     spark = build_spark(app_name="validate_aws_etl", config=config)
638 |     
    | ^^^^
639 |     try:
640 |         print("="*80)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:650:1
    |
648 |         if is_local_run and has_s3_paths:
649 |             print("âš ï¸  Running locally with S3 paths - S3 reads will fail")
650 |         
    | ^^^^^^^^
651 |         silver_results = None
652 |         gold_results = None
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:653:1
    |
651 |         silver_results = None
652 |         gold_results = None
653 |         
    | ^^^^^^^^
654 |         # Validate Silver
655 |         if not args.skip_silver:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:657:1
    |
655 |         if not args.skip_silver:
656 |             silver_results = validate_silver_layer(spark, config)
657 |         
    | ^^^^^^^^
658 |         # Validate Gold
659 |         if not args.skip_gold:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:662:1
    |
660 |             silver_dfs = silver_results.get("dataframes", {}) if silver_results else {}
661 |             gold_results = validate_gold_layer(spark, config, silver_dfs)
662 |         
    | ^^^^^^^^
663 |         # Final summary
664 |         print("\n" + "="*80)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:667:1
    |
665 |         print("ðŸ“‹ VALIDATION SUMMARY")
666 |         print("="*80)
667 |         
    | ^^^^^^^^
668 |         total_checks = 0
669 |         total_passed = 0
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:670:1
    |
668 |         total_checks = 0
669 |         total_passed = 0
670 |         
    | ^^^^^^^^
671 |         if silver_results:
672 |             silver_passed = silver_results.get("checks_passed", 0)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:677:1
    |
675 |             total_passed += silver_passed
676 |             print(f"Silver Layer: {silver_passed}/{silver_total} checks passed")
677 |         
    | ^^^^^^^^
678 |         if gold_results:
679 |             gold_passed = gold_results.get("checks_passed", 0)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:684:1
    |
682 |             total_passed += gold_passed
683 |             print(f"Gold Layer: {gold_passed}/{gold_total} checks passed")
684 |         
    | ^^^^^^^^
685 |         print(f"\nOverall: {total_passed}/{total_checks} checks passed")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:686:1
    |
685 |         print(f"\nOverall: {total_passed}/{total_checks} checks passed")
686 |         
    | ^^^^^^^^
687 |         # Exit code
688 |         if total_checks == 0:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_aws_etl.py:700:1
    |
698 |             print("âŒ Critical checks failed - pipeline may have issues")
699 |             sys.exit(1)
700 |             
    | ^^^^^^^^^^^^
701 |     except Exception as e:
702 |         logger.error(f"Validation failed: {e}", exc_info=True)
    |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> tools/validate_local_etl.py:14:1
   |
12 |   """
13 |
14 | / import sys
15 | | import argparse
16 | | import logging
17 | | from pathlib import Path
18 | | from typing import Dict, Any, List, Tuple
   | |_________________________________________^
19 |
20 |   # Add src to path
   |
help: Organize imports

UP035 `typing.Dict` is deprecated, use `dict` instead
  --> tools/validate_local_etl.py:18:1
   |
16 | import logging
17 | from pathlib import Path
18 | from typing import Dict, Any, List, Tuple
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
19 |
20 | # Add src to path
   |

UP035 `typing.List` is deprecated, use `list` instead
  --> tools/validate_local_etl.py:18:1
   |
16 | import logging
17 | from pathlib import Path
18 | from typing import Dict, Any, List, Tuple
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
19 |
20 | # Add src to path
   |

UP035 `typing.Tuple` is deprecated, use `tuple` instead
  --> tools/validate_local_etl.py:18:1
   |
16 | import logging
17 | from pathlib import Path
18 | from typing import Dict, Any, List, Tuple
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
19 |
20 | # Add src to path
   |

F401 [*] `typing.List` imported but unused
  --> tools/validate_local_etl.py:18:31
   |
16 | import logging
17 | from pathlib import Path
18 | from typing import Dict, Any, List, Tuple
   |                               ^^^^
19 |
20 | # Add src to path
   |
help: Remove unused import: `typing.List`

E402 Module level import not at top of file
  --> tools/validate_local_etl.py:26:1
   |
24 |     sys.path.insert(0, str(src_path))
25 |
26 | from pyspark.sql import SparkSession, DataFrame
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
27 | from pyspark.sql import functions as F
28 | from pyspark.sql import types as T
   |

I001 [*] Import block is un-sorted or un-formatted
  --> tools/validate_local_etl.py:26:1
   |
24 |       sys.path.insert(0, str(src_path))
25 |
26 | / from pyspark.sql import SparkSession, DataFrame
27 | | from pyspark.sql import functions as F
28 | | from pyspark.sql import types as T
29 | |
30 | | # Core utilities
31 | | from project_a.utils.spark_session import build_spark
32 | | from project_a.pyspark_interview_project.utils.config_loader import load_config_resolved
33 | | from project_a.utils.path_resolver import resolve_data_path
   | |___________________________________________________________^
34 |
35 |   logging.basicConfig(
   |
help: Organize imports

E402 Module level import not at top of file
  --> tools/validate_local_etl.py:27:1
   |
26 | from pyspark.sql import SparkSession, DataFrame
27 | from pyspark.sql import functions as F
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
28 | from pyspark.sql import types as T
   |

E402 Module level import not at top of file
  --> tools/validate_local_etl.py:28:1
   |
26 | from pyspark.sql import SparkSession, DataFrame
27 | from pyspark.sql import functions as F
28 | from pyspark.sql import types as T
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
29 |
30 | # Core utilities
   |

E402 Module level import not at top of file
  --> tools/validate_local_etl.py:31:1
   |
30 | # Core utilities
31 | from project_a.utils.spark_session import build_spark
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
32 | from project_a.pyspark_interview_project.utils.config_loader import load_config_resolved
33 | from project_a.utils.path_resolver import resolve_data_path
   |

E402 Module level import not at top of file
  --> tools/validate_local_etl.py:32:1
   |
30 | # Core utilities
31 | from project_a.utils.spark_session import build_spark
32 | from project_a.pyspark_interview_project.utils.config_loader import load_config_resolved
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
33 | from project_a.utils.path_resolver import resolve_data_path
   |

E402 Module level import not at top of file
  --> tools/validate_local_etl.py:33:1
   |
31 | from project_a.utils.spark_session import build_spark
32 | from project_a.pyspark_interview_project.utils.config_loader import load_config_resolved
33 | from project_a.utils.path_resolver import resolve_data_path
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
34 |
35 | logging.basicConfig(
   |

UP006 [*] Use `dict` instead of `Dict` for type annotation
  --> tools/validate_local_etl.py:46:13
   |
44 |     path: str,
45 |     table_name: str,
46 |     config: Dict[str, Any],
   |             ^^^^
47 |     *,
48 |     required: bool = True,
   |
help: Replace with `dict`

W293 Blank line contains whitespace
  --> tools/validate_local_etl.py:53:1
   |
51 |     """
52 |     Read a table from Silver or Gold layer with format-aware fallback.
53 |     
   | ^^^^
54 |     For local environment: parquet only (no delta attempts)
55 |     For non-local: delta first, then parquet fallback
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> tools/validate_local_etl.py:56:1
   |
54 |     For local environment: parquet only (no delta attempts)
55 |     For non-local: delta first, then parquet fallback
56 |     
   | ^^^^
57 |     Args:
58 |         spark: SparkSession
   |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
  --> tools/validate_local_etl.py:64:1
   |
62 |         required: Whether this table is required (affects error logging)
63 |         schema: Optional schema for empty DataFrame fallback
64 |         
   | ^^^^^^^^
65 |     Returns:
66 |         DataFrame or empty DataFrame with schema if not found
   |
help: Remove whitespace from blank line

I001 [*] Import block is un-sorted or un-formatted
  --> tools/validate_local_etl.py:68:5
   |
66 |           DataFrame or empty DataFrame with schema if not found
67 |       """
68 | /     import os
69 | |     from pyspark.sql.utils import AnalysisException
70 | |     from pyspark.sql import types as T
   | |______________________________________^
71 |       
72 |       environment = config.get("environment", config.get("env", "local"))
   |
help: Organize imports

W293 [*] Blank line contains whitespace
  --> tools/validate_local_etl.py:71:1
   |
69 |     from pyspark.sql.utils import AnalysisException
70 |     from pyspark.sql import types as T
71 |     
   | ^^^^
72 |     environment = config.get("environment", config.get("env", "local"))
73 |     is_local = environment in ("local", "dev_local")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tools/validate_local_etl.py:74:1
   |
72 |     environment = config.get("environment", config.get("env", "local"))
73 |     is_local = environment in ("local", "dev_local")
74 |     
   | ^^^^
75 |     # Normalize path
76 |     path = path.rstrip("/")
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
  --> tools/validate_local_etl.py:77:1
   |
75 |     # Normalize path
76 |     path = path.rstrip("/")
77 |     
   | ^^^^
78 |     # Local: parquet only (no delta attempts)
79 |     if is_local:
   |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:110:1
    |
108 |                 return spark.createDataFrame([], schema)
109 |             return spark.createDataFrame([], T.StructType([]))
110 |     
    | ^^^^
111 |     # Non-local: delta first, then parquet fallback
112 |     try:
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> tools/validate_local_etl.py:147:6
    |
145 |     show_samples: bool = True,
146 |     num_samples: int = 5
147 | ) -> Dict[str, Any]:
    |      ^^^^
148 |     """
149 |     Print a compact report for a table.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> tools/validate_local_etl.py:150:1
    |
148 |     """
149 |     Print a compact report for a table.
150 |     
    | ^^^^
151 |     Args:
152 |         df: DataFrame to report on
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> tools/validate_local_etl.py:156:1
    |
154 |         show_samples: Whether to show sample rows
155 |         num_samples: Number of sample rows to show
156 |         
    | ^^^^^^^^
157 |     Returns:
158 |         Dictionary with report metrics
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:163:1
    |
161 |     print(f"ðŸ“Š {table_name.upper()}")
162 |     print(f"{'='*80}")
163 |     
    | ^^^^
164 |     try:
165 |         row_count = df.count()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:167:1
    |
165 |         row_count = df.count()
166 |         print(f"Row Count: {row_count:,}")
167 |         
    | ^^^^^^^^
168 |         if row_count == 0:
169 |             print("âš ï¸  WARNING: Table is empty!")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:183:1
    |
181 |                 pass
182 |             return {"row_count": 0, "schema": schema_dict if schema_dict else None, "has_data": False}
183 |         
    | ^^^^^^^^
184 |         # Schema
185 |         print(f"\nSchema ({len(df.columns)} columns):")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:194:1
    |
192 |                 "nullable": field.nullable
193 |             }
194 |         
    | ^^^^^^^^
195 |         # Sample rows
196 |         if show_samples and row_count > 0:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:210:1
    |
208 |             except Exception as e:
209 |                 print(f"  âš ï¸  Could not show samples: {e}")
210 |         
    | ^^^^^^^^
211 |         return {
212 |             "row_count": row_count,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:216:1
    |
214 |             "has_data": True
215 |         }
216 |         
    | ^^^^^^^^
217 |     except Exception as e:
218 |         print(f"âŒ Error analyzing {table_name}: {e}")
    |
help: Remove whitespace from blank line

UP006 [*] Use `tuple` instead of `Tuple` for type annotation
   --> tools/validate_local_etl.py:222:58
    |
222 | def check_null_percentage(df: DataFrame, column: str) -> Tuple[int, float]:
    |                                                          ^^^^^
223 |     """
224 |     Check null percentage for a column.
    |
help: Replace with `tuple`

W293 Blank line contains whitespace
   --> tools/validate_local_etl.py:225:1
    |
223 |     """
224 |     Check null percentage for a column.
225 |     
    | ^^^^
226 |     Args:
227 |         df: DataFrame
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> tools/validate_local_etl.py:229:1
    |
227 |         df: DataFrame
228 |         column: Column name
229 |         
    | ^^^^^^^^
230 |     Returns:
231 |         Tuple of (null_count, null_percentage)
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> tools/validate_local_etl.py:251:6
    |
249 |     fact_name: str,
250 |     dim_name: str
251 | ) -> Dict[str, Any]:
    |      ^^^^
252 |     """
253 |     Check referential integrity between fact and dimension tables.
    |
help: Replace with `dict`

W293 Blank line contains whitespace
   --> tools/validate_local_etl.py:254:1
    |
252 |     """
253 |     Check referential integrity between fact and dimension tables.
254 |     
    | ^^^^
255 |     Args:
256 |         fact_df: Fact table DataFrame
    |
help: Remove whitespace from blank line

W293 Blank line contains whitespace
   --> tools/validate_local_etl.py:262:1
    |
260 |         fact_name: Fact table name
261 |         dim_name: Dimension table name
262 |         
    | ^^^^^^^^
263 |     Returns:
264 |         Dictionary with integrity check results
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:275:1
    |
273 |                 "status": "SKIPPED"
274 |             }
275 |         
    | ^^^^^^^^
276 |         # Get distinct keys from fact table
277 |         fact_keys = fact_df.select(fact_key).distinct()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:278:1
    |
276 |         # Get distinct keys from fact table
277 |         fact_keys = fact_df.select(fact_key).distinct()
278 |         
    | ^^^^^^^^
279 |         # Get distinct keys from dimension
280 |         dim_keys = dim_df.select(dim_key).distinct()
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:281:1
    |
279 |         # Get distinct keys from dimension
280 |         dim_keys = dim_df.select(dim_key).distinct()
281 |         
    | ^^^^^^^^
282 |         # Find orphans (keys in fact but not in dim)
283 |         orphans = fact_keys.join(dim_keys, fact_keys[fact_key] == dim_keys[dim_key], "left_anti")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:286:1
    |
284 |         orphan_count = orphans.count()
285 |         orphan_pct = (orphan_count / fact_total) * 100.0 if fact_total > 0 else 0.0
286 |         
    | ^^^^^^^^
287 |         status = "PASS" if orphan_count == 0 else "WARN" if orphan_pct < 5.0 else "FAIL"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:288:1
    |
287 |         status = "PASS" if orphan_count == 0 else "WARN" if orphan_pct < 5.0 else "FAIL"
288 |         
    | ^^^^^^^^
289 |         return {
290 |             "total_fact_rows": fact_total,
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> tools/validate_local_etl.py:313:13
    |
311 | def validate_silver_layer(
312 |     spark: SparkSession,
313 |     config: Dict[str, Any]
    |             ^^^^
314 | ) -> Dict[str, Any]:
315 |     """Validate Silver layer tables."""
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> tools/validate_local_etl.py:314:6
    |
312 |     spark: SparkSession,
313 |     config: Dict[str, Any]
314 | ) -> Dict[str, Any]:
    |      ^^^^
315 |     """Validate Silver layer tables."""
316 |     print("\n" + "="*80)
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:319:1
    |
317 |     print("ðŸ” VALIDATING SILVER LAYER")
318 |     print("="*80)
319 |     
    | ^^^^
320 |     silver_root = resolve_data_path(config, "silver")
321 |     tables_config = config.get("tables", {}).get("silver", {})
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:322:1
    |
320 |     silver_root = resolve_data_path(config, "silver")
321 |     tables_config = config.get("tables", {}).get("silver", {})
322 |     
    | ^^^^
323 |     silver_tables = {
324 |         "customers_silver": (tables_config.get("customers", "customers_silver"), True, None),
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:331:1
    |
329 |         "order_events_silver": (tables_config.get("order_events", "order_events_silver"), True, None),
330 |     }
331 |     
    | ^^^^
332 |     results = {}
333 |     dfs = {}
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:334:1
    |
332 |     results = {}
333 |     dfs = {}
334 |     
    | ^^^^
335 |     for table_key, (table_name, required, schema) in silver_tables.items():
336 |         path = f"{silver_root}/{table_name}"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:340:1
    |
338 |         dfs[table_key] = df
339 |         results[table_key] = print_table_report(df, table_name, show_samples=True)
340 |         
    | ^^^^^^^^
341 |         # Special handling for fx_rates_silver: warn if empty but schema is present
342 |         if table_key == "fx_rates_silver" and results[table_key].get("row_count", 0) == 0:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:349:1
    |
347 |                 else:
348 |                     logger.warning(f"  âš  {table_name} is empty and no schema found (using provided schema)")
349 |     
    | ^^^^
350 |     # Sanity checks
351 |     print(f"\n{'='*80}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:354:1
    |
352 |     print("ðŸ” SILVER LAYER SANITY CHECKS")
353 |     print(f"{'='*80}")
354 |     
    | ^^^^
355 |     checks_passed = 0
356 |     checks_total = 0
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:357:1
    |
355 |     checks_passed = 0
356 |     checks_total = 0
357 |     
    | ^^^^
358 |     # Check critical tables have data
359 |     critical_tables = ["customers_silver", "orders_silver", "products_silver"]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:361:1
    |
359 |     critical_tables = ["customers_silver", "orders_silver", "products_silver"]
360 |     optional_tables = ["fx_rates_silver"]  # Optional tables that can be empty
361 |     
    | ^^^^
362 |     for table_key in critical_tables:
363 |         checks_total += 1
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:371:1
    |
369 |             else:
370 |                 print(f"âœ— {table_key}: 0 rows (FAIL)")
371 |     
    | ^^^^
372 |     # Check optional tables (warn if empty but don't fail)
373 |     for table_key in optional_tables:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:384:1
    |
382 |                 else:
383 |                     print(f"âš  {table_key}: 0 rows and no schema (WARN - may not exist)")
384 |     
    | ^^^^
385 |     # Check null percentages for key columns
386 |     if "customers_silver" in dfs and results["customers_silver"]["has_data"]:
    |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
   --> tools/validate_local_etl.py:390:19
    |
388 |         null_count, null_pct = check_null_percentage(dfs["customers_silver"], "customer_id")
389 |         if null_pct == 0.0:
390 |             print(f"âœ“ customers_silver.customer_id: 0% null (PASS)")
    |                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
391 |             checks_passed += 1
392 |         else:
    |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:394:1
    |
392 |         else:
393 |             print(f"âš  customers_silver.customer_id: {null_pct:.2f}% null ({null_count:,} rows)")
394 |     
    | ^^^^
395 |     if "orders_silver" in dfs and results["orders_silver"]["has_data"]:
396 |         checks_total += 1
    |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
   --> tools/validate_local_etl.py:399:19
    |
397 |         null_count, null_pct = check_null_percentage(dfs["orders_silver"], "order_id")
398 |         if null_pct == 0.0:
399 |             print(f"âœ“ orders_silver.order_id: 0% null (PASS)")
    |                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
400 |             checks_passed += 1
401 |         else:
    |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:403:1
    |
401 |         else:
402 |             print(f"âš  orders_silver.order_id: {null_pct:.2f}% null ({null_count:,} rows)")
403 |         
    | ^^^^^^^^
404 |         # Check customer_id FK
405 |         checks_total += 1
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:412:1
    |
410 |         else:
411 |             print(f"âš  orders_silver.customer_id: {null_pct:.2f}% null ({null_count:,} rows)")
412 |     
    | ^^^^
413 |     print(f"\nSilver Checks: {checks_passed}/{checks_total} passed")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:414:1
    |
413 |     print(f"\nSilver Checks: {checks_passed}/{checks_total} passed")
414 |     
    | ^^^^
415 |     return {
416 |         "tables": results,
    |
help: Remove whitespace from blank line

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> tools/validate_local_etl.py:425:13
    |
423 | def validate_gold_layer(
424 |     spark: SparkSession,
425 |     config: Dict[str, Any],
    |             ^^^^
426 |     silver_dfs: Dict[str, DataFrame]
427 | ) -> Dict[str, Any]:
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> tools/validate_local_etl.py:426:17
    |
424 |     spark: SparkSession,
425 |     config: Dict[str, Any],
426 |     silver_dfs: Dict[str, DataFrame]
    |                 ^^^^
427 | ) -> Dict[str, Any]:
428 |     """Validate Gold layer tables."""
    |
help: Replace with `dict`

UP006 [*] Use `dict` instead of `Dict` for type annotation
   --> tools/validate_local_etl.py:427:6
    |
425 |     config: Dict[str, Any],
426 |     silver_dfs: Dict[str, DataFrame]
427 | ) -> Dict[str, Any]:
    |      ^^^^
428 |     """Validate Gold layer tables."""
429 |     print("\n" + "="*80)
    |
help: Replace with `dict`

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:432:1
    |
430 |     print("ðŸ” VALIDATING GOLD LAYER")
431 |     print("="*80)
432 |     
    | ^^^^
433 |     gold_root = resolve_data_path(config, "gold")
434 |     tables_config = config.get("tables", {}).get("gold", {})
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:435:1
    |
433 |     gold_root = resolve_data_path(config, "gold")
434 |     tables_config = config.get("tables", {}).get("gold", {})
435 |     
    | ^^^^
436 |     gold_tables = {
437 |         "fact_orders": tables_config.get("fact_orders", "fact_orders"),
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:444:1
    |
442 |         "product_performance": tables_config.get("product_performance", "product_performance"),
443 |     }
444 |     
    | ^^^^
445 |     results = {}
446 |     dfs = {}
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:447:1
    |
445 |     results = {}
446 |     dfs = {}
447 |     
    | ^^^^
448 |     for table_key, table_name in gold_tables.items():
449 |         path = f"{gold_root}/{table_name}"
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:453:1
    |
451 |         dfs[table_key] = df
452 |         results[table_key] = print_table_report(df, table_name, show_samples=True)
453 |     
    | ^^^^
454 |     # Sanity checks
455 |     print(f"\n{'='*80}")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:458:1
    |
456 |     print("ðŸ” GOLD LAYER SANITY CHECKS")
457 |     print(f"{'='*80}")
458 |     
    | ^^^^
459 |     checks_passed = 0
460 |     checks_total = 0
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:461:1
    |
459 |     checks_passed = 0
460 |     checks_total = 0
461 |     
    | ^^^^
462 |     # Check critical tables have data
463 |     critical_tables = ["fact_orders", "dim_customer", "dim_product"]
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:473:1
    |
471 |             else:
472 |                 print(f"âœ— {table_key}: 0 rows (FAIL)")
473 |     
    | ^^^^
474 |     # Check referential integrity
475 |     # Note: fact_orders uses surrogate keys (customer_sk, product_sk), not natural keys
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:481:1
    |
479 |             fact_df = dfs["fact_orders"]
480 |             total_fact = fact_df.count()
481 |             
    | ^^^^^^^^^^^^
482 |             # Check for missing customer_sk (should be rare, -1 indicates missing dim)
483 |             if "customer_sk" in fact_df.columns:
    |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
   --> tools/validate_local_etl.py:488:27
    |
486 |                 missing_pct = (missing_customer / total_fact) * 100.0 if total_fact > 0 else 0.0
487 |                 if missing_pct == 0.0:
488 |                     print(f"âœ“ fact_orders.customer_sk: All rows have valid customer (PASS)")
    |                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
489 |                     checks_passed += 1
490 |                 elif missing_pct < 1.0:
    |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:495:1
    |
493 |                 else:
494 |                     print(f"âœ— fact_orders.customer_sk: {missing_pct:.2f}% missing customer (FAIL)")
495 |             
    | ^^^^^^^^^^^^
496 |             # Check for missing product_sk
497 |             if "product_sk" in fact_df.columns:
    |
help: Remove whitespace from blank line

F541 [*] f-string without any placeholders
   --> tools/validate_local_etl.py:502:27
    |
500 |                 missing_pct = (missing_product / total_fact) * 100.0 if total_fact > 0 else 0.0
501 |                 if missing_pct == 0.0:
502 |                     print(f"âœ“ fact_orders.product_sk: All rows have valid product (PASS)")
    |                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
503 |                     checks_passed += 1
504 |                 elif missing_pct < 1.0:
    |
help: Remove extraneous `f` prefix

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:511:1
    |
509 |         except Exception as e:
510 |             print(f"âš  Could not check referential integrity: {e}")
511 |     
    | ^^^^
512 |     # Check key aggregations make sense
513 |     if "fact_orders" in dfs and results["fact_orders"]["has_data"]:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:524:1
    |
522 |         except Exception as e:
523 |             print(f"âš  Could not compute total revenue: {e}")
524 |     
    | ^^^^
525 |     print(f"\nGold Checks: {checks_passed}/{checks_total} passed")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:526:1
    |
525 |     print(f"\nGold Checks: {checks_passed}/{checks_total} passed")
526 |     
    | ^^^^
527 |     return {
528 |         "tables": results,
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:543:1
    |
541 |     parser.add_argument("--skip-gold", action="store_true", help="Skip Gold validation")
542 |     args = parser.parse_args()
543 |     
    | ^^^^
544 |     # Load config
545 |     if args.config:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:553:1
    |
551 |         else:
552 |             config_path = f"config/{env}.yaml"
553 |     
    | ^^^^
554 |     try:
555 |         config = load_config_resolved(config_path, env=args.env)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:559:1
    |
557 |         logger.error(f"Failed to load config: {e}")
558 |         sys.exit(1)
559 |     
    | ^^^^
560 |     # Set environment if not set
561 |     if not config.get("environment"):
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:563:1
    |
561 |     if not config.get("environment"):
562 |         config["environment"] = args.env
563 |     
    | ^^^^
564 |     # Build Spark session
565 |     spark = build_spark(app_name="validate_local_etl", config=config)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:566:1
    |
564 |     # Build Spark session
565 |     spark = build_spark(app_name="validate_local_etl", config=config)
566 |     
    | ^^^^
567 |     try:
568 |         print("="*80)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:575:1
    |
573 |         print(f"Silver Root: {resolve_data_path(config, 'silver')}")
574 |         print(f"Gold Root: {resolve_data_path(config, 'gold')}")
575 |         
    | ^^^^^^^^
576 |         silver_results = None
577 |         gold_results = None
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:578:1
    |
576 |         silver_results = None
577 |         gold_results = None
578 |         
    | ^^^^^^^^
579 |         # Validate Silver
580 |         if not args.skip_silver:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:582:1
    |
580 |         if not args.skip_silver:
581 |             silver_results = validate_silver_layer(spark, config)
582 |         
    | ^^^^^^^^
583 |         # Validate Gold
584 |         if not args.skip_gold:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:587:1
    |
585 |             silver_dfs = silver_results.get("dataframes", {}) if silver_results else {}
586 |             gold_results = validate_gold_layer(spark, config, silver_dfs)
587 |         
    | ^^^^^^^^
588 |         # Final summary
589 |         print("\n" + "="*80)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:592:1
    |
590 |         print("ðŸ“‹ VALIDATION SUMMARY")
591 |         print("="*80)
592 |         
    | ^^^^^^^^
593 |         total_checks = 0
594 |         total_passed = 0
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:595:1
    |
593 |         total_checks = 0
594 |         total_passed = 0
595 |         
    | ^^^^^^^^
596 |         if silver_results:
597 |             silver_passed = silver_results.get("checks_passed", 0)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:602:1
    |
600 |             total_passed += silver_passed
601 |             print(f"Silver Layer: {silver_passed}/{silver_total} checks passed")
602 |         
    | ^^^^^^^^
603 |         if gold_results:
604 |             gold_passed = gold_results.get("checks_passed", 0)
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:609:1
    |
607 |             total_passed += gold_passed
608 |             print(f"Gold Layer: {gold_passed}/{gold_total} checks passed")
609 |         
    | ^^^^^^^^
610 |         print(f"\nOverall: {total_passed}/{total_checks} checks passed")
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:611:1
    |
610 |         print(f"\nOverall: {total_passed}/{total_checks} checks passed")
611 |         
    | ^^^^^^^^
612 |         # Exit code
613 |         if total_checks == 0:
    |
help: Remove whitespace from blank line

W293 [*] Blank line contains whitespace
   --> tools/validate_local_etl.py:625:1
    |
623 |             print("âŒ Critical checks failed - pipeline may have issues")
624 |             sys.exit(1)
625 |             
    | ^^^^^^^^^^^^
626 |     except Exception as e:
627 |         logger.error(f"Validation failed: {e}", exc_info=True)
    |
help: Remove whitespace from blank line

Found 6582 errors.
[*] 5552 fixable with the `--fix` option (672 hidden fixes can be enabled with the `--unsafe-fixes` option).
