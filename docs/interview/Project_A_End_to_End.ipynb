{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "09991d3b",
      "metadata": {},
      "source": [
        "# Project A — End‑to‑End Implementation Guide (Beginner Friendly)\n",
        "\n",
        "Welcome! This notebook explains **how to run and understand the entire Project A data pipeline** from Bronze → Silver → Gold. It is written for beginners, with step‑by‑step instructions, commands, and explanations.\n",
        "\n",
        "**What you will learn:**\n",
        "- How the data flows through the pipeline\n",
        "- How to set up your environment\n",
        "- How to run each stage safely\n",
        "- How to troubleshoot common issues\n",
        "- How to explain the project in interviews\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "435093f8",
      "metadata": {},
      "source": [
        "## 1. Big‑Picture Architecture (Plain English)\n",
        "\n",
        "Project A is a **data engineering pipeline** with three main layers:\n",
        "\n",
        "1. **Bronze (Raw Ingest)**\n",
        "   - Raw data is ingested from multiple sources (CRM, Snowflake, Redshift, FX JSON, Kafka simulations, etc.).\n",
        "   - Minimal cleaning; keep it close to source.\n",
        "\n",
        "2. **Silver (Clean + Standardized)**\n",
        "   - Data is cleaned, normalized, and deduplicated.\n",
        "   - Business rules and data contracts are applied.\n",
        "\n",
        "3. **Gold (Analytics‑Ready)**\n",
        "   - Dimensional models (facts/dimensions) and analytics tables are built.\n",
        "   - Optimized for reporting and downstream BI.\n",
        "\n",
        "**Storage Formats:**\n",
        "- Local runs can use **Parquet** or **Delta**.\n",
        "- Iceberg is supported for ACID + time travel.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0557d94",
      "metadata": {},
      "source": [
        "## 2. Pipeline Flow Diagram\n",
        "\n",
        "```mermaid\n",
        "flowchart LR\n",
        "    A[CRM] --> B[Bronze]\n",
        "    C[Snowflake] --> B[Bronze]\n",
        "    D[Redshift] --> B[Bronze]\n",
        "    E[FX JSON] --> B[Bronze]\n",
        "    F[Kafka Sim] --> B[Bronze]\n",
        "\n",
        "    B --> S[Silver]\n",
        "    S --> G[Gold]\n",
        "    G --> BI[Analytics / BI]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58c6fef9",
      "metadata": {},
      "source": [
        "## 3. System Architecture Diagram\n",
        "\n",
        "```mermaid\n",
        "flowchart TB\n",
        "    subgraph Orchestration\n",
        "        Airflow\n",
        "    end\n",
        "\n",
        "    subgraph Compute\n",
        "        Spark\n",
        "    end\n",
        "\n",
        "    subgraph Storage\n",
        "        S3_or_Local[(S3 or Local Files)]\n",
        "        Iceberg[(Iceberg / Delta / Parquet)]\n",
        "    end\n",
        "\n",
        "    subgraph Governance\n",
        "        DQ[Data Quality Gates]\n",
        "        Lineage[Lineage + Metrics]\n",
        "    end\n",
        "\n",
        "    Airflow --> Spark\n",
        "    Spark --> Iceberg\n",
        "    Iceberg --> S3_or_Local\n",
        "    Spark --> DQ\n",
        "    Spark --> Lineage\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11f135f7",
      "metadata": {},
      "source": [
        "## 4. Prerequisites\n",
        "\n",
        "Make sure you have:\n",
        "- **Python 3.10+**\n",
        "- **Java 11 or 17** (required by Spark)\n",
        "- **Spark 3.5.x** (bundled via PySpark)\n",
        "- Optional: **Docker** (for Airflow), **AWS CLI**\n",
        "\n",
        "If you are unsure, run these commands:\n",
        "```bash\n",
        "python --version\n",
        "java -version\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca2a757b",
      "metadata": {},
      "source": [
        "## 5. Setup: Virtual Environment + Dependencies\n",
        "\n",
        "We strongly recommend a virtual environment. This keeps dependencies clean.\n",
        "```bash\n",
        "python -m venv .venv\n",
        "source .venv/bin/activate\n",
        "\n",
        "pip install -U pip\n",
        "pip install -r requirements.txt\n",
        "pip install -r requirements-dev.txt\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48c9000d",
      "metadata": {},
      "source": [
        "## 6. Project Structure Overview\n",
        "\n",
        "Key folders:\n",
        "- `data/` → sample local data\n",
        "- `jobs/` → pipeline entrypoints\n",
        "- `src/project_a/` → core library + helpers\n",
        "- `config/` and `local/config/` → configuration files\n",
        "- `tests/` → test suite\n",
        "```bash\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a70f1421",
      "metadata": {},
      "source": [
        "# See the structure quickly\n",
        "ls\n",
        "ls jobs\n",
        "ls src/project_a\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9281b67f",
      "metadata": {},
      "source": [
        "## 7. Configuration (Local vs Prod)\n",
        "\n",
        "Local configuration lives here:\n",
        "- `local/config/local.yaml`\n",
        "\n",
        "This file controls:\n",
        "- Paths to data (`paths.bronze_root`, `paths.silver_root`, `paths.gold_root`)\n",
        "- Which storage format to use (Iceberg/Delta/Parquet)\n",
        "- Spark options\n",
        "\n",
        "If you want local runs to be less strict, you can set:\n",
        "\n",
        "```yaml\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4808d56",
      "metadata": {},
      "source": [
        "# local/config/local.yaml\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b599552",
      "metadata": {},
      "source": [
        "# This prevents failure when data is incomplete\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0630b28",
      "metadata": {},
      "source": [
        "# (good for demos and learning)\n",
        "dq:\n",
        "  fail_on_error: false\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2838a242",
      "metadata": {},
      "source": [
        "## 8. How Spark is Built (Important)\n",
        "\n",
        "This project automatically builds Spark with:\n",
        "- Delta Lake support (if `delta-spark` is installed)\n",
        "- Iceberg support (if enabled in config)\n",
        "\n",
        "Local runs are forced to use **PySpark’s bundled Spark** (not Homebrew Spark 4), to avoid version conflicts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a446b128",
      "metadata": {},
      "source": [
        "## 9. Run the Pipeline Step by Step\n",
        "\n",
        "### Step 1 — FX JSON → Bronze\n",
        "This loads FX JSON and writes normalized Bronze data.\n",
        "```bash\n",
        "python3 -m project_a.pipeline.run_pipeline   --job fx_json_to_bronze   --env local   --config local/config/local.yaml\n",
        "```\n",
        "\n",
        "### Step 2 — Snowflake/CRM/Redshift → Bronze\n",
        "These jobs simulate ingestion from different sources.\n",
        "```bash\n",
        "python3 -m project_a.pipeline.run_pipeline --job snowflake_to_bronze --env local --config local/config/local.yaml\n",
        "python3 -m project_a.pipeline.run_pipeline --job crm_to_bronze --env local --config local/config/local.yaml\n",
        "python3 -m project_a.pipeline.run_pipeline --job redshift_to_bronze --env local --config local/config/local.yaml\n",
        "```\n",
        "\n",
        "### Step 3 — Bronze → Silver\n",
        "Cleans, standardizes, deduplicates.\n",
        "```bash\n",
        "python3 -m project_a.pipeline.run_pipeline --job bronze_to_silver --env local --config local/config/local.yaml\n",
        "```\n",
        "\n",
        "### Step 4 — Silver → Gold\n",
        "Builds dimensional model + analytics tables.\n",
        "```bash\n",
        "python3 -m project_a.pipeline.run_pipeline --job silver_to_gold --env local --config local/config/local.yaml\n",
        "```\n",
        "\n",
        "### One‑Command Full Pipeline\n",
        "\n",
        "Use the helper script:\n",
        "```bash\n",
        "python3 run_complete_etl.py --config local/config/local.yaml --env local\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b230bd54",
      "metadata": {},
      "source": [
        "## 10. Screenshots of Outputs\n",
        "\n",
        "Below are sample output “screenshots” generated from your local run.\n",
        "![FX Bronze Output](/Users/kunal/IdeaProjects/Project_A/docs/interview/assets/fx_bronze_sample.svg)\n",
        "![Gold Fact Orders Output](/Users/kunal/IdeaProjects/Project_A/docs/interview/assets/gold_fact_orders_sample.svg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9096200a",
      "metadata": {},
      "source": [
        "## 11. Validate Outputs\n",
        "\n",
        "Check the Gold layer output:\n",
        "```bash\n",
        "ls data/gold\n",
        "```\n",
        "\n",
        "You can also load them in Spark and inspect:\n",
        "```bash\n",
        "from project_a.utils.spark_session import build_spark\n",
        "from project_a.utils.config import load_config_resolved\n",
        "\n",
        "config = load_config_resolved('local/config/local.yaml')\n",
        "spark = build_spark(config)\n",
        "\n",
        "df = spark.read.parquet('data/gold/fact_orders')\n",
        "print(df.count())\n",
        "df.show(5, False)\n",
        "\n",
        "spark.stop()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97dfa7af",
      "metadata": {},
      "source": [
        "## 12. Data Quality (DQ) Gates\n",
        "\n",
        "The pipeline supports DQ checks (not‑null, uniqueness, range checks, contracts).\n",
        "\n",
        "- For strict mode, set `dq.fail_on_error: true`.\n",
        "- For learning/demo, keep it `false`.\n",
        "\n",
        "DQ gate jobs:\n",
        "```bash\n",
        "python3 -m project_a.pipeline.run_pipeline --job dq_silver_gate --env local --config local/config/local.yaml\n",
        "python3 -m project_a.pipeline.run_pipeline --job dq_gold_gate --env local --config local/config/local.yaml\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0237073",
      "metadata": {},
      "source": [
        "## 13. Iceberg vs Delta vs Parquet (Beginner Explanation)\n",
        "\n",
        "- **Parquet** = file format only (fast, simple, but not ACID)\n",
        "- **Delta Lake** = Parquet + transaction log (ACID, time travel)\n",
        "- **Iceberg** = table format with ACID + catalog integration\n",
        "\n",
        "In this project:\n",
        "- Local runs can use Parquet or Iceberg.\n",
        "- If Iceberg is enabled, Silver/Gold use Iceberg tables.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b51718d",
      "metadata": {},
      "source": [
        "## 14. Airflow (Optional)\n",
        "\n",
        "You can orchestrate the pipeline using Airflow.\n",
        "\n",
        "If you want to run locally with Docker:\n",
        "```bash\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4872c03c",
      "metadata": {},
      "source": [
        "# Example (if docker-compose-airflow.yml exists)\n",
        "docker compose -f docker-compose-airflow.yml up -d\n",
        "```\n",
        "\n",
        "Open Airflow UI (default `http://localhost:8080`) and trigger the DAG.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d7e223c",
      "metadata": {},
      "source": [
        "## 15. dbt (Optional)\n",
        "\n",
        "If you want dbt models:\n",
        "```bash\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "223377b9",
      "metadata": {},
      "source": [
        "# Make sure dbt profiles.yml exists\n",
        "ls dbt\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "810a8cc7",
      "metadata": {},
      "source": [
        "# Example command\n",
        "cd dbt\n",
        "python -m dbt debug\n",
        "python -m dbt run\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dfceaee",
      "metadata": {},
      "source": [
        "## 16. Tests\n",
        "\n",
        "Run the test suite:\n",
        "```bash\n",
        "pytest -q\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79a5f5c0",
      "metadata": {},
      "source": [
        "## 17. Common Errors + Fixes\n",
        "\n",
        "**Error: Spark can’t parse timestamps**\n",
        "- Fix: use robust parsing patterns in the extractor (already patched).\n",
        "\n",
        "**Error: Table not found in Iceberg**\n",
        "- This is normal on first run. The code falls back to path reads.\n",
        "\n",
        "**Error: Missing DQGate import**\n",
        "- Fixed by compatibility wrapper in `project_a.dq.gate`.\n",
        "\n",
        "**Error: Spark 4.x mismatch**\n",
        "- Local Spark is forced to PySpark distribution (Spark 3.5).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9d4baf0",
      "metadata": {},
      "source": [
        "## 18. FAQ — Interview‑Style Q&A\n",
        "\n",
        "**Q1: Why do you use Bronze/Silver/Gold layers?**  \n",
        "A: It separates concerns. Bronze keeps raw truth, Silver standardizes, Gold optimizes for analytics.\n",
        "\n",
        "**Q2: How do you guarantee data quality?**  \n",
        "A: Contracts + DQ gates (not‑null, uniqueness, range checks) and fail‑fast in strict mode.\n",
        "\n",
        "**Q3: Why Iceberg instead of only Parquet?**  \n",
        "A: Iceberg adds ACID guarantees, time travel, schema evolution, and catalog integration.\n",
        "\n",
        "**Q4: How do you avoid duplicate data?**  \n",
        "A: Deduplication logic in Silver + merge/upsert strategies.\n",
        "\n",
        "**Q5: How does this scale in production?**  \n",
        "A: Spark on EMR with S3 storage, partitioned datasets, and orchestration via Airflow.\n",
        "\n",
        "**Q6: What happens if a job fails halfway?**  \n",
        "A: Jobs are idempotent, use overwrite/merge patterns, and produce run audits for recovery.\n",
        "\n",
        "**Q7: How do you handle schema drift?**  \n",
        "A: Explicit schemas + schema drift checks and contract validation.\n",
        "\n",
        "**Q8: How do you trace lineage?**  \n",
        "A: Lineage emitters record job inputs/outputs and metrics (OpenLineage‑style).\n",
        "\n",
        "**Q9: How would you test this pipeline?**  \n",
        "A: Unit tests for transforms + integration tests for stage‑level validation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbd04370",
      "metadata": {},
      "source": [
        "## 19. Interview‑Ready Summary\n",
        "\n",
        "When asked to explain this project, say:\n",
        "\n",
        "- It ingests multi‑source data into Bronze (raw).  \n",
        "- Standardizes to Silver with explicit schema and data contracts.  \n",
        "- Builds Gold fact/dim tables (analytics ready).  \n",
        "- Includes DQ gates, lineage logging, and Iceberg/Delta support.  \n",
        "- Designed for AWS EMR with S3 + Glue catalog.  \n",
        "\n",
        "This gives you a strong, production‑grade story.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
