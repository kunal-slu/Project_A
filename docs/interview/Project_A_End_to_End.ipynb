{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Project A \u2014 End\u2011to\u2011End Implementation Guide (Beginner Friendly)\n",
        "\n",
        "Welcome. This guide explains **how to run and understand the entire Project A data pipeline** from Bronze \u2192 Silver \u2192 Gold.\n",
        "It is written for beginners with **step\u2011by\u2011step instructions**, **simple explanations**, and **interview\u2011ready framing**.\n",
        "\n",
        "**What you will learn**\n",
        "- What a data pipeline is and why Bronze/Silver/Gold exists\n",
        "- How this project is organized\n",
        "- How to set up your environment\n",
        "- How to run every stage safely\n",
        "- How to validate outputs and troubleshoot issues\n",
        "- How to explain the project in interviews\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Big\u2011Picture Architecture (Plain English)\n",
        "\n",
        "Project A is a **data engineering pipeline** with three layers:\n",
        "\n",
        "1. **Bronze (Raw Ingest)**\n",
        "   - Data is ingested from sources like CRM, Snowflake, Redshift, FX JSON, and Kafka simulations.\n",
        "   - Very little transformation. The goal is to keep raw truth.\n",
        "\n",
        "2. **Silver (Clean + Standardized)**\n",
        "   - Data is cleaned and standardized.\n",
        "   - Column names and data types are consistent.\n",
        "   - Duplicates are removed.\n",
        "\n",
        "3. **Gold (Analytics\u2011Ready)**\n",
        "   - Business\u2011ready facts and dimensions are created.\n",
        "   - These are the tables used for dashboards and analytics.\n",
        "\n",
        "**Storage formats**\n",
        "- Local runs can use **Parquet** or **Delta**.\n",
        "- **Iceberg** is supported for ACID + time travel.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Pipeline Flow Diagram\n",
        "\n",
        "```mermaid\n",
        "flowchart LR\n",
        "    A[CRM] --> B[Bronze]\n",
        "    C[Snowflake] --> B[Bronze]\n",
        "    D[Redshift] --> B[Bronze]\n",
        "    E[FX JSON] --> B[Bronze]\n",
        "    F[Kafka Sim] --> B[Bronze]\n",
        "\n",
        "    B --> S[Silver]\n",
        "    S --> G[Gold]\n",
        "    G --> BI[Analytics / BI]\n",
        "```\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. System Architecture Diagram\n",
        "\n",
        "```mermaid\n",
        "flowchart TB\n",
        "    subgraph Orchestration\n",
        "        Airflow\n",
        "    end\n",
        "\n",
        "    subgraph Compute\n",
        "        Spark\n",
        "    end\n",
        "\n",
        "    subgraph Storage\n",
        "        S3_or_Local[(S3 or Local Files)]\n",
        "        Iceberg[(Iceberg / Delta / Parquet)]\n",
        "    end\n",
        "\n",
        "    subgraph Governance\n",
        "        DQ[Data Quality Gates]\n",
        "        Lineage[Lineage + Metrics]\n",
        "    end\n",
        "\n",
        "    Airflow --> Spark\n",
        "    Spark --> Iceberg\n",
        "    Iceberg --> S3_or_Local\n",
        "    Spark --> DQ\n",
        "    Spark --> Lineage\n",
        "```\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Key Concepts (Beginner Basics)\n",
        "\n",
        "**What is a pipeline?**\n",
        "A pipeline is a sequence of steps that takes raw data and turns it into clean, reliable, and analytics\u2011ready datasets.\n",
        "\n",
        "**What is Spark?**\n",
        "Spark is a distributed compute engine. It lets us process large data quickly using DataFrames.\n",
        "\n",
        "**What is a DataFrame?**\n",
        "A DataFrame is a table\u2011like structure (rows and columns). Spark DataFrames are lazy, meaning transformations are not run until an action like `count()` or `show()` is called.\n",
        "\n",
        "**Why not just use CSV?**\n",
        "CSV is simple but slow and unstructured. Parquet, Delta, and Iceberg are columnar formats that are fast, efficient, and better for analytics.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Prerequisites\n",
        "\n",
        "You need:\n",
        "- **Python 3.10+**\n",
        "- **Java 11 or 17** (required by Spark)\n",
        "- **Spark 3.5.x** (bundled via PySpark)\n",
        "- Optional: **Docker** (for Airflow)\n",
        "- Optional: **AWS CLI** (for AWS integration)\n",
        "\n",
        "Check versions:\n",
        "```bash\n",
        "python --version\n",
        "java -version\n",
        "```\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Setup: Virtual Environment + Dependencies\n",
        "\n",
        "Create a clean environment:\n",
        "```bash\n",
        "python -m venv .venv\n",
        "source .venv/bin/activate\n",
        "\n",
        "pip install -U pip\n",
        "pip install -r requirements.txt\n",
        "pip install -r requirements-dev.txt\n",
        "```\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Project Structure Overview\n",
        "\n",
        "Key folders:\n",
        "- `data/` \u2192 sample local data\n",
        "- `jobs/` \u2192 pipeline entrypoints\n",
        "- `src/project_a/` \u2192 core library + helpers\n",
        "- `config/` and `local/config/` \u2192 configuration\n",
        "- `tests/` \u2192 test suite\n",
        "\n",
        "Quick view:\n",
        "```bash\n",
        "ls\n",
        "ls jobs\n",
        "ls src/project_a\n",
        "```\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.1 Project Folder + File Map (What Does What)\n",
        "\n",
        "This section explains **what every major folder and key file does**. I am skipping cache folders (like `.pytest_cache`, `.ruff_cache`), `.DS_Store`, and virtual environments because they are not part of the pipeline.\n",
        "\n",
        "**Core folders**\n",
        "\n",
        "| Folder | Purpose | What you should say in interviews |\n",
        "|---|---|---|\n",
        "| `jobs/` | Entry points for pipeline jobs | \u201cThese are the runnable jobs for ingest, transform, and DQ.\u201d |\n",
        "| `jobs/ingest/` | Bronze ingestion jobs | \u201cThese pull data from sources into Bronze.\u201d |\n",
        "| `jobs/transform/` | Silver and Gold transforms | \u201cThese standardize data and build analytics tables.\u201d |\n",
        "| `jobs/dq/` | Data quality jobs | \u201cThese enforce contracts and validate datasets.\u201d |\n",
        "| `src/project_a/` | Core library | \u201cReusable framework code that powers the jobs.\u201d |\n",
        "| `src/project_a/core/` | Job execution + Spark context | \u201cDefines job lifecycle, config, and Spark session setup.\u201d |\n",
        "| `src/project_a/utils/` | Helpers | \u201cConfig loading, schemas, logging, watermarking.\u201d |\n",
        "| `src/project_a/extract/` | Readers/parsers | \u201cSource\u2011specific readers for CSV/JSON.\u201d |\n",
        "| `src/project_a/schemas/` | Explicit schemas | \u201cSchema definitions for ingestion and validation.\u201d |\n",
        "| `src/project_a/contracts/` | Data contracts | \u201cSchema + quality contract definitions.\u201d |\n",
        "| `src/project_a/dq/` | DQ engine | \u201cRules and checks used by DQ jobs.\u201d |\n",
        "| `src/project_a/iceberg_utils.py` | Iceberg helpers | \u201cCreates and writes Iceberg tables.\u201d |\n",
        "| `src/project_a/delta_utils.py` | Delta helpers | \u201cDelta\u2011format IO utilities.\u201d |\n",
        "| `src/project_a/pipeline/` | Orchestration helpers | \u201cRuns jobs in correct order.\u201d |\n",
        "| `aws/` | AWS deployment artifacts | \u201cGlue/EMR configs, Airflow DAGs, Terraform.\u201d |\n",
        "| `aws/dags/` | Airflow DAGs | \u201cSchedules batch pipeline runs.\u201d |\n",
        "| `config/` | Base config files | \u201cEnvironment\u2011agnostic configuration.\u201d |\n",
        "| `local/config/` | Local configs | \u201cLocal dev overrides and paths.\u201d |\n",
        "| `data/` | Sample data | \u201cLocal Bronze/Silver/Gold data.\u201d |\n",
        "| `docs/` | Documentation | \u201cGuides + interview docs.\u201d |\n",
        "| `tests/` | Unit tests | \u201cValidates transforms and utilities.\u201d |\n",
        "| `dbt/` | dbt models (optional) | \u201cAlternative modeling layer.\u201d |\n",
        "\n",
        "**Key files (what they do)**\n",
        "\n",
        "| File | Purpose | Typical use |\n",
        "|---|---|---|\n",
        "| `run_complete_etl.py` | Runs full pipeline | One\u2011command local run |\n",
        "| `src/project_a/pipeline/run_pipeline.py` | CLI entry for a single job | `--job bronze_to_silver` etc |\n",
        "| `jobs/ingest/fx_json_to_bronze.py` | FX JSON ingestion | Normalizes FX data into Bronze |\n",
        "| `jobs/ingest/snowflake_to_bronze.py` | Snowflake ingest | Loads Snowflake export data |\n",
        "| `jobs/ingest/crm_to_bronze.py` | CRM ingest | Loads CRM data into Bronze |\n",
        "| `jobs/ingest/redshift_to_bronze.py` | Redshift ingest | Loads Redshift data |\n",
        "| `jobs/transform/bronze_to_silver.py` | Clean + standardize | Cast types, dedupe, apply rules |\n",
        "| `jobs/transform/silver_to_gold.py` | Build facts/dims | Generates analytics tables |\n",
        "| `jobs/dq/dq_gate.py` | DQ job runner | Runs contract checks |\n",
        "| `local/config/local.yaml` | Local config | Paths + storage format + DQ settings |\n",
        "| `config/contracts/*.yaml` | Contracts | Column definitions + rules |\n",
        "| `src/project_a/utils/spark_session.py` | Spark builder | Configures Spark + Iceberg/Delta |\n",
        "| `src/project_a/utils/schema_validator.py` | Schema validator | Enforces schema consistency |\n",
        "| `src/project_a/utils/watermark_utils.py` | Watermarking | Tracks latest processed time |\n",
        "| `src/project_a/iceberg_utils.py` | Iceberg writes | Creates and writes Iceberg tables |\n",
        "| `aws/dags/daily_batch_pipeline_dag.py` | Airflow DAG | Schedules batch pipeline |\n",
        "| `docker-compose-airflow.yml` | Airflow local | Brings up Airflow via Docker |\n",
        "| `Dockerfile` | Container build | Builds Spark job image |\n",
        "\n",
        "**Where Bronze/Silver/Gold live**\n",
        "- `data/bronze/` \u2192 raw ingested datasets\n",
        "- `data/silver/` \u2192 cleaned standardized datasets\n",
        "- `data/gold/` \u2192 analytics\u2011ready outputs\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.2 Tools Used (Simple Explanations)\n",
        "\n",
        "This project uses a mix of **data engineering tools** and **supporting tools**. Here is what each does in beginner terms.\n",
        "\n",
        "**Core data tools**\n",
        "- **Apache Spark (via PySpark)**: The main engine that reads, transforms, and writes large datasets. It works on DataFrames (like tables).\n",
        "- **Parquet**: A fast columnar file format. Great for analytics but not ACID.\n",
        "- **Delta Lake**: Parquet + a transaction log. Adds ACID, time travel, and schema enforcement.\n",
        "- **Apache Iceberg**: A table format with ACID, time travel, and schema evolution; works with catalogs.\n",
        "\n",
        "**Orchestration and modeling**\n",
        "- **Apache Airflow**: Schedules and runs jobs in order. Adds retries, SLAs, and dependencies.\n",
        "- **dbt** (optional): SQL\u2011based transformation and documentation layer.\n",
        "\n",
        "**Data quality and governance**\n",
        "- **Contracts**: Machine\u2011readable schema rules that define required columns, types, and constraints.\n",
        "- **DQ Gates**: Checks like not\u2011null, uniqueness, and referential integrity.\n",
        "- **Schema evolution gate**: Blocks breaking schema changes between runs.\n",
        "- **Lineage + Metrics**: Tracks what tables were read/written and job runtime details.\n",
        "\n",
        "**Local developer tools**\n",
        "- **pytest**: Runs automated tests.\n",
        "- **Docker**: Runs Airflow locally.\n",
        "- **Run scripts**:\n",
        "  - `run_complete_etl.py` = run full pipeline\n",
        "  - `scripts/run_job.sh` = run one job\n",
        "  - `scripts/run_comprehensive_dq.sh` = run full DQ\n",
        "  - `scripts/check_bronze_data.py` = sanity check Bronze data\n",
        "  - `scripts/generate_schema_baselines.py` = build schema baselines\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.3 Data Types (Beginner Explanation)\n",
        "\n",
        "This pipeline uses common data types. You\u2019ll see these across Spark, Parquet, Iceberg, and contracts.\n",
        "\n",
        "- **String**: text values like IDs, emails, and statuses. Example: `\"CUST00001\"`\n",
        "- **Integer**: whole numbers. Example: `quantity = 4`\n",
        "- **Double / Float**: decimal numbers. Example: `amount_usd = 123.45`\n",
        "- **Decimal**: fixed precision decimals for money. Example: `Decimal(10,2)` to avoid rounding errors\n",
        "- **Date**: calendar date only. Example: `2025-01-31`\n",
        "- **Timestamp**: date + time. Example: `2025-01-31 12:05:33`\n",
        "- **Boolean**: true/false. Example: `is_active = true`\n",
        "\n",
        "**Important concepts**\n",
        "- **Nullability**: If a column can be missing. Some keys are marked non\u2011nullable.\n",
        "- **Primary Key (PK)**: A unique ID for each row (e.g., `order_id`).\n",
        "- **Foreign Key (FK)**: A column that references another table (e.g., `orders.customer_id`).\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.4 Data Sources (What Feeds Bronze)\n",
        "\n",
        "**CRM (CSV)**\n",
        "- Tables: Accounts, Contacts, Opportunities\n",
        "- Purpose: Sales and customer relationship data\n",
        "- Example fields: `account_id`, `contact_id`, `opportunity_id`, `amount`, `stage`\n",
        "\n",
        "**Snowflake (CSV export simulation)**\n",
        "- Tables: Customers, Orders, Products\n",
        "- Purpose: Core business transactions\n",
        "- Example fields: `customer_id`, `order_id`, `product_id`, `amount_usd`, `order_date`\n",
        " - Incremental batches: `data/bronze/snowflake/orders/daily/date=YYYY-MM-DD/`\n",
        "\n",
        "**Redshift (CSV export simulation)**\n",
        "- Table: Customer behavior events\n",
        "- Purpose: User browsing and engagement\n",
        "- Example fields: `session_id`, `event_type`, `event_date`, `time_spent_seconds`\n",
        " - Incremental batches: `data/bronze/redshift/behavior/daily/date=YYYY-MM-DD/`\n",
        "\n",
        "**FX JSON (JSON Lines)**\n",
        "- Table: FX rates\n",
        "- Purpose: Currency conversion and normalization\n",
        "- Example fields: `trade_date`, `base_ccy`, `counter_ccy`, `rate`\n",
        "\n",
        "**Kafka (CSV simulation)**\n",
        "- Table: Order event stream\n",
        "- Purpose: Streaming\u2011style events like `ORDER_CREATED`\n",
        "- Example fields: `event_id`, `order_id`, `customer_id`, `event_type`, `event_time`\n",
        " - Incremental batches: `data/bronze/kafka/events/daily/date=YYYY-MM-DD/`\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.5 Transformations (Detailed but Simple)\n",
        "\n",
        "**Bronze \u2192 Silver (Cleaning + Standardizing)**\n",
        "- **Schema enforcement**: Ensure required columns exist and types are correct\n",
        "- **Column standardization**: Normalize names (`order_date`, `amount_usd`, etc.)\n",
        "- **Deduplication**: Remove duplicate orders or customers\n",
        "- **Type casting**: Convert strings into dates, integers, decimals\n",
        "- **Fix bad records**: Filter or quarantine invalid rows\n",
        "- **Contracts check**: Fail fast if schema/PK constraints break\n",
        "\n",
        "**Silver \u2192 Gold (Analytics\u2011Ready)**\n",
        "- **Dimensions**: Build `dim_customer`, `dim_product`\n",
        "- **Fact table**: Build `fact_orders` from orders joined to customers/products\n",
        "- **Aggregations**: Create rollups or analytics views\n",
        "- **DQ validation**: Ensure key integrity and semantic rules\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.6 Major Work in This Project (What It Actually Does)\n",
        "\n",
        "- **Ingestion**: Pulls raw data into Bronze with explicit schemas\n",
        "- **Standardization**: Cleans, aligns, and deduplicates data in Silver\n",
        "- **Modeling**: Builds Gold facts and dimensions for analytics\n",
        "- **Quality**: Runs DQ gates, schema evolution checks, and reconciliation\n",
        "- **Observability**: Captures lineage and metrics for pipeline health\n",
        "- **Publishing**: Optional exports to Snowflake/Redshift\n",
        "- **Orchestration**: Airflow schedules jobs with retries and SLAs\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Configuration (Local vs Prod)\n",
        "\n",
        "Local configuration lives here:\n",
        "- `local/config/local.yaml`\n",
        "\n",
        "This file controls:\n",
        "- Paths to data (`paths.bronze_root`, `paths.silver_root`, `paths.gold_root`)\n",
        "- Storage format (Iceberg/Delta/Parquet)\n",
        "- Spark options\n",
        "- DQ strictness\n",
        "\n",
        "Example DQ relaxed mode for learning:\n",
        "```yaml\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# local/config/local.yaml\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# This prevents failure when data is incomplete\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# (good for demos and learning)\n",
        "dq:\n",
        "  fail_on_error: false\n",
        "```\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. How Spark is Built (Important)\n",
        "\n",
        "This project automatically builds Spark with:\n",
        "- Delta Lake support (if `delta-spark` is installed)\n",
        "- Iceberg support (if enabled in config)\n",
        "\n",
        "Local runs are forced to use **PySpark\u2019s bundled Spark** (not Homebrew Spark 4), to avoid version conflicts.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Run the Pipeline Step by Step\n",
        "\n",
        "### Step 1 \u2014 FX JSON \u2192 Bronze\n",
        "Loads FX JSON and writes normalized Bronze data.\n",
        "```bash\n",
        "python3 -m project_a.pipeline.run_pipeline   --job fx_json_to_bronze   --env local   --config local/config/local.yaml\n",
        "```\n",
        "\n",
        "### Step 2 \u2014 Snowflake/CRM/Redshift \u2192 Bronze\n",
        "Simulated ingestion from multiple sources.\n",
        "```bash\n",
        "python3 -m project_a.pipeline.run_pipeline --job snowflake_to_bronze --env local --config local/config/local.yaml\n",
        "python3 -m project_a.pipeline.run_pipeline --job crm_to_bronze --env local --config local/config/local.yaml\n",
        "python3 -m project_a.pipeline.run_pipeline --job redshift_to_bronze --env local --config local/config/local.yaml\n",
        "```\n",
        "\n",
        "### Step 3 \u2014 Bronze \u2192 Silver\n",
        "Cleans, standardizes, and deduplicates data.\n",
        "```bash\n",
        "python3 -m project_a.pipeline.run_pipeline --job bronze_to_silver --env local --config local/config/local.yaml\n",
        "```\n",
        "\n",
        "### Step 4 \u2014 Silver \u2192 Gold\n",
        "Builds dimensional model + analytics tables.\n",
        "```bash\n",
        "python3 -m project_a.pipeline.run_pipeline --job silver_to_gold --env local --config local/config/local.yaml\n",
        "```\n",
        "\n",
        "### One\u2011Command Full Pipeline\n",
        "Use the helper script:\n",
        "```bash\n",
        "python3 run_complete_etl.py --config local/config/local.yaml --env local\n",
        "```\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Screenshots of Outputs\n",
        "\n",
        "Below are sample output \u201cscreenshots\u201d generated from your local run.\n",
        "\n",
        "![FX Bronze Output](/Users/kunal/IdeaProjects/Project_A/docs/interview/assets/fx_bronze_sample.svg)\n",
        "![Gold Fact Orders Output](/Users/kunal/IdeaProjects/Project_A/docs/interview/assets/gold_fact_orders_sample.svg)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Validate Outputs\n",
        "\n",
        "Check Gold output directories:\n",
        "```bash\n",
        "ls data/gold\n",
        "```\n",
        "\n",
        "Load and inspect with Spark:\n",
        "```python\n",
        "from project_a.utils.spark_session import build_spark\n",
        "from project_a.utils.config import load_config_resolved\n",
        "\n",
        "config = load_config_resolved('local/config/local.yaml')\n",
        "spark = build_spark(config)\n",
        "\n",
        "fact = spark.read.parquet('data/gold/fact_orders')\n",
        "print(fact.count())\n",
        "fact.show(5, False)\n",
        "\n",
        "spark.stop()\n",
        "```\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Data Quality (DQ) Gates\n",
        "\n",
        "DQ checks validate:\n",
        "- Not\u2011null columns\n",
        "- Uniqueness\n",
        "- Range checks\n",
        "- Schema contracts\n",
        "\n",
        "For strict mode:\n",
        "```yaml\n",
        "dq:\n",
        "  fail_on_error: true\n",
        "```\n",
        "\n",
        "DQ jobs:\n",
        "```bash\n",
        "python3 -m project_a.pipeline.run_pipeline --job dq_silver_gate --env local --config local/config/local.yaml\n",
        "python3 -m project_a.pipeline.run_pipeline --job dq_gold_gate --env local --config local/config/local.yaml\n",
        "```\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Schema Evolution Gate (Breaking\u2011Change Blocker)\n",
        "\n",
        "This project enforces **schema evolution rules** on curated layers (Silver/Gold).\n",
        "It stores a baseline schema per table in:\n",
        "\n",
        "```\n",
        "artifacts/schema_baselines/<layer>/<table>.json\n",
        "```\n",
        "\n",
        "**What is considered breaking?**\n",
        "- Removing columns\n",
        "- Changing data types (unless safe widening)\n",
        "- Making a non\u2011nullable column nullable\n",
        "- Adding non\u2011nullable columns in backward\u2011compatible mode\n",
        "\n",
        "**Why this matters in interviews:**\n",
        "It prevents silent breaking changes from corrupting downstream analytics.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Profiling + Reconciliation (Real\u2011World DQ)\n",
        "\n",
        "This project now includes **sampling\u2011based profiling** and **row\u2011level reconciliation**.\n",
        "- **Profiling** computes null rates, distinct counts, min/max, and top values on samples.\n",
        "- **Reconciliation** checks key coverage between layers (e.g., orders_silver \u2192 fact_orders).\n",
        "\n",
        "Profiles are written to:\n",
        "```\n",
        "artifacts/dq/profiles/<layer>/<table>/profile_<timestamp>.json\n",
        "```\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16. Bronze Data Sanity Check (Realism Test)\n",
        "\n",
        "You can quickly validate that Bronze data \u201clooks real\u201d using:\n",
        "\n",
        "```bash\n",
        "python3 scripts/check_bronze_data.py --config local/config/local.yaml --sample-rows 20000\n",
        "```\n",
        "\n",
        "This checks:\n",
        "- IDs are mostly non\u2011null and unique\n",
        "- Dates are not far in the future\n",
        "- Numeric columns don\u2019t go negative\n",
        "\n",
        "The report is written to:\n",
        "```\n",
        "artifacts/dq/bronze_sanity_report.json\n",
        "```\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 17. Iceberg vs Delta vs Parquet (Beginner Explanation)\n",
        "\n",
        "- **Parquet** = file format only (fast, simple, not ACID)\n",
        "- **Delta Lake** = Parquet + transaction log (ACID, time travel)\n",
        "- **Iceberg** = table format with ACID + catalog integration\n",
        "\n",
        "In this project:\n",
        "- Local runs can use Parquet or Iceberg.\n",
        "- If Iceberg is enabled, Silver/Gold use Iceberg tables.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 18. Real Kafka Stack (Streaming)\n",
        "\n",
        "This project now includes a **real Kafka stack** via Docker Compose.\n",
        "\n",
        "Start Kafka:\n",
        "```bash\n",
        "docker compose up -d zookeeper kafka kafka-init\n",
        "```\n",
        "\n",
        "Produce events (from CSV seed):\n",
        "```bash\n",
        "python3 local/scripts/kafka_producer.py --config local/config/local.yaml\n",
        "```\n",
        "\n",
        "Run the streaming job:\n",
        "```bash\n",
        "scripts/run_kafka_stream.sh local/config/local.yaml local\n",
        "```\n",
        "\n",
        "This will write live Kafka events into Bronze.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 19. Airflow (Optional)\n",
        "\n",
        "Airflow can orchestrate the pipeline with scheduling and retries.\n",
        "\n",
        "Example Docker usage:\n",
        "```bash\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Example (if docker-compose-airflow.yml exists)\n",
        "docker compose -f docker-compose-airflow.yml up -d\n",
        "```\n",
        "\n",
        "Open Airflow UI (default `http://localhost:8080`) and trigger the DAG.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 20. dbt (Optional)\n",
        "\n",
        "If dbt is included:\n",
        "```bash\n",
        "ls dbt\n",
        "cd dbt\n",
        "python -m dbt debug\n",
        "python -m dbt run\n",
        "```\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 21. Tests\n",
        "\n",
        "Run tests:\n",
        "```bash\n",
        "pytest -q\n",
        "```\n",
        "\n",
        "If tests fail, check Spark version, config paths, and missing modules.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 22. Common Errors + Fixes\n",
        "\n",
        "**Error: Spark can\u2019t parse timestamps**\n",
        "- Fix: use robust parsing patterns in the extractor.\n",
        "\n",
        "**Error: Iceberg table not found**\n",
        "- Normal on first run. Code falls back to path reads.\n",
        "\n",
        "**Error: Missing DQGate import**\n",
        "- Fixed by compatibility wrapper in `project_a.dq.gate`.\n",
        "\n",
        "**Error: Spark 4.x mismatch**\n",
        "- Local Spark is forced to PySpark distribution (Spark 3.5).\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 23. FAQ \u2014 Interview\u2011Style Q&A\n",
        "\n",
        "**Q1: Why do you use Bronze/Silver/Gold?**\n",
        "A: It separates concerns. Bronze keeps raw truth, Silver standardizes, Gold optimizes for analytics.\n",
        "\n",
        "**Q2: How do you guarantee data quality?**\n",
        "A: Contracts + DQ gates (not\u2011null, uniqueness, range checks) and fail\u2011fast in strict mode.\n",
        "\n",
        "**Q3: Why Iceberg instead of only Parquet?**\n",
        "A: Iceberg adds ACID guarantees, time travel, schema evolution, and catalog integration.\n",
        "\n",
        "**Q4: How do you avoid duplicate data?**\n",
        "A: Deduplication logic in Silver + merge/upsert strategies.\n",
        "\n",
        "**Q5: How does this scale in production?**\n",
        "A: Spark on EMR with S3 storage, partitioned datasets, orchestration via Airflow.\n",
        "\n",
        "**Q6: What happens if a job fails halfway?**\n",
        "A: Jobs are idempotent, use overwrite/merge patterns, and produce run audits for recovery.\n",
        "\n",
        "**Q7: How do you handle schema drift?**\n",
        "A: Explicit schemas + schema drift checks and contract validation.\n",
        "\n",
        "**Q8: How do you trace lineage?**\n",
        "A: Lineage emitters record job inputs/outputs and metrics (OpenLineage\u2011style).\n",
        "\n",
        "**Q9: How would you test this pipeline?**\n",
        "A: Unit tests for transforms + integration tests for stage\u2011level validation.\n",
        "\n",
        "**Q10: What would you improve?**\n",
        "A: Add CI/CD, production secrets manager, and full end\u2011to\u2011end integration tests.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 24. Interview\u2011Ready Summary\n",
        "\n",
        "When asked to explain this project, say:\n",
        "\n",
        "- It ingests multi\u2011source data into Bronze (raw).\n",
        "- Standardizes to Silver with explicit schema and data contracts.\n",
        "- Builds Gold fact/dim tables (analytics\u2011ready).\n",
        "- Includes DQ gates, lineage logging, and Iceberg/Delta support.\n",
        "- Designed for AWS EMR with S3 + Glue catalog.\n",
        "\n",
        "This gives you a strong, production\u2011grade story.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}