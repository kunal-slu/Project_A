# Project A Refactoring Progress

## ‚úÖ Completed

### 1. Core Architecture
- ‚úÖ `src/project_a/core/config.py` - ProjectConfig class
- ‚úÖ `src/project_a/core/context.py` - JobContext class (Spark + Delta builder)
- ‚úÖ `src/project_a/core/base_job.py` - BaseJob abstract class
- ‚úÖ `src/project_a/core/__init__.py` - Exports

### 2. Unified I/O
- ‚úÖ `src/project_a/io/reader.py` - Unified readers (CSV, JSON, Delta, Parquet)
- ‚úÖ `src/project_a/io/writer.py` - Unified writers (Delta/Parquet with validation)
- ‚úÖ `src/project_a/io/__init__.py` - Exports

### 3. Schema Fixes
- ‚úÖ Updated `src/project_a/schemas/bronze_schemas.py` to match ACTUAL CSV headers:
  - CRM: Id, Name, AccountId, etc. (not normalized yet)
  - Snowflake: customer_id, first_name, last_name, total_amount, etc.
  - Redshift: behavior_id, customer_id, event_timestamp, conversion_value, etc.
  - Kafka: event_id, topic, partition, value (JSON string), etc.

### 4. Single Entry Point
- ‚úÖ `jobs/run_pipeline.py` - Unified entry point routing to all jobs

## üöß In Progress

### 5. Transform Modules
- ‚è≥ `src/project_a/transform/bronze_to_silver.py` - BronzeToSilverJob (needs refactoring)
- ‚è≥ `src/project_a/transform/silver_to_gold.py` - SilverToGoldJob (needs refactoring)

### 6. Ingestion Jobs
- ‚è≥ `src/project_a/ingest/snowflake_to_bronze.py`
- ‚è≥ `src/project_a/ingest/redshift_to_bronze.py`
- ‚è≥ `src/project_a/ingest/crm_to_bronze.py`
- ‚è≥ `src/project_a/ingest/fx_to_bronze.py`
- ‚è≥ `src/project_a/ingest/kafka_events_to_bronze.py`

### 7. Streaming
- ‚è≥ `src/project_a/streaming/kafka_producer.py` - Unified producer (local + MSK)
- ‚è≥ `src/project_a/streaming/kafka_consumer.py` - Unified streaming consumer

## üìã TODO

### 8. Airflow Cleanup
- [ ] Keep only 2 DAGs:
  - `project_a_daily_pipeline.py` (Bronze‚ÜíSilver‚ÜíGold)
  - `project_a_streaming_pipeline.py` (optional)
- [ ] Use Airflow Variables for all AWS IDs
- [ ] Remove hardcoded paths
- [ ] Fix import errors

### 9. Terraform Cleanup
- [ ] Remove unused modules
- [ ] Fix duplicate resources
- [ ] Ensure `terraform fmt` and `terraform validate` pass
- [ ] Match bucket names with config

### 10. File Cleanup
- [ ] Delete unused Python files
- [ ] Delete duplicate jobs
- [ ] Remove experimental utilities
- [ ] Archive old transform scripts

### 11. Tests
- [ ] Schema validation tests
- [ ] End-to-end Bronze‚ÜíSilver‚ÜíGold test
- [ ] Kafka producer/consumer connectivity test

## üéØ Next Steps

1. **Complete Transform Modules**: Refactor `bronze_to_silver.py` and `silver_to_gold.py` to use BaseJob
2. **Create Ingestion Jobs**: Implement all Bronze ingestion jobs using BaseJob
3. **Create Streaming Jobs**: Unified Kafka producer and consumer
4. **Clean Up Airflow**: Consolidate to 2 DAGs
5. **Clean Up Terraform**: Remove unused modules
6. **Delete Unused Files**: Remove all duplicate/unused code
7. **Add Tests**: Minimal test suite

## üìù Notes

- All schemas now match actual CSV headers (not normalized)
- Transformation logic will normalize Bronze ‚Üí Silver schemas
- Path resolution is handled by `JobContext.resolve_path()`
- Configuration is centralized in `ProjectConfig`
- All jobs inherit from `BaseJob` for consistency

