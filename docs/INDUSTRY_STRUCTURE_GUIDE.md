# Industry-Standard Data Engineering Project Structure

## Overview

This project follows industry best practices for data engineering platforms, with clear separation between:
- **Shared Library** (`src/`): Reusable, testable business logic
- **AWS/EMR Jobs** (`aws/`): Cloud-specific implementations
- **Local Development** (`local/`): Local testing and development

## Directory Structure

```
Project_A/
â”‚
â”œâ”€â”€ src/                          # ğŸ“š SHARED LIBRARY (No environment-specific code)
â”‚   â””â”€â”€ project_a/
â”‚       â”œâ”€â”€ extract/              # Data extraction utilities
â”‚       â”œâ”€â”€ transform/            # Core transformation logic
â”‚       â”‚   â”œâ”€â”€ bronze_to_silver.py
â”‚       â”‚   â””â”€â”€ silver_to_gold.py
â”‚       â”œâ”€â”€ dq/                   # Data quality framework
â”‚       â”œâ”€â”€ monitoring/            # Monitoring, lineage, metrics
â”‚       â”œâ”€â”€ utils/                # Utility functions
â”‚       â”‚   â”œâ”€â”€ spark_session.py
â”‚       â”‚   â”œâ”€â”€ logging.py
â”‚       â”‚   â””â”€â”€ config.py
â”‚       â””â”€â”€ schemas/              # Schema definitions
â”‚
â”œâ”€â”€ aws/                          # â˜ï¸ AWS/EMR-SPECIFIC CODE
â”‚   â”œâ”€â”€ jobs/                     # EMR job entry points
â”‚   â”‚   â”œâ”€â”€ transform/
â”‚   â”‚   â”‚   â”œâ”€â”€ bronze_to_silver.py  # EMR entry point
â”‚   â”‚   â”‚   â””â”€â”€ silver_to_gold.py    # EMR entry point
â”‚   â”‚   â”œâ”€â”€ ingest/               # Data ingestion jobs
â”‚   â”‚   â””â”€â”€ publish/              # Data publishing jobs
â”‚   â”œâ”€â”€ scripts/                  # AWS deployment scripts
â”‚   â”‚   â”œâ”€â”€ build_dependencies_zip.sh
â”‚   â”‚   â”œâ”€â”€ sync_artifacts_to_s3.sh
â”‚   â”‚   â””â”€â”€ run_emr_steps.sh
â”‚   â”œâ”€â”€ terraform/                # Infrastructure as Code
â”‚   â”œâ”€â”€ dags/                     # Airflow DAGs
â”‚   â””â”€â”€ config/                   # AWS-specific configs
â”‚       â””â”€â”€ dev.yaml
â”‚
â”œâ”€â”€ local/                        # ğŸ’» LOCAL DEVELOPMENT
â”‚   â”œâ”€â”€ jobs/                     # Local job runners
â”‚   â”‚   â”œâ”€â”€ transform/
â”‚   â”‚   â”‚   â”œâ”€â”€ bronze_to_silver.py  # Local entry point
â”‚   â”‚   â”‚   â””â”€â”€ silver_to_gold.py    # Local entry point
â”‚   â”‚   â””â”€â”€ run_etl_pipeline.py   # Main local ETL runner
â”‚   â”œâ”€â”€ scripts/                  # Local utility scripts
â”‚   â””â”€â”€ config/                   # Local configs
â”‚       â””â”€â”€ local.yaml
â”‚
â”œâ”€â”€ config/                       # ğŸ“‹ SHARED CONFIG TEMPLATES
â”‚   â”œâ”€â”€ dev.yaml                  # Development config template
â”‚   â”œâ”€â”€ prod.yaml                 # Production config template
â”‚   â””â”€â”€ schema_definitions/       # Schema definitions
â”‚
â”œâ”€â”€ tests/                        # ğŸ§ª TESTS
â”‚   â”œâ”€â”€ unit/                     # Unit tests
â”‚   â”œâ”€â”€ integration/              # Integration tests
â”‚   â””â”€â”€ fixtures/                 # Test fixtures
â”‚
â”œâ”€â”€ data/                         # ğŸ’¾ LOCAL DATA (gitignored)
â”‚   â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ silver/
â”‚   â””â”€â”€ gold/
â”‚
â”œâ”€â”€ docs/                         # ğŸ“– DOCUMENTATION
â”‚   â”œâ”€â”€ PROJECT_STRUCTURE.md
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ scripts/                       # ğŸ”§ PROJECT-LEVEL UTILITIES
â”‚   â””â”€â”€ cleanup_unwanted_files.sh
â”‚
â”œâ”€â”€ pyproject.toml                # Python package config
â”œâ”€â”€ requirements.txt              # Production dependencies
â”œâ”€â”€ requirements-dev.txt          # Development dependencies
â””â”€â”€ README.md                     # Project documentation
```

## Key Principles

### 1. Single Source of Truth
- **Business Logic**: Only in `src/project_a/transform/`
- **Entry Points**: Separate for local (`local/jobs/`) and AWS (`aws/jobs/`)
- **No Duplication**: Each transformation has ONE implementation

### 2. Clear Separation
- **Shared Library** (`src/`): Environment-agnostic, testable, reusable
- **AWS Jobs** (`aws/jobs/`): EMR-specific entry points, S3 paths, Delta Lake
- **Local Jobs** (`local/jobs/`): Local entry points, file paths, Parquet

### 3. Configuration-Driven
- All paths in config files
- No hardcoded values
- Environment-specific configs in respective folders

### 4. Industry Standards
- **Modular**: Clear module boundaries
- **Testable**: Shared library easily unit tested
- **Scalable**: Easy to add new transformations
- **Maintainable**: Clear structure, no duplication

## Usage

### Local Development

```bash
# Run full ETL pipeline locally
python local/jobs/run_etl_pipeline.py --config local/config/local.yaml

# Run individual steps
python local/jobs/transform/bronze_to_silver.py --config local/config/local.yaml
python local/jobs/transform/silver_to_gold.py --config local/config/local.yaml
```

### AWS/EMR Execution

```bash
# Upload jobs to S3
aws s3 sync aws/jobs/ s3://bucket/jobs/

# Submit EMR job
aws emr-serverless start-job-run \
  --application-id $EMR_APP_ID \
  --execution-role-arn $EMR_ROLE_ARN \
  --job-driver '{
    "sparkSubmit": {
      "entryPoint": "s3://bucket/jobs/transform/bronze_to_silver.py",
      "entryPointArguments": ["--env", "dev", "--config", "s3://bucket/config/dev.yaml"]
    }
  }'
```

## File Organization Rules

### âœ… DO:
- Put shared business logic in `src/project_a/transform/`
- Put AWS-specific code in `aws/`
- Put local-specific code in `local/`
- Use config files for all paths and settings
- Keep entry points thin (just import and call shared library)

### âŒ DON'T:
- Duplicate transformation logic
- Hardcode paths or environment checks
- Mix AWS and local code in same file
- Put business logic in entry points
- Create multiple implementations of same transformation

## Migration Status

- âœ… Created new directory structure
- âœ… Created local job entry points
- âœ… Created AWS job entry points
- â³ Moving shared transformation logic to `src/project_a/transform/`
- â³ Updating imports and paths
- â³ Removing duplicate files

