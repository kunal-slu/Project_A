"""
Standard ETL Pipeline with Proper Delta Lake Implementation
"""
import os
import sys
import pandas as pd
import yaml
from datetime import datetime
from typing import Dict, Any
import logging
from logging.config import fileConfig

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

from .delta_lake_standard import StandardDeltaLake, create_standard_delta_tables
from .gold_writer import GoldWriter

class StandardETLPipeline:
    """
    Standard ETL Pipeline with proper Delta Lake implementation
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.delta_path = config.get("delta_path", "data/lakehouse_delta_standard")
        
    def run_pipeline(self):
        """Run the complete standard ETL pipeline"""
        logger.info("ğŸš€ Starting Standard ETL Pipeline...")
        start_time = datetime.now()
        
        try:
            # Create standard Delta Lake tables
            self.create_standard_tables()
            
            # Process data layers
            self.process_bronze_layer()
            self.process_silver_layer()
            self.process_gold_layer()
            
            # Demonstrate time travel
            self.demonstrate_time_travel()
            
            duration = datetime.now() - start_time
            logger.info(f"ğŸ‰ Pipeline completed successfully in {duration}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Pipeline failed: {e}")
            raise
    
    def create_standard_tables(self):
        """Create or update standard Delta Lake tables"""
        logger.info("ğŸ—ï¸ Creating/updating standard Delta Lake tables...")
        
        # Check if tables already exist
        if os.path.exists(self.delta_path):
            logger.info("ğŸ“Š Delta Lake tables exist - adding new versions...")
            self._add_new_versions()
        else:
            logger.info("ğŸ“Š Creating new Delta Lake tables...")
            # Create standard tables
            create_standard_delta_tables()
        
        logger.info("âœ… Standard Delta Lake tables created/updated")
    
    def _add_new_versions(self):
        """Add new versions to existing Delta Lake tables"""
        logger.info("ğŸ”„ Adding new versions to existing tables...")
        
        # Get the latest version for each table
        layers = ["bronze", "silver", "gold"]
        
        for layer in layers:
            layer_path = os.path.join(self.delta_path, layer)
            if os.path.exists(layer_path):
                for table_dir in os.listdir(layer_path):
                    table_path = os.path.join(layer_path, table_dir)
                    if os.path.isdir(table_path):
                        delta_lake = StandardDeltaLake(table_path)
                        
                        # Get current version
                        table_info = delta_lake.get_table_info()
                        current_version = table_info['versions'] - 1
                        new_version = current_version + 1
                        
                        logger.info(f"ğŸ“Š Adding version {new_version} to {layer}.{table_dir}")
                        
                        # Generate new data for the version
                        if "customers" in table_dir:
                            new_data = self._generate_customer_data()
                        elif "orders" in table_dir:
                            new_data = self._generate_order_data()
                        elif "customer_analytics" in table_dir:
                            new_data = self._generate_customer_analytics()
                        elif "monthly_revenue" in table_dir:
                            new_data = self._generate_monthly_revenue()
                        else:
                            continue
                        
                        # Append new version
                        delta_lake.append_data(new_data, table_dir, new_version)
        
        logger.info("âœ… New versions added successfully")
    
    def _generate_customer_data(self):
        """Generate new customer data for versioning"""
        import pandas as pd
        import numpy as np
        from datetime import datetime, timedelta
        
        # Generate 1000 new customers
        n_customers = 1000
        customer_ids = range(10000, 10000 + n_customers)
        names = [f"Customer_v{datetime.now().strftime('%H%M')}_{i}" for i in customer_ids]
        emails = [f"customer_{i}@example.com" for i in customer_ids]
        created_dates = [datetime.now() - timedelta(days=np.random.randint(1, 365)) for _ in range(n_customers)]
        segments = np.random.choice(['Basic', 'Standard', 'Premium'], n_customers, p=[0.4, 0.4, 0.2])
        countries = np.random.choice(['USA', 'Canada', 'UK', 'Germany', 'France'], n_customers)
        
        return pd.DataFrame({
            'customer_id': customer_ids,
            'name': names,
            'email': emails,
            'created_date': created_dates,
            'segment': segments,
            'country': countries
        })
    
    def _generate_order_data(self):
        """Generate new order data for versioning"""
        import pandas as pd
        import numpy as np
        from datetime import datetime, timedelta
        
        # Generate 5000 new orders
        n_orders = 5000
        order_ids = range(50000, 50000 + n_orders)
        customer_ids = np.random.randint(10000, 11000, n_orders)
        product_ids = np.random.randint(1, 501, n_orders)
        order_dates = [datetime.now() - timedelta(days=np.random.randint(1, 30)) for _ in range(n_orders)]
        amounts = np.random.uniform(10, 1000, n_orders)
        statuses = np.random.choice(['completed', 'pending', 'cancelled'], n_orders, p=[0.8, 0.15, 0.05])
        currencies = ['USD'] * n_orders
        
        return pd.DataFrame({
            'order_id': order_ids,
            'customer_id': customer_ids,
            'product_id': product_ids,
            'order_date': order_dates,
            'amount': amounts,
            'status': statuses,
            'currency': currencies
        })
    
    def _generate_customer_analytics(self):
        """Generate new customer analytics data for versioning"""
        import pandas as pd
        import numpy as np
        
        segments = ['Basic', 'Standard', 'Premium']
        customer_counts = [np.random.randint(300, 400) for _ in segments]
        avg_lifetime_days = [np.random.uniform(150, 250) for _ in segments]
        
        return pd.DataFrame({
            'segment': segments,
            'customer_count': customer_counts,
            'avg_lifetime_days': avg_lifetime_days
        })
    
    def _generate_monthly_revenue(self):
        """Generate new monthly revenue data for versioning"""
        import pandas as pd
        import numpy as np
        
        months = [9, 10, 11, 12]  # Next 4 months
        total_revenues = [np.random.uniform(400000, 500000) for _ in months]
        
        return pd.DataFrame({
            'month': months,
            'total_revenue': total_revenues
        })
    
    def process_bronze_layer(self):
        """Process bronze layer data"""
        logger.info("ğŸ“Š Processing bronze layer...")
        
        bronze_path = os.path.join(self.delta_path, "bronze")
        
        # Process each table in bronze layer
        for table_dir in os.listdir(bronze_path):
            table_path = os.path.join(bronze_path, table_dir)
            if os.path.isdir(table_path):
                logger.info(f"  Processing bronze table: {table_dir}")
                
                # Get table info
                delta_lake = StandardDeltaLake(table_path)
                table_info = delta_lake.get_table_info()
                logger.info(f"    Versions: {table_info['versions']}, Records: {table_info['parquet_files']}")
        
        logger.info("âœ… Bronze layer processed")
    
    def process_silver_layer(self):
        """Process silver layer data"""
        logger.info("ğŸ“Š Processing silver layer...")
        
        silver_path = os.path.join(self.delta_path, "silver")
        
        # Process each table in silver layer
        for table_dir in os.listdir(silver_path):
            table_path = os.path.join(silver_path, table_dir)
            if os.path.isdir(table_path):
                logger.info(f"  Processing silver table: {table_dir}")
                
                # Get table info
                delta_lake = StandardDeltaLake(table_path)
                table_info = delta_lake.get_table_info()
                logger.info(f"    Versions: {table_info['versions']}, Records: {table_info['parquet_files']}")
        
        logger.info("âœ… Silver layer processed")
    
    def process_gold_layer(self):
        """Process gold layer data"""
        logger.info("ğŸ“Š Processing gold layer...")
        
        gold_path = os.path.join(self.delta_path, "gold")
        
        # Process each table in gold layer
        for table_dir in os.listdir(gold_path):
            table_path = os.path.join(gold_path, table_dir)
            if os.path.isdir(table_path):
                logger.info(f"  Processing gold table: {table_dir}")
                
                # Get table info
                delta_lake = StandardDeltaLake(table_path)
                table_info = delta_lake.get_table_info()
                logger.info(f"    Versions: {table_info['versions']}, Records: {table_info['parquet_files']}")
        
        logger.info("âœ… Gold layer processed")
    
    def demonstrate_time_travel(self):
        """Demonstrate Delta Lake time travel capabilities"""
        logger.info("ğŸ• Demonstrating Delta Lake time travel...")
        
        # Show version history for each table
        layers = ["bronze", "silver", "gold"]
        
        for layer in layers:
            layer_path = os.path.join(self.delta_path, layer)
            if os.path.exists(layer_path):
                logger.info(f"ğŸ“Š {layer.upper()} LAYER:")
                
                for table_dir in os.listdir(layer_path):
                    table_path = os.path.join(layer_path, table_dir)
                    if os.path.isdir(table_path):
                        delta_lake = StandardDeltaLake(table_path)
                        delta_lake.show_version_history()
                        print()
        
        logger.info("âœ… Time travel demonstration completed")


def main():
    """Main entry point for standard ETL pipeline"""
    try:
        # Load configuration
        config_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "..", "config", "local.yaml")
        if os.path.exists(config_path):
            with open(config_path, 'r') as f:
                config_data = yaml.safe_load(f)
        else:
            config_data = {
                "delta_path": "data/lakehouse_delta_standard",
                "cloud": "local",
                "app_name": "standard_etl_pipeline"
            }
        
        # Create and run pipeline
        pipeline = StandardETLPipeline(config_data)
        success = pipeline.run_pipeline()
        
        if success:
            print("\nğŸ‰ Standard ETL Pipeline completed successfully!")
            print("ğŸ† Delta Lake tables created with proper standards!")
            print(f"ğŸ“ Check {config_data['delta_path']} for Delta Lake tables")
        else:
            print("âŒ Standard ETL Pipeline failed!")
            sys.exit(1)
            
    except Exception as e:
        print(f"âŒ Pipeline execution failed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
