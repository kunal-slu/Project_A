"""
Production ETL Pipeline - Converts existing data to Delta Lake format
"""

import os
import sys
import pandas as pd
import numpy as np
import json
from datetime import datetime
from typing import Dict, Any
import logging

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ProductionETLPipeline:
    """
    Production ETL Pipeline that creates Delta Lake tables from existing data
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.delta_path = config.get("delta_path", "data/lakehouse_delta")
        
    def create_delta_lake_structure(self, table_name: str, data: pd.DataFrame, layer: str):
        """Create Delta Lake table structure with transaction logs."""
        logger.info(f"üèóÔ∏è Creating Delta Lake structure for {table_name} in {layer} layer...")
        
        # Create directory structure
        table_dir = f"{self.delta_path}/{layer}/{table_name}"
        delta_log_dir = f"{table_dir}/_delta_log"
        
        os.makedirs(delta_log_dir, exist_ok=True)
        
        # Write data as Parquet
        parquet_file = f"{table_dir}/part-00000-{table_name}.parquet"
        data.to_parquet(parquet_file, index=False)
        
        # Create transaction log
        transaction_log = {
            "protocol": {
                "minReaderVersion": 1,
                "minWriterVersion": 2
            },
            "metaData": {
                "id": f"{table_name}-{layer}-id",
                "format": {
                    "provider": "parquet",
                    "options": {}
                },
                "schemaString": self._generate_schema_string(data),
                "partitionColumns": [],
                "createdTime": int(datetime.now().timestamp() * 1000),
                "description": f"{layer.title()} layer {table_name} data"
            },
            "add": {
                "path": f"part-00000-{table_name}.parquet",
                "partitionValues": {},
                "size": os.path.getsize(parquet_file),
                "modificationTime": int(datetime.now().timestamp() * 1000),
                "dataChange": True,
                "stats": self._generate_stats(data)
            }
        }
        
        # Write transaction log
        log_file = f"{delta_log_dir}/00000000000000000000.json"
        with open(log_file, 'w') as f:
            json.dump(transaction_log, f, indent=2)
        
        # Create checksum
        checksum_file = f"{delta_log_dir}/00000000000000000000.crc"
        with open(checksum_file, 'w') as f:
            f.write("7a8b9c0d1e2f3a4b5c6d7e8f9a0b1c2d")  # Sample checksum
        
        logger.info(f"‚úÖ Created Delta Lake table: {table_dir}")
        logger.info(f"   - Data: {parquet_file}")
        logger.info(f"   - Transaction log: {log_file}")
        logger.info(f"   - Checksum: {checksum_file}")
        
        return table_dir
    
    def _generate_schema_string(self, data: pd.DataFrame) -> str:
        """Generate Delta Lake schema string from DataFrame."""
        schema = {"type": "struct", "fields": []}
        
        for column, dtype in data.dtypes.items():
            field = {
                "name": column,
                "type": self._pandas_to_spark_type(str(dtype)),
                "nullable": True
            }
            schema["fields"].append(field)
        
        return json.dumps(schema)
    
    def _pandas_to_spark_type(self, pandas_type: str) -> str:
        """Convert pandas dtype to Spark type."""
        type_mapping = {
            'int64': 'long',
            'int32': 'integer',
            'float64': 'double',
            'float32': 'float',
            'bool': 'boolean',
            'object': 'string',
            'datetime64[ns]': 'timestamp'
        }
        return type_mapping.get(pandas_type, 'string')
    
    def _generate_stats(self, data: pd.DataFrame) -> str:
        """Generate statistics for Delta Lake."""
        stats = {
            "numRecords": len(data),
            "minValues": {},
            "maxValues": {},
            "nullCount": {}
        }
        
        for column in data.columns:
            if data[column].dtype in ['int64', 'float64']:
                stats["minValues"][column] = float(data[column].min())
                stats["maxValues"][column] = float(data[column].max())
            stats["nullCount"][column] = int(data[column].isnull().sum())
        
        return json.dumps(stats)
    
    def process_existing_data(self):
        """Process existing Parquet data and convert to Delta Lake format."""
        logger.info("üîÑ Processing existing data to Delta Lake format...")
        
        # Check if layers exist, if not create them
        if not os.path.exists(self.delta_path):
            logger.info("üìä Creating initial Delta Lake structure...")
            self._create_initial_layers()
        
        # Process Bronze layer
        self._process_layer("bronze")
        
        # Process Silver layer
        self._process_layer("silver")
        
        # Process Gold layer
        self._process_layer("gold")
        
        logger.info("‚úÖ All layers processed successfully!")
        
        # Log final counts for verification
        self._log_final_counts()
    
    def _create_initial_layers(self):
        """Create initial Delta Lake layers with sample data"""
        logger.info("üèóÔ∏è Creating initial Delta Lake layers...")
        
        # Create directory structure
        os.makedirs(self.delta_path, exist_ok=True)
        
        # Create Bronze layer data
        bronze_path = os.path.join(self.delta_path, "bronze")
        os.makedirs(bronze_path, exist_ok=True)
        
        # Create Silver layer data
        silver_path = os.path.join(self.delta_path, "silver")
        os.makedirs(silver_path, exist_ok=True)
        
        # Create Gold layer data
        gold_path = os.path.join(self.delta_path, "gold")
        os.makedirs(gold_path, exist_ok=True)
        
        # Create sample data for each layer
        self._create_sample_bronze_data()
        self._create_sample_silver_data()
        self._create_sample_gold_data()
        
        logger.info("‚úÖ Initial Delta Lake structure created")
    
    def _create_sample_bronze_data(self):
        """Create sample bronze layer data"""
        # Create sample customers data
        customers_data = pd.DataFrame({
            'customer_id': range(1, 1001),
            'name': [f'Customer_{i}' for i in range(1, 1001)],
            'email': [f'customer{i}@example.com' for i in range(1, 1001)],
            'created_date': pd.date_range('2024-01-01', periods=1000, freq='1H'),
            'segment': ['Basic'] * 400 + ['Standard'] * 400 + ['Premium'] * 200,
            'country': ['USA'] * 600 + ['Canada'] * 200 + ['UK'] * 200
        })
        
        # Create sample orders data
        orders_data = pd.DataFrame({
            'order_id': range(1, 5001),
            'customer_id': np.random.randint(1, 1001, 5000),
            'product_id': np.random.randint(1, 501, 5000),
            'order_date': pd.date_range('2024-01-01', periods=5000, freq='1H'),
            'amount': np.random.uniform(10, 1000, 5000),
            'status': np.random.choice(['completed', 'pending', 'cancelled'], 5000),
            'currency': ['USD'] * 5000
        })
        
        # Save as parquet files
        customers_data.to_parquet(f"{self.delta_path}/bronze/customers.parquet", index=False)
        orders_data.to_parquet(f"{self.delta_path}/bronze/orders.parquet", index=False)
        
        logger.info(f"‚úÖ Created bronze layer with {len(customers_data)} customers and {len(orders_data)} orders")
    
    def _create_sample_silver_data(self):
        """Create sample silver layer data"""
        # Read bronze data and add transformations
        customers_df = pd.read_parquet(f"{self.delta_path}/bronze/customers.parquet")
        orders_df = pd.read_parquet(f"{self.delta_path}/bronze/orders.parquet")
        
        # Add computed columns for silver layer
        customers_df['customer_lifetime_days'] = (datetime.now() - customers_df['created_date']).dt.days
        customers_df['is_premium'] = customers_df['segment'] == 'Premium'
        
        orders_df['order_month'] = orders_df['order_date'].dt.month
        orders_df['amount_category'] = pd.cut(orders_df['amount'], 
                                            bins=[0, 100, 500, float('inf')], 
                                            labels=['Small', 'Medium', 'Large'])
        
        # Save as parquet files
        customers_df.to_parquet(f"{self.delta_path}/silver/customers.parquet", index=False)
        orders_df.to_parquet(f"{self.delta_path}/silver/orders.parquet", index=False)
        
        logger.info(f"‚úÖ Created silver layer with transformed data")
    
    def _create_sample_gold_data(self):
        """Create sample gold layer data"""
        # Read silver data for aggregation
        customers_df = pd.read_parquet(f"{self.delta_path}/silver/customers.parquet")
        orders_df = pd.read_parquet(f"{self.delta_path}/silver/orders.parquet")
        
        # Customer analytics
        customer_analytics = customers_df.groupby('segment').agg({
            'customer_id': 'count',
            'customer_lifetime_days': 'mean'
        }).reset_index()
        customer_analytics.columns = ['segment', 'customer_count', 'avg_lifetime_days']
        
        # Order analytics
        order_analytics = orders_df.groupby('amount_category').agg({
            'order_id': 'count',
            'amount': ['sum', 'mean']
        }).reset_index()
        order_analytics.columns = ['amount_category', 'order_count', 'total_amount', 'avg_amount']
        
        # Monthly revenue
        monthly_revenue = orders_df.groupby(orders_df['order_date'].dt.month).agg({
            'amount': 'sum'
        }).reset_index()
        monthly_revenue.columns = ['month', 'total_revenue']
        
        # Save as parquet files
        customer_analytics.to_parquet(f"{self.delta_path}/gold/customer_analytics.parquet", index=False)
        order_analytics.to_parquet(f"{self.delta_path}/gold/order_analytics.parquet", index=False)
        monthly_revenue.to_parquet(f"{self.delta_path}/gold/monthly_revenue.parquet", index=False)
        
        logger.info(f"‚úÖ Created gold layer with analytics data")
    
    def _process_layer(self, layer: str):
        """Process a specific layer."""
        logger.info(f"üìä Processing {layer} layer...")
        
        layer_path = f"data/lakehouse_delta/{layer}"
        if not os.path.exists(layer_path):
            logger.warning(f"‚ö†Ô∏è Layer path not found: {layer_path}")
            return
        
        # Find Parquet files in layer
        parquet_files = [f for f in os.listdir(layer_path) if f.endswith('.parquet')]
        
        for parquet_file in parquet_files:
            try:
                # Read existing Parquet file
                file_path = f"{layer_path}/{parquet_file}"
                data = pd.read_parquet(file_path)
                
                # Extract table name
                table_name = parquet_file.replace('.parquet', '').replace(f'_{layer}', '')
                
                # Create Delta Lake structure
                self.create_delta_lake_structure(table_name, data, layer)
                
                logger.info(f"‚úÖ Processed {table_name} ({len(data)} records)")
                
            except Exception as e:
                logger.error(f"‚ùå Failed to process {parquet_file}: {e}")
    
    def demonstrate_time_travel(self):
        """Demonstrate Delta Lake time travel capabilities."""
        logger.info("üïê Demonstrating Delta Lake time travel...")
        
        # Create multiple versions for bronze customers table
        customers_path = f"{self.delta_path}/bronze/customers"
        if os.path.exists(customers_path):
            self._create_table_version(customers_path, "customers", "bronze")
            
            # Show version history
            self._show_version_history(customers_path)
        
        # Create multiple versions for silver layer tables
        silver_path = f"{self.delta_path}/silver"
        if os.path.exists(silver_path):
            for table_dir in os.listdir(silver_path):
                table_path = f"{silver_path}/{table_dir}"
                if os.path.isdir(table_path) and table_dir != "_delta_log":
                    logger.info(f"üîÑ Creating additional versions for silver table: {table_dir}")
                    self._create_silver_table_version(table_path, table_dir)
                    self._show_version_history(table_path)
        
        # Create multiple versions for gold layer tables
        gold_path = f"{self.delta_path}/gold"
        if os.path.exists(gold_path):
            for table_dir in os.listdir(gold_path):
                table_path = f"{gold_path}/{table_dir}"
                if os.path.isdir(table_path) and table_dir != "_delta_log":
                    logger.info(f"üîÑ Creating additional versions for gold table: {table_dir}")
                    self._create_gold_table_version(table_path, table_dir)
                    self._show_version_history(table_path)
    
    def _create_table_version(self, table_path: str, table_name: str, layer: str):
        """Create multiple versions of a table."""
        logger.info(f"üìù Creating multiple versions of {table_name}...")
        
        # Read existing data
        data_file = f"{table_path}/part-00000-{table_name}.parquet"
        if os.path.exists(data_file):
            data = pd.read_parquet(data_file)
            
            # Create 3 versions
            for version in range(1, 4):
                # Modify data (add new records)
                new_data = data.copy()
                new_records = data.head(25 * version).copy()  # Add more records each version
                new_records = new_records.reset_index(drop=True)  # Reset index to avoid length mismatch
                
                # Create new customer IDs as a list with correct length
                num_new_records = len(new_records)
                new_customer_ids = list(range(len(data) + (version-1)*25 + 1, len(data) + (version-1)*25 + 1 + num_new_records))
                new_names = [f'Customer_v{version}_{i}' for i in new_customer_ids]
                
                # Assign new values with correct length
                new_records = new_records.copy()
                new_records['customer_id'] = new_customer_ids
                new_records['name'] = new_names
                
                updated_data = pd.concat([data, new_records], ignore_index=True)
                
                # Write new version
                new_parquet_file = f"{table_path}/part-0000{version}-{table_name}-v{version+1}.parquet"
                updated_data.to_parquet(new_parquet_file, index=False)
                
                # Create transaction log
                delta_log_dir = f"{table_path}/_delta_log"
                transaction_log = {
                    "protocol": {
                        "minReaderVersion": 1,
                        "minWriterVersion": 2
                    },
                    "add": {
                        "path": f"part-0000{version}-{table_name}-v{version+1}.parquet",
                        "partitionValues": {},
                        "size": os.path.getsize(new_parquet_file),
                        "modificationTime": int(datetime.now().timestamp() * 1000) + version,
                        "dataChange": True,
                        "stats": self._generate_stats(updated_data)
                    }
                }
                
                # Write transaction log
                log_file = f"{delta_log_dir}/0000000000000000000{version}.json"
                with open(log_file, 'w') as f:
                    json.dump(transaction_log, f, indent=2)
                
                # Create checksum
                checksum_file = f"{delta_log_dir}/0000000000000000000{version}.crc"
                with open(checksum_file, 'w') as f:
                    f.write(f"8c9d0e1f2a3b4c5d6e7f8a9b0c1d2e3{version}")
                
                logger.info(f"‚úÖ Created version {version} of {table_name} ({len(updated_data)} records)")
    
    def _create_silver_table_version(self, table_path: str, table_name: str):
        """Create multiple versions for silver layer tables."""
        logger.info(f"üìù Creating multiple versions for silver table: {table_name}...")
        
        # Read existing data
        data_file = f"{table_path}/part-00000-{table_name}.parquet"
        if os.path.exists(data_file):
            data = pd.read_parquet(data_file)
            
            # Create 2 versions
            for version in range(1, 3):
                # Modify data based on table type and actual columns
                new_data = data.copy()
                
                if "customers" in table_name.lower():
                    # For customers, update segment and add new customers
                    segment_map = {
                        'Basic': 'Premium', 
                        'Premium': 'Enterprise', 
                        'Enterprise': 'Basic'
                    }
                    new_data['segment'] = new_data['segment'].map(segment_map).fillna('Premium')
                    # Add new customers
                    new_customers = data.head(15 * version).copy()
                    new_customers = new_customers.reset_index(drop=True)  # Reset index to avoid length mismatch
                    
                    # Create new customer IDs as a list with correct length
                    num_new_customers = len(new_customers)
                    new_customer_ids = list(range(len(data) + (version-1)*15 + 1, len(data) + (version-1)*15 + 1 + num_new_customers))
                    new_names = [f'Silver_v{version}_{i}' for i in new_customer_ids]
                    
                    # Assign new values with correct length
                    new_customers = new_customers.copy()
                    new_customers['customer_id'] = new_customer_ids
                    new_customers['name'] = new_names
                    new_data = pd.concat([data, new_customers], ignore_index=True)
                elif "orders" in table_name.lower():
                    # For orders, update amounts and status
                    if 'total_amount' in new_data.columns:
                        new_data['total_amount'] = new_data['total_amount'] * (1.08 ** version)  # Increasing multiplier
                    elif 'amount' in new_data.columns:
                        new_data['amount'] = new_data['amount'] * (1.08 ** version)  # Increasing multiplier
                    new_data['status'] = new_data['status'].map({
                        'Completed': 'Processing',
                        'Processing': 'Completed',
                        'Cancelled': 'Completed',
                        'COMPLETED': 'PROCESSING',
                        'PROCESSING': 'COMPLETED',
                        'PENDING': 'COMPLETED'
                    }).fillna('Completed')
                
                # Write new version
                new_parquet_file = f"{table_path}/part-0000{version}-{table_name}-v{version+1}.parquet"
                new_data.to_parquet(new_parquet_file, index=False)
                
                # Create transaction log
                delta_log_dir = f"{table_path}/_delta_log"
                transaction_log = {
                    "protocol": {
                        "minReaderVersion": 1,
                        "minWriterVersion": 2
                    },
                    "add": {
                        "path": f"part-0000{version}-{table_name}-v{version+1}.parquet",
                        "partitionValues": {},
                        "size": os.path.getsize(new_parquet_file),
                        "modificationTime": int(datetime.now().timestamp() * 1000) + version,
                        "dataChange": True,
                        "stats": self._generate_stats(new_data)
                    }
                }
                
                # Write transaction log
                log_file = f"{delta_log_dir}/0000000000000000000{version}.json"
                with open(log_file, 'w') as f:
                    json.dump(transaction_log, f, indent=2)
                
                # Create checksum
                checksum_file = f"{delta_log_dir}/0000000000000000000{version}.crc"
                with open(checksum_file, 'w') as f:
                    f.write(f"8c9d0e1f2a3b4c5d6e7f8a9b0c1d2e3{version}")
                
                logger.info(f"‚úÖ Created version {version} of silver {table_name} ({len(new_data)} records)")
    
    def _create_gold_table_version(self, table_path: str, table_name: str):
        """Create multiple versions for gold layer tables."""
        logger.info(f"üìù Creating multiple versions for gold table: {table_name}...")
        
        # Read existing data
        data_file = f"{table_path}/part-00000-{table_name}.parquet"
        if os.path.exists(data_file):
            data = pd.read_parquet(data_file)
            
            # Create 2 versions
            for version in range(1, 3):
                # Modify data based on table type and actual columns
                new_data = data.copy()
                
                if "customer_analytics" in table_name.lower():
                    # For customer analytics, increase customer counts
                    new_data['customer_count'] = new_data['customer_count'] + (10 * version)
                    new_data['avg_lifetime_days'] = new_data['avg_lifetime_days'] + (5 * version)
                elif "monthly_revenue" in table_name.lower():
                    # For monthly revenue, increase revenue and add new months
                    new_data['total_revenue'] = new_data['total_revenue'] * (1.15 ** version)  # Increasing multiplier
                    new_data['month'] = new_data['month'] + version  # Next months
                elif "order_analytics" in table_name.lower():
                    # For order analytics, increase counts and amounts
                    new_data['order_count'] = new_data['order_count'] + (5 * version)
                    new_data['total_amount'] = new_data['total_amount'] * (1.1 ** version)  # Increasing multiplier
                    new_data['avg_amount'] = new_data['avg_amount'] * (1.05 ** version)  # Increasing multiplier
                else:
                    # For other tables, just duplicate with modified IDs
                    if 'id' in new_data.columns:
                        new_data['id'] = new_data['id'] + (1000 * version)
                
                # Write new version
                new_parquet_file = f"{table_path}/part-0000{version}-{table_name}-v{version+1}.parquet"
                new_data.to_parquet(new_parquet_file, index=False)
                
                # Create transaction log
                delta_log_dir = f"{table_path}/_delta_log"
                transaction_log = {
                    "protocol": {
                        "minReaderVersion": 1,
                        "minWriterVersion": 2
                    },
                    "add": {
                        "path": f"part-0000{version}-{table_name}-v{version+1}.parquet",
                        "partitionValues": {},
                        "size": os.path.getsize(new_parquet_file),
                        "modificationTime": int(datetime.now().timestamp() * 1000) + version,
                        "dataChange": True,
                        "stats": self._generate_stats(new_data)
                    }
                }
                
                # Write transaction log
                log_file = f"{delta_log_dir}/0000000000000000000{version}.json"
                with open(log_file, 'w') as f:
                    json.dump(transaction_log, f, indent=2)
                
                # Create checksum
                checksum_file = f"{delta_log_dir}/0000000000000000000{version}.crc"
                with open(checksum_file, 'w') as f:
                    f.write(f"8c9d0e1f2a3b4c5d6e7f8a9b0c1d2e3{version}")
                
                logger.info(f"‚úÖ Created version {version} of gold {table_name} ({len(new_data)} records)")
    
    def _show_version_history(self, table_path: str):
        """Show version history of a Delta Lake table."""
        logger.info(f"üìä Version history for {table_path}:")
        
        delta_log_dir = f"{table_path}/_delta_log"
        if os.path.exists(delta_log_dir):
            log_files = [f for f in os.listdir(delta_log_dir) if f.endswith('.json')]
            
            for i, log_file in enumerate(sorted(log_files)):
                log_path = f"{delta_log_dir}/{log_file}"
                with open(log_path, 'r') as f:
                    log_data = json.load(f)
                
                if 'metaData' in log_data:
                    # Initial version
                    logger.info(f"   Version {i}: Initial load")
                    logger.info(f"      Records: {json.loads(log_data['add']['stats'])['numRecords']}")
                else:
                    # Update version
                    logger.info(f"   Version {i}: Update")
                    logger.info(f"      Records: {json.loads(log_data['add']['stats'])['numRecords']}")
    
    def run_pipeline(self):
        """Run the complete production pipeline."""
        logger.info("üöÄ Starting Production ETL Pipeline...")
        start_time = datetime.now()
        
        try:
            # Process existing data
            self.process_existing_data()
            
            # Demonstrate time travel
            self.demonstrate_time_travel()
            
            duration = datetime.now() - start_time
            logger.info(f"üéâ Pipeline completed successfully in {duration}")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Pipeline failed: {e}")
            raise
    
    def _log_final_counts(self):
        """Log final row counts for verification"""
        logger.info("üìä Final Delta Lake table counts:")
        
        layers = ["bronze", "silver", "gold"]
        for layer in layers:
            layer_path = os.path.join(self.delta_path, layer)
            if os.path.exists(layer_path):
                for table_dir in os.listdir(layer_path):
                    table_path = os.path.join(layer_path, table_dir)
                    if os.path.isdir(table_path):
                        parquet_files = [f for f in os.listdir(table_path) if f.endswith('.parquet')]
                        if parquet_files:
                            latest_file = max(parquet_files)
                            file_path = os.path.join(table_path, latest_file)
                            try:
                                df = pd.read_parquet(file_path)
                                logger.info(f"  üìÑ {layer}.{table_dir}: {len(df):,} rows")
                            except Exception as e:
                                logger.warning(f"  ‚ö†Ô∏è {layer}.{table_dir}: Could not read - {e}")
        
        logger.info("‚úÖ Final counts logged")

def main():
    """Main entry point."""
    config = {
        "base_path": "data/lakehouse",
        "delta_path": "data/lakehouse_delta"
    }
    
    pipeline = ProductionETLPipeline(config)
    success = pipeline.run_pipeline()
    
    if success:
        print("\nüéâ Production ETL Pipeline completed successfully!")
        print("üèÜ Delta Lake tables created with time travel capabilities!")
        print(f"üìÅ Check {config['delta_path']} for Delta Lake tables")
    else:
        print("‚ùå Production ETL Pipeline failed!")
        sys.exit(1)

if __name__ == "__main__":
    main()
