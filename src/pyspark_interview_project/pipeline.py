import logging
import os
import time
from typing import Dict

from pyspark.sql import SparkSession

from .extract import (extract_customers, extract_products, extract_orders_json,
                     extract_returns, extract_exchange_rates, extract_inventory_snapshots)
from .transform import (enrich_customers, enrich_products, clean_orders,
                       build_fact_orders, sql_vs_dsl_demo, udf_examples, window_functions_demo)
from .utils.spark_session import build_spark
from .validate import ValidateOutput
from .performance_optimizer import create_performance_optimizer
from .io_utils import write_delta
from .logging_config import log_metric, log_pipeline_event
from .metrics import create_metrics
from .dq.runner import run_dq, DQResult

logger = logging.getLogger(__name__)


def run_pipeline(spark: SparkSession, cfg: Dict, run_id: str):
    """Main pipeline execution function with enhanced observability"""
    
    # Initialize metrics
    metrics = create_metrics(cfg, run_id)
    
    # Log pipeline start
    log_pipeline_event(logger, "pipeline_execution_started", "main", run_id)
    
    # Create output directories
    os.makedirs(os.path.dirname(cfg.get("output", {}).get("parquet_path", "data/output_data/final.parquet")), exist_ok=True)
    os.makedirs(os.path.dirname(cfg.get("output", {}).get("delta_path", "data/output_data/final_delta")), exist_ok=True)
    
    # Lakehouse layers
    bronze_base = cfg.get("output", {}).get("bronze_path", "data/lakehouse/bronze")
    silver_base = cfg.get("output", {}).get("silver_path", "data/lakehouse/silver")
    gold_base = cfg.get("output", {}).get("gold_path", "data/lakehouse/gold")
    
    for p in (bronze_base, silver_base, gold_base):
        os.makedirs(p, exist_ok=True)
    os.makedirs("data/metrics", exist_ok=True)

    pipeline_start_time = time.time()

    # Extract data with metrics tracking
    log_pipeline_event(logger, "extraction_started", "extract", run_id)
    extract_start = time.time()
    
    customers = extract_customers(spark, cfg.get("input", {}).get("customer_path", "data/input_data/customers.csv"))
    products = extract_products(spark, cfg.get("input", {}).get("product_path", "data/input_data/products.csv"))
    orders = extract_orders_json(spark, cfg.get("input", {}).get("orders_path", "data/input_data/orders.json"))
    returns = extract_returns(spark, cfg.get("input", {}).get("returns_path", "data/input_data/returns.json"))
    rates = extract_exchange_rates(spark, cfg.get("input", {}).get("exchange_rates_path", "data/input_data/exchange_rates.csv"))
    inventory = extract_inventory_snapshots(spark, cfg.get("input", {}).get("inventory_path", "data/input_data/inventory_snapshots.csv"))
    
    # Log extraction metrics
    extract_duration = time.time() - extract_start
    log_metric(logger, "pipeline.extract.duration.seconds", extract_duration, run_id)
    metrics.timing("pipeline.extract.duration", extract_duration * 1000, {"stage": "extract"})
    
    # Log row counts for each dataset
    datasets = {
        "customers": customers,
        "products": products, 
        "orders": orders,
        "returns": returns,
        "rates": rates,
        "inventory": inventory
    }
    
    for name, df in datasets.items():
        try:
            count = df.count()
            log_metric(logger, f"pipeline.extract.{name}.row_count", count, run_id)
            metrics.pipeline_metric("extract", f"{name}_rows", count)
        except Exception as e:
            logger.warning(f"Could not count rows for {name}: {e}")
    
    log_pipeline_event(logger, "extraction_completed", "extract", run_id, 
                      duration_seconds=extract_duration)

    # Initialize performance optimizer
    log_pipeline_event(logger, "optimization_started", "optimize", run_id)
    optimize_start = time.time()
    
    performance_optimizer = create_performance_optimizer(spark, cfg)

    # Apply performance optimizations to datasets
    datasets_to_optimize = {
        "customers": customers,
        "products": products,
        "orders": orders,
        "returns_raw": returns,
        "rates": rates,
        "inventory": inventory
    }

    # Run full optimization pipeline
    optimized_datasets = performance_optimizer.run_full_optimization_pipeline(datasets_to_optimize)
    
    optimize_duration = time.time() - optimize_start
    log_metric(logger, "pipeline.optimize.duration.seconds", optimize_duration, run_id)
    metrics.timing("pipeline.optimize.duration", optimize_duration * 1000, {"stage": "optimize"})
    
    log_pipeline_event(logger, "optimization_completed", "optimize", run_id,
                      duration_seconds=optimize_duration)

    # Bronze layer with data quality checks
    log_pipeline_event(logger, "bronze_layer_started", "bronze", run_id)
    bronze_start = time.time()
    
    bronze_tables = {
        "customers_raw": optimized_datasets["customers"],
        "products_raw": optimized_datasets["products"],
        "orders_raw": optimized_datasets["orders"],
        "returns_raw": optimized_datasets["returns_raw"],
        "fx_rates": optimized_datasets["rates"],
        "inventory_snapshots": optimized_datasets["inventory"]
    }
    
    for table_name, df in bronze_tables.items():
        # Run data quality checks
        try:
            dq_result = run_dq(df, key_cols=["id"], required_cols=["id"])
            log_metric(logger, f"pipeline.bronze.{table_name}.row_count", dq_result.stats.get("rows", 0), run_id)
            log_metric(logger, f"pipeline.bronze.{table_name}.null_keys", dq_result.issues.get("null_keys", 0), run_id)
            
            # Log DQ metrics
            metrics.data_quality_metric(table_name, "row_count", dq_result.stats.get("rows", 0))
            metrics.data_quality_metric(table_name, "null_keys", dq_result.issues.get("null_keys", 0))
            
            if dq_result.critical_fail:
                logger.error(f"Critical DQ failure for {table_name}: {dq_result.issues}")
                metrics.data_quality_metric(table_name, "critical_failures", 1)
        except Exception as e:
            logger.warning(f"DQ check failed for {table_name}: {e}")
        
        # Write to bronze
        write_delta(df, f"{bronze_base}/{table_name}", mode="overwrite")
    
    bronze_duration = time.time() - bronze_start
    log_metric(logger, "pipeline.bronze.duration.seconds", bronze_duration, run_id)
    metrics.timing("pipeline.bronze.duration", bronze_duration * 1000, {"stage": "bronze"})
    
    log_pipeline_event(logger, "bronze_layer_completed", "bronze", run_id,
                      duration_seconds=bronze_duration)

    # Transform data for Silver layer
    log_pipeline_event(logger, "silver_layer_started", "silver", run_id)
    silver_start = time.time()
    
    try:
        customers_enriched = enrich_customers(optimized_datasets["customers"])
        products_enriched = enrich_products(optimized_datasets["products"])
        orders_enriched = clean_orders(optimized_datasets["orders"])
        
        # Log transformation success
        log_pipeline_event(logger, "transformations_successful", "silver", run_id)
        
    except Exception as e:
        logger.warning(f"Transformation failed (likely UDF issue with mock session): {e}")
        logger.info("Using original datasets for Silver layer")
        customers_enriched = optimized_datasets["customers"]
        products_enriched = optimized_datasets["products"]
        orders_enriched = optimized_datasets["orders"]
        
        # Log transformation failure
        log_pipeline_event(logger, "transformations_failed", "silver", run_id, error=str(e))
        metrics.incr("pipeline.transform.errors", 1, {"error_type": "udf_failure"})

    # Silver layer (cleaned, enriched data) with DQ checks
    silver_tables = {
        "customers_enriched": customers_enriched,
        "products_enriched": products_enriched,
        "orders_enriched": orders_enriched
    }
    
    for table_name, df in silver_tables.items():
        # Run data quality checks
        try:
            dq_result = run_dq(df, key_cols=["id"], required_cols=["id"])
            log_metric(logger, f"pipeline.silver.{table_name}.row_count", dq_result.stats.get("rows", 0), run_id)
            log_metric(logger, f"pipeline.silver.{table_name}.null_keys", dq_result.issues.get("null_keys", 0), run_id)
            
            metrics.data_quality_metric(table_name, "row_count", dq_result.stats.get("rows", 0))
            metrics.data_quality_metric(table_name, "null_keys", dq_result.issues.get("null_keys", 0))
        except Exception as e:
            logger.warning(f"DQ check failed for {table_name}: {e}")
        
        # Write to silver
        write_delta(df, f"{silver_base}/{table_name}", mode="overwrite")
    
    silver_duration = time.time() - silver_start
    log_metric(logger, "pipeline.silver.duration.seconds", silver_duration, run_id)
    metrics.timing("pipeline.silver.duration", silver_duration * 1000, {"stage": "silver"})
    
    log_pipeline_event(logger, "silver_layer_completed", "silver", run_id,
                      duration_seconds=silver_duration)

    # Gold layer (business-ready data)
    log_pipeline_event(logger, "gold_layer_started", "gold", run_id)
    gold_start = time.time()
    
    fact_orders = build_fact_orders(orders_enriched)
    
    # DQ check for gold layer
    try:
        dq_result = run_dq(fact_orders, key_cols=["order_id"], required_cols=["order_id"])
        log_metric(logger, "pipeline.gold.fact_orders.row_count", dq_result.stats.get("rows", 0), run_id)
        log_metric(logger, "pipeline.gold.fact_orders.null_keys", dq_result.issues.get("null_keys", 0), run_id)
        
        metrics.data_quality_metric("fact_orders", "row_count", dq_result.stats.get("rows", 0))
        metrics.data_quality_metric("fact_orders", "null_keys", dq_result.issues.get("null_keys", 0))
    except Exception as e:
        logger.warning(f"DQ check failed for fact_orders: {e}")
    
    write_delta(fact_orders, f"{gold_base}/fact_orders", mode="overwrite")
    
    gold_duration = time.time() - gold_start
    log_metric(logger, "pipeline.gold.duration.seconds", gold_duration, run_id)
    metrics.timing("pipeline.gold.duration", gold_duration * 1000, {"stage": "gold"})
    
    log_pipeline_event(logger, "gold_layer_completed", "gold", run_id,
                      duration_seconds=gold_duration)

    # Final outputs
    log_pipeline_event(logger, "final_outputs_started", "output", run_id)
    output_start = time.time()
    
    write_delta(fact_orders, cfg.get("output", {}).get("parquet_path", "data/output_data/final.parquet"), mode="overwrite")
    write_delta(fact_orders, cfg.get("output", {}).get("delta_path", "data/output_data/final_delta"), mode="overwrite")
    
    output_duration = time.time() - output_start
    log_metric(logger, "pipeline.output.duration.seconds", output_duration, run_id)
    metrics.timing("pipeline.output.duration", output_duration * 1000, {"stage": "output"})
    
    log_pipeline_event(logger, "final_outputs_completed", "output", run_id,
                      duration_seconds=output_duration)

    # Validation
    log_pipeline_event(logger, "validation_started", "validation", run_id)
    validation_start = time.time()
    
    try:
        validator = ValidateOutput(spark)
        validator.validate_parquet(spark, cfg.get("output", {}).get("parquet_path", "data/output_data/final.parquet"))
        
        validation_duration = time.time() - validation_start
        log_metric(logger, "pipeline.validation.duration.seconds", validation_duration, run_id)
        metrics.timing("pipeline.validation.duration", validation_duration * 1000, {"stage": "validation"})
        
        log_pipeline_event(logger, "validation_completed", "validation", run_id,
                          duration_seconds=validation_duration)
        
    except Exception as e:
        logger.warning(f"Validation failed (likely mock session issue): {e}")
        logger.info("Skipping validation in mock environment")
        
        validation_duration = time.time() - validation_start
        log_metric(logger, "pipeline.validation.duration.seconds", validation_duration, run_id)
        metrics.incr("pipeline.validation.errors", 1, {"error_type": "mock_session"})
        
        log_pipeline_event(logger, "validation_skipped", "validation", run_id,
                          duration_seconds=validation_duration, error=str(e))

    # Final pipeline metrics
    total_duration = time.time() - pipeline_start_time
    log_metric(logger, "pipeline.total.duration.seconds", total_duration, run_id)
    metrics.timing("pipeline.total.duration", total_duration * 1000, {"stage": "total"})
    
    log_pipeline_event(logger, "pipeline_completed", "completion", run_id,
                      duration_seconds=total_duration, status="success")
    
    logger.info(f"Pipeline completed in {total_duration:.2f} seconds")


def main(config_path="config/config.yaml"):
    """Legacy main function for backward compatibility"""
    from .config_loader import load_config
    from .utils import get_spark_session
    from .monitoring import create_monitor
    
    cfg = load_config(config_path)
    spark = get_spark_session(cfg)
    monitor = create_monitor(spark, cfg)
    try:
        with monitor.monitor_pipeline("pde_etl"):
            run_pipeline(spark, cfg, "legacy_run")
    finally:
        spark.stop()
