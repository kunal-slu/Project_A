# Alertmanager configuration

global:
  resolve_timeout: 5m
  # Slack webhook URL (set via environment or secret)
  slack_api_url: '${SLACK_WEBHOOK_URL}'

# Route tree for notifications
route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default'
  
  routes:
    # Critical alerts go to pagerduty and slack
    - match:
        severity: critical
      receiver: critical-alerts
      continue: false
    
    # Warning alerts go to slack only
    - match:
        severity: warning
      receiver: warning-alerts
      continue: false
    
    # Component-specific routing
    - match:
        component: data_quality
      receiver: dq-team
      continue: true
    
    - match:
        component: delta_lake
      receiver: platform-team
      continue: true

# Alert receivers
receivers:
  - name: 'default'
    email_configs:
      - to: 'alerts@example.com'
        from: 'alertmanager@example.com'
        smarthost: 'smtp.example.com:587'
        auth_username: 'alerts@example.com'
        auth_password: '${SMTP_PASSWORD}'
        headers:
          Subject: '[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .GroupLabels.alertname }}'

  - name: 'critical-alerts'
    slack_configs:
      - channel: '#etl-critical-alerts'
        title: 'üö® CRITICAL: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Labels.alertname }}
          *Summary:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Severity:* {{ .Labels.severity }}
          *Job:* {{ .Labels.job_name }}
          *Environment:* {{ .Labels.environment }}
          {{ end }}
        send_resolved: true
        color: danger
        actions:
          - type: button
            text: 'View in Grafana üìä'
            url: 'http://localhost:3000/d/etl-overview'
          - type: button
            text: 'View in Prometheus üìà'
            url: 'http://localhost:9090/alerts'
    
    pagerduty_configs:
      - service_key: '${PAGERDUTY_SERVICE_KEY}'
        description: '{{ .GroupLabels.alertname }}: {{ .CommonAnnotations.summary }}'

  - name: 'warning-alerts'
    slack_configs:
      - channel: '#etl-warnings'
        title: '‚ö†Ô∏è WARNING: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Labels.alertname }}
          *Summary:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Job:* {{ .Labels.job_name }}
          {{ end }}
        send_resolved: true
        color: warning

  - name: 'dq-team'
    slack_configs:
      - channel: '#data-quality-team'
        title: 'Data Quality Alert: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Table:* {{ .Labels.table }}
          *Check Type:* {{ .Labels.check_type }}
          *Description:* {{ .Annotations.description }}
          {{ end }}
        send_resolved: true

  - name: 'platform-team'
    slack_configs:
      - channel: '#platform-team'
        title: 'Platform Alert: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Component:* {{ .Labels.component }}
          *Description:* {{ .Annotations.description }}
          {{ end }}
        send_resolved: true

# Inhibition rules to prevent alert storms
inhibit_rules:
  # Inhibit warning alerts if critical alert is firing for same job
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['job_name', 'environment']
  
  # Inhibit downstream alerts if upstream is failing
  - source_match:
      stage: 'extract'
    target_match:
      stage: 'transform'
    equal: ['job_name']
  
  - source_match:
      stage: 'transform'
    target_match:
      stage: 'load'
    equal: ['job_name']

