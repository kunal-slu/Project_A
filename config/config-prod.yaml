# Production Configuration for PySpark Data Engineering Pipeline
# This configuration is optimized for production environments with enterprise-grade settings

# Environment Configuration
environment:
  name: "production"
  region: "us-west-2"
  timezone: "UTC"
  
# Spark Configuration
spark:
  app_name: "PySpark-Data-Engineering-Prod"
  master: "yarn"
  deploy_mode: "cluster"
  
  # Performance Tuning
  executor:
    cores: 4
    memory: "8g"
    memory_overhead: "2g"
    instances: 10
    max_instances: 20
    
  driver:
    memory: "4g"
    memory_overhead: "1g"
    
  # Adaptive Query Execution
  sql:
    adaptive:
      enabled: true
      coalesce_partitions:
        enabled: true
        min_partition_num: 1
        max_partition_num: 100
      skew_join:
        enabled: true
        skew_partition_threshold: 256
        skew_partition_factor: 5
      local_shuffle_reader:
        enabled: true
        
  # Dynamic Partition Pruning
  sql:
    dynamic_partition_pruning:
      enabled: true
      reuse_work: true
      
  # Cost-based Optimization
  sql:
    cost_based_optimization:
      enabled: true
      join_reorder:
        enabled: true
        depth: 12
        
  # Serialization
  serializer: "org.apache.spark.serializer.KryoSerializer"
  kryo:
    registration_required: false
    unsafe: true
    
  # Memory Management
  memory:
    fraction: 0.8
    storage_fraction: 0.3
    off_heap:
      enabled: true
      size: "2g"
      
  # Shuffle
  shuffle:
    file:
      buffer: "32k"
    io:
      max_retries: 3
      retry_wait: "5s"
      
  # Network
  network:
    timeout: "800s"
    io:
      max_retries: 3
      retry_wait: "5s"
      
  # File System
  hadoop:
    fs:
      s3:
        max_connections: 1000
        max_connections_per_host: 100
        connection_timeout: "200000"
        socket_timeout: "200000"
        retry_policy: "exponential"
        max_retry_attempts: 10
        
# Delta Lake Configuration
delta:
  # Performance
  auto_optimize:
    optimize_write: true
    auto_compact: true
    
  # Change Data Feed
  enable_change_data_feed: true
  
  # Schema Evolution
  schema_evolution:
    mode: "additive"
    allow_incompatible_schema: false
    
  # Data Quality
  data_quality:
    enabled: true
    mode: "strict"
    
  # Retention
  retention:
    log_retention_days: 30
    deleted_file_retention_days: 7
    
  # Z-Order Optimization
  optimize:
    auto: true
    interval: "1 hour"
    zorder:
      columns: ["order_date", "customer_id", "product_id"]
      
# Data Sources
sources:
  customers:
    path: "s3://prod-data-lake/bronze/customers/"
    format: "delta"
    schema_evolution: true
    
  orders:
    path: "s3://prod-data-lake/bronze/orders/"
    format: "delta"
    schema_evolution: true
    
  products:
    path: "s3://prod-data-lake/bronze/products/"
    format: "delta"
    schema_evolution: true
    
  exchange_rates:
    path: "s3://prod-data-lake/bronze/exchange_rates/"
    format: "delta"
    schema_evolution: true
    
  inventory:
    path: "s3://prod-data-lake/bronze/inventory/"
    format: "delta"
    schema_evolution: true
    
  returns:
    path: "s3://prod-data-lake/bronze/returns/"
    format: "delta"
    schema_evolution: true

# Data Destinations
destinations:
  bronze:
    base_path: "s3://prod-data-lake/bronze/"
    format: "delta"
    partition_by: ["year", "month", "day"]
    
  silver:
    base_path: "s3://prod-data-lake/silver/"
    format: "delta"
    partition_by: ["year", "month", "day"]
    
  gold:
    base_path: "s3://prod-data-lake/gold/"
    format: "delta"
    partition_by: ["year", "month", "day"]
    
  output:
    base_path: "s3://prod-data-lake/output/"
    format: "parquet"
    partition_by: ["year", "month", "day"]

# Data Quality Configuration
data_quality:
  enabled: true
  strict_mode: true
  
  # Validation Rules
  rules:
    customers:
      - column: "customer_id"
        rule: "not_null"
        severity: "error"
      - column: "email"
        rule: "email_format"
        severity: "error"
      - column: "age"
        rule: "range"
        min: 13
        max: 120
        severity: "warning"
        
    orders:
      - column: "order_id"
        rule: "not_null"
        severity: "error"
      - column: "order_date"
        rule: "date_range"
        min: "2020-01-01"
        max: "current_date"
        severity: "error"
      - column: "total_amount"
        rule: "positive"
        severity: "error"
        
    products:
      - column: "product_id"
        rule: "not_null"
        severity: "error"
      - column: "price"
        rule: "positive"
        severity: "error"
        
  # Anomaly Detection
  anomaly_detection:
    enabled: true
    methods:
      - "statistical"
      - "isolation_forest"
      - "dbscan"
    threshold: 0.95
    
  # Data Profiling
  profiling:
    enabled: true
    sample_size: 10000
    include_histograms: true
    include_correlations: true

# Monitoring and Alerting
monitoring:
  enabled: true
  
  # Metrics Collection
  metrics:
    spark_metrics: true
    custom_metrics: true
    data_quality_metrics: true
    
  # Alerting
  alerts:
    pipeline_failure: true
    data_quality_violations: true
    performance_degradation: true
    resource_utilization: true
    
  # Thresholds
  thresholds:
    pipeline_runtime_minutes: 120
    data_quality_violation_rate: 0.01
    executor_memory_utilization: 0.85
    driver_memory_utilization: 0.80
    
  # Notification Channels
  notifications:
    slack:
      enabled: true
      webhook_url: "${SLACK_WEBHOOK_URL}"
      channel: "#data-engineering-alerts"
    email:
      enabled: true
      recipients: ["data-team@company.com", "oncall@company.com"]
    pagerduty:
      enabled: true
      service_key: "${PAGERDUTY_SERVICE_KEY}"

# Security Configuration
security:
  # Data Encryption
  encryption:
    at_rest: true
    in_transit: true
    algorithm: "AES-256"
    
  # Access Control
  access_control:
    enabled: true
    method: "iam"
    
  # Audit Logging
  audit:
    enabled: true
    level: "detailed"
    retention_days: 2555
    
  # Data Masking
  data_masking:
    enabled: true
    sensitive_columns:
      - "ssn"
      - "credit_card"
      - "phone"
    method: "hash"

# Error Handling and Recovery
error_handling:
  # Retry Configuration
  retry:
    max_attempts: 3
    initial_delay: "5s"
    max_delay: "60s"
    backoff_multiplier: 2
    
  # Dead Letter Queue
  dead_letter_queue:
    enabled: true
    path: "s3://prod-data-lake/dlq/"
    retention_days: 30
    
  # Circuit Breaker
  circuit_breaker:
    enabled: true
    failure_threshold: 5
    recovery_timeout: "300s"
    
  # Graceful Degradation
  graceful_degradation:
    enabled: true
    fallback_strategies:
      - "use_cached_data"
      - "use_historical_data"
      - "skip_optional_steps"

# Performance Optimization
performance:
  # Caching Strategy
  caching:
    enabled: true
    strategy: "lru"
    max_memory: "4g"
    
  # Broadcast Joins
  broadcast_join:
    enabled: true
    threshold: "10m"
    
  # Skew Handling
  skew_handling:
    enabled: true
    method: "salting"
    num_salt_partitions: 10
    
  # Partition Optimization
  partitioning:
    strategy: "date_based"
    partition_size_target: "128m"
    max_partitions: 10000
    
  # Compression
  compression:
    format: "snappy"
    level: 1

# Logging Configuration
logging:
  level: "INFO"
  format: "json"
  
  # Appenders
  appenders:
    console:
      enabled: true
    file:
      enabled: true
      path: "/var/log/pyspark-pipeline/"
      max_size: "100MB"
      max_files: 10
    s3:
      enabled: true
      bucket: "prod-logs"
      prefix: "pyspark-pipeline/"
      
  # Structured Logging
  structured:
    enabled: true
    include:
      - "timestamp"
      - "level"
      - "logger"
      - "message"
      - "context"
      - "performance_metrics"

# Scheduling and Orchestration
scheduling:
  # Pipeline Schedule
  schedule:
    frequency: "hourly"
    start_time: "00:00"
    timezone: "UTC"
    
  # Dependencies
  dependencies:
    - "data_ingestion"
    - "data_validation"
    - "data_transformation"
    - "data_quality_check"
    - "data_loading"
    
  # SLA Configuration
  sla:
    data_freshness_minutes: 60
    pipeline_runtime_minutes: 120
    data_quality_threshold: 0.99
    
  # Resource Allocation
  resources:
    priority: "high"
    preemption: false
    resource_guarantees: true

# Data Lineage and Metadata
metadata:
  # Data Lineage
  lineage:
    enabled: true
    granularity: "column_level"
    storage: "graph_database"
    
  # Data Catalog
  catalog:
    enabled: true
    provider: "aws_glue"
    auto_discovery: true
    
  # Schema Registry
  schema_registry:
    enabled: true
    provider: "aws_glue"
    versioning: true
    
  # Business Glossary
  business_glossary:
    enabled: true
    categories:
      - "customer_data"
      - "financial_data"
      - "operational_data"
      - "compliance_data"

# Compliance and Governance
compliance:
  # Data Classification
  data_classification:
    enabled: true
    levels:
      - "public"
      - "internal"
      - "confidential"
      - "restricted"
      
  # Data Retention
  retention:
    enabled: true
    policies:
      customer_data: "7_years"
      financial_data: "7_years"
      operational_data: "3_years"
      logs: "1_year"
      
  # Data Privacy
  privacy:
    gdpr_compliance: true
    data_minimization: true
    right_to_forget: true
    
  # Audit and Reporting
  audit:
    enabled: true
    frequency: "monthly"
    reports:
      - "data_access_logs"
      - "data_quality_metrics"
      - "compliance_status"
      - "security_incidents"

# Disaster Recovery
disaster_recovery:
  # Backup Strategy
  backup:
    enabled: true
    frequency: "daily"
    retention: "30_days"
    cross_region: true
    
  # Replication
  replication:
    enabled: true
    regions:
      - "us-east-1"
      - "eu-west-1"
    sync_mode: "asynchronous"
    
  # Recovery Time Objective (RTO)
  rto: "4_hours"
  
  # Recovery Point Objective (RPO)
  rpo: "1_hour"
  
  # Failover
  failover:
    enabled: true
    automatic: false
    health_check_interval: "5_minutes"