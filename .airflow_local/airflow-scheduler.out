  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2025-10-19T22:03:49.603-0500[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2025-10-19T22:03:49.605-0500[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: LocalExecutor[0m
[[34m2025-10-19T22:03:49.630-0500[0m] {[34mscheduler_job_runner.py:[0m799} INFO[0m - Starting the scheduler[0m
[[34m2025-10-19T22:03:49.630-0500[0m] {[34mscheduler_job_runner.py:[0m806} INFO[0m - Processing each file at most -1 times[0m
[[34m2025-10-19T22:03:50.229-0500[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 9030[0m
[[34m2025-10-19T22:03:50.231-0500[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-19T22:03:51.103-0500[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[[34m2025-10-19T22:04:21.489-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: production_etl_dag.start_pipeline manual__2025-10-20T03:04:21+00:00 [scheduled]>[0m
[[34m2025-10-19T22:04:21.490-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG production_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:04:21.490-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: production_etl_dag.start_pipeline manual__2025-10-20T03:04:21+00:00 [scheduled]>[0m
[[34m2025-10-19T22:04:21.492-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='production_etl_dag', task_id='start_pipeline', run_id='manual__2025-10-20T03:04:21+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-10-19T22:04:21.492-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'production_etl_dag', 'start_pipeline', 'manual__2025-10-20T03:04:21+00:00', '--local', '--subdir', 'DAGS_FOLDER/production_etl_dag.py'][0m
[[34m2025-10-19T22:04:21.493-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'production_etl_dag', 'start_pipeline', 'manual__2025-10-20T03:04:21+00:00', '--local', '--subdir', 'DAGS_FOLDER/production_etl_dag.py'][0m
[[34m2025-10-19T22:04:21.618-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /Users/kunal/IdeaProjects/pyspark_data_engineer_project/***/dags/production_etl_dag.py[0m
[[34m2025-10-19T22:04:21.672-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: production_etl_dag.start_pipeline manual__2025-10-20T03:04:21+00:00 [queued]> on host kunals-macbook-air.local[0m
[[34m2025-10-19T22:04:22.545-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='production_etl_dag', task_id='start_pipeline', run_id='manual__2025-10-20T03:04:21+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T22:04:22.548-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=production_etl_dag, task_id=start_pipeline, run_id=manual__2025-10-20T03:04:21+00:00, map_index=-1, run_start_date=2025-10-20 03:04:21.735095+00:00, run_end_date=None, run_duration=None, state=running, executor_state=success, try_number=1, max_tries=1, job_id=6, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2025-10-20 03:04:21.491287+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:04:30.422-0500[0m] {[34mscheduler_job_runner.py:[0m1725} WARNING[0m - Failing (1) jobs without heartbeat after 2025-10-20 02:59:30.420731+00:00[0m
[[34m2025-10-19T22:04:30.423-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - Detected zombie job: {'full_filepath': '/Users/kunal/IdeaProjects/pyspark_data_engineer_project/airflow/dags/production_etl_dag.py', 'processor_subdir': '/Users/kunal/IdeaProjects/pyspark_data_engineer_project/airflow/dags', 'msg': "{'DAG Id': 'production_etl_dag', 'Task Id': 'start_pipeline', 'Run Id': 'manual__2025-10-20T03:04:21+00:00', 'Hostname': 'kunals-macbook-air.local'}", 'simple_task_instance': <airflow.models.taskinstance.SimpleTaskInstance object at 0x105443bd0>, 'is_failure_callback': True} (See https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/tasks.html#zombie-undead-tasks)[0m
[[34m2025-10-19T22:08:50.300-0500[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-19T22:09:31.654-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: production_etl_dag.start_pipeline manual__2025-10-20T03:04:21+00:00 [scheduled]>[0m
[[34m2025-10-19T22:09:31.656-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG production_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:09:31.657-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: production_etl_dag.start_pipeline manual__2025-10-20T03:04:21+00:00 [scheduled]>[0m
[[34m2025-10-19T22:09:31.659-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='production_etl_dag', task_id='start_pipeline', run_id='manual__2025-10-20T03:04:21+00:00', try_number=2, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-10-19T22:09:31.660-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'production_etl_dag', 'start_pipeline', 'manual__2025-10-20T03:04:21+00:00', '--local', '--subdir', 'DAGS_FOLDER/production_etl_dag.py'][0m
[[34m2025-10-19T22:09:31.663-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'production_etl_dag', 'start_pipeline', 'manual__2025-10-20T03:04:21+00:00', '--local', '--subdir', 'DAGS_FOLDER/production_etl_dag.py'][0m
[[34m2025-10-19T22:09:31.846-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /Users/kunal/IdeaProjects/pyspark_data_engineer_project/***/dags/production_etl_dag.py[0m
[[34m2025-10-19T22:09:31.911-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: production_etl_dag.start_pipeline manual__2025-10-20T03:04:21+00:00 [queued]> on host kunals-macbook-air.local[0m
[[34m2025-10-19T22:09:32.738-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: production_etl_dag.run_etl manual__2025-10-20T03:04:21+00:00 [scheduled]>[0m
[[34m2025-10-19T22:09:32.740-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG production_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:09:32.741-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: production_etl_dag.run_etl manual__2025-10-20T03:04:21+00:00 [scheduled]>[0m
[[34m2025-10-19T22:09:32.743-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='production_etl_dag', task_id='run_etl', run_id='manual__2025-10-20T03:04:21+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T22:09:32.744-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'production_etl_dag', 'run_etl', 'manual__2025-10-20T03:04:21+00:00', '--local', '--subdir', 'DAGS_FOLDER/production_etl_dag.py'][0m
[[34m2025-10-19T22:09:32.745-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='production_etl_dag', task_id='start_pipeline', run_id='manual__2025-10-20T03:04:21+00:00', try_number=2, map_index=-1)[0m
[[34m2025-10-19T22:09:32.746-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'production_etl_dag', 'run_etl', 'manual__2025-10-20T03:04:21+00:00', '--local', '--subdir', 'DAGS_FOLDER/production_etl_dag.py'][0m
[[34m2025-10-19T22:09:32.749-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=production_etl_dag, task_id=start_pipeline, run_id=manual__2025-10-20T03:04:21+00:00, map_index=-1, run_start_date=2025-10-20 03:09:31.971642+00:00, run_end_date=2025-10-20 03:09:32.160484+00:00, run_duration=0.188842, state=success, executor_state=success, try_number=2, max_tries=1, job_id=7, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2025-10-20 03:09:31.658656+00:00, queued_by_job_id=5, pid=11189[0m
[[34m2025-10-19T22:09:32.886-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /Users/kunal/IdeaProjects/pyspark_data_engineer_project/***/dags/production_etl_dag.py[0m
[[34m2025-10-19T22:09:32.938-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: production_etl_dag.run_etl manual__2025-10-20T03:04:21+00:00 [queued]> on host kunals-macbook-air.local[0m
[[34m2025-10-19T22:13:50.332-0500[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-19T22:18:50.360-0500[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-19T22:23:50.400-0500[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-19T22:28:50.467-0500[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-19T22:29:21.941-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: delta_lake_etl_pipeline_dag.start_pipeline manual__2025-10-20T03:29:21.172528+00:00 [scheduled]>[0m
[[34m2025-10-19T22:29:21.942-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG delta_lake_etl_pipeline_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:29:21.942-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: delta_lake_etl_pipeline_dag.start_pipeline manual__2025-10-20T03:29:21.172528+00:00 [scheduled]>[0m
[[34m2025-10-19T22:29:21.943-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='delta_lake_etl_pipeline_dag', task_id='start_pipeline', run_id='manual__2025-10-20T03:29:21.172528+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-10-19T22:29:21.943-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'delta_lake_etl_pipeline_dag', 'start_pipeline', 'manual__2025-10-20T03:29:21.172528+00:00', '--local', '--subdir', 'DAGS_FOLDER/delta_lake_etl_pipeline_dag.py'][0m
[[34m2025-10-19T22:29:21.945-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'delta_lake_etl_pipeline_dag', 'start_pipeline', 'manual__2025-10-20T03:29:21.172528+00:00', '--local', '--subdir', 'DAGS_FOLDER/delta_lake_etl_pipeline_dag.py'][0m
[[34m2025-10-19T22:29:22.120-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /Users/kunal/IdeaProjects/pyspark_data_engineer_project/***/dags/delta_lake_etl_pipeline_dag.py[0m
[[34m2025-10-19T22:29:22.175-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: delta_lake_etl_pipeline_dag.start_pipeline manual__2025-10-20T03:29:21.172528+00:00 [queued]> on host kunals-macbook-air.local[0m
[[34m2025-10-19T22:29:23.021-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: delta_lake_etl_pipeline_dag.run_etl_pipeline manual__2025-10-20T03:29:21.172528+00:00 [scheduled]>[0m
[[34m2025-10-19T22:29:23.021-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG delta_lake_etl_pipeline_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:29:23.021-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: delta_lake_etl_pipeline_dag.run_etl_pipeline manual__2025-10-20T03:29:21.172528+00:00 [scheduled]>[0m
[[34m2025-10-19T22:29:23.022-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='delta_lake_etl_pipeline_dag', task_id='run_etl_pipeline', run_id='manual__2025-10-20T03:29:21.172528+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T22:29:23.023-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'delta_lake_etl_pipeline_dag', 'run_etl_pipeline', 'manual__2025-10-20T03:29:21.172528+00:00', '--local', '--subdir', 'DAGS_FOLDER/delta_lake_etl_pipeline_dag.py'][0m
[[34m2025-10-19T22:29:23.023-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='delta_lake_etl_pipeline_dag', task_id='start_pipeline', run_id='manual__2025-10-20T03:29:21.172528+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T22:29:23.024-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'delta_lake_etl_pipeline_dag', 'run_etl_pipeline', 'manual__2025-10-20T03:29:21.172528+00:00', '--local', '--subdir', 'DAGS_FOLDER/delta_lake_etl_pipeline_dag.py'][0m
[[34m2025-10-19T22:29:23.025-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=delta_lake_etl_pipeline_dag, task_id=start_pipeline, run_id=manual__2025-10-20T03:29:21.172528+00:00, map_index=-1, run_start_date=2025-10-20 03:29:22.236146+00:00, run_end_date=2025-10-20 03:29:22.367081+00:00, run_duration=0.130935, state=success, executor_state=success, try_number=1, max_tries=0, job_id=9, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2025-10-20 03:29:21.943108+00:00, queued_by_job_id=5, pid=18086[0m
[[34m2025-10-19T22:29:23.129-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /Users/kunal/IdeaProjects/pyspark_data_engineer_project/***/dags/delta_lake_etl_pipeline_dag.py[0m
[[34m2025-10-19T22:29:23.175-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: delta_lake_etl_pipeline_dag.run_etl_pipeline manual__2025-10-20T03:29:21.172528+00:00 [queued]> on host kunals-macbook-air.local[0m
[[34m2025-10-19T22:29:24.314-0500[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG delta_lake_maintenance is at (or above) max_active_runs (2 of 1), not creating any more runs[0m
[[34m2025-10-19T22:29:24.335-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: delta_lake_maintenance.perform_delta_lake_maintenance scheduled__2025-10-19T00:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:29:24.335-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG delta_lake_maintenance has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:29:24.336-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: delta_lake_maintenance.perform_delta_lake_maintenance scheduled__2025-10-19T00:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:29:24.337-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='delta_lake_maintenance', task_id='perform_delta_lake_maintenance', run_id='scheduled__2025-10-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T22:29:24.337-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'delta_lake_maintenance', 'perform_delta_lake_maintenance', 'scheduled__2025-10-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/delta_lake_maintenance_dag.py'][0m
[[34m2025-10-19T22:29:24.338-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='delta_lake_etl_pipeline_dag', task_id='run_etl_pipeline', run_id='manual__2025-10-20T03:29:21.172528+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T22:29:24.340-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=delta_lake_etl_pipeline_dag, task_id=run_etl_pipeline, run_id=manual__2025-10-20T03:29:21.172528+00:00, map_index=-1, run_start_date=2025-10-20 03:29:23.220751+00:00, run_end_date=None, run_duration=None, state=running, executor_state=success, try_number=1, max_tries=0, job_id=10, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2025-10-20 03:29:23.022259+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:29:24.340-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'delta_lake_maintenance', 'perform_delta_lake_maintenance', 'scheduled__2025-10-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/delta_lake_maintenance_dag.py'][0m
[[34m2025-10-19T22:29:24.507-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /Users/kunal/IdeaProjects/pyspark_data_engineer_project/***/dags/production/delta_lake_maintenance_dag.py[0m
[[34m2025-10-19T22:29:24.559-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: delta_lake_maintenance.perform_delta_lake_maintenance scheduled__2025-10-19T00:00:00+00:00 [queued]> on host kunals-macbook-air.local[0m
[[34m2025-10-19T22:29:25.094-0500[0m] {[34mscheduler_job_runner.py:[0m1725} WARNING[0m - Failing (1) jobs without heartbeat after 2025-10-20 03:24:25.092954+00:00[0m
[[34m2025-10-19T22:29:25.095-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - Detected zombie job: {'full_filepath': '/Users/kunal/IdeaProjects/pyspark_data_engineer_project/airflow/dags/delta_lake_etl_pipeline_dag.py', 'processor_subdir': '/Users/kunal/IdeaProjects/pyspark_data_engineer_project/airflow/dags', 'msg': "{'DAG Id': 'delta_lake_etl_pipeline_dag', 'Task Id': 'run_etl_pipeline', 'Run Id': 'manual__2025-10-20T03:29:21.172528+00:00', 'Hostname': 'kunals-macbook-air.local'}", 'simple_task_instance': <airflow.models.taskinstance.SimpleTaskInstance object at 0x113920850>, 'is_failure_callback': True} (See https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/tasks.html#zombie-undead-tasks)[0m
[[34m2025-10-19T22:29:26.108-0500[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG external_data_ingestion is at (or above) max_active_runs (2 of 1), not creating any more runs[0m
[[34m2025-10-19T22:29:26.140-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='delta_lake_maintenance', task_id='perform_delta_lake_maintenance', run_id='scheduled__2025-10-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T22:29:26.143-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=delta_lake_maintenance, task_id=perform_delta_lake_maintenance, run_id=scheduled__2025-10-19T00:00:00+00:00, map_index=-1, run_start_date=2025-10-20 03:29:24.605700+00:00, run_end_date=2025-10-20 03:29:25.119596+00:00, run_duration=0.513896, state=up_for_retry, executor_state=success, try_number=1, max_tries=2, job_id=11, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 03:29:24.336542+00:00, queued_by_job_id=5, pid=18129[0m
[[34m2025-10-19T22:29:27.205-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: external_data_ingestion.ingest_hubspot_data scheduled__2025-10-20T02:00:00+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_snowflake_data scheduled__2025-10-20T02:00:00+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_redshift_data scheduled__2025-10-20T02:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:29:27.205-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:29:27.206-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 1/4 running and queued tasks[0m
[[34m2025-10-19T22:29:27.206-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 2/4 running and queued tasks[0m
[[34m2025-10-19T22:29:27.207-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: external_data_ingestion.ingest_hubspot_data scheduled__2025-10-20T02:00:00+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_snowflake_data scheduled__2025-10-20T02:00:00+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_redshift_data scheduled__2025-10-20T02:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:29:27.209-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_hubspot_data', run_id='scheduled__2025-10-20T02:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T22:29:27.209-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_hubspot_data', 'scheduled__2025-10-20T02:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:29:27.210-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_snowflake_data', run_id='scheduled__2025-10-20T02:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T22:29:27.210-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_snowflake_data', 'scheduled__2025-10-20T02:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:29:27.212-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_redshift_data', run_id='scheduled__2025-10-20T02:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T22:29:27.213-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_redshift_data', 'scheduled__2025-10-20T02:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:29:27.214-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_hubspot_data', 'scheduled__2025-10-20T02:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:29:27.214-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_snowflake_data', 'scheduled__2025-10-20T02:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:29:27.215-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_redshift_data', 'scheduled__2025-10-20T02:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:29:27.268-0500[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun delta_lake_etl_pipeline_dag @ 2025-10-20 03:29:21.172528+00:00: manual__2025-10-20T03:29:21.172528+00:00, state:running, queued_at: 2025-10-20 03:29:21.183123+00:00. externally triggered: True> failed[0m
[[34m2025-10-19T22:29:27.270-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=delta_lake_etl_pipeline_dag, execution_date=2025-10-20 03:29:21.172528+00:00, run_id=manual__2025-10-20T03:29:21.172528+00:00, run_start_date=2025-10-20 03:29:21.928953+00:00, run_end_date=2025-10-20 03:29:27.270715+00:00, run_duration=5.341762, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-10-20 03:29:21.172528+00:00, data_interval_end=2025-10-20 03:29:21.172528+00:00, dag_hash=85046a1f12d9c1464c081606161a0adc[0m
[[34m2025-10-19T22:29:27.445-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /Users/kunal/IdeaProjects/pyspark_data_engineer_project/***/dags/production/external_data_ingestion_dag.py[0m
[[34m2025-10-19T22:29:27.450-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /Users/kunal/IdeaProjects/pyspark_data_engineer_project/***/dags/production/external_data_ingestion_dag.py[0m
[[34m2025-10-19T22:29:27.457-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /Users/kunal/IdeaProjects/pyspark_data_engineer_project/***/dags/production/external_data_ingestion_dag.py[0m
[[34m2025-10-19T22:29:27.546-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: external_data_ingestion.ingest_snowflake_data scheduled__2025-10-20T02:00:00+00:00 [queued]> on host kunals-macbook-air.local[0m
[[34m2025-10-19T22:29:27.557-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: external_data_ingestion.ingest_hubspot_data scheduled__2025-10-20T02:00:00+00:00 [queued]> on host kunals-macbook-air.local[0m
[[34m2025-10-19T22:29:27.562-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: external_data_ingestion.ingest_redshift_data scheduled__2025-10-20T02:00:00+00:00 [queued]> on host kunals-macbook-air.local[0m
[[34m2025-10-19T22:29:28.323-0500[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG standard_etl_dag is at (or above) max_active_runs (2 of 1), not creating any more runs[0m
[[34m2025-10-19T22:29:28.363-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_redshift_data', run_id='scheduled__2025-10-20T02:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T22:29:28.364-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_hubspot_data', run_id='scheduled__2025-10-20T02:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T22:29:28.364-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_snowflake_data', run_id='scheduled__2025-10-20T02:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T22:29:28.368-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_hubspot_data, run_id=scheduled__2025-10-20T02:00:00+00:00, map_index=-1, run_start_date=2025-10-20 03:29:27.640739+00:00, run_end_date=2025-10-20 03:29:27.918036+00:00, run_duration=0.277297, state=success, executor_state=success, try_number=1, max_tries=2, job_id=13, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 03:29:27.208199+00:00, queued_by_job_id=5, pid=18180[0m
[[34m2025-10-19T22:29:28.368-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_redshift_data, run_id=scheduled__2025-10-20T02:00:00+00:00, map_index=-1, run_start_date=2025-10-20 03:29:27.642878+00:00, run_end_date=2025-10-20 03:29:27.911867+00:00, run_duration=0.268989, state=success, executor_state=success, try_number=1, max_tries=2, job_id=14, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 03:29:27.208199+00:00, queued_by_job_id=5, pid=18179[0m
[[34m2025-10-19T22:29:28.369-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_snowflake_data, run_id=scheduled__2025-10-20T02:00:00+00:00, map_index=-1, run_start_date=2025-10-20 03:29:27.631162+00:00, run_end_date=2025-10-20 03:29:27.912841+00:00, run_duration=0.281679, state=success, executor_state=success, try_number=1, max_tries=2, job_id=12, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 03:29:27.208199+00:00, queued_by_job_id=5, pid=18178[0m
[[34m2025-10-19T22:29:29.399-0500[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG working_etl_dag is at (or above) max_active_runs (2 of 1), not creating any more runs[0m
[[34m2025-10-19T22:29:29.460-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:20:00+00:00 [scheduled]>
	<TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:10:00+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_kafka_streams scheduled__2025-10-20T02:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:29:29.464-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG working_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:29:29.465-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG standard_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:29:29.466-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:29:29.467-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:20:00+00:00 [scheduled]>
	<TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:10:00+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_kafka_streams scheduled__2025-10-20T02:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:29:29.471-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:20:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
[[34m2025-10-19T22:29:29.472-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:20:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T22:29:29.473-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='standard_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:10:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-10-19T22:29:29.473-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'standard_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:10:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/standard_etl_dag.py'][0m
[[34m2025-10-19T22:29:29.474-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_kafka_streams', run_id='scheduled__2025-10-20T02:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T22:29:29.475-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_kafka_streams', 'scheduled__2025-10-20T02:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:29:29.476-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:20:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T22:29:29.476-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_kafka_streams', 'scheduled__2025-10-20T02:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:29:29.476-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'standard_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:10:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/standard_etl_dag.py'][0m
[[34m2025-10-19T22:29:29.731-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /Users/kunal/IdeaProjects/pyspark_data_engineer_project/***/dags/production/external_data_ingestion_dag.py[0m
[[34m2025-10-19T22:29:29.733-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /Users/kunal/IdeaProjects/pyspark_data_engineer_project/***/dags/standard_etl_dag.py[0m
[[34m2025-10-19T22:29:29.745-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /Users/kunal/IdeaProjects/pyspark_data_engineer_project/***/dags/working_etl_dag.py[0m
[[34m2025-10-19T22:29:29.837-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:10:00+00:00 [queued]> on host kunals-macbook-air.local[0m
[[34m2025-10-19T22:29:29.840-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: external_data_ingestion.ingest_kafka_streams scheduled__2025-10-20T02:00:00+00:00 [queued]> on host kunals-macbook-air.local[0m
[[34m2025-10-19T22:29:29.848-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:20:00+00:00 [queued]> on host kunals-macbook-air.local[0m
[[34m2025-10-19T22:29:31.125-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_kafka_streams', run_id='scheduled__2025-10-20T02:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T22:29:31.125-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='standard_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:10:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T22:29:31.126-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:20:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T22:29:31.129-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=standard_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:10:00+00:00, map_index=-1, run_start_date=2025-10-20 03:29:29.909128+00:00, run_end_date=None, run_duration=None, state=running, executor_state=success, try_number=1, max_tries=1, job_id=15, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2025-10-20 03:29:29.469635+00:00, queued_by_job_id=5, pid=18251[0m
[[34m2025-10-19T22:29:31.130-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:20:00+00:00, map_index=-1, run_start_date=2025-10-20 03:29:29.922351+00:00, run_end_date=None, run_duration=None, state=running, executor_state=success, try_number=1, max_tries=1, job_id=17, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-10-20 03:29:29.469635+00:00, queued_by_job_id=5, pid=18255[0m
[[34m2025-10-19T22:29:31.130-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_kafka_streams, run_id=scheduled__2025-10-20T02:00:00+00:00, map_index=-1, run_start_date=2025-10-20 03:29:29.910160+00:00, run_end_date=2025-10-20 03:29:30.144028+00:00, run_duration=0.233868, state=success, executor_state=success, try_number=1, max_tries=2, job_id=16, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 03:29:29.469635+00:00, queued_by_job_id=5, pid=18252[0m
[[34m2025-10-19T22:29:32.164-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: external_data_ingestion.ingest_fx_rates scheduled__2025-10-20T02:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:29:32.164-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:29:32.165-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: external_data_ingestion.ingest_fx_rates scheduled__2025-10-20T02:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:29:32.166-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_fx_rates', run_id='scheduled__2025-10-20T02:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T22:29:32.166-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_fx_rates', 'scheduled__2025-10-20T02:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:29:32.167-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_fx_rates', 'scheduled__2025-10-20T02:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:29:32.266-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /Users/kunal/IdeaProjects/pyspark_data_engineer_project/***/dags/production/external_data_ingestion_dag.py[0m
[[34m2025-10-19T22:29:32.317-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: external_data_ingestion.ingest_fx_rates scheduled__2025-10-20T02:00:00+00:00 [queued]> on host kunals-macbook-air.local[0m
[[34m2025-10-19T22:29:33.248-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: external_data_ingestion.consolidate_ingestion_results scheduled__2025-10-20T02:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:29:33.249-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:29:33.249-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: external_data_ingestion.consolidate_ingestion_results scheduled__2025-10-20T02:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:29:33.250-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='consolidate_ingestion_results', run_id='scheduled__2025-10-20T02:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2025-10-19T22:29:33.250-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'consolidate_ingestion_results', 'scheduled__2025-10-20T02:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:29:33.251-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_fx_rates', run_id='scheduled__2025-10-20T02:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T22:29:33.251-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'consolidate_ingestion_results', 'scheduled__2025-10-20T02:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:29:33.259-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_fx_rates, run_id=scheduled__2025-10-20T02:00:00+00:00, map_index=-1, run_start_date=2025-10-20 03:29:32.363712+00:00, run_end_date=2025-10-20 03:29:32.487218+00:00, run_duration=0.123506, state=success, executor_state=success, try_number=1, max_tries=2, job_id=18, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 03:29:32.165587+00:00, queued_by_job_id=5, pid=18307[0m
[[34m2025-10-19T22:29:33.343-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /Users/kunal/IdeaProjects/pyspark_data_engineer_project/***/dags/production/external_data_ingestion_dag.py[0m
[[34m2025-10-19T22:29:33.393-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: external_data_ingestion.consolidate_ingestion_results scheduled__2025-10-20T02:00:00+00:00 [queued]> on host kunals-macbook-air.local[0m
[[34m2025-10-19T22:29:34.331-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: external_data_ingestion.update_ingestion_metrics scheduled__2025-10-20T02:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:29:34.331-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:29:34.331-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: external_data_ingestion.update_ingestion_metrics scheduled__2025-10-20T02:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:29:34.332-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='update_ingestion_metrics', run_id='scheduled__2025-10-20T02:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-10-19T22:29:34.333-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'update_ingestion_metrics', 'scheduled__2025-10-20T02:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:29:34.333-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='consolidate_ingestion_results', run_id='scheduled__2025-10-20T02:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T22:29:34.333-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'update_ingestion_metrics', 'scheduled__2025-10-20T02:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:29:34.340-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=consolidate_ingestion_results, run_id=scheduled__2025-10-20T02:00:00+00:00, map_index=-1, run_start_date=2025-10-20 03:29:33.443381+00:00, run_end_date=2025-10-20 03:29:33.578563+00:00, run_duration=0.135182, state=success, executor_state=success, try_number=1, max_tries=2, job_id=19, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-10-20 03:29:33.249580+00:00, queued_by_job_id=5, pid=18327[0m
[[34m2025-10-19T22:29:34.425-0500[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /Users/kunal/IdeaProjects/pyspark_data_engineer_project/***/dags/production/external_data_ingestion_dag.py[0m
[[34m2025-10-19T22:29:34.478-0500[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: external_data_ingestion.update_ingestion_metrics scheduled__2025-10-20T02:00:00+00:00 [queued]> on host kunals-macbook-air.local[0m
[[34m2025-10-19T22:29:35.124-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='update_ingestion_metrics', run_id='scheduled__2025-10-20T02:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T22:29:35.126-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=update_ingestion_metrics, run_id=scheduled__2025-10-20T02:00:00+00:00, map_index=-1, run_start_date=2025-10-20 03:29:34.530156+00:00, run_end_date=2025-10-20 03:29:34.650722+00:00, run_duration=0.120566, state=up_for_retry, executor_state=success, try_number=1, max_tries=2, job_id=20, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-10-20 03:29:34.332210+00:00, queued_by_job_id=5, pid=18359[0m
[[34m2025-10-19T22:29:35.142-0500[0m] {[34mscheduler_job_runner.py:[0m1725} WARNING[0m - Failing (2) jobs without heartbeat after 2025-10-20 03:24:35.140664+00:00[0m
[[34m2025-10-19T22:29:35.143-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - Detected zombie job: {'full_filepath': '/Users/kunal/IdeaProjects/pyspark_data_engineer_project/airflow/dags/standard_etl_dag.py', 'processor_subdir': '/Users/kunal/IdeaProjects/pyspark_data_engineer_project/airflow/dags', 'msg': "{'DAG Id': 'standard_etl_dag', 'Task Id': 'start_pipeline', 'Run Id': 'scheduled__2025-10-20T03:10:00+00:00', 'Hostname': 'kunals-macbook-air.local'}", 'simple_task_instance': <airflow.models.taskinstance.SimpleTaskInstance object at 0x113e303d0>, 'is_failure_callback': True} (See https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/tasks.html#zombie-undead-tasks)[0m
[[34m2025-10-19T22:29:35.145-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - Detected zombie job: {'full_filepath': '/Users/kunal/IdeaProjects/pyspark_data_engineer_project/airflow/dags/working_etl_dag.py', 'processor_subdir': '/Users/kunal/IdeaProjects/pyspark_data_engineer_project/airflow/dags', 'msg': "{'DAG Id': 'working_etl_dag', 'Task Id': 'start_pipeline', 'Run Id': 'scheduled__2025-10-20T03:20:00+00:00', 'Hostname': 'kunals-macbook-air.local'}", 'simple_task_instance': <airflow.models.taskinstance.SimpleTaskInstance object at 0x11373bfd0>, 'is_failure_callback': True} (See https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/tasks.html#zombie-undead-tasks)[0m
[[34m2025-10-19T22:31:35.971-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:20:00+00:00 [scheduled]>
	<TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:10:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:31:35.972-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG working_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:31:35.972-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG standard_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:31:35.972-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:20:00+00:00 [scheduled]>
	<TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:10:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:31:35.974-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:20:00+00:00', try_number=2, map_index=-1) to executor with priority 5 and queue default[0m
[[34m2025-10-19T22:31:35.974-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:20:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T22:31:35.975-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='standard_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:10:00+00:00', try_number=2, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-10-19T22:31:35.975-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'standard_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:10:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/standard_etl_dag.py'][0m
[[34m2025-10-19T22:31:35.977-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:20:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T22:31:35.977-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'standard_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:10:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/standard_etl_dag.py'][0m
[[34m2025-10-19T22:31:37.082-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='standard_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:10:00+00:00', try_number=2, map_index=-1)[0m
[[34m2025-10-19T22:31:37.083-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:20:00+00:00', try_number=2, map_index=-1)[0m
[[34m2025-10-19T22:31:37.087-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=standard_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:10:00+00:00, map_index=-1, run_start_date=2025-10-20 03:29:29.909128+00:00, run_end_date=2025-10-20 03:29:35.789767+00:00, run_duration=5.880639, state=queued, executor_state=failed, try_number=2, max_tries=1, job_id=15, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2025-10-20 03:31:35.973428+00:00, queued_by_job_id=5, pid=18251[0m
[[34m2025-10-19T22:31:37.087-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:10:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:31:37.091-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:10:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:31:37.097-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=standard_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:10:00+00:00, execution_date=20251020T031000, start_date=20251020T032929, end_date=20251020T033137[0m
[[34m2025-10-19T22:31:37.105-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:20:00+00:00, map_index=-1, run_start_date=2025-10-20 03:29:29.922351+00:00, run_end_date=2025-10-20 03:29:35.789777+00:00, run_duration=5.867426, state=queued, executor_state=failed, try_number=2, max_tries=1, job_id=17, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-10-20 03:31:35.973428+00:00, queued_by_job_id=5, pid=18255[0m
[[34m2025-10-19T22:31:37.106-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:20:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:31:37.108-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:20:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:31:37.112-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:20:00+00:00, execution_date=20251020T032000, start_date=20251020T032929, end_date=20251020T033137[0m
[[34m2025-10-19T22:31:40.716-0500[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun standard_etl_dag @ 2025-10-20 03:10:00+00:00: scheduled__2025-10-20T03:10:00+00:00, state:running, queued_at: 2025-10-20 03:29:28.319788+00:00. externally triggered: False> failed[0m
[[34m2025-10-19T22:31:40.717-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=standard_etl_dag, execution_date=2025-10-20 03:10:00+00:00, run_id=scheduled__2025-10-20T03:10:00+00:00, run_start_date=2025-10-20 03:29:28.329821+00:00, run_end_date=2025-10-20 03:31:40.717513+00:00, run_duration=132.387692, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2025-10-20 03:10:00+00:00, data_interval_end=2025-10-20 03:20:00+00:00, dag_hash=9421653b83515a7f1a16a29fc8f09179[0m
[[34m2025-10-19T22:31:40.720-0500[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG standard_etl_dag is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-10-19T22:31:41.013-0500[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun working_etl_dag @ 2025-10-20 03:20:00+00:00: scheduled__2025-10-20T03:20:00+00:00, state:running, queued_at: 2025-10-20 03:29:29.391301+00:00. externally triggered: False> failed[0m
[[34m2025-10-19T22:31:41.013-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=working_etl_dag, execution_date=2025-10-20 03:20:00+00:00, run_id=scheduled__2025-10-20T03:20:00+00:00, run_start_date=2025-10-20 03:29:29.406478+00:00, run_end_date=2025-10-20 03:31:41.013902+00:00, run_duration=131.607424, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2025-10-20 03:20:00+00:00, data_interval_end=2025-10-20 03:25:00+00:00, dag_hash=277a4a4ca5035f6f5dc345a5179a577e[0m
[[34m2025-10-19T22:31:41.015-0500[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG working_etl_dag is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-10-19T22:31:41.029-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: standard_etl_dag.start_pipeline manual__2025-10-20T03:29:27.701879+00:00 [scheduled]>[0m
[[34m2025-10-19T22:31:41.030-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG standard_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:31:41.030-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: standard_etl_dag.start_pipeline manual__2025-10-20T03:29:27.701879+00:00 [scheduled]>[0m
[[34m2025-10-19T22:31:41.032-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='standard_etl_dag', task_id='start_pipeline', run_id='manual__2025-10-20T03:29:27.701879+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-10-19T22:31:41.032-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'standard_etl_dag', 'start_pipeline', 'manual__2025-10-20T03:29:27.701879+00:00', '--local', '--subdir', 'DAGS_FOLDER/standard_etl_dag.py'][0m
[[34m2025-10-19T22:31:41.034-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'standard_etl_dag', 'start_pipeline', 'manual__2025-10-20T03:29:27.701879+00:00', '--local', '--subdir', 'DAGS_FOLDER/standard_etl_dag.py'][0m
[[34m2025-10-19T22:31:41.080-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: working_etl_dag.start_pipeline manual__2025-10-20T03:29:29.111326+00:00 [scheduled]>[0m
[[34m2025-10-19T22:31:41.080-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG working_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:31:41.081-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: working_etl_dag.start_pipeline manual__2025-10-20T03:29:29.111326+00:00 [scheduled]>[0m
[[34m2025-10-19T22:31:41.082-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='manual__2025-10-20T03:29:29.111326+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
[[34m2025-10-19T22:31:41.082-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'manual__2025-10-20T03:29:29.111326+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T22:31:41.083-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'manual__2025-10-20T03:29:29.111326+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T22:31:42.171-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='standard_etl_dag', task_id='start_pipeline', run_id='manual__2025-10-20T03:29:27.701879+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T22:31:42.171-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='manual__2025-10-20T03:29:29.111326+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T22:31:42.173-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=standard_etl_dag, task_id=start_pipeline, run_id=manual__2025-10-20T03:29:27.701879+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2025-10-20 03:31:41.031040+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:31:42.174-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: standard_etl_dag.start_pipeline manual__2025-10-20T03:29:27.701879+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:31:42.176-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: standard_etl_dag.start_pipeline manual__2025-10-20T03:29:27.701879+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:31:42.180-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=standard_etl_dag, task_id=start_pipeline, run_id=manual__2025-10-20T03:29:27.701879+00:00, execution_date=20251020T032927, start_date=, end_date=20251020T033142[0m
[[34m2025-10-19T22:31:42.183-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=working_etl_dag, task_id=start_pipeline, run_id=manual__2025-10-20T03:29:29.111326+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-10-20 03:31:41.081506+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:31:42.183-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline manual__2025-10-20T03:29:29.111326+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:31:42.186-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline manual__2025-10-20T03:29:29.111326+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:31:42.189-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=working_etl_dag, task_id=start_pipeline, run_id=manual__2025-10-20T03:29:29.111326+00:00, execution_date=20251020T032929, start_date=, end_date=20251020T033142[0m
[[34m2025-10-19T22:33:43.206-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: working_etl_dag.start_pipeline manual__2025-10-20T03:29:29.111326+00:00 [scheduled]>
	<TaskInstance: standard_etl_dag.start_pipeline manual__2025-10-20T03:29:27.701879+00:00 [scheduled]>[0m
[[34m2025-10-19T22:33:43.208-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG working_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:33:43.208-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG standard_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:33:43.208-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: working_etl_dag.start_pipeline manual__2025-10-20T03:29:29.111326+00:00 [scheduled]>
	<TaskInstance: standard_etl_dag.start_pipeline manual__2025-10-20T03:29:27.701879+00:00 [scheduled]>[0m
[[34m2025-10-19T22:33:43.210-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='manual__2025-10-20T03:29:29.111326+00:00', try_number=2, map_index=-1) to executor with priority 5 and queue default[0m
[[34m2025-10-19T22:33:43.210-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'manual__2025-10-20T03:29:29.111326+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T22:33:43.211-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='standard_etl_dag', task_id='start_pipeline', run_id='manual__2025-10-20T03:29:27.701879+00:00', try_number=2, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-10-19T22:33:43.211-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'standard_etl_dag', 'start_pipeline', 'manual__2025-10-20T03:29:27.701879+00:00', '--local', '--subdir', 'DAGS_FOLDER/standard_etl_dag.py'][0m
[[34m2025-10-19T22:33:43.213-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'manual__2025-10-20T03:29:29.111326+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T22:33:43.214-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'standard_etl_dag', 'start_pipeline', 'manual__2025-10-20T03:29:27.701879+00:00', '--local', '--subdir', 'DAGS_FOLDER/standard_etl_dag.py'][0m
[[34m2025-10-19T22:33:44.326-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='manual__2025-10-20T03:29:29.111326+00:00', try_number=2, map_index=-1)[0m
[[34m2025-10-19T22:33:44.326-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='standard_etl_dag', task_id='start_pipeline', run_id='manual__2025-10-20T03:29:27.701879+00:00', try_number=2, map_index=-1)[0m
[[34m2025-10-19T22:33:44.328-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=standard_etl_dag, task_id=start_pipeline, run_id=manual__2025-10-20T03:29:27.701879+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 03:31:42.178038+00:00, run_duration=None, state=queued, executor_state=failed, try_number=2, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2025-10-20 03:33:43.209375+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:33:44.329-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: standard_etl_dag.start_pipeline manual__2025-10-20T03:29:27.701879+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:33:44.333-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: standard_etl_dag.start_pipeline manual__2025-10-20T03:29:27.701879+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:33:44.337-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=standard_etl_dag, task_id=start_pipeline, run_id=manual__2025-10-20T03:29:27.701879+00:00, execution_date=20251020T032927, start_date=, end_date=20251020T033344[0m
[[34m2025-10-19T22:33:44.342-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=working_etl_dag, task_id=start_pipeline, run_id=manual__2025-10-20T03:29:29.111326+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 03:31:42.187326+00:00, run_duration=None, state=queued, executor_state=failed, try_number=2, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-10-20 03:33:43.209375+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:33:44.342-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline manual__2025-10-20T03:29:29.111326+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:33:44.345-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline manual__2025-10-20T03:29:29.111326+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:33:44.349-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=working_etl_dag, task_id=start_pipeline, run_id=manual__2025-10-20T03:29:29.111326+00:00, execution_date=20251020T032929, start_date=, end_date=20251020T033344[0m
[[34m2025-10-19T22:33:48.240-0500[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun standard_etl_dag @ 2025-10-20 03:29:27.701879+00:00: manual__2025-10-20T03:29:27.701879+00:00, state:running, queued_at: 2025-10-20 03:29:27.729016+00:00. externally triggered: True> failed[0m
[[34m2025-10-19T22:33:48.241-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=standard_etl_dag, execution_date=2025-10-20 03:29:27.701879+00:00, run_id=manual__2025-10-20T03:29:27.701879+00:00, run_start_date=2025-10-20 03:31:41.000451+00:00, run_end_date=2025-10-20 03:33:48.241007+00:00, run_duration=127.240556, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-10-20 03:19:27.701879+00:00, data_interval_end=2025-10-20 03:29:27.701879+00:00, dag_hash=9421653b83515a7f1a16a29fc8f09179[0m
[[34m2025-10-19T22:33:49.280-0500[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun working_etl_dag @ 2025-10-20 03:29:29.111326+00:00: manual__2025-10-20T03:29:29.111326+00:00, state:running, queued_at: 2025-10-20 03:29:29.122218+00:00. externally triggered: True> failed[0m
[[34m2025-10-19T22:33:49.280-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=working_etl_dag, execution_date=2025-10-20 03:29:29.111326+00:00, run_id=manual__2025-10-20T03:29:29.111326+00:00, run_start_date=2025-10-20 03:31:41.053989+00:00, run_end_date=2025-10-20 03:33:49.280838+00:00, run_duration=128.226849, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-10-20 03:24:29.111326+00:00, data_interval_end=2025-10-20 03:29:29.111326+00:00, dag_hash=277a4a4ca5035f6f5dc345a5179a577e[0m
[[34m2025-10-19T22:33:50.513-0500[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-19T22:34:17.361-0500[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG working_etl_dag is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-10-19T22:34:17.366-0500[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG standard_etl_dag is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-10-19T22:34:17.400-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:25:00+00:00 [scheduled]>
	<TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:20:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:34:17.401-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG working_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:34:17.401-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG standard_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:34:17.401-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:25:00+00:00 [scheduled]>
	<TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:20:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:34:17.403-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:25:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
[[34m2025-10-19T22:34:17.403-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:25:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T22:34:17.403-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='standard_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:20:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-10-19T22:34:17.404-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'standard_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:20:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/standard_etl_dag.py'][0m
[[34m2025-10-19T22:34:17.405-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'standard_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:20:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/standard_etl_dag.py'][0m
[[34m2025-10-19T22:34:17.406-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:25:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T22:34:18.510-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='standard_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:20:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T22:34:18.511-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:25:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T22:34:18.514-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:25:00+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-10-20 03:34:17.402156+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:34:18.514-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:25:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:34:18.518-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:25:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:34:18.521-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:25:00+00:00, execution_date=20251020T032500, start_date=, end_date=20251020T033418[0m
[[34m2025-10-19T22:34:18.527-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=standard_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:20:00+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2025-10-20 03:34:17.402156+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:34:18.527-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:20:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:34:18.530-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:20:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:34:18.533-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=standard_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:20:00+00:00, execution_date=20251020T032000, start_date=, end_date=20251020T033418[0m
[[34m2025-10-19T22:34:35.658-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: external_data_ingestion.update_ingestion_metrics scheduled__2025-10-20T02:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:34:35.658-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:34:35.658-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: external_data_ingestion.update_ingestion_metrics scheduled__2025-10-20T02:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:34:35.659-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='update_ingestion_metrics', run_id='scheduled__2025-10-20T02:00:00+00:00', try_number=2, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-10-19T22:34:35.659-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'update_ingestion_metrics', 'scheduled__2025-10-20T02:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:34:35.660-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'update_ingestion_metrics', 'scheduled__2025-10-20T02:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:34:36.419-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='update_ingestion_metrics', run_id='scheduled__2025-10-20T02:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2025-10-19T22:34:36.421-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=update_ingestion_metrics, run_id=scheduled__2025-10-20T02:00:00+00:00, map_index=-1, run_start_date=2025-10-20 03:29:34.530156+00:00, run_end_date=2025-10-20 03:29:34.650722+00:00, run_duration=0.120566, state=queued, executor_state=failed, try_number=2, max_tries=2, job_id=20, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-10-20 03:34:35.658999+00:00, queued_by_job_id=5, pid=18359[0m
[[34m2025-10-19T22:34:36.421-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.update_ingestion_metrics scheduled__2025-10-20T02:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:34:36.423-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.update_ingestion_metrics scheduled__2025-10-20T02:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:34:36.428-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=external_data_ingestion, task_id=update_ingestion_metrics, run_id=scheduled__2025-10-20T02:00:00+00:00, execution_date=20251020T020000, start_date=20251020T032934, end_date=20251020T033436[0m
[[34m2025-10-19T22:36:18.561-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:25:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:36:18.563-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG working_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:36:18.564-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:25:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:36:18.565-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:25:00+00:00', try_number=2, map_index=-1) to executor with priority 5 and queue default[0m
[[34m2025-10-19T22:36:18.566-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:25:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T22:36:18.567-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:25:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T22:36:18.651-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:20:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:36:18.652-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG standard_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:36:18.654-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:20:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:36:18.658-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='standard_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:20:00+00:00', try_number=2, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-10-19T22:36:18.659-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'standard_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:20:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/standard_etl_dag.py'][0m
[[34m2025-10-19T22:36:18.687-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'standard_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:20:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/standard_etl_dag.py'][0m
[[34m2025-10-19T22:36:19.950-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:25:00+00:00', try_number=2, map_index=-1)[0m
[[34m2025-10-19T22:36:19.951-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='standard_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:20:00+00:00', try_number=2, map_index=-1)[0m
[[34m2025-10-19T22:36:19.954-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:25:00+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 03:34:18.519778+00:00, run_duration=None, state=queued, executor_state=failed, try_number=2, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-10-20 03:36:18.564717+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:36:19.955-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:25:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:36:19.962-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:25:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:36:19.968-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:25:00+00:00, execution_date=20251020T032500, start_date=, end_date=20251020T033619[0m
[[34m2025-10-19T22:36:19.975-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=standard_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:20:00+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 03:34:18.531381+00:00, run_duration=None, state=queued, executor_state=failed, try_number=2, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2025-10-20 03:36:18.656416+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:36:19.975-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:20:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:36:19.980-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:20:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:36:19.985-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=standard_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:20:00+00:00, execution_date=20251020T032000, start_date=, end_date=20251020T033619[0m
[[34m2025-10-19T22:36:23.592-0500[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun standard_etl_dag @ 2025-10-20 03:20:00+00:00: scheduled__2025-10-20T03:20:00+00:00, state:running, queued_at: 2025-10-20 03:34:17.363002+00:00. externally triggered: False> failed[0m
[[34m2025-10-19T22:36:23.592-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=standard_etl_dag, execution_date=2025-10-20 03:20:00+00:00, run_id=scheduled__2025-10-20T03:20:00+00:00, run_start_date=2025-10-20 03:34:17.370820+00:00, run_end_date=2025-10-20 03:36:23.592819+00:00, run_duration=126.221999, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2025-10-20 03:20:00+00:00, data_interval_end=2025-10-20 03:30:00+00:00, dag_hash=9421653b83515a7f1a16a29fc8f09179[0m
[[34m2025-10-19T22:36:23.595-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for standard_etl_dag to 2025-10-20 03:30:00+00:00, run_after=2025-10-20 03:40:00+00:00[0m
[[34m2025-10-19T22:36:24.525-0500[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun working_etl_dag @ 2025-10-20 03:25:00+00:00: scheduled__2025-10-20T03:25:00+00:00, state:running, queued_at: 2025-10-20 03:34:17.354789+00:00. externally triggered: False> failed[0m
[[34m2025-10-19T22:36:24.525-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=working_etl_dag, execution_date=2025-10-20 03:25:00+00:00, run_id=scheduled__2025-10-20T03:25:00+00:00, run_start_date=2025-10-20 03:34:17.371264+00:00, run_end_date=2025-10-20 03:36:24.525763+00:00, run_duration=127.154499, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2025-10-20 03:25:00+00:00, data_interval_end=2025-10-20 03:30:00+00:00, dag_hash=277a4a4ca5035f6f5dc345a5179a577e[0m
[[34m2025-10-19T22:36:24.527-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for working_etl_dag to 2025-10-20 03:30:00+00:00, run_after=2025-10-20 03:35:00+00:00[0m
[[34m2025-10-19T22:36:25.566-0500[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG working_etl_dag is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-10-19T22:36:25.593-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:30:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:36:25.593-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG working_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:36:25.594-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:30:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:36:25.594-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:30:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
[[34m2025-10-19T22:36:25.595-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:30:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T22:36:25.596-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:30:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T22:36:26.672-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:30:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T22:36:26.674-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:30:00+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-10-20 03:36:25.594405+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:36:26.675-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:30:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:36:26.678-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:30:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:36:26.681-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:30:00+00:00, execution_date=20251020T033000, start_date=, end_date=20251020T033626[0m
[[34m2025-10-19T22:38:27.017-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:30:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:38:27.019-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG working_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:38:27.019-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:30:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:38:27.021-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:30:00+00:00', try_number=2, map_index=-1) to executor with priority 5 and queue default[0m
[[34m2025-10-19T22:38:27.021-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:30:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T22:38:27.023-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:30:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T22:38:27.717-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:30:00+00:00', try_number=2, map_index=-1)[0m
[[34m2025-10-19T22:38:27.720-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:30:00+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 03:36:26.679402+00:00, run_duration=None, state=queued, executor_state=failed, try_number=2, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-10-20 03:38:27.020313+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:38:27.721-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:30:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:38:27.726-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:30:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:38:27.731-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:30:00+00:00, execution_date=20251020T033000, start_date=, end_date=20251020T033827[0m
[[34m2025-10-19T22:38:32.916-0500[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun working_etl_dag @ 2025-10-20 03:30:00+00:00: scheduled__2025-10-20T03:30:00+00:00, state:running, queued_at: 2025-10-20 03:36:25.562621+00:00. externally triggered: False> failed[0m
[[34m2025-10-19T22:38:32.916-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=working_etl_dag, execution_date=2025-10-20 03:30:00+00:00, run_id=scheduled__2025-10-20T03:30:00+00:00, run_start_date=2025-10-20 03:36:25.570101+00:00, run_end_date=2025-10-20 03:38:32.916740+00:00, run_duration=127.346639, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2025-10-20 03:30:00+00:00, data_interval_end=2025-10-20 03:35:00+00:00, dag_hash=277a4a4ca5035f6f5dc345a5179a577e[0m
[[34m2025-10-19T22:38:32.918-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for working_etl_dag to 2025-10-20 03:35:00+00:00, run_after=2025-10-20 03:40:00+00:00[0m
[[34m2025-10-19T22:38:50.553-0500[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-19T22:39:37.429-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: external_data_ingestion.update_ingestion_metrics scheduled__2025-10-20T02:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:39:37.430-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:39:37.431-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: external_data_ingestion.update_ingestion_metrics scheduled__2025-10-20T02:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:39:37.432-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='update_ingestion_metrics', run_id='scheduled__2025-10-20T02:00:00+00:00', try_number=3, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-10-19T22:39:37.432-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'update_ingestion_metrics', 'scheduled__2025-10-20T02:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:39:37.433-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'update_ingestion_metrics', 'scheduled__2025-10-20T02:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:39:37.983-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='update_ingestion_metrics', run_id='scheduled__2025-10-20T02:00:00+00:00', try_number=3, map_index=-1)[0m
[[34m2025-10-19T22:39:37.985-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=update_ingestion_metrics, run_id=scheduled__2025-10-20T02:00:00+00:00, map_index=-1, run_start_date=2025-10-20 03:29:34.530156+00:00, run_end_date=2025-10-20 03:34:36.424915+00:00, run_duration=301.894759, state=queued, executor_state=failed, try_number=3, max_tries=2, job_id=20, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-10-20 03:39:37.431519+00:00, queued_by_job_id=5, pid=18359[0m
[[34m2025-10-19T22:39:37.986-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.update_ingestion_metrics scheduled__2025-10-20T02:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:39:37.989-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.update_ingestion_metrics scheduled__2025-10-20T02:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:39:37.993-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=external_data_ingestion, task_id=update_ingestion_metrics, run_id=scheduled__2025-10-20T02:00:00+00:00, execution_date=20251020T020000, start_date=20251020T032934, end_date=20251020T033937[0m
[[34m2025-10-19T22:39:38.006-0500[0m] {[34mconfiguration.py:[0m1053} WARNING[0m - section/key [smtp/smtp_user] not found in config[0m
[[34m2025-10-19T22:39:38.006-0500[0m] {[34memail.py:[0m271} INFO[0m - Email alerting: attempt 1[0m
[[34m2025-10-19T22:39:38.008-0500[0m] {[34mconfiguration.py:[0m1053} WARNING[0m - section/key [smtp/smtp_user] not found in config[0m
[[34m2025-10-19T22:39:38.008-0500[0m] {[34memail.py:[0m271} INFO[0m - Email alerting: attempt 1[0m
[[34m2025-10-19T22:39:38.009-0500[0m] {[34mtaskinstance.py:[0m879} ERROR[0m - Failed to send email to: ['data-team@company.com'][0m
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1063, in _email_alert
    send_email(task.email, subject, html_content)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 877, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 3163, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1065, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 61] Connection refused
[[34m2025-10-19T22:39:39.039-0500[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun external_data_ingestion @ 2025-10-20 02:00:00+00:00: scheduled__2025-10-20T02:00:00+00:00, state:running, queued_at: 2025-10-20 03:29:26.104117+00:00. externally triggered: False> failed[0m
[[34m2025-10-19T22:39:39.040-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=external_data_ingestion, execution_date=2025-10-20 02:00:00+00:00, run_id=scheduled__2025-10-20T02:00:00+00:00, run_start_date=2025-10-20 03:29:26.113515+00:00, run_end_date=2025-10-20 03:39:39.040344+00:00, run_duration=612.926829, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2025-10-20 02:00:00+00:00, data_interval_end=2025-10-20 03:00:00+00:00, dag_hash=fef5ae2a608c839f01e36de13cff2f9b[0m
[[34m2025-10-19T22:39:39.041-0500[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG external_data_ingestion is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-10-19T22:39:40.399-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: external_data_ingestion.ingest_hubspot_data manual__2025-10-20T03:29:25.390413+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_snowflake_data manual__2025-10-20T03:29:25.390413+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_redshift_data manual__2025-10-20T03:29:25.390413+00:00 [scheduled]>[0m
[[34m2025-10-19T22:39:40.399-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:39:40.400-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 1/4 running and queued tasks[0m
[[34m2025-10-19T22:39:40.400-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 2/4 running and queued tasks[0m
[[34m2025-10-19T22:39:40.400-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: external_data_ingestion.ingest_hubspot_data manual__2025-10-20T03:29:25.390413+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_snowflake_data manual__2025-10-20T03:29:25.390413+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_redshift_data manual__2025-10-20T03:29:25.390413+00:00 [scheduled]>[0m
[[34m2025-10-19T22:39:40.401-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_hubspot_data', run_id='manual__2025-10-20T03:29:25.390413+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T22:39:40.401-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_hubspot_data', 'manual__2025-10-20T03:29:25.390413+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:39:40.402-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_snowflake_data', run_id='manual__2025-10-20T03:29:25.390413+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T22:39:40.402-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_snowflake_data', 'manual__2025-10-20T03:29:25.390413+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:39:40.402-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_redshift_data', run_id='manual__2025-10-20T03:29:25.390413+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T22:39:40.402-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_redshift_data', 'manual__2025-10-20T03:29:25.390413+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:39:40.403-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_redshift_data', 'manual__2025-10-20T03:29:25.390413+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:39:40.404-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_hubspot_data', 'manual__2025-10-20T03:29:25.390413+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:39:40.404-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_snowflake_data', 'manual__2025-10-20T03:29:25.390413+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:39:41.483-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_redshift_data', run_id='manual__2025-10-20T03:29:25.390413+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T22:39:41.484-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_hubspot_data', run_id='manual__2025-10-20T03:29:25.390413+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T22:39:41.484-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_snowflake_data', run_id='manual__2025-10-20T03:29:25.390413+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T22:39:41.486-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_hubspot_data, run_id=manual__2025-10-20T03:29:25.390413+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=2, job_id=None, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 03:39:40.401031+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:39:41.487-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_hubspot_data manual__2025-10-20T03:29:25.390413+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:39:41.490-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_hubspot_data manual__2025-10-20T03:29:25.390413+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:39:41.493-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=external_data_ingestion, task_id=ingest_hubspot_data, run_id=manual__2025-10-20T03:29:25.390413+00:00, execution_date=20251020T032925, start_date=, end_date=20251020T033941[0m
[[34m2025-10-19T22:39:41.497-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_snowflake_data, run_id=manual__2025-10-20T03:29:25.390413+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=2, job_id=None, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 03:39:40.401031+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:39:41.497-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_snowflake_data manual__2025-10-20T03:29:25.390413+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:39:41.500-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_snowflake_data manual__2025-10-20T03:29:25.390413+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:39:41.504-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=external_data_ingestion, task_id=ingest_snowflake_data, run_id=manual__2025-10-20T03:29:25.390413+00:00, execution_date=20251020T032925, start_date=, end_date=20251020T033941[0m
[[34m2025-10-19T22:39:41.507-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_redshift_data, run_id=manual__2025-10-20T03:29:25.390413+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=2, job_id=None, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 03:39:40.401031+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:39:41.507-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_redshift_data manual__2025-10-20T03:29:25.390413+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:39:41.510-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_redshift_data manual__2025-10-20T03:29:25.390413+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:39:41.513-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=external_data_ingestion, task_id=ingest_redshift_data, run_id=manual__2025-10-20T03:29:25.390413+00:00, execution_date=20251020T032925, start_date=, end_date=20251020T033941[0m
[[34m2025-10-19T22:39:42.553-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: external_data_ingestion.ingest_kafka_streams manual__2025-10-20T03:29:25.390413+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_fx_rates manual__2025-10-20T03:29:25.390413+00:00 [scheduled]>[0m
[[34m2025-10-19T22:39:42.553-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:39:42.554-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 1/4 running and queued tasks[0m
[[34m2025-10-19T22:39:42.554-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: external_data_ingestion.ingest_kafka_streams manual__2025-10-20T03:29:25.390413+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_fx_rates manual__2025-10-20T03:29:25.390413+00:00 [scheduled]>[0m
[[34m2025-10-19T22:39:42.555-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_kafka_streams', run_id='manual__2025-10-20T03:29:25.390413+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T22:39:42.555-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_kafka_streams', 'manual__2025-10-20T03:29:25.390413+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:39:42.556-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_fx_rates', run_id='manual__2025-10-20T03:29:25.390413+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T22:39:42.556-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_fx_rates', 'manual__2025-10-20T03:29:25.390413+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:39:42.557-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_kafka_streams', 'manual__2025-10-20T03:29:25.390413+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:39:42.557-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_fx_rates', 'manual__2025-10-20T03:29:25.390413+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:39:43.633-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_kafka_streams', run_id='manual__2025-10-20T03:29:25.390413+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T22:39:43.634-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_fx_rates', run_id='manual__2025-10-20T03:29:25.390413+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T22:39:43.636-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_kafka_streams, run_id=manual__2025-10-20T03:29:25.390413+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=2, job_id=None, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 03:39:42.554750+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:39:43.636-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_kafka_streams manual__2025-10-20T03:29:25.390413+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:39:43.639-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_kafka_streams manual__2025-10-20T03:29:25.390413+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:39:43.642-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=external_data_ingestion, task_id=ingest_kafka_streams, run_id=manual__2025-10-20T03:29:25.390413+00:00, execution_date=20251020T032925, start_date=, end_date=20251020T033943[0m
[[34m2025-10-19T22:39:43.645-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_fx_rates, run_id=manual__2025-10-20T03:29:25.390413+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=2, job_id=None, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 03:39:42.554750+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:39:43.645-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_fx_rates manual__2025-10-20T03:29:25.390413+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:39:43.648-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_fx_rates manual__2025-10-20T03:29:25.390413+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:39:43.651-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=external_data_ingestion, task_id=ingest_fx_rates, run_id=manual__2025-10-20T03:29:25.390413+00:00, execution_date=20251020T032925, start_date=, end_date=20251020T033943[0m
[[34m2025-10-19T22:40:00.148-0500[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG working_etl_dag is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-10-19T22:40:00.152-0500[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG standard_etl_dag is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-10-19T22:40:00.184-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:35:00+00:00 [scheduled]>
	<TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:30:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:40:00.184-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG working_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:40:00.184-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG standard_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:40:00.184-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:35:00+00:00 [scheduled]>
	<TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:30:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:40:00.186-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:35:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
[[34m2025-10-19T22:40:00.186-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:35:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T22:40:00.186-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='standard_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:30:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-10-19T22:40:00.186-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'standard_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:30:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/standard_etl_dag.py'][0m
[[34m2025-10-19T22:40:00.187-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:35:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T22:40:00.187-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'standard_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:30:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/standard_etl_dag.py'][0m
[[34m2025-10-19T22:40:00.571-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:35:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T22:40:00.571-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='standard_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:30:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T22:40:00.573-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:35:00+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-10-20 03:40:00.185312+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:40:00.573-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:35:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:40:00.576-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:35:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:40:00.579-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:35:00+00:00, execution_date=20251020T033500, start_date=, end_date=20251020T034000[0m
[[34m2025-10-19T22:40:00.583-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=standard_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:30:00+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2025-10-20 03:40:00.185312+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:40:00.583-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:30:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:40:00.585-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:30:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:40:00.588-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=standard_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:30:00+00:00, execution_date=20251020T033000, start_date=, end_date=20251020T034000[0m
[[34m2025-10-19T22:42:00.731-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:35:00+00:00 [scheduled]>
	<TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:30:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:42:00.732-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG working_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:42:00.732-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG standard_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:42:00.732-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:35:00+00:00 [scheduled]>
	<TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:30:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:42:00.733-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:35:00+00:00', try_number=2, map_index=-1) to executor with priority 5 and queue default[0m
[[34m2025-10-19T22:42:00.734-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:35:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T22:42:00.734-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='standard_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:30:00+00:00', try_number=2, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-10-19T22:42:00.734-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'standard_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:30:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/standard_etl_dag.py'][0m
[[34m2025-10-19T22:42:00.737-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:35:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T22:42:00.737-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'standard_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:30:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/standard_etl_dag.py'][0m
[[34m2025-10-19T22:42:01.722-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='standard_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:30:00+00:00', try_number=2, map_index=-1)[0m
[[34m2025-10-19T22:42:01.722-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:35:00+00:00', try_number=2, map_index=-1)[0m
[[34m2025-10-19T22:42:01.725-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:35:00+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 03:40:00.578126+00:00, run_duration=None, state=queued, executor_state=failed, try_number=2, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-10-20 03:42:00.733189+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:42:01.725-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:35:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:42:01.729-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:35:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:42:01.733-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:35:00+00:00, execution_date=20251020T033500, start_date=, end_date=20251020T034201[0m
[[34m2025-10-19T22:42:01.737-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=standard_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:30:00+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 03:40:00.586795+00:00, run_duration=None, state=queued, executor_state=failed, try_number=2, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2025-10-20 03:42:00.733189+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:42:01.738-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:30:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:42:01.740-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:30:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:42:01.743-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=standard_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:30:00+00:00, execution_date=20251020T033000, start_date=, end_date=20251020T034201[0m
[[34m2025-10-19T22:42:05.897-0500[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun standard_etl_dag @ 2025-10-20 03:30:00+00:00: scheduled__2025-10-20T03:30:00+00:00, state:running, queued_at: 2025-10-20 03:40:00.149423+00:00. externally triggered: False> failed[0m
[[34m2025-10-19T22:42:05.897-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=standard_etl_dag, execution_date=2025-10-20 03:30:00+00:00, run_id=scheduled__2025-10-20T03:30:00+00:00, run_start_date=2025-10-20 03:40:00.155847+00:00, run_end_date=2025-10-20 03:42:05.897638+00:00, run_duration=125.741791, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2025-10-20 03:30:00+00:00, data_interval_end=2025-10-20 03:40:00+00:00, dag_hash=9421653b83515a7f1a16a29fc8f09179[0m
[[34m2025-10-19T22:42:05.898-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for standard_etl_dag to 2025-10-20 03:40:00+00:00, run_after=2025-10-20 03:50:00+00:00[0m
[[34m2025-10-19T22:42:06.768-0500[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun working_etl_dag @ 2025-10-20 03:35:00+00:00: scheduled__2025-10-20T03:35:00+00:00, state:running, queued_at: 2025-10-20 03:40:00.144839+00:00. externally triggered: False> failed[0m
[[34m2025-10-19T22:42:06.769-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=working_etl_dag, execution_date=2025-10-20 03:35:00+00:00, run_id=scheduled__2025-10-20T03:35:00+00:00, run_start_date=2025-10-20 03:40:00.156144+00:00, run_end_date=2025-10-20 03:42:06.769277+00:00, run_duration=126.613133, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2025-10-20 03:35:00+00:00, data_interval_end=2025-10-20 03:40:00+00:00, dag_hash=277a4a4ca5035f6f5dc345a5179a577e[0m
[[34m2025-10-19T22:42:06.770-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for working_etl_dag to 2025-10-20 03:40:00+00:00, run_after=2025-10-20 03:45:00+00:00[0m
[[34m2025-10-19T22:42:31.967-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: delta_lake_maintenance.perform_delta_lake_maintenance scheduled__2025-10-19T00:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:42:31.968-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG delta_lake_maintenance has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:42:31.969-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: delta_lake_maintenance.perform_delta_lake_maintenance scheduled__2025-10-19T00:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:42:31.970-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='delta_lake_maintenance', task_id='perform_delta_lake_maintenance', run_id='scheduled__2025-10-19T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T22:42:31.970-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'delta_lake_maintenance', 'perform_delta_lake_maintenance', 'scheduled__2025-10-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/delta_lake_maintenance_dag.py'][0m
[[34m2025-10-19T22:42:31.971-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'delta_lake_maintenance', 'perform_delta_lake_maintenance', 'scheduled__2025-10-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/delta_lake_maintenance_dag.py'][0m
[[34m2025-10-19T22:42:33.040-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='delta_lake_maintenance', task_id='perform_delta_lake_maintenance', run_id='scheduled__2025-10-19T00:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2025-10-19T22:42:33.041-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=delta_lake_maintenance, task_id=perform_delta_lake_maintenance, run_id=scheduled__2025-10-19T00:00:00+00:00, map_index=-1, run_start_date=2025-10-20 03:29:24.605700+00:00, run_end_date=2025-10-20 03:29:25.119596+00:00, run_duration=0.513896, state=queued, executor_state=failed, try_number=2, max_tries=2, job_id=11, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 03:42:31.969494+00:00, queued_by_job_id=5, pid=18129[0m
[[34m2025-10-19T22:42:33.042-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: delta_lake_maintenance.perform_delta_lake_maintenance scheduled__2025-10-19T00:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:42:33.044-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: delta_lake_maintenance.perform_delta_lake_maintenance scheduled__2025-10-19T00:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:42:33.047-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=delta_lake_maintenance, task_id=perform_delta_lake_maintenance, run_id=scheduled__2025-10-19T00:00:00+00:00, execution_date=20251019T000000, start_date=20251020T032924, end_date=20251020T034233[0m
[[34m2025-10-19T22:43:50.596-0500[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-19T22:44:41.933-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: external_data_ingestion.ingest_hubspot_data manual__2025-10-20T03:29:25.390413+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_snowflake_data manual__2025-10-20T03:29:25.390413+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_redshift_data manual__2025-10-20T03:29:25.390413+00:00 [scheduled]>[0m
[[34m2025-10-19T22:44:41.934-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:44:41.935-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 1/4 running and queued tasks[0m
[[34m2025-10-19T22:44:41.935-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 2/4 running and queued tasks[0m
[[34m2025-10-19T22:44:41.935-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: external_data_ingestion.ingest_hubspot_data manual__2025-10-20T03:29:25.390413+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_snowflake_data manual__2025-10-20T03:29:25.390413+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_redshift_data manual__2025-10-20T03:29:25.390413+00:00 [scheduled]>[0m
[[34m2025-10-19T22:44:41.936-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_hubspot_data', run_id='manual__2025-10-20T03:29:25.390413+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T22:44:41.936-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_hubspot_data', 'manual__2025-10-20T03:29:25.390413+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:44:41.937-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_snowflake_data', run_id='manual__2025-10-20T03:29:25.390413+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T22:44:41.937-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_snowflake_data', 'manual__2025-10-20T03:29:25.390413+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:44:41.937-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_redshift_data', run_id='manual__2025-10-20T03:29:25.390413+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T22:44:41.937-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_redshift_data', 'manual__2025-10-20T03:29:25.390413+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:44:41.939-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_hubspot_data', 'manual__2025-10-20T03:29:25.390413+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:44:41.939-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_snowflake_data', 'manual__2025-10-20T03:29:25.390413+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:44:41.939-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_redshift_data', 'manual__2025-10-20T03:29:25.390413+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:44:43.004-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_snowflake_data', run_id='manual__2025-10-20T03:29:25.390413+00:00', try_number=2, map_index=-1)[0m
[[34m2025-10-19T22:44:43.004-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_redshift_data', run_id='manual__2025-10-20T03:29:25.390413+00:00', try_number=2, map_index=-1)[0m
[[34m2025-10-19T22:44:43.004-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_hubspot_data', run_id='manual__2025-10-20T03:29:25.390413+00:00', try_number=2, map_index=-1)[0m
[[34m2025-10-19T22:44:43.007-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_hubspot_data, run_id=manual__2025-10-20T03:29:25.390413+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 03:39:41.491765+00:00, run_duration=None, state=queued, executor_state=failed, try_number=2, max_tries=2, job_id=None, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 03:44:41.935921+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:44:43.007-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_hubspot_data manual__2025-10-20T03:29:25.390413+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:44:43.010-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_hubspot_data manual__2025-10-20T03:29:25.390413+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:44:43.014-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=external_data_ingestion, task_id=ingest_hubspot_data, run_id=manual__2025-10-20T03:29:25.390413+00:00, execution_date=20251020T032925, start_date=, end_date=20251020T034443[0m
[[34m2025-10-19T22:44:43.017-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_snowflake_data, run_id=manual__2025-10-20T03:29:25.390413+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 03:39:41.502270+00:00, run_duration=None, state=queued, executor_state=failed, try_number=2, max_tries=2, job_id=None, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 03:44:41.935921+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:44:43.017-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_snowflake_data manual__2025-10-20T03:29:25.390413+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:44:43.020-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_snowflake_data manual__2025-10-20T03:29:25.390413+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:44:43.023-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=external_data_ingestion, task_id=ingest_snowflake_data, run_id=manual__2025-10-20T03:29:25.390413+00:00, execution_date=20251020T032925, start_date=, end_date=20251020T034443[0m
[[34m2025-10-19T22:44:43.026-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_redshift_data, run_id=manual__2025-10-20T03:29:25.390413+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 03:39:41.511903+00:00, run_duration=None, state=queued, executor_state=failed, try_number=2, max_tries=2, job_id=None, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 03:44:41.935921+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:44:43.026-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_redshift_data manual__2025-10-20T03:29:25.390413+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:44:43.028-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_redshift_data manual__2025-10-20T03:29:25.390413+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:44:43.031-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=external_data_ingestion, task_id=ingest_redshift_data, run_id=manual__2025-10-20T03:29:25.390413+00:00, execution_date=20251020T032925, start_date=, end_date=20251020T034443[0m
[[34m2025-10-19T22:44:44.069-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: external_data_ingestion.ingest_kafka_streams manual__2025-10-20T03:29:25.390413+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_fx_rates manual__2025-10-20T03:29:25.390413+00:00 [scheduled]>[0m
[[34m2025-10-19T22:44:44.069-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:44:44.070-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 1/4 running and queued tasks[0m
[[34m2025-10-19T22:44:44.070-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: external_data_ingestion.ingest_kafka_streams manual__2025-10-20T03:29:25.390413+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_fx_rates manual__2025-10-20T03:29:25.390413+00:00 [scheduled]>[0m
[[34m2025-10-19T22:44:44.071-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_kafka_streams', run_id='manual__2025-10-20T03:29:25.390413+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T22:44:44.071-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_kafka_streams', 'manual__2025-10-20T03:29:25.390413+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:44:44.071-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_fx_rates', run_id='manual__2025-10-20T03:29:25.390413+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T22:44:44.072-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_fx_rates', 'manual__2025-10-20T03:29:25.390413+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:44:44.072-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_kafka_streams', 'manual__2025-10-20T03:29:25.390413+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:44:44.073-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_fx_rates', 'manual__2025-10-20T03:29:25.390413+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:44:45.137-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_fx_rates', run_id='manual__2025-10-20T03:29:25.390413+00:00', try_number=2, map_index=-1)[0m
[[34m2025-10-19T22:44:45.138-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_kafka_streams', run_id='manual__2025-10-20T03:29:25.390413+00:00', try_number=2, map_index=-1)[0m
[[34m2025-10-19T22:44:45.139-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_kafka_streams, run_id=manual__2025-10-20T03:29:25.390413+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 03:39:43.640658+00:00, run_duration=None, state=queued, executor_state=failed, try_number=2, max_tries=2, job_id=None, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 03:44:44.070678+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:44:45.140-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_kafka_streams manual__2025-10-20T03:29:25.390413+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:44:45.142-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_kafka_streams manual__2025-10-20T03:29:25.390413+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:44:45.145-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=external_data_ingestion, task_id=ingest_kafka_streams, run_id=manual__2025-10-20T03:29:25.390413+00:00, execution_date=20251020T032925, start_date=, end_date=20251020T034445[0m
[[34m2025-10-19T22:44:45.148-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_fx_rates, run_id=manual__2025-10-20T03:29:25.390413+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 03:39:43.649584+00:00, run_duration=None, state=queued, executor_state=failed, try_number=2, max_tries=2, job_id=None, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 03:44:44.070678+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:44:45.148-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_fx_rates manual__2025-10-20T03:29:25.390413+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:44:45.150-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_fx_rates manual__2025-10-20T03:29:25.390413+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:44:45.153-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=external_data_ingestion, task_id=ingest_fx_rates, run_id=manual__2025-10-20T03:29:25.390413+00:00, execution_date=20251020T032925, start_date=, end_date=20251020T034445[0m
[[34m2025-10-19T22:45:00.469-0500[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG working_etl_dag is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-10-19T22:45:00.508-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:40:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:45:00.510-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG working_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:45:00.511-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:40:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:45:00.513-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:40:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
[[34m2025-10-19T22:45:00.513-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:40:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T22:45:00.516-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:40:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T22:45:01.619-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:40:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T22:45:01.622-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:40:00+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-10-20 03:45:00.511952+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:45:01.622-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:40:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:45:01.630-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:40:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:45:01.634-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:40:00+00:00, execution_date=20251020T034000, start_date=, end_date=20251020T034501[0m
[[34m2025-10-19T22:47:02.104-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:40:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:47:02.105-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG working_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:47:02.105-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:40:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:47:02.106-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:40:00+00:00', try_number=2, map_index=-1) to executor with priority 5 and queue default[0m
[[34m2025-10-19T22:47:02.107-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:40:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T22:47:02.108-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:40:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T22:47:03.179-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:40:00+00:00', try_number=2, map_index=-1)[0m
[[34m2025-10-19T22:47:03.181-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:40:00+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 03:45:01.631947+00:00, run_duration=None, state=queued, executor_state=failed, try_number=2, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-10-20 03:47:02.106062+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:47:03.182-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:40:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:47:03.185-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:40:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:47:03.188-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:40:00+00:00, execution_date=20251020T034000, start_date=, end_date=20251020T034703[0m
[[34m2025-10-19T22:47:08.356-0500[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun working_etl_dag @ 2025-10-20 03:40:00+00:00: scheduled__2025-10-20T03:40:00+00:00, state:running, queued_at: 2025-10-20 03:45:00.463625+00:00. externally triggered: False> failed[0m
[[34m2025-10-19T22:47:08.356-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=working_etl_dag, execution_date=2025-10-20 03:40:00+00:00, run_id=scheduled__2025-10-20T03:40:00+00:00, run_start_date=2025-10-20 03:45:00.476318+00:00, run_end_date=2025-10-20 03:47:08.356795+00:00, run_duration=127.880477, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2025-10-20 03:40:00+00:00, data_interval_end=2025-10-20 03:45:00+00:00, dag_hash=277a4a4ca5035f6f5dc345a5179a577e[0m
[[34m2025-10-19T22:47:08.358-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for working_etl_dag to 2025-10-20 03:45:00+00:00, run_after=2025-10-20 03:50:00+00:00[0m
[[34m2025-10-19T22:48:50.642-0500[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-19T22:49:43.817-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: external_data_ingestion.ingest_hubspot_data manual__2025-10-20T03:29:25.390413+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_snowflake_data manual__2025-10-20T03:29:25.390413+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_redshift_data manual__2025-10-20T03:29:25.390413+00:00 [scheduled]>[0m
[[34m2025-10-19T22:49:43.819-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:49:43.820-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 1/4 running and queued tasks[0m
[[34m2025-10-19T22:49:43.821-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 2/4 running and queued tasks[0m
[[34m2025-10-19T22:49:43.826-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: external_data_ingestion.ingest_hubspot_data manual__2025-10-20T03:29:25.390413+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_snowflake_data manual__2025-10-20T03:29:25.390413+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_redshift_data manual__2025-10-20T03:29:25.390413+00:00 [scheduled]>[0m
[[34m2025-10-19T22:49:43.833-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_hubspot_data', run_id='manual__2025-10-20T03:29:25.390413+00:00', try_number=3, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T22:49:43.833-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_hubspot_data', 'manual__2025-10-20T03:29:25.390413+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:49:43.833-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_snowflake_data', run_id='manual__2025-10-20T03:29:25.390413+00:00', try_number=3, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T22:49:43.834-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_snowflake_data', 'manual__2025-10-20T03:29:25.390413+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:49:43.834-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_redshift_data', run_id='manual__2025-10-20T03:29:25.390413+00:00', try_number=3, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T22:49:43.834-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_redshift_data', 'manual__2025-10-20T03:29:25.390413+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:49:43.841-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_snowflake_data', 'manual__2025-10-20T03:29:25.390413+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:49:43.841-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_hubspot_data', 'manual__2025-10-20T03:29:25.390413+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:49:43.841-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_redshift_data', 'manual__2025-10-20T03:29:25.390413+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:49:44.979-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_hubspot_data', run_id='manual__2025-10-20T03:29:25.390413+00:00', try_number=3, map_index=-1)[0m
[[34m2025-10-19T22:49:44.980-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_snowflake_data', run_id='manual__2025-10-20T03:29:25.390413+00:00', try_number=3, map_index=-1)[0m
[[34m2025-10-19T22:49:44.980-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_redshift_data', run_id='manual__2025-10-20T03:29:25.390413+00:00', try_number=3, map_index=-1)[0m
[[34m2025-10-19T22:49:44.983-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_hubspot_data, run_id=manual__2025-10-20T03:29:25.390413+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 03:44:43.012205+00:00, run_duration=None, state=queued, executor_state=failed, try_number=3, max_tries=2, job_id=None, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 03:49:43.828738+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:49:44.983-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_hubspot_data manual__2025-10-20T03:29:25.390413+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:49:44.987-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_hubspot_data manual__2025-10-20T03:29:25.390413+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:49:44.991-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=external_data_ingestion, task_id=ingest_hubspot_data, run_id=manual__2025-10-20T03:29:25.390413+00:00, execution_date=20251020T032925, start_date=, end_date=20251020T034944[0m
[[34m2025-10-19T22:49:44.997-0500[0m] {[34mconfiguration.py:[0m1053} WARNING[0m - section/key [smtp/smtp_user] not found in config[0m
[[34m2025-10-19T22:49:44.998-0500[0m] {[34memail.py:[0m271} INFO[0m - Email alerting: attempt 1[0m
[[34m2025-10-19T22:49:45.000-0500[0m] {[34mconfiguration.py:[0m1053} WARNING[0m - section/key [smtp/smtp_user] not found in config[0m
[[34m2025-10-19T22:49:45.000-0500[0m] {[34memail.py:[0m271} INFO[0m - Email alerting: attempt 1[0m
[[34m2025-10-19T22:49:45.001-0500[0m] {[34mtaskinstance.py:[0m879} ERROR[0m - Failed to send email to: ['data-team@company.com'][0m
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1063, in _email_alert
    send_email(task.email, subject, html_content)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 877, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 3163, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1065, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 61] Connection refused
[[34m2025-10-19T22:49:45.006-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_snowflake_data, run_id=manual__2025-10-20T03:29:25.390413+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 03:44:43.022019+00:00, run_duration=None, state=queued, executor_state=failed, try_number=3, max_tries=2, job_id=None, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 03:49:43.828738+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:49:45.007-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_snowflake_data manual__2025-10-20T03:29:25.390413+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:49:45.009-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_snowflake_data manual__2025-10-20T03:29:25.390413+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:49:45.012-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=external_data_ingestion, task_id=ingest_snowflake_data, run_id=manual__2025-10-20T03:29:25.390413+00:00, execution_date=20251020T032925, start_date=, end_date=20251020T034945[0m
[[34m2025-10-19T22:49:45.016-0500[0m] {[34mconfiguration.py:[0m1053} WARNING[0m - section/key [smtp/smtp_user] not found in config[0m
[[34m2025-10-19T22:49:45.016-0500[0m] {[34memail.py:[0m271} INFO[0m - Email alerting: attempt 1[0m
[[34m2025-10-19T22:49:45.018-0500[0m] {[34mconfiguration.py:[0m1053} WARNING[0m - section/key [smtp/smtp_user] not found in config[0m
[[34m2025-10-19T22:49:45.019-0500[0m] {[34memail.py:[0m271} INFO[0m - Email alerting: attempt 1[0m
[[34m2025-10-19T22:49:45.019-0500[0m] {[34mtaskinstance.py:[0m879} ERROR[0m - Failed to send email to: ['data-team@company.com'][0m
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1063, in _email_alert
    send_email(task.email, subject, html_content)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 877, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 3163, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1065, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 61] Connection refused
[[34m2025-10-19T22:49:45.024-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_redshift_data, run_id=manual__2025-10-20T03:29:25.390413+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 03:44:43.029565+00:00, run_duration=None, state=queued, executor_state=failed, try_number=3, max_tries=2, job_id=None, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 03:49:43.828738+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:49:45.024-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_redshift_data manual__2025-10-20T03:29:25.390413+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:49:45.027-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_redshift_data manual__2025-10-20T03:29:25.390413+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:49:45.030-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=external_data_ingestion, task_id=ingest_redshift_data, run_id=manual__2025-10-20T03:29:25.390413+00:00, execution_date=20251020T032925, start_date=, end_date=20251020T034945[0m
[[34m2025-10-19T22:49:45.034-0500[0m] {[34mconfiguration.py:[0m1053} WARNING[0m - section/key [smtp/smtp_user] not found in config[0m
[[34m2025-10-19T22:49:45.035-0500[0m] {[34memail.py:[0m271} INFO[0m - Email alerting: attempt 1[0m
[[34m2025-10-19T22:49:45.037-0500[0m] {[34mconfiguration.py:[0m1053} WARNING[0m - section/key [smtp/smtp_user] not found in config[0m
[[34m2025-10-19T22:49:45.037-0500[0m] {[34memail.py:[0m271} INFO[0m - Email alerting: attempt 1[0m
[[34m2025-10-19T22:49:45.038-0500[0m] {[34mtaskinstance.py:[0m879} ERROR[0m - Failed to send email to: ['data-team@company.com'][0m
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1063, in _email_alert
    send_email(task.email, subject, html_content)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 877, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 3163, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1065, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 61] Connection refused
[[34m2025-10-19T22:49:45.434-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: external_data_ingestion.ingest_kafka_streams manual__2025-10-20T03:29:25.390413+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_fx_rates manual__2025-10-20T03:29:25.390413+00:00 [scheduled]>[0m
[[34m2025-10-19T22:49:45.435-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:49:45.435-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 1/4 running and queued tasks[0m
[[34m2025-10-19T22:49:45.436-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: external_data_ingestion.ingest_kafka_streams manual__2025-10-20T03:29:25.390413+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_fx_rates manual__2025-10-20T03:29:25.390413+00:00 [scheduled]>[0m
[[34m2025-10-19T22:49:45.437-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_kafka_streams', run_id='manual__2025-10-20T03:29:25.390413+00:00', try_number=3, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T22:49:45.437-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_kafka_streams', 'manual__2025-10-20T03:29:25.390413+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:49:45.437-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_fx_rates', run_id='manual__2025-10-20T03:29:25.390413+00:00', try_number=3, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T22:49:45.437-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_fx_rates', 'manual__2025-10-20T03:29:25.390413+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:49:45.438-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_kafka_streams', 'manual__2025-10-20T03:29:25.390413+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:49:45.438-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_fx_rates', 'manual__2025-10-20T03:29:25.390413+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T22:49:46.510-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_fx_rates', run_id='manual__2025-10-20T03:29:25.390413+00:00', try_number=3, map_index=-1)[0m
[[34m2025-10-19T22:49:46.510-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_kafka_streams', run_id='manual__2025-10-20T03:29:25.390413+00:00', try_number=3, map_index=-1)[0m
[[34m2025-10-19T22:49:46.512-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_kafka_streams, run_id=manual__2025-10-20T03:29:25.390413+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 03:44:45.143719+00:00, run_duration=None, state=queued, executor_state=failed, try_number=3, max_tries=2, job_id=None, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 03:49:45.436359+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:49:46.512-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_kafka_streams manual__2025-10-20T03:29:25.390413+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:49:46.515-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_kafka_streams manual__2025-10-20T03:29:25.390413+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:49:46.518-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=external_data_ingestion, task_id=ingest_kafka_streams, run_id=manual__2025-10-20T03:29:25.390413+00:00, execution_date=20251020T032925, start_date=, end_date=20251020T034946[0m
[[34m2025-10-19T22:49:46.521-0500[0m] {[34mconfiguration.py:[0m1053} WARNING[0m - section/key [smtp/smtp_user] not found in config[0m
[[34m2025-10-19T22:49:46.521-0500[0m] {[34memail.py:[0m271} INFO[0m - Email alerting: attempt 1[0m
[[34m2025-10-19T22:49:46.523-0500[0m] {[34mconfiguration.py:[0m1053} WARNING[0m - section/key [smtp/smtp_user] not found in config[0m
[[34m2025-10-19T22:49:46.523-0500[0m] {[34memail.py:[0m271} INFO[0m - Email alerting: attempt 1[0m
[[34m2025-10-19T22:49:46.524-0500[0m] {[34mtaskinstance.py:[0m879} ERROR[0m - Failed to send email to: ['data-team@company.com'][0m
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1063, in _email_alert
    send_email(task.email, subject, html_content)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 877, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 3163, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1065, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 61] Connection refused
[[34m2025-10-19T22:49:46.528-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_fx_rates, run_id=manual__2025-10-20T03:29:25.390413+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 03:44:45.151771+00:00, run_duration=None, state=queued, executor_state=failed, try_number=3, max_tries=2, job_id=None, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 03:49:45.436359+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:49:46.528-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_fx_rates manual__2025-10-20T03:29:25.390413+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:49:46.531-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_fx_rates manual__2025-10-20T03:29:25.390413+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:49:46.539-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=external_data_ingestion, task_id=ingest_fx_rates, run_id=manual__2025-10-20T03:29:25.390413+00:00, execution_date=20251020T032925, start_date=, end_date=20251020T034946[0m
[[34m2025-10-19T22:49:46.542-0500[0m] {[34mconfiguration.py:[0m1053} WARNING[0m - section/key [smtp/smtp_user] not found in config[0m
[[34m2025-10-19T22:49:46.542-0500[0m] {[34memail.py:[0m271} INFO[0m - Email alerting: attempt 1[0m
[[34m2025-10-19T22:49:46.544-0500[0m] {[34mconfiguration.py:[0m1053} WARNING[0m - section/key [smtp/smtp_user] not found in config[0m
[[34m2025-10-19T22:49:46.544-0500[0m] {[34memail.py:[0m271} INFO[0m - Email alerting: attempt 1[0m
[[34m2025-10-19T22:49:46.545-0500[0m] {[34mtaskinstance.py:[0m879} ERROR[0m - Failed to send email to: ['data-team@company.com'][0m
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1063, in _email_alert
    send_email(task.email, subject, html_content)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 877, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 3163, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1065, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 61] Connection refused
[[34m2025-10-19T22:49:47.571-0500[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun external_data_ingestion @ 2025-10-20 03:29:25.390413+00:00: manual__2025-10-20T03:29:25.390413+00:00, state:running, queued_at: 2025-10-20 03:29:25.397710+00:00. externally triggered: True> failed[0m
[[34m2025-10-19T22:49:47.572-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=external_data_ingestion, execution_date=2025-10-20 03:29:25.390413+00:00, run_id=manual__2025-10-20T03:29:25.390413+00:00, run_start_date=2025-10-20 03:39:40.073029+00:00, run_end_date=2025-10-20 03:49:47.572197+00:00, run_duration=607.499168, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-10-20 02:00:00+00:00, data_interval_end=2025-10-20 03:00:00+00:00, dag_hash=fef5ae2a608c839f01e36de13cff2f9b[0m
[[34m2025-10-19T22:50:00.534-0500[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG standard_etl_dag is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-10-19T22:50:00.538-0500[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG working_etl_dag is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-10-19T22:50:00.565-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:45:00+00:00 [scheduled]>
	<TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:40:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:50:00.565-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG working_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:50:00.566-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG standard_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:50:00.566-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:45:00+00:00 [scheduled]>
	<TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:40:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:50:00.567-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:45:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
[[34m2025-10-19T22:50:00.567-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T22:50:00.568-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='standard_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:40:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-10-19T22:50:00.568-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'standard_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:40:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/standard_etl_dag.py'][0m
[[34m2025-10-19T22:50:00.569-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T22:50:00.569-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'standard_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:40:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/standard_etl_dag.py'][0m
[[34m2025-10-19T22:50:00.985-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:45:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T22:50:00.986-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='standard_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:40:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T22:50:00.988-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=standard_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:40:00+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2025-10-20 03:50:00.566776+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:50:00.989-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:40:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:50:00.994-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:40:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:50:01.006-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=standard_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:40:00+00:00, execution_date=20251020T034000, start_date=, end_date=20251020T035000[0m
[[34m2025-10-19T22:50:01.017-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:45:00+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-10-20 03:50:00.566776+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:50:01.019-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:45:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:50:01.037-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:45:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:50:01.047-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:45:00+00:00, execution_date=20251020T034500, start_date=, end_date=20251020T035001[0m
[[34m2025-10-19T22:52:01.240-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:45:00+00:00 [scheduled]>
	<TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:40:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:52:01.241-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG working_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:52:01.241-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG standard_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:52:01.242-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:45:00+00:00 [scheduled]>
	<TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:40:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:52:01.243-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:45:00+00:00', try_number=2, map_index=-1) to executor with priority 5 and queue default[0m
[[34m2025-10-19T22:52:01.243-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T22:52:01.243-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='standard_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:40:00+00:00', try_number=2, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-10-19T22:52:01.244-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'standard_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:40:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/standard_etl_dag.py'][0m
[[34m2025-10-19T22:52:01.245-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'standard_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:40:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/standard_etl_dag.py'][0m
[[34m2025-10-19T22:52:01.245-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:45:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T22:52:01.602-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='standard_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:40:00+00:00', try_number=2, map_index=-1)[0m
[[34m2025-10-19T22:52:01.621-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=standard_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:40:00+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 03:50:00.996146+00:00, run_duration=None, state=queued, executor_state=failed, try_number=2, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2025-10-20 03:52:01.242560+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:52:01.623-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:40:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:52:01.735-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:40:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:52:01.746-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=standard_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:40:00+00:00, execution_date=20251020T034000, start_date=, end_date=20251020T035201[0m
[[34m2025-10-19T22:52:02.978-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:45:00+00:00', try_number=2, map_index=-1)[0m
[[34m2025-10-19T22:52:02.981-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:45:00+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 03:50:01.044026+00:00, run_duration=None, state=queued, executor_state=failed, try_number=2, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-10-20 03:52:01.242560+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:52:02.982-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:45:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:52:02.987-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:45:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:52:02.994-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:45:00+00:00, execution_date=20251020T034500, start_date=, end_date=20251020T035202[0m
[[34m2025-10-19T22:52:06.177-0500[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun standard_etl_dag @ 2025-10-20 03:40:00+00:00: scheduled__2025-10-20T03:40:00+00:00, state:running, queued_at: 2025-10-20 03:50:00.530152+00:00. externally triggered: False> failed[0m
[[34m2025-10-19T22:52:06.177-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=standard_etl_dag, execution_date=2025-10-20 03:40:00+00:00, run_id=scheduled__2025-10-20T03:40:00+00:00, run_start_date=2025-10-20 03:50:00.542246+00:00, run_end_date=2025-10-20 03:52:06.177667+00:00, run_duration=125.635421, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2025-10-20 03:40:00+00:00, data_interval_end=2025-10-20 03:50:00+00:00, dag_hash=9421653b83515a7f1a16a29fc8f09179[0m
[[34m2025-10-19T22:52:06.179-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for standard_etl_dag to 2025-10-20 03:50:00+00:00, run_after=2025-10-20 04:00:00+00:00[0m
[[34m2025-10-19T22:52:07.979-0500[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun working_etl_dag @ 2025-10-20 03:45:00+00:00: scheduled__2025-10-20T03:45:00+00:00, state:running, queued_at: 2025-10-20 03:50:00.535743+00:00. externally triggered: False> failed[0m
[[34m2025-10-19T22:52:07.981-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=working_etl_dag, execution_date=2025-10-20 03:45:00+00:00, run_id=scheduled__2025-10-20T03:45:00+00:00, run_start_date=2025-10-20 03:50:00.542659+00:00, run_end_date=2025-10-20 03:52:07.981214+00:00, run_duration=127.438555, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2025-10-20 03:45:00+00:00, data_interval_end=2025-10-20 03:50:00+00:00, dag_hash=277a4a4ca5035f6f5dc345a5179a577e[0m
[[34m2025-10-19T22:52:07.982-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for working_etl_dag to 2025-10-20 03:50:00+00:00, run_after=2025-10-20 03:55:00+00:00[0m
[[34m2025-10-19T22:53:50.681-0500[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-19T22:55:00.246-0500[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG working_etl_dag is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-10-19T22:55:00.271-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:50:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:55:00.271-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG working_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:55:00.271-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:50:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:55:00.272-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:50:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
[[34m2025-10-19T22:55:00.272-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T22:55:00.274-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T22:55:01.341-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:50:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T22:55:01.343-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:50:00+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-10-20 03:55:00.272045+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:55:01.344-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:50:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:55:01.348-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:50:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:55:01.351-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:50:00+00:00, execution_date=20251020T035000, start_date=, end_date=20251020T035501[0m
[[34m2025-10-19T22:57:02.195-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:50:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:57:02.196-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG working_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T22:57:02.197-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:50:00+00:00 [scheduled]>[0m
[[34m2025-10-19T22:57:02.198-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:50:00+00:00', try_number=2, map_index=-1) to executor with priority 5 and queue default[0m
[[34m2025-10-19T22:57:02.198-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T22:57:02.200-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T22:57:03.039-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:50:00+00:00', try_number=2, map_index=-1)[0m
[[34m2025-10-19T22:57:03.041-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:50:00+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 03:55:01.349527+00:00, run_duration=None, state=queued, executor_state=failed, try_number=2, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-10-20 03:57:02.197517+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T22:57:03.041-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:50:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:57:03.044-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:50:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T22:57:03.047-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:50:00+00:00, execution_date=20251020T035000, start_date=, end_date=20251020T035703[0m
[[34m2025-10-19T22:57:07.225-0500[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun working_etl_dag @ 2025-10-20 03:50:00+00:00: scheduled__2025-10-20T03:50:00+00:00, state:running, queued_at: 2025-10-20 03:55:00.242766+00:00. externally triggered: False> failed[0m
[[34m2025-10-19T22:57:07.226-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=working_etl_dag, execution_date=2025-10-20 03:50:00+00:00, run_id=scheduled__2025-10-20T03:50:00+00:00, run_start_date=2025-10-20 03:55:00.252640+00:00, run_end_date=2025-10-20 03:57:07.226411+00:00, run_duration=126.973771, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2025-10-20 03:50:00+00:00, data_interval_end=2025-10-20 03:55:00+00:00, dag_hash=277a4a4ca5035f6f5dc345a5179a577e[0m
[[34m2025-10-19T22:57:07.227-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for working_etl_dag to 2025-10-20 03:55:00+00:00, run_after=2025-10-20 04:00:00+00:00[0m
[[34m2025-10-19T22:58:50.721-0500[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-19T23:00:00.317-0500[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG external_data_ingestion is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-10-19T23:00:00.321-0500[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG working_etl_dag is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-10-19T23:00:00.323-0500[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG standard_etl_dag is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-10-19T23:00:00.353-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:55:00+00:00 [scheduled]>
	<TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:50:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:00:00.353-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG working_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T23:00:00.353-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG standard_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T23:00:00.354-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:55:00+00:00 [scheduled]>
	<TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:50:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:00:00.355-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:55:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
[[34m2025-10-19T23:00:00.355-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:55:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T23:00:00.355-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='standard_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:50:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-10-19T23:00:00.356-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'standard_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/standard_etl_dag.py'][0m
[[34m2025-10-19T23:00:00.357-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'standard_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/standard_etl_dag.py'][0m
[[34m2025-10-19T23:00:00.357-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:55:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T23:00:00.399-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: external_data_ingestion.ingest_hubspot_data scheduled__2025-10-20T03:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:00:00.399-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 0/4 running and queued tasks[0m
[[34m2025-10-19T23:00:00.400-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: external_data_ingestion.ingest_hubspot_data scheduled__2025-10-20T03:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:00:00.401-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_hubspot_data', run_id='scheduled__2025-10-20T03:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T23:00:00.401-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_hubspot_data', 'scheduled__2025-10-20T03:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T23:00:00.405-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_hubspot_data', 'scheduled__2025-10-20T03:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T23:00:01.490-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='standard_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:50:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T23:00:01.490-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:55:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T23:00:01.491-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_hubspot_data', run_id='scheduled__2025-10-20T03:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T23:00:01.494-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_hubspot_data, run_id=scheduled__2025-10-20T03:00:00+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=2, job_id=None, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 04:00:00.400591+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T23:00:01.494-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_hubspot_data scheduled__2025-10-20T03:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:00:01.498-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_hubspot_data scheduled__2025-10-20T03:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:00:01.501-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=external_data_ingestion, task_id=ingest_hubspot_data, run_id=scheduled__2025-10-20T03:00:00+00:00, execution_date=20251020T030000, start_date=, end_date=20251020T040001[0m
[[34m2025-10-19T23:00:01.505-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:55:00+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-10-20 04:00:00.354429+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T23:00:01.505-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:55:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:00:01.509-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:55:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:00:01.512-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:55:00+00:00, execution_date=20251020T035500, start_date=, end_date=20251020T040001[0m
[[34m2025-10-19T23:00:01.516-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=standard_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:50:00+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2025-10-20 04:00:00.354429+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T23:00:01.516-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:50:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:00:01.519-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:50:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:00:01.522-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=standard_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:50:00+00:00, execution_date=20251020T035000, start_date=, end_date=20251020T040001[0m
[[34m2025-10-19T23:00:02.568-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: external_data_ingestion.ingest_snowflake_data scheduled__2025-10-20T03:00:00+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_redshift_data scheduled__2025-10-20T03:00:00+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_kafka_streams scheduled__2025-10-20T03:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:00:02.568-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 0/4 running and queued tasks[0m
[[34m2025-10-19T23:00:02.569-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 1/4 running and queued tasks[0m
[[34m2025-10-19T23:00:02.569-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 2/4 running and queued tasks[0m
[[34m2025-10-19T23:00:02.569-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: external_data_ingestion.ingest_snowflake_data scheduled__2025-10-20T03:00:00+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_redshift_data scheduled__2025-10-20T03:00:00+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_kafka_streams scheduled__2025-10-20T03:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:00:02.571-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_snowflake_data', run_id='scheduled__2025-10-20T03:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T23:00:02.571-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_snowflake_data', 'scheduled__2025-10-20T03:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T23:00:02.572-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_redshift_data', run_id='scheduled__2025-10-20T03:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T23:00:02.572-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_redshift_data', 'scheduled__2025-10-20T03:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T23:00:02.572-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_kafka_streams', run_id='scheduled__2025-10-20T03:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T23:00:02.573-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_kafka_streams', 'scheduled__2025-10-20T03:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T23:00:02.574-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_snowflake_data', 'scheduled__2025-10-20T03:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T23:00:02.574-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_redshift_data', 'scheduled__2025-10-20T03:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T23:00:02.575-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_kafka_streams', 'scheduled__2025-10-20T03:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T23:00:03.669-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_redshift_data', run_id='scheduled__2025-10-20T03:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T23:00:03.669-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_kafka_streams', run_id='scheduled__2025-10-20T03:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T23:00:03.670-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_snowflake_data', run_id='scheduled__2025-10-20T03:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T23:00:03.672-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_snowflake_data, run_id=scheduled__2025-10-20T03:00:00+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=2, job_id=None, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 04:00:02.570421+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T23:00:03.672-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_snowflake_data scheduled__2025-10-20T03:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:00:03.675-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_snowflake_data scheduled__2025-10-20T03:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:00:03.679-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=external_data_ingestion, task_id=ingest_snowflake_data, run_id=scheduled__2025-10-20T03:00:00+00:00, execution_date=20251020T030000, start_date=, end_date=20251020T040003[0m
[[34m2025-10-19T23:00:03.684-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_redshift_data, run_id=scheduled__2025-10-20T03:00:00+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=2, job_id=None, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 04:00:02.570421+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T23:00:03.684-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_redshift_data scheduled__2025-10-20T03:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:00:03.687-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_redshift_data scheduled__2025-10-20T03:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:00:03.691-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=external_data_ingestion, task_id=ingest_redshift_data, run_id=scheduled__2025-10-20T03:00:00+00:00, execution_date=20251020T030000, start_date=, end_date=20251020T040003[0m
[[34m2025-10-19T23:00:03.694-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_kafka_streams, run_id=scheduled__2025-10-20T03:00:00+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=2, job_id=None, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 04:00:02.570421+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T23:00:03.695-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_kafka_streams scheduled__2025-10-20T03:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:00:03.698-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_kafka_streams scheduled__2025-10-20T03:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:00:03.701-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=external_data_ingestion, task_id=ingest_kafka_streams, run_id=scheduled__2025-10-20T03:00:00+00:00, execution_date=20251020T030000, start_date=, end_date=20251020T040003[0m
[[34m2025-10-19T23:00:04.750-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: external_data_ingestion.ingest_fx_rates scheduled__2025-10-20T03:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:00:04.750-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 0/4 running and queued tasks[0m
[[34m2025-10-19T23:00:04.750-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: external_data_ingestion.ingest_fx_rates scheduled__2025-10-20T03:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:00:04.751-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_fx_rates', run_id='scheduled__2025-10-20T03:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T23:00:04.751-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_fx_rates', 'scheduled__2025-10-20T03:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T23:00:04.752-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_fx_rates', 'scheduled__2025-10-20T03:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T23:00:05.403-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_fx_rates', run_id='scheduled__2025-10-20T03:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T23:00:05.405-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_fx_rates, run_id=scheduled__2025-10-20T03:00:00+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=2, job_id=None, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 04:00:04.751142+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T23:00:05.405-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_fx_rates scheduled__2025-10-20T03:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:00:05.408-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_fx_rates scheduled__2025-10-20T03:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:00:05.411-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=external_data_ingestion, task_id=ingest_fx_rates, run_id=scheduled__2025-10-20T03:00:00+00:00, execution_date=20251020T030000, start_date=, end_date=20251020T040005[0m
[[34m2025-10-19T23:02:02.487-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:55:00+00:00 [scheduled]>
	<TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:50:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:02:02.488-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG working_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T23:02:02.488-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG standard_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T23:02:02.488-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:55:00+00:00 [scheduled]>
	<TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:50:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:02:02.489-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:55:00+00:00', try_number=2, map_index=-1) to executor with priority 5 and queue default[0m
[[34m2025-10-19T23:02:02.490-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:55:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T23:02:02.490-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='standard_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:50:00+00:00', try_number=2, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-10-19T23:02:02.490-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'standard_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/standard_etl_dag.py'][0m
[[34m2025-10-19T23:02:02.492-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:55:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T23:02:02.492-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'standard_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T03:50:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/standard_etl_dag.py'][0m
[[34m2025-10-19T23:02:03.569-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:55:00+00:00', try_number=2, map_index=-1)[0m
[[34m2025-10-19T23:02:03.569-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='standard_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T03:50:00+00:00', try_number=2, map_index=-1)[0m
[[34m2025-10-19T23:02:03.573-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:55:00+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 04:00:01.511058+00:00, run_duration=None, state=queued, executor_state=failed, try_number=2, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-10-20 04:02:02.489111+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T23:02:03.573-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:55:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:02:03.577-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T03:55:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:02:03.580-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:55:00+00:00, execution_date=20251020T035500, start_date=, end_date=20251020T040203[0m
[[34m2025-10-19T23:02:03.584-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=standard_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:50:00+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 04:00:01.520701+00:00, run_duration=None, state=queued, executor_state=failed, try_number=2, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2025-10-20 04:02:02.489111+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T23:02:03.584-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:50:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:02:03.588-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T03:50:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:02:03.591-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=standard_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T03:50:00+00:00, execution_date=20251020T035000, start_date=, end_date=20251020T040203[0m
[[34m2025-10-19T23:02:06.467-0500[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun standard_etl_dag @ 2025-10-20 03:50:00+00:00: scheduled__2025-10-20T03:50:00+00:00, state:running, queued_at: 2025-10-20 04:00:00.322027+00:00. externally triggered: False> failed[0m
[[34m2025-10-19T23:02:06.468-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=standard_etl_dag, execution_date=2025-10-20 03:50:00+00:00, run_id=scheduled__2025-10-20T03:50:00+00:00, run_start_date=2025-10-20 04:00:00.326988+00:00, run_end_date=2025-10-20 04:02:06.468168+00:00, run_duration=126.14118, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2025-10-20 03:50:00+00:00, data_interval_end=2025-10-20 04:00:00+00:00, dag_hash=9421653b83515a7f1a16a29fc8f09179[0m
[[34m2025-10-19T23:02:06.469-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for standard_etl_dag to 2025-10-20 04:00:00+00:00, run_after=2025-10-20 04:10:00+00:00[0m
[[34m2025-10-19T23:02:07.513-0500[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun working_etl_dag @ 2025-10-20 03:55:00+00:00: scheduled__2025-10-20T03:55:00+00:00, state:running, queued_at: 2025-10-20 04:00:00.319274+00:00. externally triggered: False> failed[0m
[[34m2025-10-19T23:02:07.514-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=working_etl_dag, execution_date=2025-10-20 03:55:00+00:00, run_id=scheduled__2025-10-20T03:55:00+00:00, run_start_date=2025-10-20 04:00:00.327225+00:00, run_end_date=2025-10-20 04:02:07.514396+00:00, run_duration=127.187171, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2025-10-20 03:55:00+00:00, data_interval_end=2025-10-20 04:00:00+00:00, dag_hash=277a4a4ca5035f6f5dc345a5179a577e[0m
[[34m2025-10-19T23:02:07.516-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for working_etl_dag to 2025-10-20 04:00:00+00:00, run_after=2025-10-20 04:05:00+00:00[0m
[[34m2025-10-19T23:03:50.762-0500[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-19T23:05:00.145-0500[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG working_etl_dag is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-10-19T23:05:00.203-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T04:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:05:00.204-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG working_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T23:05:00.204-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T04:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:05:00.206-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T04:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
[[34m2025-10-19T23:05:00.207-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T04:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T23:05:00.210-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T04:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T23:05:01.318-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T04:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T23:05:01.320-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T04:00:00+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-10-20 04:05:00.205334+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T23:05:01.321-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T04:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:05:01.327-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T04:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:05:01.332-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T04:00:00+00:00, execution_date=20251020T040000, start_date=, end_date=20251020T040501[0m
[[34m2025-10-19T23:05:02.379-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: external_data_ingestion.ingest_hubspot_data scheduled__2025-10-20T03:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:05:02.380-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 0/4 running and queued tasks[0m
[[34m2025-10-19T23:05:02.380-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: external_data_ingestion.ingest_hubspot_data scheduled__2025-10-20T03:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:05:02.381-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_hubspot_data', run_id='scheduled__2025-10-20T03:00:00+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T23:05:02.381-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_hubspot_data', 'scheduled__2025-10-20T03:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T23:05:02.383-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_hubspot_data', 'scheduled__2025-10-20T03:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T23:05:03.397-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_hubspot_data', run_id='scheduled__2025-10-20T03:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2025-10-19T23:05:03.399-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_hubspot_data, run_id=scheduled__2025-10-20T03:00:00+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 04:00:01.499314+00:00, run_duration=None, state=queued, executor_state=failed, try_number=2, max_tries=2, job_id=None, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 04:05:02.380758+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T23:05:03.400-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_hubspot_data scheduled__2025-10-20T03:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:05:03.402-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_hubspot_data scheduled__2025-10-20T03:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:05:03.405-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=external_data_ingestion, task_id=ingest_hubspot_data, run_id=scheduled__2025-10-20T03:00:00+00:00, execution_date=20251020T030000, start_date=, end_date=20251020T040503[0m
[[34m2025-10-19T23:05:04.450-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
	<TaskInstance: external_data_ingestion.ingest_snowflake_data scheduled__2025-10-20T03:00:00+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_redshift_data scheduled__2025-10-20T03:00:00+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_kafka_streams scheduled__2025-10-20T03:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:05:04.451-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 0/4 running and queued tasks[0m
[[34m2025-10-19T23:05:04.451-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 1/4 running and queued tasks[0m
[[34m2025-10-19T23:05:04.451-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 2/4 running and queued tasks[0m
[[34m2025-10-19T23:05:04.451-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: external_data_ingestion.ingest_snowflake_data scheduled__2025-10-20T03:00:00+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_redshift_data scheduled__2025-10-20T03:00:00+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_kafka_streams scheduled__2025-10-20T03:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:05:04.452-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_snowflake_data', run_id='scheduled__2025-10-20T03:00:00+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T23:05:04.453-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_snowflake_data', 'scheduled__2025-10-20T03:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T23:05:04.453-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_redshift_data', run_id='scheduled__2025-10-20T03:00:00+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T23:05:04.453-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_redshift_data', 'scheduled__2025-10-20T03:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T23:05:04.453-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_kafka_streams', run_id='scheduled__2025-10-20T03:00:00+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T23:05:04.454-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_kafka_streams', 'scheduled__2025-10-20T03:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T23:05:04.455-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_redshift_data', 'scheduled__2025-10-20T03:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T23:05:04.455-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_kafka_streams', 'scheduled__2025-10-20T03:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T23:05:04.455-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_snowflake_data', 'scheduled__2025-10-20T03:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T23:05:04.944-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_redshift_data', run_id='scheduled__2025-10-20T03:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2025-10-19T23:05:04.945-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_kafka_streams', run_id='scheduled__2025-10-20T03:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2025-10-19T23:05:04.945-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_snowflake_data', run_id='scheduled__2025-10-20T03:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2025-10-19T23:05:04.947-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_snowflake_data, run_id=scheduled__2025-10-20T03:00:00+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 04:00:03.677227+00:00, run_duration=None, state=queued, executor_state=failed, try_number=2, max_tries=2, job_id=None, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 04:05:04.452211+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T23:05:04.947-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_snowflake_data scheduled__2025-10-20T03:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:05:04.950-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_snowflake_data scheduled__2025-10-20T03:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:05:04.954-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=external_data_ingestion, task_id=ingest_snowflake_data, run_id=scheduled__2025-10-20T03:00:00+00:00, execution_date=20251020T030000, start_date=, end_date=20251020T040504[0m
[[34m2025-10-19T23:05:04.957-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_redshift_data, run_id=scheduled__2025-10-20T03:00:00+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 04:00:03.689150+00:00, run_duration=None, state=queued, executor_state=failed, try_number=2, max_tries=2, job_id=None, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 04:05:04.452211+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T23:05:04.958-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_redshift_data scheduled__2025-10-20T03:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:05:04.961-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_redshift_data scheduled__2025-10-20T03:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:05:04.963-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=external_data_ingestion, task_id=ingest_redshift_data, run_id=scheduled__2025-10-20T03:00:00+00:00, execution_date=20251020T030000, start_date=, end_date=20251020T040504[0m
[[34m2025-10-19T23:05:04.966-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_kafka_streams, run_id=scheduled__2025-10-20T03:00:00+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 04:00:03.699363+00:00, run_duration=None, state=queued, executor_state=failed, try_number=2, max_tries=2, job_id=None, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 04:05:04.452211+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T23:05:04.967-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_kafka_streams scheduled__2025-10-20T03:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:05:04.969-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_kafka_streams scheduled__2025-10-20T03:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:05:04.972-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=external_data_ingestion, task_id=ingest_kafka_streams, run_id=scheduled__2025-10-20T03:00:00+00:00, execution_date=20251020T030000, start_date=, end_date=20251020T040504[0m
[[34m2025-10-19T23:05:06.019-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: external_data_ingestion.ingest_fx_rates scheduled__2025-10-20T03:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:05:06.020-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 0/4 running and queued tasks[0m
[[34m2025-10-19T23:05:06.020-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: external_data_ingestion.ingest_fx_rates scheduled__2025-10-20T03:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:05:06.021-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_fx_rates', run_id='scheduled__2025-10-20T03:00:00+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T23:05:06.021-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_fx_rates', 'scheduled__2025-10-20T03:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T23:05:06.022-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_fx_rates', 'scheduled__2025-10-20T03:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T23:05:07.097-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_fx_rates', run_id='scheduled__2025-10-20T03:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2025-10-19T23:05:07.099-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_fx_rates, run_id=scheduled__2025-10-20T03:00:00+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 04:00:05.409412+00:00, run_duration=None, state=queued, executor_state=failed, try_number=2, max_tries=2, job_id=None, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 04:05:06.020798+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T23:05:07.099-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_fx_rates scheduled__2025-10-20T03:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:05:07.102-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_fx_rates scheduled__2025-10-20T03:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:05:07.107-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=external_data_ingestion, task_id=ingest_fx_rates, run_id=scheduled__2025-10-20T03:00:00+00:00, execution_date=20251020T030000, start_date=, end_date=20251020T040507[0m
[[34m2025-10-19T23:07:01.617-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T04:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:07:01.618-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG working_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T23:07:01.618-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T04:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:07:01.619-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T04:00:00+00:00', try_number=2, map_index=-1) to executor with priority 5 and queue default[0m
[[34m2025-10-19T23:07:01.620-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T04:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T23:07:01.621-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T04:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T23:07:02.695-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T04:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2025-10-19T23:07:02.697-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T04:00:00+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 04:05:01.329610+00:00, run_duration=None, state=queued, executor_state=failed, try_number=2, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-10-20 04:07:01.619230+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T23:07:02.698-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T04:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:07:02.701-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T04:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:07:02.704-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T04:00:00+00:00, execution_date=20251020T040000, start_date=, end_date=20251020T040702[0m
[[34m2025-10-19T23:07:07.625-0500[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun working_etl_dag @ 2025-10-20 04:00:00+00:00: scheduled__2025-10-20T04:00:00+00:00, state:running, queued_at: 2025-10-20 04:05:00.139827+00:00. externally triggered: False> failed[0m
[[34m2025-10-19T23:07:07.627-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=working_etl_dag, execution_date=2025-10-20 04:00:00+00:00, run_id=scheduled__2025-10-20T04:00:00+00:00, run_start_date=2025-10-20 04:05:00.169921+00:00, run_end_date=2025-10-20 04:07:07.626960+00:00, run_duration=127.457039, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2025-10-20 04:00:00+00:00, data_interval_end=2025-10-20 04:05:00+00:00, dag_hash=277a4a4ca5035f6f5dc345a5179a577e[0m
[[34m2025-10-19T23:07:07.629-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for working_etl_dag to 2025-10-20 04:05:00+00:00, run_after=2025-10-20 04:10:00+00:00[0m
[[34m2025-10-19T23:08:50.800-0500[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-10-19T23:10:00.925-0500[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG working_etl_dag is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-10-19T23:10:00.930-0500[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG standard_etl_dag is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-10-19T23:10:00.968-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T04:05:00+00:00 [scheduled]>
	<TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T04:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:10:00.969-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG working_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T23:10:00.969-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG standard_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T23:10:00.969-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T04:05:00+00:00 [scheduled]>
	<TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T04:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:10:00.972-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T04:05:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
[[34m2025-10-19T23:10:00.972-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T04:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T23:10:00.972-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='standard_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T04:00:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-10-19T23:10:00.973-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'standard_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T04:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/standard_etl_dag.py'][0m
[[34m2025-10-19T23:10:00.974-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T04:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T23:10:00.975-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'standard_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T04:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/standard_etl_dag.py'][0m
[[34m2025-10-19T23:10:02.067-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T04:05:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T23:10:02.067-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='standard_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T04:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T23:10:02.070-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T04:05:00+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-10-20 04:10:00.970218+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T23:10:02.070-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T04:05:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:10:02.074-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T04:05:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:10:02.077-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T04:05:00+00:00, execution_date=20251020T040500, start_date=, end_date=20251020T041002[0m
[[34m2025-10-19T23:10:02.081-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=standard_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T04:00:00+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2025-10-20 04:10:00.970218+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T23:10:02.081-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T04:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:10:02.084-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T04:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:10:02.087-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=standard_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T04:00:00+00:00, execution_date=20251020T040000, start_date=, end_date=20251020T041002[0m
[[34m2025-10-19T23:10:04.174-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: external_data_ingestion.ingest_hubspot_data scheduled__2025-10-20T03:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:10:04.175-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 0/4 running and queued tasks[0m
[[34m2025-10-19T23:10:04.175-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: external_data_ingestion.ingest_hubspot_data scheduled__2025-10-20T03:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:10:04.176-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_hubspot_data', run_id='scheduled__2025-10-20T03:00:00+00:00', try_number=3, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T23:10:04.176-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_hubspot_data', 'scheduled__2025-10-20T03:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T23:10:04.178-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_hubspot_data', 'scheduled__2025-10-20T03:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T23:10:05.262-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: external_data_ingestion.ingest_snowflake_data scheduled__2025-10-20T03:00:00+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_redshift_data scheduled__2025-10-20T03:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:10:05.263-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 1/4 running and queued tasks[0m
[[34m2025-10-19T23:10:05.263-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 2/4 running and queued tasks[0m
[[34m2025-10-19T23:10:05.263-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: external_data_ingestion.ingest_snowflake_data scheduled__2025-10-20T03:00:00+00:00 [scheduled]>
	<TaskInstance: external_data_ingestion.ingest_redshift_data scheduled__2025-10-20T03:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:10:05.264-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_snowflake_data', run_id='scheduled__2025-10-20T03:00:00+00:00', try_number=3, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T23:10:05.264-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_snowflake_data', 'scheduled__2025-10-20T03:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T23:10:05.265-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_redshift_data', run_id='scheduled__2025-10-20T03:00:00+00:00', try_number=3, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T23:10:05.265-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_redshift_data', 'scheduled__2025-10-20T03:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T23:10:05.266-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_hubspot_data', run_id='scheduled__2025-10-20T03:00:00+00:00', try_number=3, map_index=-1)[0m
[[34m2025-10-19T23:10:05.266-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_snowflake_data', 'scheduled__2025-10-20T03:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T23:10:05.266-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_redshift_data', 'scheduled__2025-10-20T03:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T23:10:05.277-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_hubspot_data, run_id=scheduled__2025-10-20T03:00:00+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 04:05:03.403802+00:00, run_duration=None, state=queued, executor_state=failed, try_number=3, max_tries=2, job_id=None, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 04:10:04.175944+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T23:10:05.277-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_hubspot_data scheduled__2025-10-20T03:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:10:05.280-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_hubspot_data scheduled__2025-10-20T03:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:10:05.284-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=external_data_ingestion, task_id=ingest_hubspot_data, run_id=scheduled__2025-10-20T03:00:00+00:00, execution_date=20251020T030000, start_date=, end_date=20251020T041005[0m
[[34m2025-10-19T23:10:05.303-0500[0m] {[34mconfiguration.py:[0m1053} WARNING[0m - section/key [smtp/smtp_user] not found in config[0m
[[34m2025-10-19T23:10:05.304-0500[0m] {[34memail.py:[0m271} INFO[0m - Email alerting: attempt 1[0m
[[34m2025-10-19T23:10:05.306-0500[0m] {[34mconfiguration.py:[0m1053} WARNING[0m - section/key [smtp/smtp_user] not found in config[0m
[[34m2025-10-19T23:10:05.306-0500[0m] {[34memail.py:[0m271} INFO[0m - Email alerting: attempt 1[0m
[[34m2025-10-19T23:10:05.307-0500[0m] {[34mtaskinstance.py:[0m879} ERROR[0m - Failed to send email to: ['data-team@company.com'][0m
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1063, in _email_alert
    send_email(task.email, subject, html_content)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 877, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 3163, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1065, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 61] Connection refused
[[34m2025-10-19T23:10:05.356-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: external_data_ingestion.ingest_kafka_streams scheduled__2025-10-20T03:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:10:05.356-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 2/4 running and queued tasks[0m
[[34m2025-10-19T23:10:05.356-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: external_data_ingestion.ingest_kafka_streams scheduled__2025-10-20T03:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:10:05.357-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_kafka_streams', run_id='scheduled__2025-10-20T03:00:00+00:00', try_number=3, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T23:10:05.358-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_kafka_streams', 'scheduled__2025-10-20T03:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T23:10:05.358-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_kafka_streams', 'scheduled__2025-10-20T03:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T23:10:06.046-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_redshift_data', run_id='scheduled__2025-10-20T03:00:00+00:00', try_number=3, map_index=-1)[0m
[[34m2025-10-19T23:10:06.046-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_snowflake_data', run_id='scheduled__2025-10-20T03:00:00+00:00', try_number=3, map_index=-1)[0m
[[34m2025-10-19T23:10:06.047-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_kafka_streams', run_id='scheduled__2025-10-20T03:00:00+00:00', try_number=3, map_index=-1)[0m
[[34m2025-10-19T23:10:06.049-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_snowflake_data, run_id=scheduled__2025-10-20T03:00:00+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 04:05:04.952148+00:00, run_duration=None, state=queued, executor_state=failed, try_number=3, max_tries=2, job_id=None, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 04:10:05.263979+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T23:10:06.049-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_snowflake_data scheduled__2025-10-20T03:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:10:06.051-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_snowflake_data scheduled__2025-10-20T03:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:10:06.054-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=external_data_ingestion, task_id=ingest_snowflake_data, run_id=scheduled__2025-10-20T03:00:00+00:00, execution_date=20251020T030000, start_date=, end_date=20251020T041006[0m
[[34m2025-10-19T23:10:06.058-0500[0m] {[34mconfiguration.py:[0m1053} WARNING[0m - section/key [smtp/smtp_user] not found in config[0m
[[34m2025-10-19T23:10:06.058-0500[0m] {[34memail.py:[0m271} INFO[0m - Email alerting: attempt 1[0m
[[34m2025-10-19T23:10:06.060-0500[0m] {[34mconfiguration.py:[0m1053} WARNING[0m - section/key [smtp/smtp_user] not found in config[0m
[[34m2025-10-19T23:10:06.060-0500[0m] {[34memail.py:[0m271} INFO[0m - Email alerting: attempt 1[0m
[[34m2025-10-19T23:10:06.061-0500[0m] {[34mtaskinstance.py:[0m879} ERROR[0m - Failed to send email to: ['data-team@company.com'][0m
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1063, in _email_alert
    send_email(task.email, subject, html_content)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 877, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 3163, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1065, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 61] Connection refused
[[34m2025-10-19T23:10:06.064-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_redshift_data, run_id=scheduled__2025-10-20T03:00:00+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 04:05:04.962119+00:00, run_duration=None, state=queued, executor_state=failed, try_number=3, max_tries=2, job_id=None, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 04:10:05.263979+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T23:10:06.065-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_redshift_data scheduled__2025-10-20T03:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:10:06.067-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_redshift_data scheduled__2025-10-20T03:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:10:06.070-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=external_data_ingestion, task_id=ingest_redshift_data, run_id=scheduled__2025-10-20T03:00:00+00:00, execution_date=20251020T030000, start_date=, end_date=20251020T041006[0m
[[34m2025-10-19T23:10:06.074-0500[0m] {[34mconfiguration.py:[0m1053} WARNING[0m - section/key [smtp/smtp_user] not found in config[0m
[[34m2025-10-19T23:10:06.074-0500[0m] {[34memail.py:[0m271} INFO[0m - Email alerting: attempt 1[0m
[[34m2025-10-19T23:10:06.076-0500[0m] {[34mconfiguration.py:[0m1053} WARNING[0m - section/key [smtp/smtp_user] not found in config[0m
[[34m2025-10-19T23:10:06.076-0500[0m] {[34memail.py:[0m271} INFO[0m - Email alerting: attempt 1[0m
[[34m2025-10-19T23:10:06.076-0500[0m] {[34mtaskinstance.py:[0m879} ERROR[0m - Failed to send email to: ['data-team@company.com'][0m
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1063, in _email_alert
    send_email(task.email, subject, html_content)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 877, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 3163, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1065, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 61] Connection refused
[[34m2025-10-19T23:10:06.080-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_kafka_streams, run_id=scheduled__2025-10-20T03:00:00+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 04:05:04.970902+00:00, run_duration=None, state=queued, executor_state=failed, try_number=3, max_tries=2, job_id=None, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 04:10:05.357163+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T23:10:06.080-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_kafka_streams scheduled__2025-10-20T03:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:10:06.082-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_kafka_streams scheduled__2025-10-20T03:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:10:06.085-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=external_data_ingestion, task_id=ingest_kafka_streams, run_id=scheduled__2025-10-20T03:00:00+00:00, execution_date=20251020T030000, start_date=, end_date=20251020T041006[0m
[[34m2025-10-19T23:10:06.088-0500[0m] {[34mconfiguration.py:[0m1053} WARNING[0m - section/key [smtp/smtp_user] not found in config[0m
[[34m2025-10-19T23:10:06.089-0500[0m] {[34memail.py:[0m271} INFO[0m - Email alerting: attempt 1[0m
[[34m2025-10-19T23:10:06.090-0500[0m] {[34mconfiguration.py:[0m1053} WARNING[0m - section/key [smtp/smtp_user] not found in config[0m
[[34m2025-10-19T23:10:06.090-0500[0m] {[34memail.py:[0m271} INFO[0m - Email alerting: attempt 1[0m
[[34m2025-10-19T23:10:06.091-0500[0m] {[34mtaskinstance.py:[0m879} ERROR[0m - Failed to send email to: ['data-team@company.com'][0m
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1063, in _email_alert
    send_email(task.email, subject, html_content)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 877, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 3163, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1065, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 61] Connection refused
[[34m2025-10-19T23:10:07.368-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: external_data_ingestion.ingest_fx_rates scheduled__2025-10-20T03:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:10:07.368-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG external_data_ingestion has 0/4 running and queued tasks[0m
[[34m2025-10-19T23:10:07.368-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: external_data_ingestion.ingest_fx_rates scheduled__2025-10-20T03:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:10:07.369-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_fx_rates', run_id='scheduled__2025-10-20T03:00:00+00:00', try_number=3, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T23:10:07.370-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_fx_rates', 'scheduled__2025-10-20T03:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T23:10:07.370-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'external_data_ingestion', 'ingest_fx_rates', 'scheduled__2025-10-20T03:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/external_data_ingestion_dag.py'][0m
[[34m2025-10-19T23:10:08.453-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='external_data_ingestion', task_id='ingest_fx_rates', run_id='scheduled__2025-10-20T03:00:00+00:00', try_number=3, map_index=-1)[0m
[[34m2025-10-19T23:10:08.454-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=external_data_ingestion, task_id=ingest_fx_rates, run_id=scheduled__2025-10-20T03:00:00+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 04:05:07.104476+00:00, run_duration=None, state=queued, executor_state=failed, try_number=3, max_tries=2, job_id=None, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 04:10:07.369290+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T23:10:08.455-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_fx_rates scheduled__2025-10-20T03:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:10:08.457-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: external_data_ingestion.ingest_fx_rates scheduled__2025-10-20T03:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:10:08.460-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=external_data_ingestion, task_id=ingest_fx_rates, run_id=scheduled__2025-10-20T03:00:00+00:00, execution_date=20251020T030000, start_date=, end_date=20251020T041008[0m
[[34m2025-10-19T23:10:08.464-0500[0m] {[34mconfiguration.py:[0m1053} WARNING[0m - section/key [smtp/smtp_user] not found in config[0m
[[34m2025-10-19T23:10:08.464-0500[0m] {[34memail.py:[0m271} INFO[0m - Email alerting: attempt 1[0m
[[34m2025-10-19T23:10:08.466-0500[0m] {[34mconfiguration.py:[0m1053} WARNING[0m - section/key [smtp/smtp_user] not found in config[0m
[[34m2025-10-19T23:10:08.466-0500[0m] {[34memail.py:[0m271} INFO[0m - Email alerting: attempt 1[0m
[[34m2025-10-19T23:10:08.467-0500[0m] {[34mtaskinstance.py:[0m879} ERROR[0m - Failed to send email to: ['data-team@company.com'][0m
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1063, in _email_alert
    send_email(task.email, subject, html_content)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 877, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 3163, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1065, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 61] Connection refused
[[34m2025-10-19T23:10:09.498-0500[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun external_data_ingestion @ 2025-10-20 03:00:00+00:00: scheduled__2025-10-20T03:00:00+00:00, state:running, queued_at: 2025-10-20 04:00:00.313216+00:00. externally triggered: False> failed[0m
[[34m2025-10-19T23:10:09.498-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=external_data_ingestion, execution_date=2025-10-20 03:00:00+00:00, run_id=scheduled__2025-10-20T03:00:00+00:00, run_start_date=2025-10-20 04:00:00.326643+00:00, run_end_date=2025-10-20 04:10:09.498605+00:00, run_duration=609.171962, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2025-10-20 03:00:00+00:00, data_interval_end=2025-10-20 04:00:00+00:00, dag_hash=fef5ae2a608c839f01e36de13cff2f9b[0m
[[34m2025-10-19T23:10:09.500-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for external_data_ingestion to 2025-10-20 04:00:00+00:00, run_after=2025-10-20 05:00:00+00:00[0m
[[34m2025-10-19T23:12:02.137-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T04:05:00+00:00 [scheduled]>
	<TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T04:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:12:02.138-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG working_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T23:12:02.139-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG standard_etl_dag has 0/4 running and queued tasks[0m
[[34m2025-10-19T23:12:02.139-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T04:05:00+00:00 [scheduled]>
	<TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T04:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:12:02.140-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T04:05:00+00:00', try_number=2, map_index=-1) to executor with priority 5 and queue default[0m
[[34m2025-10-19T23:12:02.140-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T04:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T23:12:02.141-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='standard_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T04:00:00+00:00', try_number=2, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-10-19T23:12:02.141-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'standard_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T04:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/standard_etl_dag.py'][0m
[[34m2025-10-19T23:12:02.143-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'standard_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T04:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/standard_etl_dag.py'][0m
[[34m2025-10-19T23:12:02.143-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'working_etl_dag', 'start_pipeline', 'scheduled__2025-10-20T04:05:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/working_etl_dag.py'][0m
[[34m2025-10-19T23:12:03.231-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='standard_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T04:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2025-10-19T23:12:03.232-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='working_etl_dag', task_id='start_pipeline', run_id='scheduled__2025-10-20T04:05:00+00:00', try_number=2, map_index=-1)[0m
[[34m2025-10-19T23:12:03.234-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T04:05:00+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 04:10:02.075725+00:00, run_duration=None, state=queued, executor_state=failed, try_number=2, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=5, operator=BashOperator, queued_dttm=2025-10-20 04:12:02.139809+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T23:12:03.235-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T04:05:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:12:03.238-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: working_etl_dag.start_pipeline scheduled__2025-10-20T04:05:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:12:03.241-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=working_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T04:05:00+00:00, execution_date=20251020T040500, start_date=, end_date=20251020T041203[0m
[[34m2025-10-19T23:12:03.250-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=standard_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T04:00:00+00:00, map_index=-1, run_start_date=None, run_end_date=2025-10-20 04:10:02.085886+00:00, run_duration=None, state=queued, executor_state=failed, try_number=2, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2025-10-20 04:12:02.139809+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T23:12:03.250-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T04:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:12:03.253-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: standard_etl_dag.start_pipeline scheduled__2025-10-20T04:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:12:03.257-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=standard_etl_dag, task_id=start_pipeline, run_id=scheduled__2025-10-20T04:00:00+00:00, execution_date=20251020T040000, start_date=, end_date=20251020T041203[0m
[[34m2025-10-19T23:12:06.784-0500[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun standard_etl_dag @ 2025-10-20 04:00:00+00:00: scheduled__2025-10-20T04:00:00+00:00, state:running, queued_at: 2025-10-20 04:10:00.926789+00:00. externally triggered: False> failed[0m
[[34m2025-10-19T23:12:06.785-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=standard_etl_dag, execution_date=2025-10-20 04:00:00+00:00, run_id=scheduled__2025-10-20T04:00:00+00:00, run_start_date=2025-10-20 04:10:00.935405+00:00, run_end_date=2025-10-20 04:12:06.785648+00:00, run_duration=125.850243, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2025-10-20 04:00:00+00:00, data_interval_end=2025-10-20 04:10:00+00:00, dag_hash=9421653b83515a7f1a16a29fc8f09179[0m
[[34m2025-10-19T23:12:06.787-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for standard_etl_dag to 2025-10-20 04:10:00+00:00, run_after=2025-10-20 04:20:00+00:00[0m
[[34m2025-10-19T23:12:07.175-0500[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun working_etl_dag @ 2025-10-20 04:05:00+00:00: scheduled__2025-10-20T04:05:00+00:00, state:running, queued_at: 2025-10-20 04:10:00.919975+00:00. externally triggered: False> failed[0m
[[34m2025-10-19T23:12:07.175-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=working_etl_dag, execution_date=2025-10-20 04:05:00+00:00, run_id=scheduled__2025-10-20T04:05:00+00:00, run_start_date=2025-10-20 04:10:00.935909+00:00, run_end_date=2025-10-20 04:12:07.175644+00:00, run_duration=126.239735, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2025-10-20 04:05:00+00:00, data_interval_end=2025-10-20 04:10:00+00:00, dag_hash=277a4a4ca5035f6f5dc345a5179a577e[0m
[[34m2025-10-19T23:12:07.177-0500[0m] {[34mdag.py:[0m3947} INFO[0m - Setting next_dagrun for working_etl_dag to 2025-10-20 04:10:00+00:00, run_after=2025-10-20 04:15:00+00:00[0m
[[34m2025-10-19T23:12:33.380-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: delta_lake_maintenance.perform_delta_lake_maintenance scheduled__2025-10-19T00:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:12:33.381-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG delta_lake_maintenance has 0/4 running and queued tasks[0m
[[34m2025-10-19T23:12:33.381-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: delta_lake_maintenance.perform_delta_lake_maintenance scheduled__2025-10-19T00:00:00+00:00 [scheduled]>[0m
[[34m2025-10-19T23:12:33.382-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='delta_lake_maintenance', task_id='perform_delta_lake_maintenance', run_id='scheduled__2025-10-19T00:00:00+00:00', try_number=3, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T23:12:33.382-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'delta_lake_maintenance', 'perform_delta_lake_maintenance', 'scheduled__2025-10-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/delta_lake_maintenance_dag.py'][0m
[[34m2025-10-19T23:12:33.383-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'delta_lake_maintenance', 'perform_delta_lake_maintenance', 'scheduled__2025-10-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/delta_lake_maintenance_dag.py'][0m
[[34m2025-10-19T23:12:34.445-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='delta_lake_maintenance', task_id='perform_delta_lake_maintenance', run_id='scheduled__2025-10-19T00:00:00+00:00', try_number=3, map_index=-1)[0m
[[34m2025-10-19T23:12:34.447-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=delta_lake_maintenance, task_id=perform_delta_lake_maintenance, run_id=scheduled__2025-10-19T00:00:00+00:00, map_index=-1, run_start_date=2025-10-20 03:29:24.605700+00:00, run_end_date=2025-10-20 03:42:33.045708+00:00, run_duration=788.440008, state=queued, executor_state=failed, try_number=3, max_tries=2, job_id=11, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 04:12:33.382009+00:00, queued_by_job_id=5, pid=18129[0m
[[34m2025-10-19T23:12:34.447-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: delta_lake_maintenance.perform_delta_lake_maintenance scheduled__2025-10-19T00:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:12:34.450-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: delta_lake_maintenance.perform_delta_lake_maintenance scheduled__2025-10-19T00:00:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:12:34.452-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=delta_lake_maintenance, task_id=perform_delta_lake_maintenance, run_id=scheduled__2025-10-19T00:00:00+00:00, execution_date=20251019T000000, start_date=20251020T032924, end_date=20251020T041234[0m
[[34m2025-10-19T23:12:34.457-0500[0m] {[34mconfiguration.py:[0m1053} WARNING[0m - section/key [smtp/smtp_user] not found in config[0m
[[34m2025-10-19T23:12:34.457-0500[0m] {[34memail.py:[0m271} INFO[0m - Email alerting: attempt 1[0m
[[34m2025-10-19T23:12:34.459-0500[0m] {[34mconfiguration.py:[0m1053} WARNING[0m - section/key [smtp/smtp_user] not found in config[0m
[[34m2025-10-19T23:12:34.459-0500[0m] {[34memail.py:[0m271} INFO[0m - Email alerting: attempt 1[0m
[[34m2025-10-19T23:12:34.460-0500[0m] {[34mtaskinstance.py:[0m879} ERROR[0m - Failed to send email to: ['data-team@company.com'][0m
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1063, in _email_alert
    send_email(task.email, subject, html_content)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 877, in _handle_failure
    task_instance.email_alert(error, failure_context["task"])
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 3163, in email_alert
    _email_alert(task_instance=self, exception=exception, task=task)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1065, in _email_alert
    send_email(task.email, subject, html_content_err)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 80, in send_email
    return backend(
           ^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 154, in send_email_smtp
    send_mime_email(e_from=mail_from, e_to=recipients, mime_msg=msg, conn_id=conn_id, dryrun=dryrun)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 273, in send_mime_email
    smtp_conn = _get_smtp_connection(smtp_host, smtp_port, smtp_timeout, smtp_ssl)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/utils/email.py", line 317, in _get_smtp_connection
    return smtplib.SMTP(host=host, port=port, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 255, in __init__
    (code, msg) = self.connect(host, port)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 341, in connect
    self.sock = self._get_socket(host, port, self.timeout)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/smtplib.py", line 312, in _get_socket
    return socket.create_connection((host, port), timeout,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 851, in create_connection
    raise exceptions[0]
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py", line 836, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 61] Connection refused
[[34m2025-10-19T23:12:36.905-0500[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun delta_lake_maintenance @ 2025-10-19 00:00:00+00:00: scheduled__2025-10-19T00:00:00+00:00, state:running, queued_at: 2025-10-20 03:29:24.307740+00:00. externally triggered: False> failed[0m
[[34m2025-10-19T23:12:36.906-0500[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=delta_lake_maintenance, execution_date=2025-10-19 00:00:00+00:00, run_id=scheduled__2025-10-19T00:00:00+00:00, run_start_date=2025-10-20 03:29:24.318252+00:00, run_end_date=2025-10-20 04:12:36.906064+00:00, run_duration=2592.587812, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2025-10-19 00:00:00+00:00, data_interval_end=2025-10-20 00:00:00+00:00, dag_hash=ab503ea7c662099dc8c91614ae75e0a2[0m
[[34m2025-10-19T23:12:36.908-0500[0m] {[34mscheduler_job_runner.py:[0m1331} INFO[0m - DAG delta_lake_maintenance is at (or above) max_active_runs (1 of 1), not creating any more runs[0m
[[34m2025-10-19T23:12:37.388-0500[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: delta_lake_maintenance.perform_delta_lake_maintenance manual__2025-10-20T03:29:23.560172+00:00 [scheduled]>[0m
[[34m2025-10-19T23:12:37.389-0500[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG delta_lake_maintenance has 0/4 running and queued tasks[0m
[[34m2025-10-19T23:12:37.389-0500[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: delta_lake_maintenance.perform_delta_lake_maintenance manual__2025-10-20T03:29:23.560172+00:00 [scheduled]>[0m
[[34m2025-10-19T23:12:37.391-0500[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='delta_lake_maintenance', task_id='perform_delta_lake_maintenance', run_id='manual__2025-10-20T03:29:23.560172+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-10-19T23:12:37.391-0500[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'delta_lake_maintenance', 'perform_delta_lake_maintenance', 'manual__2025-10-20T03:29:23.560172+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/delta_lake_maintenance_dag.py'][0m
[[34m2025-10-19T23:12:37.393-0500[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'delta_lake_maintenance', 'perform_delta_lake_maintenance', 'manual__2025-10-20T03:29:23.560172+00:00', '--local', '--subdir', 'DAGS_FOLDER/production/delta_lake_maintenance_dag.py'][0m
[[34m2025-10-19T23:12:38.478-0500[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='delta_lake_maintenance', task_id='perform_delta_lake_maintenance', run_id='manual__2025-10-20T03:29:23.560172+00:00', try_number=1, map_index=-1)[0m
[[34m2025-10-19T23:12:38.480-0500[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=delta_lake_maintenance, task_id=perform_delta_lake_maintenance, run_id=manual__2025-10-20T03:29:23.560172+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=2, job_id=None, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-10-20 04:12:37.390364+00:00, queued_by_job_id=5, pid=None[0m
[[34m2025-10-19T23:12:38.481-0500[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: delta_lake_maintenance.perform_delta_lake_maintenance manual__2025-10-20T03:29:23.560172+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:12:38.485-0500[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - The executor reported that the task instance <TaskInstance: delta_lake_maintenance.perform_delta_lake_maintenance manual__2025-10-20T03:29:23.560172+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2025-10-19T23:12:38.488-0500[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as UP_FOR_RETRY. dag_id=delta_lake_maintenance, task_id=perform_delta_lake_maintenance, run_id=manual__2025-10-20T03:29:23.560172+00:00, execution_date=20251020T032923, start_date=, end_date=20251020T041238[0m
[[34m2025-10-19T23:13:12.665-0500[0m] {[34mscheduler_job_runner.py:[0m863} ERROR[0m - Exception when executing SchedulerJob._run_scheduler_loop[0m
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py", line 846, in _execute
    self._run_scheduler_loop()
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _run_scheduler_loop
    self.job.executor.heartbeat()
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/executors/base_executor.py", line 244, in heartbeat
    self.sync()
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/executors/local_executor.py", line 395, in sync
    self.impl.sync()
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/executors/local_executor.py", line 338, in sync
    results = self.executor.result_queue.get_nowait()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in get_nowait
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py", line 206, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py", line 427, in _send_bytes
    self._send(header + buf)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py", line 384, in _send
    n = write(self._handle, buf)
        ^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe
[[34m2025-10-19T23:13:12.760-0500[0m] {[34mlocal_executor.py:[0m403} INFO[0m - Shutting down LocalExecutor; waiting for running tasks to finish.  Signal again if you don't want to wait.[0m
[[34m2025-10-19T23:13:12.761-0500[0m] {[34mscheduler_job_runner.py:[0m869} ERROR[0m - Exception when executing Executor.end[0m
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py", line 846, in _execute
    self._run_scheduler_loop()
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _run_scheduler_loop
    self.job.executor.heartbeat()
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/executors/base_executor.py", line 244, in heartbeat
    self.sync()
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/executors/local_executor.py", line 395, in sync
    self.impl.sync()
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/executors/local_executor.py", line 338, in sync
    results = self.executor.result_queue.get_nowait()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in get_nowait
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py", line 206, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py", line 427, in _send_bytes
    self._send(header + buf)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py", line 384, in _send
    n = write(self._handle, buf)
        ^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py", line 867, in _execute
    self.job.executor.end()
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/executors/local_executor.py", line 407, in end
    self.impl.end()
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/airflow/executors/local_executor.py", line 351, in end
    self.queue.put((None, None))
  File "<string>", line 2, in put
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/managers.py", line 821, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py", line 206, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py", line 427, in _send_bytes
    self._send(header + buf)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py", line 384, in _send
    n = write(self._handle, buf)
        ^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe
[[34m2025-10-19T23:13:12.842-0500[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 9030. PIDs of all processes in the group: [][0m
[[34m2025-10-19T23:13:12.844-0500[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 9030[0m
[[34m2025-10-19T23:13:12.846-0500[0m] {[34mprocess_utils.py:[0m101} INFO[0m - Sending the signal 15 to process 9030 as process group is missing.[0m
[[34m2025-10-19T23:13:12.848-0500[0m] {[34mscheduler_job_runner.py:[0m875} INFO[0m - Exited execute loop[0m
